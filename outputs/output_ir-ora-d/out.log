2022-03-30 18:45:03,970 - INFO - allennlp.common.params - random_seed = 42
2022-03-30 18:45:03,970 - INFO - allennlp.common.params - numpy_seed = 42
2022-03-30 18:45:03,970 - INFO - allennlp.common.params - pytorch_seed = 42
2022-03-30 18:45:04,680 - INFO - allennlp.common.checks - Pytorch version: 1.7.1
2022-03-30 18:45:04,681 - INFO - allennlp.common.params - type = default
2022-03-30 18:45:04,681 - INFO - allennlp.common.params - dataset_reader.type = strategy_qa_reader
2022-03-30 18:45:04,682 - INFO - allennlp.common.params - dataset_reader.lazy = False
2022-03-30 18:45:04,682 - INFO - allennlp.common.params - dataset_reader.cache_directory = None
2022-03-30 18:45:04,682 - INFO - allennlp.common.params - dataset_reader.max_instances = None
2022-03-30 18:45:04,682 - INFO - allennlp.common.params - dataset_reader.manual_distributed_sharding = False
2022-03-30 18:45:04,682 - INFO - allennlp.common.params - dataset_reader.manual_multi_process_sharding = False
2022-03-30 18:45:04,682 - INFO - allennlp.common.params - dataset_reader.tokenizer_wrapper.type = default
2022-03-30 18:45:04,682 - INFO - allennlp.common.params - dataset_reader.tokenizer_wrapper.pretrained_model = roberta-large
2022-03-30 18:45:04,682 - INFO - allennlp.common.params - dataset_reader.tokenizer_wrapper.call_kwargs.return_offsets_mapping = True
2022-03-30 18:45:04,682 - INFO - allennlp.common.params - dataset_reader.tokenizer_wrapper.call_kwargs.return_special_tokens_mask = True
2022-03-30 18:45:04,683 - INFO - allennlp.common.params - dataset_reader.tokenizer_wrapper.call_kwargs.truncation = only_first
2022-03-30 18:45:05,890 - INFO - allennlp.common.params - dataset_reader.save_tokenizer = True
2022-03-30 18:45:05,890 - INFO - allennlp.common.params - dataset_reader.is_training = True
2022-03-30 18:45:05,890 - INFO - allennlp.common.params - dataset_reader.pickle.action = None
2022-03-30 18:45:05,890 - INFO - allennlp.common.params - dataset_reader.pickle.file_name = None
2022-03-30 18:45:05,890 - INFO - allennlp.common.params - dataset_reader.pickle.path = ../pickle
2022-03-30 18:45:05,890 - INFO - allennlp.common.params - dataset_reader.pickle.save_even_when_max_instances = False
2022-03-30 18:45:05,890 - INFO - allennlp.common.params - dataset_reader.paragraphs_source = IR-ORA-D
2022-03-30 18:45:05,890 - INFO - allennlp.common.params - dataset_reader.answer_last_decomposition_step = False
2022-03-30 18:45:05,891 - INFO - allennlp.common.params - dataset_reader.skip_if_context_missing = True
2022-03-30 18:45:05,891 - INFO - allennlp.common.params - dataset_reader.paragraphs_limit = 10
2022-03-30 18:45:05,891 - INFO - allennlp.common.params - dataset_reader.generated_decompositions_paths = None
2022-03-30 18:45:05,891 - INFO - allennlp.common.params - dataset_reader.save_elasticsearch_cache = True
2022-03-30 18:45:06,351 - INFO - filelock - Lock 140246233526032 acquired on cache.lock
2022-03-30 18:45:07,107 - INFO - filelock - Lock 140246233526032 released on cache.lock
2022-03-30 18:45:07,107 - INFO - allennlp.common.params - train_data_path = data/strategyqa/train.json
2022-03-30 18:45:07,109 - INFO - allennlp.common.params - vocabulary = <allennlp.common.lazy.Lazy object at 0x7f8da9af9b50>
2022-03-30 18:45:07,109 - INFO - allennlp.common.params - datasets_for_vocab_creation = None
2022-03-30 18:45:07,110 - INFO - allennlp.common.params - validation_dataset_reader.type = strategy_qa_reader
2022-03-30 18:45:07,110 - INFO - allennlp.common.params - validation_dataset_reader.lazy = False
2022-03-30 18:45:07,110 - INFO - allennlp.common.params - validation_dataset_reader.cache_directory = None
2022-03-30 18:45:07,110 - INFO - allennlp.common.params - validation_dataset_reader.max_instances = None
2022-03-30 18:45:07,110 - INFO - allennlp.common.params - validation_dataset_reader.manual_distributed_sharding = False
2022-03-30 18:45:07,110 - INFO - allennlp.common.params - validation_dataset_reader.manual_multi_process_sharding = False
2022-03-30 18:45:07,110 - INFO - allennlp.common.params - validation_dataset_reader.tokenizer_wrapper.type = default
2022-03-30 18:45:07,110 - INFO - allennlp.common.params - validation_dataset_reader.tokenizer_wrapper.pretrained_model = roberta-large
2022-03-30 18:45:07,111 - INFO - allennlp.common.params - validation_dataset_reader.tokenizer_wrapper.call_kwargs.return_offsets_mapping = True
2022-03-30 18:45:07,111 - INFO - allennlp.common.params - validation_dataset_reader.tokenizer_wrapper.call_kwargs.return_special_tokens_mask = True
2022-03-30 18:45:07,111 - INFO - allennlp.common.params - validation_dataset_reader.tokenizer_wrapper.call_kwargs.truncation = only_first
2022-03-30 18:45:08,331 - INFO - allennlp.common.params - validation_dataset_reader.save_tokenizer = False
2022-03-30 18:45:08,331 - INFO - allennlp.common.params - validation_dataset_reader.is_training = False
2022-03-30 18:45:08,331 - INFO - allennlp.common.params - validation_dataset_reader.pickle.action = None
2022-03-30 18:45:08,331 - INFO - allennlp.common.params - validation_dataset_reader.pickle.file_name = None
2022-03-30 18:45:08,331 - INFO - allennlp.common.params - validation_dataset_reader.pickle.path = ../pickle
2022-03-30 18:45:08,331 - INFO - allennlp.common.params - validation_dataset_reader.pickle.save_even_when_max_instances = False
2022-03-30 18:45:08,331 - INFO - allennlp.common.params - validation_dataset_reader.paragraphs_source = IR-ORA-D
2022-03-30 18:45:08,331 - INFO - allennlp.common.params - validation_dataset_reader.answer_last_decomposition_step = False
2022-03-30 18:45:08,331 - INFO - allennlp.common.params - validation_dataset_reader.skip_if_context_missing = True
2022-03-30 18:45:08,332 - INFO - allennlp.common.params - validation_dataset_reader.paragraphs_limit = 10
2022-03-30 18:45:08,332 - INFO - allennlp.common.params - validation_dataset_reader.generated_decompositions_paths = None
2022-03-30 18:45:08,332 - INFO - allennlp.common.params - validation_dataset_reader.save_elasticsearch_cache = True
2022-03-30 18:45:08,332 - INFO - filelock - Lock 140246233764624 acquired on cache.lock
2022-03-30 18:45:09,245 - INFO - filelock - Lock 140246233764624 released on cache.lock
2022-03-30 18:45:09,245 - INFO - allennlp.common.params - validation_data_path = data/strategyqa/dev.json
2022-03-30 18:45:09,245 - INFO - allennlp.common.params - validation_data_loader = None
2022-03-30 18:45:09,245 - INFO - allennlp.common.params - test_data_path = None
2022-03-30 18:45:09,245 - INFO - allennlp.common.params - evaluate_on_test = True
2022-03-30 18:45:09,245 - INFO - allennlp.common.params - batch_weight_key = 
2022-03-30 18:45:09,245 - INFO - allennlp.training.util - Reading training data from data/strategyqa/train.json
2022-03-30 18:45:09,246 - INFO - tqdm - reading instances: 0it [00:00, ?it/s]
2022-03-30 18:45:09,246 - INFO - src.data.dataset_readers.base.base_dataset_reader - Reading the dataset from scratch
2022-03-30 18:45:09,246 - INFO - src.data.dataset_readers.strategy_qa_reader - Reading the dataset:
2022-03-30 18:45:09,246 - INFO - src.data.dataset_readers.strategy_qa_reader - Reading file at data/strategyqa/train.json
2022-03-30 18:45:17,504 - INFO - filelock - Lock 140245574837136 acquired on cache.lock
2022-03-30 18:45:18,369 - INFO - filelock - Lock 140245574837136 released on cache.lock
2022-03-30 18:45:18,409 - INFO - allennlp.training.util - Reading validation data from data/strategyqa/dev.json
2022-03-30 18:45:18,410 - INFO - tqdm - reading instances: 0it [00:00, ?it/s]
2022-03-30 18:45:18,410 - INFO - src.data.dataset_readers.base.base_dataset_reader - Reading the dataset from scratch
2022-03-30 18:45:18,410 - INFO - src.data.dataset_readers.strategy_qa_reader - Reading the dataset:
2022-03-30 18:45:18,410 - INFO - src.data.dataset_readers.strategy_qa_reader - Reading file at data/strategyqa/dev.json
2022-03-30 18:45:18,923 - INFO - src.data.dataset_readers.utils.elasticsearch_utils - Empty results from Elasticsearch for: actions commercial pilots take concerning engine noise arriving departing Orange County, California?
2022-03-30 18:45:19,295 - INFO - filelock - Lock 140245649849936 acquired on cache.lock
2022-03-30 18:45:20,145 - INFO - filelock - Lock 140245649849936 released on cache.lock
2022-03-30 18:45:20,183 - INFO - allennlp.common.params - type = from_instances
2022-03-30 18:45:20,183 - INFO - allennlp.common.params - min_count = None
2022-03-30 18:45:20,183 - INFO - allennlp.common.params - max_vocab_size = None
2022-03-30 18:45:20,183 - INFO - allennlp.common.params - non_padded_namespaces = ('*tags', '*labels')
2022-03-30 18:45:20,183 - INFO - allennlp.common.params - pretrained_files = None
2022-03-30 18:45:20,184 - INFO - allennlp.common.params - only_include_pretrained_words = False
2022-03-30 18:45:20,184 - INFO - allennlp.common.params - tokens_to_add = None
2022-03-30 18:45:20,184 - INFO - allennlp.common.params - min_pretrained_embeddings = None
2022-03-30 18:45:20,184 - INFO - allennlp.common.params - padding_token = @@PADDING@@
2022-03-30 18:45:20,184 - INFO - allennlp.common.params - oov_token = @@UNKNOWN@@
2022-03-30 18:45:20,184 - INFO - allennlp.data.vocabulary - Fitting token dictionary from dataset.
2022-03-30 18:45:20,184 - INFO - tqdm - building vocab: 0it [00:00, ?it/s]
2022-03-30 18:45:20,190 - INFO - allennlp.common.params - model.type = hf_classifier
2022-03-30 18:45:20,190 - INFO - allennlp.common.params - model.regularizer = None
2022-03-30 18:45:20,191 - INFO - allennlp.common.params - model.pretrained_model = roberta-large
2022-03-30 18:45:20,191 - INFO - allennlp.common.params - model.tokenizer_wrapper.type = default
2022-03-30 18:45:20,191 - INFO - allennlp.common.params - model.tokenizer_wrapper.pretrained_model = roberta-large
2022-03-30 18:45:20,191 - INFO - allennlp.common.params - model.tokenizer_wrapper.call_kwargs.return_offsets_mapping = True
2022-03-30 18:45:20,191 - INFO - allennlp.common.params - model.tokenizer_wrapper.call_kwargs.return_special_tokens_mask = True
2022-03-30 18:45:20,191 - INFO - allennlp.common.params - model.tokenizer_wrapper.call_kwargs.truncation = only_first
2022-03-30 18:45:21,402 - INFO - allennlp.common.params - model.num_labels = 2
2022-03-30 18:45:21,402 - INFO - allennlp.common.params - model.label_namespace = labels
2022-03-30 18:45:21,402 - INFO - allennlp.common.params - model.transformer_weights_path = None
2022-03-30 18:45:21,403 - INFO - allennlp.common.params - model.initializer.regexes.0.1.type = pretrained
2022-03-30 18:45:21,403 - INFO - allennlp.common.params - model.initializer.regexes.0.1.weights_file_path = output_roberta-star/model.tar.gz
2022-03-30 18:45:21,405 - INFO - allennlp.models.archival - extracting archive file output_roberta-star/model.tar.gz to temp dir /tmp/tmpbqxnl11x
2022-03-30 18:45:33,778 - INFO - allennlp.models.archival - removing temporary unarchived model dir at /tmp/tmpbqxnl11x
2022-03-30 18:45:33,952 - INFO - allennlp.common.params - model.initializer.prevent_regexes = None
2022-03-30 18:45:48,708 - INFO - allennlp.nn.initializers - Initializing parameters
2022-03-30 18:45:48,708 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.embeddings.word_embeddings.weight using .* initializer
2022-03-30 18:45:48,718 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.embeddings.position_embeddings.weight using .* initializer
2022-03-30 18:45:48,718 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.embeddings.token_type_embeddings.weight using .* initializer
2022-03-30 18:45:48,718 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.embeddings.LayerNorm.weight using .* initializer
2022-03-30 18:45:48,718 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.embeddings.LayerNorm.bias using .* initializer
2022-03-30 18:45:48,718 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.self.query.weight using .* initializer
2022-03-30 18:45:48,719 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.self.query.bias using .* initializer
2022-03-30 18:45:48,719 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.self.key.weight using .* initializer
2022-03-30 18:45:48,719 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.self.key.bias using .* initializer
2022-03-30 18:45:48,719 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.self.value.weight using .* initializer
2022-03-30 18:45:48,720 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.self.value.bias using .* initializer
2022-03-30 18:45:48,720 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.output.dense.weight using .* initializer
2022-03-30 18:45:48,720 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.output.dense.bias using .* initializer
2022-03-30 18:45:48,720 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.output.LayerNorm.weight using .* initializer
2022-03-30 18:45:48,720 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.output.LayerNorm.bias using .* initializer
2022-03-30 18:45:48,720 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.intermediate.dense.weight using .* initializer
2022-03-30 18:45:48,721 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.intermediate.dense.bias using .* initializer
2022-03-30 18:45:48,721 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.output.dense.weight using .* initializer
2022-03-30 18:45:48,722 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.output.dense.bias using .* initializer
2022-03-30 18:45:48,723 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.output.LayerNorm.weight using .* initializer
2022-03-30 18:45:48,723 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.output.LayerNorm.bias using .* initializer
2022-03-30 18:45:48,723 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.self.query.weight using .* initializer
2022-03-30 18:45:48,723 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.self.query.bias using .* initializer
2022-03-30 18:45:48,723 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.self.key.weight using .* initializer
2022-03-30 18:45:48,724 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.self.key.bias using .* initializer
2022-03-30 18:45:48,724 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.self.value.weight using .* initializer
2022-03-30 18:45:48,724 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.self.value.bias using .* initializer
2022-03-30 18:45:48,724 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.output.dense.weight using .* initializer
2022-03-30 18:45:48,724 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.output.dense.bias using .* initializer
2022-03-30 18:45:48,725 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.output.LayerNorm.weight using .* initializer
2022-03-30 18:45:48,725 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.output.LayerNorm.bias using .* initializer
2022-03-30 18:45:48,725 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.intermediate.dense.weight using .* initializer
2022-03-30 18:45:48,726 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.intermediate.dense.bias using .* initializer
2022-03-30 18:45:48,726 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.output.dense.weight using .* initializer
2022-03-30 18:45:48,727 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.output.dense.bias using .* initializer
2022-03-30 18:45:48,727 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.output.LayerNorm.weight using .* initializer
2022-03-30 18:45:48,727 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.output.LayerNorm.bias using .* initializer
2022-03-30 18:45:48,727 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.self.query.weight using .* initializer
2022-03-30 18:45:48,727 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.self.query.bias using .* initializer
2022-03-30 18:45:48,728 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.self.key.weight using .* initializer
2022-03-30 18:45:48,728 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.self.key.bias using .* initializer
2022-03-30 18:45:48,728 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.self.value.weight using .* initializer
2022-03-30 18:45:48,728 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.self.value.bias using .* initializer
2022-03-30 18:45:48,729 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.output.dense.weight using .* initializer
2022-03-30 18:45:48,729 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.output.dense.bias using .* initializer
2022-03-30 18:45:48,729 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.output.LayerNorm.weight using .* initializer
2022-03-30 18:45:48,729 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.output.LayerNorm.bias using .* initializer
2022-03-30 18:45:48,729 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.intermediate.dense.weight using .* initializer
2022-03-30 18:45:48,730 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.intermediate.dense.bias using .* initializer
2022-03-30 18:45:48,730 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.output.dense.weight using .* initializer
2022-03-30 18:45:48,731 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.output.dense.bias using .* initializer
2022-03-30 18:45:48,731 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.output.LayerNorm.weight using .* initializer
2022-03-30 18:45:48,731 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.output.LayerNorm.bias using .* initializer
2022-03-30 18:45:48,732 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.self.query.weight using .* initializer
2022-03-30 18:45:48,732 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.self.query.bias using .* initializer
2022-03-30 18:45:48,732 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.self.key.weight using .* initializer
2022-03-30 18:45:48,732 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.self.key.bias using .* initializer
2022-03-30 18:45:48,732 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.self.value.weight using .* initializer
2022-03-30 18:45:48,733 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.self.value.bias using .* initializer
2022-03-30 18:45:48,733 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.output.dense.weight using .* initializer
2022-03-30 18:45:48,733 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.output.dense.bias using .* initializer
2022-03-30 18:45:48,733 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.output.LayerNorm.weight using .* initializer
2022-03-30 18:45:48,734 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.output.LayerNorm.bias using .* initializer
2022-03-30 18:45:48,734 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.intermediate.dense.weight using .* initializer
2022-03-30 18:45:48,735 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.intermediate.dense.bias using .* initializer
2022-03-30 18:45:48,735 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.output.dense.weight using .* initializer
2022-03-30 18:45:48,736 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.output.dense.bias using .* initializer
2022-03-30 18:45:48,736 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.output.LayerNorm.weight using .* initializer
2022-03-30 18:45:48,736 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.output.LayerNorm.bias using .* initializer
2022-03-30 18:45:48,736 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.self.query.weight using .* initializer
2022-03-30 18:45:48,736 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.self.query.bias using .* initializer
2022-03-30 18:45:48,736 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.self.key.weight using .* initializer
2022-03-30 18:45:48,737 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.self.key.bias using .* initializer
2022-03-30 18:45:48,737 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.self.value.weight using .* initializer
2022-03-30 18:45:48,737 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.self.value.bias using .* initializer
2022-03-30 18:45:48,737 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.output.dense.weight using .* initializer
2022-03-30 18:45:48,738 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.output.dense.bias using .* initializer
2022-03-30 18:45:48,738 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.output.LayerNorm.weight using .* initializer
2022-03-30 18:45:48,738 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.output.LayerNorm.bias using .* initializer
2022-03-30 18:45:48,738 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.intermediate.dense.weight using .* initializer
2022-03-30 18:45:48,739 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.intermediate.dense.bias using .* initializer
2022-03-30 18:45:48,739 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.output.dense.weight using .* initializer
2022-03-30 18:45:48,740 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.output.dense.bias using .* initializer
2022-03-30 18:45:48,740 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.output.LayerNorm.weight using .* initializer
2022-03-30 18:45:48,740 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.output.LayerNorm.bias using .* initializer
2022-03-30 18:45:48,740 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.self.query.weight using .* initializer
2022-03-30 18:45:48,741 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.self.query.bias using .* initializer
2022-03-30 18:45:48,741 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.self.key.weight using .* initializer
2022-03-30 18:45:48,741 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.self.key.bias using .* initializer
2022-03-30 18:45:48,741 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.self.value.weight using .* initializer
2022-03-30 18:45:48,742 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.self.value.bias using .* initializer
2022-03-30 18:45:48,742 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.output.dense.weight using .* initializer
2022-03-30 18:45:48,742 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.output.dense.bias using .* initializer
2022-03-30 18:45:48,742 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.output.LayerNorm.weight using .* initializer
2022-03-30 18:45:48,742 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.output.LayerNorm.bias using .* initializer
2022-03-30 18:45:48,742 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.intermediate.dense.weight using .* initializer
2022-03-30 18:45:48,743 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.intermediate.dense.bias using .* initializer
2022-03-30 18:45:48,744 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.output.dense.weight using .* initializer
2022-03-30 18:45:48,744 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.output.dense.bias using .* initializer
2022-03-30 18:45:48,745 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.output.LayerNorm.weight using .* initializer
2022-03-30 18:45:48,745 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.output.LayerNorm.bias using .* initializer
2022-03-30 18:45:48,745 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.self.query.weight using .* initializer
2022-03-30 18:45:48,745 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.self.query.bias using .* initializer
2022-03-30 18:45:48,745 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.self.key.weight using .* initializer
2022-03-30 18:45:48,746 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.self.key.bias using .* initializer
2022-03-30 18:45:48,746 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.self.value.weight using .* initializer
2022-03-30 18:45:48,746 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.self.value.bias using .* initializer
2022-03-30 18:45:48,746 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.output.dense.weight using .* initializer
2022-03-30 18:45:48,746 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.output.dense.bias using .* initializer
2022-03-30 18:45:48,747 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.output.LayerNorm.weight using .* initializer
2022-03-30 18:45:48,747 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.output.LayerNorm.bias using .* initializer
2022-03-30 18:45:48,747 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.intermediate.dense.weight using .* initializer
2022-03-30 18:45:48,748 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.intermediate.dense.bias using .* initializer
2022-03-30 18:45:48,748 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.output.dense.weight using .* initializer
2022-03-30 18:45:48,749 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.output.dense.bias using .* initializer
2022-03-30 18:45:48,749 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.output.LayerNorm.weight using .* initializer
2022-03-30 18:45:48,749 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.output.LayerNorm.bias using .* initializer
2022-03-30 18:45:48,749 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.self.query.weight using .* initializer
2022-03-30 18:45:48,749 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.self.query.bias using .* initializer
2022-03-30 18:45:48,750 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.self.key.weight using .* initializer
2022-03-30 18:45:48,750 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.self.key.bias using .* initializer
2022-03-30 18:45:48,750 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.self.value.weight using .* initializer
2022-03-30 18:45:48,750 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.self.value.bias using .* initializer
2022-03-30 18:45:48,750 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.output.dense.weight using .* initializer
2022-03-30 18:45:48,751 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.output.dense.bias using .* initializer
2022-03-30 18:45:48,751 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.output.LayerNorm.weight using .* initializer
2022-03-30 18:45:48,751 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.output.LayerNorm.bias using .* initializer
2022-03-30 18:45:48,751 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.intermediate.dense.weight using .* initializer
2022-03-30 18:45:48,752 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.intermediate.dense.bias using .* initializer
2022-03-30 18:45:48,752 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.output.dense.weight using .* initializer
2022-03-30 18:45:48,753 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.output.dense.bias using .* initializer
2022-03-30 18:45:48,753 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.output.LayerNorm.weight using .* initializer
2022-03-30 18:45:48,753 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.output.LayerNorm.bias using .* initializer
2022-03-30 18:45:48,754 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.self.query.weight using .* initializer
2022-03-30 18:45:48,754 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.self.query.bias using .* initializer
2022-03-30 18:45:48,754 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.self.key.weight using .* initializer
2022-03-30 18:45:48,754 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.self.key.bias using .* initializer
2022-03-30 18:45:48,754 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.self.value.weight using .* initializer
2022-03-30 18:45:48,755 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.self.value.bias using .* initializer
2022-03-30 18:45:48,755 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.output.dense.weight using .* initializer
2022-03-30 18:45:48,755 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.output.dense.bias using .* initializer
2022-03-30 18:45:48,755 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.output.LayerNorm.weight using .* initializer
2022-03-30 18:45:48,756 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.output.LayerNorm.bias using .* initializer
2022-03-30 18:45:48,756 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.intermediate.dense.weight using .* initializer
2022-03-30 18:45:48,757 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.intermediate.dense.bias using .* initializer
2022-03-30 18:45:48,757 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.output.dense.weight using .* initializer
2022-03-30 18:45:48,758 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.output.dense.bias using .* initializer
2022-03-30 18:45:48,758 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.output.LayerNorm.weight using .* initializer
2022-03-30 18:45:48,758 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.output.LayerNorm.bias using .* initializer
2022-03-30 18:45:48,758 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.self.query.weight using .* initializer
2022-03-30 18:45:48,758 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.self.query.bias using .* initializer
2022-03-30 18:45:48,758 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.self.key.weight using .* initializer
2022-03-30 18:45:48,759 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.self.key.bias using .* initializer
2022-03-30 18:45:48,759 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.self.value.weight using .* initializer
2022-03-30 18:45:48,759 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.self.value.bias using .* initializer
2022-03-30 18:45:48,759 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.output.dense.weight using .* initializer
2022-03-30 18:45:48,760 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.output.dense.bias using .* initializer
2022-03-30 18:45:48,760 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.output.LayerNorm.weight using .* initializer
2022-03-30 18:45:48,760 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.output.LayerNorm.bias using .* initializer
2022-03-30 18:45:48,760 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.intermediate.dense.weight using .* initializer
2022-03-30 18:45:48,761 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.intermediate.dense.bias using .* initializer
2022-03-30 18:45:48,761 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.output.dense.weight using .* initializer
2022-03-30 18:45:48,762 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.output.dense.bias using .* initializer
2022-03-30 18:45:48,762 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.output.LayerNorm.weight using .* initializer
2022-03-30 18:45:48,762 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.output.LayerNorm.bias using .* initializer
2022-03-30 18:45:48,762 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.self.query.weight using .* initializer
2022-03-30 18:45:48,763 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.self.query.bias using .* initializer
2022-03-30 18:45:48,763 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.self.key.weight using .* initializer
2022-03-30 18:45:48,763 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.self.key.bias using .* initializer
2022-03-30 18:45:48,763 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.self.value.weight using .* initializer
2022-03-30 18:45:48,764 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.self.value.bias using .* initializer
2022-03-30 18:45:48,764 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.output.dense.weight using .* initializer
2022-03-30 18:45:48,764 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.output.dense.bias using .* initializer
2022-03-30 18:45:48,764 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.output.LayerNorm.weight using .* initializer
2022-03-30 18:45:48,764 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.output.LayerNorm.bias using .* initializer
2022-03-30 18:45:48,765 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.intermediate.dense.weight using .* initializer
2022-03-30 18:45:48,765 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.intermediate.dense.bias using .* initializer
2022-03-30 18:45:48,766 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.output.dense.weight using .* initializer
2022-03-30 18:45:48,767 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.output.dense.bias using .* initializer
2022-03-30 18:45:48,767 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.output.LayerNorm.weight using .* initializer
2022-03-30 18:45:48,767 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.output.LayerNorm.bias using .* initializer
2022-03-30 18:45:48,767 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.self.query.weight using .* initializer
2022-03-30 18:45:48,767 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.self.query.bias using .* initializer
2022-03-30 18:45:48,767 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.self.key.weight using .* initializer
2022-03-30 18:45:48,768 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.self.key.bias using .* initializer
2022-03-30 18:45:48,768 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.self.value.weight using .* initializer
2022-03-30 18:45:48,768 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.self.value.bias using .* initializer
2022-03-30 18:45:48,768 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.output.dense.weight using .* initializer
2022-03-30 18:45:48,769 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.output.dense.bias using .* initializer
2022-03-30 18:45:48,769 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.output.LayerNorm.weight using .* initializer
2022-03-30 18:45:48,769 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.output.LayerNorm.bias using .* initializer
2022-03-30 18:45:48,769 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.intermediate.dense.weight using .* initializer
2022-03-30 18:45:48,770 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.intermediate.dense.bias using .* initializer
2022-03-30 18:45:48,770 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.output.dense.weight using .* initializer
2022-03-30 18:45:48,771 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.output.dense.bias using .* initializer
2022-03-30 18:45:48,771 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.output.LayerNorm.weight using .* initializer
2022-03-30 18:45:48,771 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.output.LayerNorm.bias using .* initializer
2022-03-30 18:45:48,771 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.self.query.weight using .* initializer
2022-03-30 18:45:48,772 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.self.query.bias using .* initializer
2022-03-30 18:45:48,772 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.self.key.weight using .* initializer
2022-03-30 18:45:48,772 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.self.key.bias using .* initializer
2022-03-30 18:45:48,772 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.self.value.weight using .* initializer
2022-03-30 18:45:48,773 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.self.value.bias using .* initializer
2022-03-30 18:45:48,773 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.output.dense.weight using .* initializer
2022-03-30 18:45:48,773 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.output.dense.bias using .* initializer
2022-03-30 18:45:48,773 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.output.LayerNorm.weight using .* initializer
2022-03-30 18:45:48,773 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.output.LayerNorm.bias using .* initializer
2022-03-30 18:45:48,773 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.intermediate.dense.weight using .* initializer
2022-03-30 18:45:48,774 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.intermediate.dense.bias using .* initializer
2022-03-30 18:45:48,774 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.output.dense.weight using .* initializer
2022-03-30 18:45:48,775 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.output.dense.bias using .* initializer
2022-03-30 18:45:48,776 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.output.LayerNorm.weight using .* initializer
2022-03-30 18:45:48,776 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.output.LayerNorm.bias using .* initializer
2022-03-30 18:45:48,776 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.self.query.weight using .* initializer
2022-03-30 18:45:48,776 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.self.query.bias using .* initializer
2022-03-30 18:45:48,776 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.self.key.weight using .* initializer
2022-03-30 18:45:48,777 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.self.key.bias using .* initializer
2022-03-30 18:45:48,777 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.self.value.weight using .* initializer
2022-03-30 18:45:48,777 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.self.value.bias using .* initializer
2022-03-30 18:45:48,777 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.output.dense.weight using .* initializer
2022-03-30 18:45:48,778 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.output.dense.bias using .* initializer
2022-03-30 18:45:48,778 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.output.LayerNorm.weight using .* initializer
2022-03-30 18:45:48,778 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.output.LayerNorm.bias using .* initializer
2022-03-30 18:45:48,778 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.intermediate.dense.weight using .* initializer
2022-03-30 18:45:48,779 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.intermediate.dense.bias using .* initializer
2022-03-30 18:45:48,779 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.output.dense.weight using .* initializer
2022-03-30 18:45:48,780 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.output.dense.bias using .* initializer
2022-03-30 18:45:48,780 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.output.LayerNorm.weight using .* initializer
2022-03-30 18:45:48,780 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.output.LayerNorm.bias using .* initializer
2022-03-30 18:45:48,780 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.self.query.weight using .* initializer
2022-03-30 18:45:48,781 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.self.query.bias using .* initializer
2022-03-30 18:45:48,781 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.self.key.weight using .* initializer
2022-03-30 18:45:48,781 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.self.key.bias using .* initializer
2022-03-30 18:45:48,781 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.self.value.weight using .* initializer
2022-03-30 18:45:48,782 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.self.value.bias using .* initializer
2022-03-30 18:45:48,782 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.output.dense.weight using .* initializer
2022-03-30 18:45:48,782 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.output.dense.bias using .* initializer
2022-03-30 18:45:48,782 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.output.LayerNorm.weight using .* initializer
2022-03-30 18:45:48,782 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.output.LayerNorm.bias using .* initializer
2022-03-30 18:45:48,782 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.intermediate.dense.weight using .* initializer
2022-03-30 18:45:48,783 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.intermediate.dense.bias using .* initializer
2022-03-30 18:45:48,783 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.output.dense.weight using .* initializer
2022-03-30 18:45:48,784 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.output.dense.bias using .* initializer
2022-03-30 18:45:48,784 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.output.LayerNorm.weight using .* initializer
2022-03-30 18:45:48,785 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.output.LayerNorm.bias using .* initializer
2022-03-30 18:45:48,785 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.self.query.weight using .* initializer
2022-03-30 18:45:48,785 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.self.query.bias using .* initializer
2022-03-30 18:45:48,785 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.self.key.weight using .* initializer
2022-03-30 18:45:48,786 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.self.key.bias using .* initializer
2022-03-30 18:45:48,786 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.self.value.weight using .* initializer
2022-03-30 18:45:48,786 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.self.value.bias using .* initializer
2022-03-30 18:45:48,786 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.output.dense.weight using .* initializer
2022-03-30 18:45:48,786 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.output.dense.bias using .* initializer
2022-03-30 18:45:48,787 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.output.LayerNorm.weight using .* initializer
2022-03-30 18:45:48,787 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.output.LayerNorm.bias using .* initializer
2022-03-30 18:45:48,787 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.intermediate.dense.weight using .* initializer
2022-03-30 18:45:48,788 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.intermediate.dense.bias using .* initializer
2022-03-30 18:45:48,788 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.output.dense.weight using .* initializer
2022-03-30 18:45:48,789 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.output.dense.bias using .* initializer
2022-03-30 18:45:48,789 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.output.LayerNorm.weight using .* initializer
2022-03-30 18:45:48,789 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.output.LayerNorm.bias using .* initializer
2022-03-30 18:45:48,789 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.self.query.weight using .* initializer
2022-03-30 18:45:48,789 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.self.query.bias using .* initializer
2022-03-30 18:45:48,790 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.self.key.weight using .* initializer
2022-03-30 18:45:48,790 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.self.key.bias using .* initializer
2022-03-30 18:45:48,790 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.self.value.weight using .* initializer
2022-03-30 18:45:48,790 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.self.value.bias using .* initializer
2022-03-30 18:45:48,791 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.output.dense.weight using .* initializer
2022-03-30 18:45:48,791 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.output.dense.bias using .* initializer
2022-03-30 18:45:48,791 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.output.LayerNorm.weight using .* initializer
2022-03-30 18:45:48,791 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.output.LayerNorm.bias using .* initializer
2022-03-30 18:45:48,791 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.intermediate.dense.weight using .* initializer
2022-03-30 18:45:48,792 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.intermediate.dense.bias using .* initializer
2022-03-30 18:45:48,792 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.output.dense.weight using .* initializer
2022-03-30 18:45:48,793 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.output.dense.bias using .* initializer
2022-03-30 18:45:48,793 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.output.LayerNorm.weight using .* initializer
2022-03-30 18:45:48,794 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.output.LayerNorm.bias using .* initializer
2022-03-30 18:45:48,794 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.self.query.weight using .* initializer
2022-03-30 18:45:48,794 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.self.query.bias using .* initializer
2022-03-30 18:45:48,794 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.self.key.weight using .* initializer
2022-03-30 18:45:48,794 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.self.key.bias using .* initializer
2022-03-30 18:45:48,795 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.self.value.weight using .* initializer
2022-03-30 18:45:48,795 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.self.value.bias using .* initializer
2022-03-30 18:45:48,795 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.output.dense.weight using .* initializer
2022-03-30 18:45:48,795 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.output.dense.bias using .* initializer
2022-03-30 18:45:48,796 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.output.LayerNorm.weight using .* initializer
2022-03-30 18:45:48,796 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.output.LayerNorm.bias using .* initializer
2022-03-30 18:45:48,796 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.intermediate.dense.weight using .* initializer
2022-03-30 18:45:48,797 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.intermediate.dense.bias using .* initializer
2022-03-30 18:45:48,797 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.output.dense.weight using .* initializer
2022-03-30 18:45:48,798 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.output.dense.bias using .* initializer
2022-03-30 18:45:48,798 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.output.LayerNorm.weight using .* initializer
2022-03-30 18:45:48,798 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.output.LayerNorm.bias using .* initializer
2022-03-30 18:45:48,798 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.self.query.weight using .* initializer
2022-03-30 18:45:48,798 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.self.query.bias using .* initializer
2022-03-30 18:45:48,799 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.self.key.weight using .* initializer
2022-03-30 18:45:48,799 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.self.key.bias using .* initializer
2022-03-30 18:45:48,799 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.self.value.weight using .* initializer
2022-03-30 18:45:48,799 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.self.value.bias using .* initializer
2022-03-30 18:45:48,800 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.output.dense.weight using .* initializer
2022-03-30 18:45:48,800 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.output.dense.bias using .* initializer
2022-03-30 18:45:48,800 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.output.LayerNorm.weight using .* initializer
2022-03-30 18:45:48,800 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.output.LayerNorm.bias using .* initializer
2022-03-30 18:45:48,800 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.intermediate.dense.weight using .* initializer
2022-03-30 18:45:48,801 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.intermediate.dense.bias using .* initializer
2022-03-30 18:45:48,801 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.output.dense.weight using .* initializer
2022-03-30 18:45:48,802 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.output.dense.bias using .* initializer
2022-03-30 18:45:48,802 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.output.LayerNorm.weight using .* initializer
2022-03-30 18:45:48,803 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.output.LayerNorm.bias using .* initializer
2022-03-30 18:45:48,803 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.self.query.weight using .* initializer
2022-03-30 18:45:48,803 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.self.query.bias using .* initializer
2022-03-30 18:45:48,803 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.self.key.weight using .* initializer
2022-03-30 18:45:48,804 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.self.key.bias using .* initializer
2022-03-30 18:45:48,804 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.self.value.weight using .* initializer
2022-03-30 18:45:48,804 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.self.value.bias using .* initializer
2022-03-30 18:45:48,804 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.output.dense.weight using .* initializer
2022-03-30 18:45:48,805 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.output.dense.bias using .* initializer
2022-03-30 18:45:48,805 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.output.LayerNorm.weight using .* initializer
2022-03-30 18:45:48,805 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.output.LayerNorm.bias using .* initializer
2022-03-30 18:45:48,805 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.intermediate.dense.weight using .* initializer
2022-03-30 18:45:48,806 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.intermediate.dense.bias using .* initializer
2022-03-30 18:45:48,806 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.output.dense.weight using .* initializer
2022-03-30 18:45:48,807 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.output.dense.bias using .* initializer
2022-03-30 18:45:48,807 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.output.LayerNorm.weight using .* initializer
2022-03-30 18:45:48,807 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.output.LayerNorm.bias using .* initializer
2022-03-30 18:45:48,808 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.self.query.weight using .* initializer
2022-03-30 18:45:48,808 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.self.query.bias using .* initializer
2022-03-30 18:45:48,808 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.self.key.weight using .* initializer
2022-03-30 18:45:48,808 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.self.key.bias using .* initializer
2022-03-30 18:45:48,809 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.self.value.weight using .* initializer
2022-03-30 18:45:48,809 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.self.value.bias using .* initializer
2022-03-30 18:45:48,809 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.output.dense.weight using .* initializer
2022-03-30 18:45:48,809 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.output.dense.bias using .* initializer
2022-03-30 18:45:48,810 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.output.LayerNorm.weight using .* initializer
2022-03-30 18:45:48,810 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.output.LayerNorm.bias using .* initializer
2022-03-30 18:45:48,810 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.intermediate.dense.weight using .* initializer
2022-03-30 18:45:48,811 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.intermediate.dense.bias using .* initializer
2022-03-30 18:45:48,811 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.output.dense.weight using .* initializer
2022-03-30 18:45:48,812 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.output.dense.bias using .* initializer
2022-03-30 18:45:48,812 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.output.LayerNorm.weight using .* initializer
2022-03-30 18:45:48,812 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.output.LayerNorm.bias using .* initializer
2022-03-30 18:45:48,812 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.self.query.weight using .* initializer
2022-03-30 18:45:48,812 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.self.query.bias using .* initializer
2022-03-30 18:45:48,813 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.self.key.weight using .* initializer
2022-03-30 18:45:48,813 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.self.key.bias using .* initializer
2022-03-30 18:45:48,813 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.self.value.weight using .* initializer
2022-03-30 18:45:48,813 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.self.value.bias using .* initializer
2022-03-30 18:45:48,814 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.output.dense.weight using .* initializer
2022-03-30 18:45:48,814 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.output.dense.bias using .* initializer
2022-03-30 18:45:48,814 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.output.LayerNorm.weight using .* initializer
2022-03-30 18:45:48,814 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.output.LayerNorm.bias using .* initializer
2022-03-30 18:45:48,814 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.intermediate.dense.weight using .* initializer
2022-03-30 18:45:48,815 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.intermediate.dense.bias using .* initializer
2022-03-30 18:45:48,815 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.output.dense.weight using .* initializer
2022-03-30 18:45:48,816 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.output.dense.bias using .* initializer
2022-03-30 18:45:48,816 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.output.LayerNorm.weight using .* initializer
2022-03-30 18:45:48,816 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.output.LayerNorm.bias using .* initializer
2022-03-30 18:45:48,817 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.self.query.weight using .* initializer
2022-03-30 18:45:48,817 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.self.query.bias using .* initializer
2022-03-30 18:45:48,817 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.self.key.weight using .* initializer
2022-03-30 18:45:48,817 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.self.key.bias using .* initializer
2022-03-30 18:45:48,818 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.self.value.weight using .* initializer
2022-03-30 18:45:48,818 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.self.value.bias using .* initializer
2022-03-30 18:45:48,818 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.output.dense.weight using .* initializer
2022-03-30 18:45:48,818 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.output.dense.bias using .* initializer
2022-03-30 18:45:48,819 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.output.LayerNorm.weight using .* initializer
2022-03-30 18:45:48,819 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.output.LayerNorm.bias using .* initializer
2022-03-30 18:45:48,819 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.intermediate.dense.weight using .* initializer
2022-03-30 18:45:48,820 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.intermediate.dense.bias using .* initializer
2022-03-30 18:45:48,820 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.output.dense.weight using .* initializer
2022-03-30 18:45:48,821 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.output.dense.bias using .* initializer
2022-03-30 18:45:48,821 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.output.LayerNorm.weight using .* initializer
2022-03-30 18:45:48,821 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.output.LayerNorm.bias using .* initializer
2022-03-30 18:45:48,821 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.self.query.weight using .* initializer
2022-03-30 18:45:48,821 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.self.query.bias using .* initializer
2022-03-30 18:45:48,822 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.self.key.weight using .* initializer
2022-03-30 18:45:48,822 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.self.key.bias using .* initializer
2022-03-30 18:45:48,822 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.self.value.weight using .* initializer
2022-03-30 18:45:48,822 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.self.value.bias using .* initializer
2022-03-30 18:45:48,823 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.output.dense.weight using .* initializer
2022-03-30 18:45:48,823 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.output.dense.bias using .* initializer
2022-03-30 18:45:48,823 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.output.LayerNorm.weight using .* initializer
2022-03-30 18:45:48,823 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.output.LayerNorm.bias using .* initializer
2022-03-30 18:45:48,823 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.intermediate.dense.weight using .* initializer
2022-03-30 18:45:48,824 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.intermediate.dense.bias using .* initializer
2022-03-30 18:45:48,824 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.output.dense.weight using .* initializer
2022-03-30 18:45:48,825 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.output.dense.bias using .* initializer
2022-03-30 18:45:48,825 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.output.LayerNorm.weight using .* initializer
2022-03-30 18:45:48,825 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.output.LayerNorm.bias using .* initializer
2022-03-30 18:45:48,826 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.pooler.dense.weight using .* initializer
2022-03-30 18:45:48,826 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.pooler.dense.bias using .* initializer
2022-03-30 18:45:48,826 - INFO - allennlp.nn.initializers - Initializing _classifier.classifier.dense.weight using .* initializer
2022-03-30 18:45:48,826 - INFO - allennlp.nn.initializers - Initializing _classifier.classifier.dense.bias using .* initializer
2022-03-30 18:45:48,827 - INFO - allennlp.nn.initializers - Initializing _classifier.classifier.out_proj.weight using .* initializer
2022-03-30 18:45:48,827 - INFO - allennlp.nn.initializers - Initializing _classifier.classifier.out_proj.bias using .* initializer
2022-03-30 18:45:48,827 - INFO - allennlp.nn.initializers - Done initializing parameters; the following parameters are using their default initialization from their code
2022-03-30 18:45:49,384 - INFO - filelock - Lock 140245574836944 acquired on ./output_ir-ora-d/vocabulary/.lock
2022-03-30 18:45:49,384 - INFO - filelock - Lock 140245574836944 released on ./output_ir-ora-d/vocabulary/.lock
2022-03-30 18:45:49,385 - INFO - allennlp.common.params - data_loader.type = pytorch_dataloader
2022-03-30 18:45:49,385 - INFO - allennlp.common.params - data_loader.batch_size = 1
2022-03-30 18:45:49,385 - INFO - allennlp.common.params - data_loader.shuffle = False
2022-03-30 18:45:49,385 - INFO - allennlp.common.params - data_loader.sampler = None
2022-03-30 18:45:49,385 - INFO - allennlp.common.params - data_loader.num_workers = 0
2022-03-30 18:45:49,386 - INFO - allennlp.common.params - data_loader.pin_memory = False
2022-03-30 18:45:49,386 - INFO - allennlp.common.params - data_loader.drop_last = False
2022-03-30 18:45:49,386 - INFO - allennlp.common.params - data_loader.timeout = 0
2022-03-30 18:45:49,386 - INFO - allennlp.common.params - data_loader.worker_init_fn = None
2022-03-30 18:45:49,386 - INFO - allennlp.common.params - data_loader.multiprocessing_context = None
2022-03-30 18:45:49,386 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None
2022-03-30 18:45:49,386 - INFO - allennlp.common.params - data_loader.batch_sampler.type = bucket
2022-03-30 18:45:49,386 - INFO - allennlp.common.params - data_loader.batch_sampler.batch_size = 2
2022-03-30 18:45:49,387 - INFO - allennlp.common.params - data_loader.batch_sampler.sorting_keys = None
2022-03-30 18:45:49,387 - INFO - allennlp.common.params - data_loader.batch_sampler.padding_noise = 0.1
2022-03-30 18:45:49,387 - INFO - allennlp.common.params - data_loader.batch_sampler.drop_last = False
2022-03-30 18:45:49,387 - INFO - allennlp.common.params - data_loader.type = pytorch_dataloader
2022-03-30 18:45:49,387 - INFO - allennlp.common.params - data_loader.batch_size = 1
2022-03-30 18:45:49,388 - INFO - allennlp.common.params - data_loader.shuffle = False
2022-03-30 18:45:49,388 - INFO - allennlp.common.params - data_loader.sampler = None
2022-03-30 18:45:49,388 - INFO - allennlp.common.params - data_loader.num_workers = 0
2022-03-30 18:45:49,388 - INFO - allennlp.common.params - data_loader.pin_memory = False
2022-03-30 18:45:49,388 - INFO - allennlp.common.params - data_loader.drop_last = False
2022-03-30 18:45:49,388 - INFO - allennlp.common.params - data_loader.timeout = 0
2022-03-30 18:45:49,388 - INFO - allennlp.common.params - data_loader.worker_init_fn = None
2022-03-30 18:45:49,388 - INFO - allennlp.common.params - data_loader.multiprocessing_context = None
2022-03-30 18:45:49,388 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None
2022-03-30 18:45:49,389 - INFO - allennlp.common.params - data_loader.batch_sampler.type = bucket
2022-03-30 18:45:49,389 - INFO - allennlp.common.params - data_loader.batch_sampler.batch_size = 2
2022-03-30 18:45:49,389 - INFO - allennlp.common.params - data_loader.batch_sampler.sorting_keys = None
2022-03-30 18:45:49,389 - INFO - allennlp.common.params - data_loader.batch_sampler.padding_noise = 0.1
2022-03-30 18:45:49,389 - INFO - allennlp.common.params - data_loader.batch_sampler.drop_last = False
2022-03-30 18:45:49,389 - INFO - allennlp.common.params - trainer.type = gradient_descent
2022-03-30 18:45:49,390 - INFO - allennlp.common.params - trainer.patience = None
2022-03-30 18:45:49,390 - INFO - allennlp.common.params - trainer.validation_metric = +accuracy
2022-03-30 18:45:49,390 - INFO - allennlp.common.params - trainer.num_epochs = 15
2022-03-30 18:45:49,390 - INFO - allennlp.common.params - trainer.cuda_device = 0
2022-03-30 18:45:49,390 - INFO - allennlp.common.params - trainer.grad_norm = None
2022-03-30 18:45:49,390 - INFO - allennlp.common.params - trainer.grad_clipping = None
2022-03-30 18:45:49,390 - INFO - allennlp.common.params - trainer.distributed = False
2022-03-30 18:45:49,390 - INFO - allennlp.common.params - trainer.world_size = 1
2022-03-30 18:45:49,391 - INFO - allennlp.common.params - trainer.num_gradient_accumulation_steps = 16
2022-03-30 18:45:49,391 - INFO - allennlp.common.params - trainer.use_amp = False
2022-03-30 18:45:49,391 - INFO - allennlp.common.params - trainer.no_grad = None
2022-03-30 18:45:49,391 - INFO - allennlp.common.params - trainer.momentum_scheduler = None
2022-03-30 18:45:49,391 - INFO - allennlp.common.params - trainer.tensorboard_writer = <allennlp.common.lazy.Lazy object at 0x7f8d9f69ee90>
2022-03-30 18:45:49,391 - INFO - allennlp.common.params - trainer.moving_average = None
2022-03-30 18:45:49,391 - INFO - allennlp.common.params - trainer.checkpointer = <allennlp.common.lazy.Lazy object at 0x7f8d9f69ef10>
2022-03-30 18:45:49,391 - INFO - allennlp.common.params - trainer.batch_callbacks = None
2022-03-30 18:45:49,391 - INFO - allennlp.common.params - trainer.epoch_callbacks = None
2022-03-30 18:45:49,392 - INFO - allennlp.common.params - trainer.end_callbacks = None
2022-03-30 18:45:49,392 - INFO - allennlp.common.params - trainer.trainer_callbacks = None
2022-03-30 18:45:52,221 - INFO - allennlp.common.params - trainer.optimizer.type = huggingface_adamw
2022-03-30 18:45:52,222 - INFO - allennlp.common.params - trainer.optimizer.parameter_groups = None
2022-03-30 18:45:52,222 - INFO - allennlp.common.params - trainer.optimizer.lr = 1e-05
2022-03-30 18:45:52,222 - INFO - allennlp.common.params - trainer.optimizer.betas = [0.9, 0.98]
2022-03-30 18:45:52,222 - INFO - allennlp.common.params - trainer.optimizer.eps = 1e-06
2022-03-30 18:45:52,222 - INFO - allennlp.common.params - trainer.optimizer.weight_decay = 0.1
2022-03-30 18:45:52,223 - INFO - allennlp.common.params - trainer.optimizer.correct_bias = True
2022-03-30 18:45:52,223 - INFO - allennlp.training.optimizers - Number of trainable parameters: 356411394
2022-03-30 18:45:52,225 - INFO - allennlp.common.util - The following parameters are Frozen (without gradient):
2022-03-30 18:45:52,228 - INFO - allennlp.common.util - The following parameters are Tunable (with gradient):
2022-03-30 18:45:52,228 - INFO - allennlp.common.util - _classifier.roberta.embeddings.word_embeddings.weight
2022-03-30 18:45:52,228 - INFO - allennlp.common.util - _classifier.roberta.embeddings.position_embeddings.weight
2022-03-30 18:45:52,228 - INFO - allennlp.common.util - _classifier.roberta.embeddings.token_type_embeddings.weight
2022-03-30 18:45:52,228 - INFO - allennlp.common.util - _classifier.roberta.embeddings.LayerNorm.weight
2022-03-30 18:45:52,228 - INFO - allennlp.common.util - _classifier.roberta.embeddings.LayerNorm.bias
2022-03-30 18:45:52,228 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.self.query.weight
2022-03-30 18:45:52,228 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.self.query.bias
2022-03-30 18:45:52,228 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.self.key.weight
2022-03-30 18:45:52,228 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.self.key.bias
2022-03-30 18:45:52,228 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.self.value.weight
2022-03-30 18:45:52,229 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.self.value.bias
2022-03-30 18:45:52,229 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.output.dense.weight
2022-03-30 18:45:52,229 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.output.dense.bias
2022-03-30 18:45:52,229 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.output.LayerNorm.weight
2022-03-30 18:45:52,229 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.output.LayerNorm.bias
2022-03-30 18:45:52,229 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.intermediate.dense.weight
2022-03-30 18:45:52,229 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.intermediate.dense.bias
2022-03-30 18:45:52,229 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.output.dense.weight
2022-03-30 18:45:52,229 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.output.dense.bias
2022-03-30 18:45:52,229 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.output.LayerNorm.weight
2022-03-30 18:45:52,229 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.output.LayerNorm.bias
2022-03-30 18:45:52,230 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.self.query.weight
2022-03-30 18:45:52,230 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.self.query.bias
2022-03-30 18:45:52,230 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.self.key.weight
2022-03-30 18:45:52,230 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.self.key.bias
2022-03-30 18:45:52,230 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.self.value.weight
2022-03-30 18:45:52,230 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.self.value.bias
2022-03-30 18:45:52,230 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.output.dense.weight
2022-03-30 18:45:52,230 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.output.dense.bias
2022-03-30 18:45:52,230 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.output.LayerNorm.weight
2022-03-30 18:45:52,230 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.output.LayerNorm.bias
2022-03-30 18:45:52,230 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.intermediate.dense.weight
2022-03-30 18:45:52,230 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.intermediate.dense.bias
2022-03-30 18:45:52,231 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.output.dense.weight
2022-03-30 18:45:52,231 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.output.dense.bias
2022-03-30 18:45:52,231 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.output.LayerNorm.weight
2022-03-30 18:45:52,231 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.output.LayerNorm.bias
2022-03-30 18:45:52,231 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.self.query.weight
2022-03-30 18:45:52,231 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.self.query.bias
2022-03-30 18:45:52,231 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.self.key.weight
2022-03-30 18:45:52,231 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.self.key.bias
2022-03-30 18:45:52,231 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.self.value.weight
2022-03-30 18:45:52,231 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.self.value.bias
2022-03-30 18:45:52,231 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.output.dense.weight
2022-03-30 18:45:52,232 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.output.dense.bias
2022-03-30 18:45:52,232 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.output.LayerNorm.weight
2022-03-30 18:45:52,232 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.output.LayerNorm.bias
2022-03-30 18:45:52,232 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.intermediate.dense.weight
2022-03-30 18:45:52,232 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.intermediate.dense.bias
2022-03-30 18:45:52,232 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.output.dense.weight
2022-03-30 18:45:52,232 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.output.dense.bias
2022-03-30 18:45:52,232 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.output.LayerNorm.weight
2022-03-30 18:45:52,232 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.output.LayerNorm.bias
2022-03-30 18:45:52,232 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.self.query.weight
2022-03-30 18:45:52,232 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.self.query.bias
2022-03-30 18:45:52,233 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.self.key.weight
2022-03-30 18:45:52,233 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.self.key.bias
2022-03-30 18:45:52,233 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.self.value.weight
2022-03-30 18:45:52,233 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.self.value.bias
2022-03-30 18:45:52,233 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.output.dense.weight
2022-03-30 18:45:52,233 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.output.dense.bias
2022-03-30 18:45:52,233 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.output.LayerNorm.weight
2022-03-30 18:45:52,233 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.output.LayerNorm.bias
2022-03-30 18:45:52,233 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.intermediate.dense.weight
2022-03-30 18:45:52,233 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.intermediate.dense.bias
2022-03-30 18:45:52,233 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.output.dense.weight
2022-03-30 18:45:52,234 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.output.dense.bias
2022-03-30 18:45:52,234 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.output.LayerNorm.weight
2022-03-30 18:45:52,234 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.output.LayerNorm.bias
2022-03-30 18:45:52,234 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.self.query.weight
2022-03-30 18:45:52,234 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.self.query.bias
2022-03-30 18:45:52,234 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.self.key.weight
2022-03-30 18:45:52,234 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.self.key.bias
2022-03-30 18:45:52,234 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.self.value.weight
2022-03-30 18:45:52,234 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.self.value.bias
2022-03-30 18:45:52,234 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.output.dense.weight
2022-03-30 18:45:52,234 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.output.dense.bias
2022-03-30 18:45:52,234 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.output.LayerNorm.weight
2022-03-30 18:45:52,235 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.output.LayerNorm.bias
2022-03-30 18:45:52,235 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.intermediate.dense.weight
2022-03-30 18:45:52,235 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.intermediate.dense.bias
2022-03-30 18:45:52,235 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.output.dense.weight
2022-03-30 18:45:52,235 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.output.dense.bias
2022-03-30 18:45:52,235 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.output.LayerNorm.weight
2022-03-30 18:45:52,235 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.output.LayerNorm.bias
2022-03-30 18:45:52,235 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.self.query.weight
2022-03-30 18:45:52,235 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.self.query.bias
2022-03-30 18:45:52,235 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.self.key.weight
2022-03-30 18:45:52,235 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.self.key.bias
2022-03-30 18:45:52,236 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.self.value.weight
2022-03-30 18:45:52,236 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.self.value.bias
2022-03-30 18:45:52,236 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.output.dense.weight
2022-03-30 18:45:52,236 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.output.dense.bias
2022-03-30 18:45:52,236 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.output.LayerNorm.weight
2022-03-30 18:45:52,236 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.output.LayerNorm.bias
2022-03-30 18:45:52,236 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.intermediate.dense.weight
2022-03-30 18:45:52,236 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.intermediate.dense.bias
2022-03-30 18:45:52,236 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.output.dense.weight
2022-03-30 18:45:52,236 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.output.dense.bias
2022-03-30 18:45:52,236 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.output.LayerNorm.weight
2022-03-30 18:45:52,236 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.output.LayerNorm.bias
2022-03-30 18:45:52,237 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.self.query.weight
2022-03-30 18:45:52,237 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.self.query.bias
2022-03-30 18:45:52,237 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.self.key.weight
2022-03-30 18:45:52,237 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.self.key.bias
2022-03-30 18:45:52,237 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.self.value.weight
2022-03-30 18:45:52,237 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.self.value.bias
2022-03-30 18:45:52,237 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.output.dense.weight
2022-03-30 18:45:52,237 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.output.dense.bias
2022-03-30 18:45:52,237 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.output.LayerNorm.weight
2022-03-30 18:45:52,237 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.output.LayerNorm.bias
2022-03-30 18:45:52,237 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.intermediate.dense.weight
2022-03-30 18:45:52,237 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.intermediate.dense.bias
2022-03-30 18:45:52,238 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.output.dense.weight
2022-03-30 18:45:52,238 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.output.dense.bias
2022-03-30 18:45:52,238 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.output.LayerNorm.weight
2022-03-30 18:45:52,238 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.output.LayerNorm.bias
2022-03-30 18:45:52,238 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.self.query.weight
2022-03-30 18:45:52,238 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.self.query.bias
2022-03-30 18:45:52,238 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.self.key.weight
2022-03-30 18:45:52,238 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.self.key.bias
2022-03-30 18:45:52,238 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.self.value.weight
2022-03-30 18:45:52,238 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.self.value.bias
2022-03-30 18:45:52,238 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.output.dense.weight
2022-03-30 18:45:52,238 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.output.dense.bias
2022-03-30 18:45:52,239 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.output.LayerNorm.weight
2022-03-30 18:45:52,239 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.output.LayerNorm.bias
2022-03-30 18:45:52,239 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.intermediate.dense.weight
2022-03-30 18:45:52,239 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.intermediate.dense.bias
2022-03-30 18:45:52,239 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.output.dense.weight
2022-03-30 18:45:52,239 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.output.dense.bias
2022-03-30 18:45:52,239 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.output.LayerNorm.weight
2022-03-30 18:45:52,239 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.output.LayerNorm.bias
2022-03-30 18:45:52,239 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.self.query.weight
2022-03-30 18:45:52,239 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.self.query.bias
2022-03-30 18:45:52,239 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.self.key.weight
2022-03-30 18:45:52,239 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.self.key.bias
2022-03-30 18:45:52,240 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.self.value.weight
2022-03-30 18:45:52,240 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.self.value.bias
2022-03-30 18:45:52,240 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.output.dense.weight
2022-03-30 18:45:52,240 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.output.dense.bias
2022-03-30 18:45:52,240 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.output.LayerNorm.weight
2022-03-30 18:45:52,240 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.output.LayerNorm.bias
2022-03-30 18:45:52,240 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.intermediate.dense.weight
2022-03-30 18:45:52,240 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.intermediate.dense.bias
2022-03-30 18:45:52,240 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.output.dense.weight
2022-03-30 18:45:52,240 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.output.dense.bias
2022-03-30 18:45:52,240 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.output.LayerNorm.weight
2022-03-30 18:45:52,240 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.output.LayerNorm.bias
2022-03-30 18:45:52,241 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.self.query.weight
2022-03-30 18:45:52,241 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.self.query.bias
2022-03-30 18:45:52,241 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.self.key.weight
2022-03-30 18:45:52,241 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.self.key.bias
2022-03-30 18:45:52,241 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.self.value.weight
2022-03-30 18:45:52,241 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.self.value.bias
2022-03-30 18:45:52,241 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.output.dense.weight
2022-03-30 18:45:52,241 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.output.dense.bias
2022-03-30 18:45:52,241 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.output.LayerNorm.weight
2022-03-30 18:45:52,241 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.output.LayerNorm.bias
2022-03-30 18:45:52,241 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.intermediate.dense.weight
2022-03-30 18:45:52,241 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.intermediate.dense.bias
2022-03-30 18:45:52,242 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.output.dense.weight
2022-03-30 18:45:52,242 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.output.dense.bias
2022-03-30 18:45:52,242 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.output.LayerNorm.weight
2022-03-30 18:45:52,242 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.output.LayerNorm.bias
2022-03-30 18:45:52,242 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.self.query.weight
2022-03-30 18:45:52,242 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.self.query.bias
2022-03-30 18:45:52,242 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.self.key.weight
2022-03-30 18:45:52,242 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.self.key.bias
2022-03-30 18:45:52,242 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.self.value.weight
2022-03-30 18:45:52,242 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.self.value.bias
2022-03-30 18:45:52,242 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.output.dense.weight
2022-03-30 18:45:52,243 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.output.dense.bias
2022-03-30 18:45:52,243 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.output.LayerNorm.weight
2022-03-30 18:45:52,243 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.output.LayerNorm.bias
2022-03-30 18:45:52,243 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.intermediate.dense.weight
2022-03-30 18:45:52,243 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.intermediate.dense.bias
2022-03-30 18:45:52,243 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.output.dense.weight
2022-03-30 18:45:52,243 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.output.dense.bias
2022-03-30 18:45:52,243 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.output.LayerNorm.weight
2022-03-30 18:45:52,243 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.output.LayerNorm.bias
2022-03-30 18:45:52,243 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.self.query.weight
2022-03-30 18:45:52,243 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.self.query.bias
2022-03-30 18:45:52,244 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.self.key.weight
2022-03-30 18:45:52,244 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.self.key.bias
2022-03-30 18:45:52,244 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.self.value.weight
2022-03-30 18:45:52,244 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.self.value.bias
2022-03-30 18:45:52,244 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.output.dense.weight
2022-03-30 18:45:52,244 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.output.dense.bias
2022-03-30 18:45:52,244 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.output.LayerNorm.weight
2022-03-30 18:45:52,244 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.output.LayerNorm.bias
2022-03-30 18:45:52,244 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.intermediate.dense.weight
2022-03-30 18:45:52,244 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.intermediate.dense.bias
2022-03-30 18:45:52,244 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.output.dense.weight
2022-03-30 18:45:52,244 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.output.dense.bias
2022-03-30 18:45:52,245 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.output.LayerNorm.weight
2022-03-30 18:45:52,245 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.output.LayerNorm.bias
2022-03-30 18:45:52,245 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.self.query.weight
2022-03-30 18:45:52,245 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.self.query.bias
2022-03-30 18:45:52,245 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.self.key.weight
2022-03-30 18:45:52,245 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.self.key.bias
2022-03-30 18:45:52,245 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.self.value.weight
2022-03-30 18:45:52,245 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.self.value.bias
2022-03-30 18:45:52,245 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.output.dense.weight
2022-03-30 18:45:52,245 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.output.dense.bias
2022-03-30 18:45:52,245 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.output.LayerNorm.weight
2022-03-30 18:45:52,245 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.output.LayerNorm.bias
2022-03-30 18:45:52,246 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.intermediate.dense.weight
2022-03-30 18:45:52,246 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.intermediate.dense.bias
2022-03-30 18:45:52,246 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.output.dense.weight
2022-03-30 18:45:52,246 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.output.dense.bias
2022-03-30 18:45:52,246 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.output.LayerNorm.weight
2022-03-30 18:45:52,246 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.output.LayerNorm.bias
2022-03-30 18:45:52,246 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.self.query.weight
2022-03-30 18:45:52,246 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.self.query.bias
2022-03-30 18:45:52,246 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.self.key.weight
2022-03-30 18:45:52,246 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.self.key.bias
2022-03-30 18:45:52,246 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.self.value.weight
2022-03-30 18:45:52,247 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.self.value.bias
2022-03-30 18:45:52,247 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.output.dense.weight
2022-03-30 18:45:52,247 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.output.dense.bias
2022-03-30 18:45:52,247 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.output.LayerNorm.weight
2022-03-30 18:45:52,247 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.output.LayerNorm.bias
2022-03-30 18:45:52,247 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.intermediate.dense.weight
2022-03-30 18:45:52,247 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.intermediate.dense.bias
2022-03-30 18:45:52,247 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.output.dense.weight
2022-03-30 18:45:52,247 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.output.dense.bias
2022-03-30 18:45:52,247 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.output.LayerNorm.weight
2022-03-30 18:45:52,247 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.output.LayerNorm.bias
2022-03-30 18:45:52,247 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.self.query.weight
2022-03-30 18:45:52,248 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.self.query.bias
2022-03-30 18:45:52,248 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.self.key.weight
2022-03-30 18:45:52,248 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.self.key.bias
2022-03-30 18:45:52,248 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.self.value.weight
2022-03-30 18:45:52,248 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.self.value.bias
2022-03-30 18:45:52,248 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.output.dense.weight
2022-03-30 18:45:52,248 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.output.dense.bias
2022-03-30 18:45:52,248 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.output.LayerNorm.weight
2022-03-30 18:45:52,248 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.output.LayerNorm.bias
2022-03-30 18:45:52,248 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.intermediate.dense.weight
2022-03-30 18:45:52,248 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.intermediate.dense.bias
2022-03-30 18:45:52,248 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.output.dense.weight
2022-03-30 18:45:52,249 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.output.dense.bias
2022-03-30 18:45:52,249 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.output.LayerNorm.weight
2022-03-30 18:45:52,249 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.output.LayerNorm.bias
2022-03-30 18:45:52,249 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.self.query.weight
2022-03-30 18:45:52,249 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.self.query.bias
2022-03-30 18:45:52,249 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.self.key.weight
2022-03-30 18:45:52,249 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.self.key.bias
2022-03-30 18:45:52,249 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.self.value.weight
2022-03-30 18:45:52,249 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.self.value.bias
2022-03-30 18:45:52,249 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.output.dense.weight
2022-03-30 18:45:52,249 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.output.dense.bias
2022-03-30 18:45:52,249 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.output.LayerNorm.weight
2022-03-30 18:45:52,250 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.output.LayerNorm.bias
2022-03-30 18:45:52,250 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.intermediate.dense.weight
2022-03-30 18:45:52,250 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.intermediate.dense.bias
2022-03-30 18:45:52,250 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.output.dense.weight
2022-03-30 18:45:52,250 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.output.dense.bias
2022-03-30 18:45:52,250 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.output.LayerNorm.weight
2022-03-30 18:45:52,250 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.output.LayerNorm.bias
2022-03-30 18:45:52,250 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.self.query.weight
2022-03-30 18:45:52,250 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.self.query.bias
2022-03-30 18:45:52,250 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.self.key.weight
2022-03-30 18:45:52,250 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.self.key.bias
2022-03-30 18:45:52,251 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.self.value.weight
2022-03-30 18:45:52,251 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.self.value.bias
2022-03-30 18:45:52,251 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.output.dense.weight
2022-03-30 18:45:52,251 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.output.dense.bias
2022-03-30 18:45:52,251 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.output.LayerNorm.weight
2022-03-30 18:45:52,251 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.output.LayerNorm.bias
2022-03-30 18:45:52,251 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.intermediate.dense.weight
2022-03-30 18:45:52,251 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.intermediate.dense.bias
2022-03-30 18:45:52,251 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.output.dense.weight
2022-03-30 18:45:52,251 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.output.dense.bias
2022-03-30 18:45:52,251 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.output.LayerNorm.weight
2022-03-30 18:45:52,251 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.output.LayerNorm.bias
2022-03-30 18:45:52,252 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.self.query.weight
2022-03-30 18:45:52,252 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.self.query.bias
2022-03-30 18:45:52,252 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.self.key.weight
2022-03-30 18:45:52,252 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.self.key.bias
2022-03-30 18:45:52,252 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.self.value.weight
2022-03-30 18:45:52,252 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.self.value.bias
2022-03-30 18:45:52,252 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.output.dense.weight
2022-03-30 18:45:52,252 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.output.dense.bias
2022-03-30 18:45:52,252 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.output.LayerNorm.weight
2022-03-30 18:45:52,252 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.output.LayerNorm.bias
2022-03-30 18:45:52,252 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.intermediate.dense.weight
2022-03-30 18:45:52,253 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.intermediate.dense.bias
2022-03-30 18:45:52,253 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.output.dense.weight
2022-03-30 18:45:52,253 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.output.dense.bias
2022-03-30 18:45:52,253 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.output.LayerNorm.weight
2022-03-30 18:45:52,253 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.output.LayerNorm.bias
2022-03-30 18:45:52,253 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.self.query.weight
2022-03-30 18:45:52,253 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.self.query.bias
2022-03-30 18:45:52,253 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.self.key.weight
2022-03-30 18:45:52,253 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.self.key.bias
2022-03-30 18:45:52,253 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.self.value.weight
2022-03-30 18:45:52,253 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.self.value.bias
2022-03-30 18:45:52,254 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.output.dense.weight
2022-03-30 18:45:52,254 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.output.dense.bias
2022-03-30 18:45:52,254 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.output.LayerNorm.weight
2022-03-30 18:45:52,254 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.output.LayerNorm.bias
2022-03-30 18:45:52,254 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.intermediate.dense.weight
2022-03-30 18:45:52,254 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.intermediate.dense.bias
2022-03-30 18:45:52,254 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.output.dense.weight
2022-03-30 18:45:52,254 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.output.dense.bias
2022-03-30 18:45:52,254 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.output.LayerNorm.weight
2022-03-30 18:45:52,254 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.output.LayerNorm.bias
2022-03-30 18:45:52,254 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.self.query.weight
2022-03-30 18:45:52,254 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.self.query.bias
2022-03-30 18:45:52,255 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.self.key.weight
2022-03-30 18:45:52,255 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.self.key.bias
2022-03-30 18:45:52,255 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.self.value.weight
2022-03-30 18:45:52,255 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.self.value.bias
2022-03-30 18:45:52,255 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.output.dense.weight
2022-03-30 18:45:52,255 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.output.dense.bias
2022-03-30 18:45:52,255 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.output.LayerNorm.weight
2022-03-30 18:45:52,255 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.output.LayerNorm.bias
2022-03-30 18:45:52,255 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.intermediate.dense.weight
2022-03-30 18:45:52,255 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.intermediate.dense.bias
2022-03-30 18:45:52,255 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.output.dense.weight
2022-03-30 18:45:52,256 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.output.dense.bias
2022-03-30 18:45:52,256 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.output.LayerNorm.weight
2022-03-30 18:45:52,256 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.output.LayerNorm.bias
2022-03-30 18:45:52,256 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.self.query.weight
2022-03-30 18:45:52,256 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.self.query.bias
2022-03-30 18:45:52,256 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.self.key.weight
2022-03-30 18:45:52,256 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.self.key.bias
2022-03-30 18:45:52,256 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.self.value.weight
2022-03-30 18:45:52,256 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.self.value.bias
2022-03-30 18:45:52,256 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.output.dense.weight
2022-03-30 18:45:52,256 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.output.dense.bias
2022-03-30 18:45:52,257 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.output.LayerNorm.weight
2022-03-30 18:45:52,257 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.output.LayerNorm.bias
2022-03-30 18:45:52,257 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.intermediate.dense.weight
2022-03-30 18:45:52,257 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.intermediate.dense.bias
2022-03-30 18:45:52,257 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.output.dense.weight
2022-03-30 18:45:52,257 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.output.dense.bias
2022-03-30 18:45:52,257 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.output.LayerNorm.weight
2022-03-30 18:45:52,257 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.output.LayerNorm.bias
2022-03-30 18:45:52,257 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.self.query.weight
2022-03-30 18:45:52,257 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.self.query.bias
2022-03-30 18:45:52,257 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.self.key.weight
2022-03-30 18:45:52,258 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.self.key.bias
2022-03-30 18:45:52,258 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.self.value.weight
2022-03-30 18:45:52,258 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.self.value.bias
2022-03-30 18:45:52,258 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.output.dense.weight
2022-03-30 18:45:52,258 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.output.dense.bias
2022-03-30 18:45:52,258 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.output.LayerNorm.weight
2022-03-30 18:45:52,258 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.output.LayerNorm.bias
2022-03-30 18:45:52,258 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.intermediate.dense.weight
2022-03-30 18:45:52,258 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.intermediate.dense.bias
2022-03-30 18:45:52,258 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.output.dense.weight
2022-03-30 18:45:52,258 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.output.dense.bias
2022-03-30 18:45:52,259 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.output.LayerNorm.weight
2022-03-30 18:45:52,259 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.output.LayerNorm.bias
2022-03-30 18:45:52,259 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.self.query.weight
2022-03-30 18:45:52,259 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.self.query.bias
2022-03-30 18:45:52,259 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.self.key.weight
2022-03-30 18:45:52,259 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.self.key.bias
2022-03-30 18:45:52,259 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.self.value.weight
2022-03-30 18:45:52,259 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.self.value.bias
2022-03-30 18:45:52,259 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.output.dense.weight
2022-03-30 18:45:52,259 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.output.dense.bias
2022-03-30 18:45:52,259 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.output.LayerNorm.weight
2022-03-30 18:45:52,260 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.output.LayerNorm.bias
2022-03-30 18:45:52,260 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.intermediate.dense.weight
2022-03-30 18:45:52,260 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.intermediate.dense.bias
2022-03-30 18:45:52,260 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.output.dense.weight
2022-03-30 18:45:52,260 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.output.dense.bias
2022-03-30 18:45:52,260 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.output.LayerNorm.weight
2022-03-30 18:45:52,260 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.output.LayerNorm.bias
2022-03-30 18:45:52,260 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.self.query.weight
2022-03-30 18:45:52,260 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.self.query.bias
2022-03-30 18:45:52,260 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.self.key.weight
2022-03-30 18:45:52,261 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.self.key.bias
2022-03-30 18:45:52,261 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.self.value.weight
2022-03-30 18:45:52,261 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.self.value.bias
2022-03-30 18:45:52,261 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.output.dense.weight
2022-03-30 18:45:52,261 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.output.dense.bias
2022-03-30 18:45:52,261 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.output.LayerNorm.weight
2022-03-30 18:45:52,261 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.output.LayerNorm.bias
2022-03-30 18:45:52,261 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.intermediate.dense.weight
2022-03-30 18:45:52,261 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.intermediate.dense.bias
2022-03-30 18:45:52,261 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.output.dense.weight
2022-03-30 18:45:52,261 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.output.dense.bias
2022-03-30 18:45:52,262 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.output.LayerNorm.weight
2022-03-30 18:45:52,262 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.output.LayerNorm.bias
2022-03-30 18:45:52,262 - INFO - allennlp.common.util - _classifier.roberta.pooler.dense.weight
2022-03-30 18:45:52,262 - INFO - allennlp.common.util - _classifier.roberta.pooler.dense.bias
2022-03-30 18:45:52,262 - INFO - allennlp.common.util - _classifier.classifier.dense.weight
2022-03-30 18:45:52,262 - INFO - allennlp.common.util - _classifier.classifier.dense.bias
2022-03-30 18:45:52,262 - INFO - allennlp.common.util - _classifier.classifier.out_proj.weight
2022-03-30 18:45:52,262 - INFO - allennlp.common.util - _classifier.classifier.out_proj.bias
2022-03-30 18:45:52,262 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.type = slanted_triangular
2022-03-30 18:45:52,263 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.cut_frac = 0.06
2022-03-30 18:45:52,263 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.ratio = 32
2022-03-30 18:45:52,263 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.last_epoch = -1
2022-03-30 18:45:52,263 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.gradual_unfreezing = False
2022-03-30 18:45:52,263 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.discriminative_fine_tuning = False
2022-03-30 18:45:52,263 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.decay_factor = 0.38
2022-03-30 18:45:52,263 - INFO - allennlp.common.params - type = default
2022-03-30 18:45:52,264 - INFO - allennlp.common.params - keep_serialized_model_every_num_seconds = None
2022-03-30 18:45:52,264 - INFO - allennlp.common.params - num_serialized_models_to_keep = 2
2022-03-30 18:45:52,264 - INFO - allennlp.common.params - model_save_interval = None
2022-03-30 18:45:52,264 - INFO - allennlp.common.params - summary_interval = 100
2022-03-30 18:45:52,264 - INFO - allennlp.common.params - histogram_interval = None
2022-03-30 18:45:52,264 - INFO - allennlp.common.params - batch_size_interval = None
2022-03-30 18:45:52,264 - INFO - allennlp.common.params - should_log_parameter_statistics = True
2022-03-30 18:45:52,264 - INFO - allennlp.common.params - should_log_learning_rate = False
2022-03-30 18:45:52,265 - INFO - allennlp.common.params - get_batch_num_total = None
2022-03-30 18:45:52,267 - WARNING - allennlp.training.trainer - You provided a validation dataset but patience was set to None, meaning that early stopping is disabled
2022-03-30 18:45:52,359 - INFO - allennlp.training.trainer - Beginning training.
2022-03-30 18:45:52,359 - INFO - allennlp.training.trainer - Epoch 0/14
2022-03-30 18:45:52,359 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-03-30 18:45:52,359 - INFO - allennlp.training.trainer - GPU 0 memory usage: 1.3G
2022-03-30 18:45:52,361 - INFO - allennlp.training.trainer - Training
2022-03-30 18:45:52,361 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-03-30 18:45:52,361 - INFO - allennlp.data.samplers.bucket_batch_sampler - No sorting keys given; trying to guess a good one
2022-03-30 18:45:52,361 - INFO - allennlp.data.samplers.bucket_batch_sampler - Using ['tokens'] as the sorting keys
2022-03-30 18:46:02,688 - INFO - tqdm - accuracy: 0.4688, batch_loss: 1.5490, loss: 1.5490 ||:   2%|1         | 1/65 [00:10<11:00, 10.33s/it]
2022-03-30 18:46:13,208 - INFO - tqdm - accuracy: 0.5156, batch_loss: 1.5481, loss: 1.5485 ||:   3%|3         | 2/65 [00:20<10:57, 10.44s/it]
2022-03-30 18:46:23,875 - INFO - tqdm - accuracy: 0.5208, batch_loss: 2.0186, loss: 1.7052 ||:   5%|4         | 3/65 [00:31<10:53, 10.54s/it]
2022-03-30 18:46:34,670 - INFO - tqdm - accuracy: 0.5469, batch_loss: 1.3413, loss: 1.6143 ||:   6%|6         | 4/65 [00:42<10:49, 10.64s/it]
2022-03-30 18:46:45,542 - INFO - tqdm - accuracy: 0.5312, batch_loss: 1.2133, loss: 1.5341 ||:   8%|7         | 5/65 [00:53<10:43, 10.73s/it]
2022-03-30 18:46:56,642 - INFO - tqdm - accuracy: 0.5156, batch_loss: 1.6996, loss: 1.5617 ||:   9%|9         | 6/65 [01:04<10:40, 10.85s/it]
2022-03-30 18:47:07,996 - INFO - tqdm - accuracy: 0.5223, batch_loss: 1.4528, loss: 1.5461 ||:  11%|#         | 7/65 [01:15<10:38, 11.02s/it]
2022-03-30 18:47:19,631 - INFO - tqdm - accuracy: 0.5156, batch_loss: 1.5003, loss: 1.5404 ||:  12%|#2        | 8/65 [01:27<10:39, 11.21s/it]
2022-03-30 18:47:31,294 - INFO - tqdm - accuracy: 0.5104, batch_loss: 1.6758, loss: 1.5554 ||:  14%|#3        | 9/65 [01:38<10:35, 11.35s/it]
2022-03-30 18:47:42,574 - INFO - tqdm - accuracy: 0.4969, batch_loss: 1.5000, loss: 1.5499 ||:  15%|#5        | 10/65 [01:50<10:23, 11.33s/it]
2022-03-30 18:47:53,826 - INFO - tqdm - accuracy: 0.4943, batch_loss: 1.4277, loss: 1.5388 ||:  17%|#6        | 11/65 [02:01<10:10, 11.31s/it]
2022-03-30 18:48:05,070 - INFO - tqdm - accuracy: 0.5104, batch_loss: 0.9454, loss: 1.4893 ||:  18%|#8        | 12/65 [02:12<09:58, 11.29s/it]
2022-03-30 18:48:16,375 - INFO - tqdm - accuracy: 0.5144, batch_loss: 0.8647, loss: 1.4413 ||:  20%|##        | 13/65 [02:24<09:47, 11.29s/it]
2022-03-30 18:48:27,640 - INFO - tqdm - accuracy: 0.5134, batch_loss: 1.3311, loss: 1.4334 ||:  22%|##1       | 14/65 [02:35<09:35, 11.28s/it]
2022-03-30 18:48:39,121 - INFO - tqdm - accuracy: 0.5167, batch_loss: 1.0015, loss: 1.4046 ||:  23%|##3       | 15/65 [02:46<09:27, 11.34s/it]
2022-03-30 18:48:50,581 - INFO - tqdm - accuracy: 0.5059, batch_loss: 1.0311, loss: 1.3813 ||:  25%|##4       | 16/65 [02:58<09:17, 11.38s/it]
2022-03-30 18:49:02,024 - INFO - tqdm - accuracy: 0.5110, batch_loss: 0.5861, loss: 1.3345 ||:  26%|##6       | 17/65 [03:09<09:07, 11.40s/it]
2022-03-30 18:49:13,417 - INFO - tqdm - accuracy: 0.5156, batch_loss: 0.8127, loss: 1.3055 ||:  28%|##7       | 18/65 [03:21<08:55, 11.40s/it]
2022-03-30 18:49:24,774 - INFO - tqdm - accuracy: 0.5181, batch_loss: 0.8276, loss: 1.2804 ||:  29%|##9       | 19/65 [03:32<08:43, 11.38s/it]
2022-03-30 18:49:35,931 - INFO - tqdm - accuracy: 0.5250, batch_loss: 0.6319, loss: 1.2479 ||:  31%|###       | 20/65 [03:43<08:29, 11.32s/it]
2022-03-30 18:49:47,344 - INFO - tqdm - accuracy: 0.5193, batch_loss: 0.9864, loss: 1.2355 ||:  32%|###2      | 21/65 [03:54<08:19, 11.35s/it]
2022-03-30 18:49:58,663 - INFO - tqdm - accuracy: 0.5213, batch_loss: 0.7503, loss: 1.2134 ||:  34%|###3      | 22/65 [04:06<08:07, 11.34s/it]
2022-03-30 18:50:10,114 - INFO - tqdm - accuracy: 0.5204, batch_loss: 0.7883, loss: 1.1949 ||:  35%|###5      | 23/65 [04:17<07:57, 11.37s/it]
2022-03-30 18:50:21,564 - INFO - tqdm - accuracy: 0.5182, batch_loss: 0.7611, loss: 1.1769 ||:  37%|###6      | 24/65 [04:29<07:47, 11.39s/it]
2022-03-30 18:50:33,021 - INFO - tqdm - accuracy: 0.5138, batch_loss: 0.8458, loss: 1.1636 ||:  38%|###8      | 25/65 [04:40<07:36, 11.41s/it]
2022-03-30 18:50:44,484 - INFO - tqdm - accuracy: 0.5096, batch_loss: 0.7295, loss: 1.1469 ||:  40%|####      | 26/65 [04:52<07:25, 11.43s/it]
2022-03-30 18:50:55,929 - INFO - tqdm - accuracy: 0.5127, batch_loss: 0.6514, loss: 1.1286 ||:  42%|####1     | 27/65 [05:03<07:14, 11.43s/it]
2022-03-30 18:51:07,402 - INFO - tqdm - accuracy: 0.5123, batch_loss: 0.7323, loss: 1.1144 ||:  43%|####3     | 28/65 [05:15<07:03, 11.45s/it]
2022-03-30 18:51:18,848 - INFO - tqdm - accuracy: 0.5097, batch_loss: 0.7301, loss: 1.1012 ||:  45%|####4     | 29/65 [05:26<06:52, 11.45s/it]
2022-03-30 18:51:30,315 - INFO - tqdm - accuracy: 0.5073, batch_loss: 0.7439, loss: 1.0893 ||:  46%|####6     | 30/65 [05:37<06:40, 11.45s/it]
2022-03-30 18:51:41,777 - INFO - tqdm - accuracy: 0.5101, batch_loss: 0.6687, loss: 1.0757 ||:  48%|####7     | 31/65 [05:49<06:29, 11.45s/it]
2022-03-30 18:51:53,241 - INFO - tqdm - accuracy: 0.5088, batch_loss: 0.7439, loss: 1.0653 ||:  49%|####9     | 32/65 [06:00<06:18, 11.46s/it]
2022-03-30 18:52:04,703 - INFO - tqdm - accuracy: 0.5076, batch_loss: 0.7022, loss: 1.0543 ||:  51%|#####     | 33/65 [06:12<06:06, 11.46s/it]
2022-03-30 18:52:16,148 - INFO - tqdm - accuracy: 0.5074, batch_loss: 0.6999, loss: 1.0439 ||:  52%|#####2    | 34/65 [06:23<05:55, 11.45s/it]
2022-03-30 18:52:27,594 - INFO - tqdm - accuracy: 0.5071, batch_loss: 0.7122, loss: 1.0344 ||:  54%|#####3    | 35/65 [06:35<05:43, 11.45s/it]
2022-03-30 18:52:39,045 - INFO - tqdm - accuracy: 0.5061, batch_loss: 0.6850, loss: 1.0247 ||:  55%|#####5    | 36/65 [06:46<05:32, 11.45s/it]
2022-03-30 18:52:50,460 - INFO - tqdm - accuracy: 0.5025, batch_loss: 0.7111, loss: 1.0162 ||:  57%|#####6    | 37/65 [06:58<05:20, 11.44s/it]
2022-03-30 18:53:01,860 - INFO - tqdm - accuracy: 0.5041, batch_loss: 0.6732, loss: 1.0072 ||:  58%|#####8    | 38/65 [07:09<05:08, 11.43s/it]
2022-03-30 18:53:13,270 - INFO - tqdm - accuracy: 0.5048, batch_loss: 0.6896, loss: 0.9991 ||:  60%|######    | 39/65 [07:20<04:56, 11.42s/it]
2022-03-30 18:53:24,680 - INFO - tqdm - accuracy: 0.5078, batch_loss: 0.6668, loss: 0.9908 ||:  62%|######1   | 40/65 [07:32<04:45, 11.42s/it]
2022-03-30 18:53:36,072 - INFO - tqdm - accuracy: 0.5046, batch_loss: 0.7377, loss: 0.9846 ||:  63%|######3   | 41/65 [07:43<04:33, 11.41s/it]
2022-03-30 18:53:47,461 - INFO - tqdm - accuracy: 0.5067, batch_loss: 0.6600, loss: 0.9769 ||:  65%|######4   | 42/65 [07:55<04:22, 11.40s/it]
2022-03-30 18:53:58,836 - INFO - tqdm - accuracy: 0.5029, batch_loss: 0.7340, loss: 0.9712 ||:  66%|######6   | 43/65 [08:06<04:10, 11.40s/it]
2022-03-30 18:54:10,218 - INFO - tqdm - accuracy: 0.5057, batch_loss: 0.6494, loss: 0.9639 ||:  68%|######7   | 44/65 [08:17<03:59, 11.39s/it]
2022-03-30 18:54:21,609 - INFO - tqdm - accuracy: 0.5062, batch_loss: 0.6971, loss: 0.9580 ||:  69%|######9   | 45/65 [08:29<03:47, 11.39s/it]
2022-03-30 18:54:32,984 - INFO - tqdm - accuracy: 0.5068, batch_loss: 0.6830, loss: 0.9520 ||:  71%|#######   | 46/65 [08:40<03:36, 11.39s/it]
2022-03-30 18:54:44,388 - INFO - tqdm - accuracy: 0.5086, batch_loss: 0.6785, loss: 0.9462 ||:  72%|#######2  | 47/65 [08:52<03:25, 11.39s/it]
2022-03-30 18:54:55,810 - INFO - tqdm - accuracy: 0.5072, batch_loss: 0.7286, loss: 0.9416 ||:  74%|#######3  | 48/65 [09:03<03:13, 11.40s/it]
2022-03-30 18:55:07,229 - INFO - tqdm - accuracy: 0.5077, batch_loss: 0.6875, loss: 0.9365 ||:  75%|#######5  | 49/65 [09:14<03:02, 11.41s/it]
2022-03-30 18:55:18,681 - INFO - tqdm - accuracy: 0.5088, batch_loss: 0.6865, loss: 0.9315 ||:  77%|#######6  | 50/65 [09:26<02:51, 11.42s/it]
2022-03-30 18:55:30,129 - INFO - tqdm - accuracy: 0.5074, batch_loss: 0.7242, loss: 0.9274 ||:  78%|#######8  | 51/65 [09:37<02:39, 11.43s/it]
2022-03-30 18:55:41,580 - INFO - tqdm - accuracy: 0.5102, batch_loss: 0.6428, loss: 0.9219 ||:  80%|########  | 52/65 [09:49<02:28, 11.44s/it]
2022-03-30 18:55:53,030 - INFO - tqdm - accuracy: 0.5112, batch_loss: 0.6869, loss: 0.9175 ||:  82%|########1 | 53/65 [10:00<02:17, 11.44s/it]
2022-03-30 18:56:04,136 - INFO - tqdm - accuracy: 0.5159, batch_loss: 0.6165, loss: 0.9119 ||:  83%|########3 | 54/65 [10:11<02:04, 11.34s/it]
2022-03-30 18:56:15,587 - INFO - tqdm - accuracy: 0.5134, batch_loss: 0.7288, loss: 0.9086 ||:  85%|########4 | 55/65 [10:23<01:53, 11.37s/it]
2022-03-30 18:56:27,031 - INFO - tqdm - accuracy: 0.5137, batch_loss: 0.6932, loss: 0.9047 ||:  86%|########6 | 56/65 [10:34<01:42, 11.39s/it]
2022-03-30 18:56:38,474 - INFO - tqdm - accuracy: 0.5091, batch_loss: 0.7964, loss: 0.9028 ||:  88%|########7 | 57/65 [10:46<01:31, 11.41s/it]
2022-03-30 18:56:49,923 - INFO - tqdm - accuracy: 0.5089, batch_loss: 0.6769, loss: 0.8989 ||:  89%|########9 | 58/65 [10:57<01:19, 11.42s/it]
2022-03-30 18:57:01,375 - INFO - tqdm - accuracy: 0.5093, batch_loss: 0.7125, loss: 0.8958 ||:  91%|######### | 59/65 [11:09<01:08, 11.43s/it]
2022-03-30 18:57:12,833 - INFO - tqdm - accuracy: 0.5117, batch_loss: 0.6738, loss: 0.8921 ||:  92%|#########2| 60/65 [11:20<00:57, 11.44s/it]
2022-03-30 18:57:24,287 - INFO - tqdm - accuracy: 0.5115, batch_loss: 0.6856, loss: 0.8887 ||:  94%|#########3| 61/65 [11:31<00:45, 11.44s/it]
2022-03-30 18:57:35,738 - INFO - tqdm - accuracy: 0.5124, batch_loss: 0.6830, loss: 0.8854 ||:  95%|#########5| 62/65 [11:43<00:34, 11.45s/it]
2022-03-30 18:57:47,188 - INFO - tqdm - accuracy: 0.5117, batch_loss: 0.7115, loss: 0.8826 ||:  97%|#########6| 63/65 [11:54<00:22, 11.45s/it]
2022-03-30 18:57:58,638 - INFO - tqdm - accuracy: 0.5125, batch_loss: 0.6992, loss: 0.8798 ||:  98%|#########8| 64/65 [12:06<00:11, 11.45s/it]
2022-03-30 18:58:03,721 - INFO - tqdm - accuracy: 0.5119, batch_loss: 0.7233, loss: 0.8773 ||: 100%|##########| 65/65 [12:11<00:00,  9.54s/it]
2022-03-30 18:58:03,723 - INFO - tqdm - accuracy: 0.5119, batch_loss: 0.7233, loss: 0.8773 ||: 100%|##########| 65/65 [12:11<00:00, 11.25s/it]
2022-03-30 18:58:06,587 - INFO - allennlp.training.trainer - Validating
2022-03-30 18:58:06,589 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-03-30 18:58:06,589 - INFO - allennlp.data.samplers.bucket_batch_sampler - No sorting keys given; trying to guess a good one
2022-03-30 18:58:06,589 - INFO - allennlp.data.samplers.bucket_batch_sampler - Using ['tokens'] as the sorting keys
2022-03-30 18:58:16,651 - INFO - tqdm - accuracy: 0.5000, batch_loss: 0.6410, loss: 0.6960 ||:  35%|###4      | 40/115 [00:10<00:18,  3.99it/s]
2022-03-30 18:58:26,674 - INFO - tqdm - accuracy: 0.4625, batch_loss: 0.6486, loss: 0.6995 ||:  70%|######9   | 80/115 [00:20<00:08,  4.02it/s]
2022-03-30 18:58:35,420 - INFO - tqdm - accuracy: 0.4716, batch_loss: 0.7945, loss: 0.6986 ||: 100%|##########| 115/115 [00:28<00:00,  3.94it/s]
2022-03-30 18:58:35,420 - INFO - tqdm - accuracy: 0.4716, batch_loss: 0.7945, loss: 0.6986 ||: 100%|##########| 115/115 [00:28<00:00,  3.99it/s]
2022-03-30 18:58:35,420 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-03-30 18:58:35,421 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.512  |     0.472
2022-03-30 18:58:35,422 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  1359.606  |       N/A
2022-03-30 18:58:35,423 - INFO - allennlp.training.tensorboard_writer - loss               |     0.877  |     0.699
2022-03-30 18:58:35,423 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5793.102  |       N/A
2022-03-30 18:58:41,513 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to './output_ir-ora-d/best.th'.
2022-03-30 18:58:42,729 - INFO - allennlp.training.trainer - Epoch duration: 0:12:50.370176
2022-03-30 18:58:42,729 - INFO - allennlp.training.trainer - Estimated training time remaining: 2:59:45
2022-03-30 18:58:42,729 - INFO - allennlp.training.trainer - Epoch 1/14
2022-03-30 18:58:42,730 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-03-30 18:58:42,730 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-03-30 18:58:42,731 - INFO - allennlp.training.trainer - Training
2022-03-30 18:58:42,731 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-03-30 18:58:54,029 - INFO - tqdm - accuracy: 0.3438, batch_loss: 0.7153, loss: 0.7153 ||:   2%|1         | 1/65 [00:11<12:03, 11.30s/it]
2022-03-30 18:59:05,595 - INFO - tqdm - accuracy: 0.4531, batch_loss: 0.6846, loss: 0.7000 ||:   3%|3         | 2/65 [00:22<12:01, 11.46s/it]
2022-03-30 18:59:17,291 - INFO - tqdm - accuracy: 0.4271, batch_loss: 0.7152, loss: 0.7051 ||:   5%|4         | 3/65 [00:34<11:57, 11.57s/it]
2022-03-30 18:59:28,747 - INFO - tqdm - accuracy: 0.4688, batch_loss: 0.6796, loss: 0.6987 ||:   6%|6         | 4/65 [00:46<11:42, 11.52s/it]
2022-03-30 18:59:40,086 - INFO - tqdm - accuracy: 0.4688, batch_loss: 0.6868, loss: 0.6963 ||:   8%|7         | 5/65 [00:57<11:27, 11.46s/it]
2022-03-30 18:59:51,378 - INFO - tqdm - accuracy: 0.5052, batch_loss: 0.6825, loss: 0.6940 ||:   9%|9         | 6/65 [01:08<11:12, 11.40s/it]
2022-03-30 19:00:02,703 - INFO - tqdm - accuracy: 0.5089, batch_loss: 0.6949, loss: 0.6941 ||:  11%|#         | 7/65 [01:19<10:59, 11.38s/it]
2022-03-30 19:00:14,108 - INFO - tqdm - accuracy: 0.5195, batch_loss: 0.6819, loss: 0.6926 ||:  12%|#2        | 8/65 [01:31<10:48, 11.38s/it]
2022-03-30 19:00:25,581 - INFO - tqdm - accuracy: 0.5312, batch_loss: 0.6657, loss: 0.6896 ||:  14%|#3        | 9/65 [01:42<10:39, 11.41s/it]
2022-03-30 19:00:37,061 - INFO - tqdm - accuracy: 0.5281, batch_loss: 0.6689, loss: 0.6875 ||:  15%|#5        | 10/65 [01:54<10:28, 11.43s/it]
2022-03-30 19:00:48,497 - INFO - tqdm - accuracy: 0.5227, batch_loss: 0.7294, loss: 0.6913 ||:  17%|#6        | 11/65 [02:05<10:17, 11.43s/it]
2022-03-30 19:00:59,903 - INFO - tqdm - accuracy: 0.5260, batch_loss: 0.6778, loss: 0.6902 ||:  18%|#8        | 12/65 [02:17<10:05, 11.43s/it]
2022-03-30 19:01:11,271 - INFO - tqdm - accuracy: 0.5337, batch_loss: 0.6708, loss: 0.6887 ||:  20%|##        | 13/65 [02:28<09:53, 11.41s/it]
2022-03-30 19:01:22,514 - INFO - tqdm - accuracy: 0.5312, batch_loss: 0.7205, loss: 0.6910 ||:  22%|##1       | 14/65 [02:39<09:39, 11.36s/it]
2022-03-30 19:01:33,884 - INFO - tqdm - accuracy: 0.5292, batch_loss: 0.6931, loss: 0.6911 ||:  23%|##3       | 15/65 [02:51<09:28, 11.36s/it]
2022-03-30 19:01:45,286 - INFO - tqdm - accuracy: 0.5312, batch_loss: 0.6831, loss: 0.6906 ||:  25%|##4       | 16/65 [03:02<09:17, 11.37s/it]
2022-03-30 19:01:56,655 - INFO - tqdm - accuracy: 0.5312, batch_loss: 0.7200, loss: 0.6924 ||:  26%|##6       | 17/65 [03:13<09:05, 11.37s/it]
2022-03-30 19:02:08,038 - INFO - tqdm - accuracy: 0.5365, batch_loss: 0.6291, loss: 0.6888 ||:  28%|##7       | 18/65 [03:25<08:54, 11.38s/it]
2022-03-30 19:02:19,430 - INFO - tqdm - accuracy: 0.5362, batch_loss: 0.6821, loss: 0.6885 ||:  29%|##9       | 19/65 [03:36<08:43, 11.38s/it]
2022-03-30 19:02:30,824 - INFO - tqdm - accuracy: 0.5422, batch_loss: 0.6668, loss: 0.6874 ||:  31%|###       | 20/65 [03:48<08:32, 11.38s/it]
2022-03-30 19:02:42,222 - INFO - tqdm - accuracy: 0.5446, batch_loss: 0.6473, loss: 0.6855 ||:  32%|###2      | 21/65 [03:59<08:21, 11.39s/it]
2022-03-30 19:02:53,507 - INFO - tqdm - accuracy: 0.5440, batch_loss: 0.6778, loss: 0.6851 ||:  34%|###3      | 22/65 [04:10<08:08, 11.36s/it]
2022-03-30 19:03:04,903 - INFO - tqdm - accuracy: 0.5462, batch_loss: 0.6676, loss: 0.6844 ||:  35%|###5      | 23/65 [04:22<07:57, 11.37s/it]
2022-03-30 19:03:16,314 - INFO - tqdm - accuracy: 0.5417, batch_loss: 0.7016, loss: 0.6851 ||:  37%|###6      | 24/65 [04:33<07:46, 11.38s/it]
2022-03-30 19:03:27,705 - INFO - tqdm - accuracy: 0.5463, batch_loss: 0.6431, loss: 0.6834 ||:  38%|###8      | 25/65 [04:44<07:35, 11.38s/it]
2022-03-30 19:03:39,113 - INFO - tqdm - accuracy: 0.5481, batch_loss: 0.6670, loss: 0.6828 ||:  40%|####      | 26/65 [04:56<07:24, 11.39s/it]
2022-03-30 19:03:50,525 - INFO - tqdm - accuracy: 0.5486, batch_loss: 0.6598, loss: 0.6819 ||:  42%|####1     | 27/65 [05:07<07:13, 11.40s/it]
2022-03-30 19:04:01,941 - INFO - tqdm - accuracy: 0.5513, batch_loss: 0.6623, loss: 0.6812 ||:  43%|####3     | 28/65 [05:19<07:01, 11.40s/it]
2022-03-30 19:04:13,349 - INFO - tqdm - accuracy: 0.5539, batch_loss: 0.6620, loss: 0.6806 ||:  45%|####4     | 29/65 [05:30<06:50, 11.40s/it]
2022-03-30 19:04:24,739 - INFO - tqdm - accuracy: 0.5531, batch_loss: 0.7036, loss: 0.6813 ||:  46%|####6     | 30/65 [05:42<06:39, 11.40s/it]
2022-03-30 19:04:36,154 - INFO - tqdm - accuracy: 0.5554, batch_loss: 0.6624, loss: 0.6807 ||:  48%|####7     | 31/65 [05:53<06:27, 11.40s/it]
2022-03-30 19:04:47,576 - INFO - tqdm - accuracy: 0.5537, batch_loss: 0.6471, loss: 0.6797 ||:  49%|####9     | 32/65 [06:04<06:16, 11.41s/it]
2022-03-30 19:04:58,986 - INFO - tqdm - accuracy: 0.5540, batch_loss: 0.6887, loss: 0.6800 ||:  51%|#####     | 33/65 [06:16<06:05, 11.41s/it]
2022-03-30 19:05:10,401 - INFO - tqdm - accuracy: 0.5570, batch_loss: 0.6588, loss: 0.6793 ||:  52%|#####2    | 34/65 [06:27<05:53, 11.41s/it]
2022-03-30 19:05:22,107 - INFO - tqdm - accuracy: 0.5571, batch_loss: 0.6727, loss: 0.6791 ||:  54%|#####3    | 35/65 [06:39<05:44, 11.50s/it]
2022-03-30 19:05:33,485 - INFO - tqdm - accuracy: 0.5651, batch_loss: 0.6186, loss: 0.6775 ||:  55%|#####5    | 36/65 [06:50<05:32, 11.46s/it]
2022-03-30 19:05:44,871 - INFO - tqdm - accuracy: 0.5676, batch_loss: 0.5817, loss: 0.6749 ||:  57%|#####6    | 37/65 [07:02<05:20, 11.44s/it]
2022-03-30 19:05:56,273 - INFO - tqdm - accuracy: 0.5691, batch_loss: 0.6878, loss: 0.6752 ||:  58%|#####8    | 38/65 [07:13<05:08, 11.43s/it]
2022-03-30 19:06:07,673 - INFO - tqdm - accuracy: 0.5697, batch_loss: 0.6549, loss: 0.6747 ||:  60%|######    | 39/65 [07:24<04:56, 11.42s/it]
2022-03-30 19:06:19,094 - INFO - tqdm - accuracy: 0.5734, batch_loss: 0.6267, loss: 0.6735 ||:  62%|######1   | 40/65 [07:36<04:45, 11.42s/it]
2022-03-30 19:06:30,518 - INFO - tqdm - accuracy: 0.5739, batch_loss: 0.6267, loss: 0.6724 ||:  63%|######3   | 41/65 [07:47<04:34, 11.42s/it]
2022-03-30 19:06:41,899 - INFO - tqdm - accuracy: 0.5781, batch_loss: 0.6345, loss: 0.6715 ||:  65%|######4   | 42/65 [07:59<04:22, 11.41s/it]
2022-03-30 19:06:53,318 - INFO - tqdm - accuracy: 0.5785, batch_loss: 0.6820, loss: 0.6717 ||:  66%|######6   | 43/65 [08:10<04:11, 11.41s/it]
2022-03-30 19:07:04,737 - INFO - tqdm - accuracy: 0.5803, batch_loss: 0.6619, loss: 0.6715 ||:  68%|######7   | 44/65 [08:22<03:59, 11.41s/it]
2022-03-30 19:07:16,159 - INFO - tqdm - accuracy: 0.5813, batch_loss: 0.5842, loss: 0.6695 ||:  69%|######9   | 45/65 [08:33<03:48, 11.42s/it]
2022-03-30 19:07:27,570 - INFO - tqdm - accuracy: 0.5842, batch_loss: 0.6486, loss: 0.6691 ||:  71%|#######   | 46/65 [08:44<03:36, 11.41s/it]
2022-03-30 19:07:38,991 - INFO - tqdm - accuracy: 0.5864, batch_loss: 0.6524, loss: 0.6687 ||:  72%|#######2  | 47/65 [08:56<03:25, 11.42s/it]
2022-03-30 19:07:50,408 - INFO - tqdm - accuracy: 0.5859, batch_loss: 0.6806, loss: 0.6690 ||:  74%|#######3  | 48/65 [09:07<03:14, 11.42s/it]
2022-03-30 19:08:01,816 - INFO - tqdm - accuracy: 0.5874, batch_loss: 0.7497, loss: 0.6706 ||:  75%|#######5  | 49/65 [09:19<03:02, 11.41s/it]
2022-03-30 19:08:13,221 - INFO - tqdm - accuracy: 0.5894, batch_loss: 0.6349, loss: 0.6699 ||:  77%|#######6  | 50/65 [09:30<02:51, 11.41s/it]
2022-03-30 19:08:24,612 - INFO - tqdm - accuracy: 0.5858, batch_loss: 0.8003, loss: 0.6725 ||:  78%|#######8  | 51/65 [09:41<02:39, 11.41s/it]
2022-03-30 19:08:35,996 - INFO - tqdm - accuracy: 0.5841, batch_loss: 0.6903, loss: 0.6728 ||:  80%|########  | 52/65 [09:53<02:28, 11.40s/it]
2022-03-30 19:08:47,383 - INFO - tqdm - accuracy: 0.5855, batch_loss: 0.6139, loss: 0.6717 ||:  82%|########1 | 53/65 [10:04<02:16, 11.40s/it]
2022-03-30 19:08:58,793 - INFO - tqdm - accuracy: 0.5845, batch_loss: 0.7169, loss: 0.6725 ||:  83%|########3 | 54/65 [10:16<02:05, 11.40s/it]
2022-03-30 19:09:10,195 - INFO - tqdm - accuracy: 0.5875, batch_loss: 0.5394, loss: 0.6701 ||:  85%|########4 | 55/65 [10:27<01:54, 11.40s/it]
2022-03-30 19:09:21,598 - INFO - tqdm - accuracy: 0.5893, batch_loss: 0.5789, loss: 0.6685 ||:  86%|########6 | 56/65 [10:38<01:42, 11.40s/it]
2022-03-30 19:09:32,764 - INFO - tqdm - accuracy: 0.5899, batch_loss: 0.6290, loss: 0.6678 ||:  88%|########7 | 57/65 [10:50<01:30, 11.33s/it]
2022-03-30 19:09:44,175 - INFO - tqdm - accuracy: 0.5889, batch_loss: 0.7422, loss: 0.6691 ||:  89%|########9 | 58/65 [11:01<01:19, 11.35s/it]
2022-03-30 19:09:55,585 - INFO - tqdm - accuracy: 0.5906, batch_loss: 0.6120, loss: 0.6681 ||:  91%|######### | 59/65 [11:12<01:08, 11.37s/it]
2022-03-30 19:10:06,989 - INFO - tqdm - accuracy: 0.5896, batch_loss: 0.7587, loss: 0.6696 ||:  92%|#########2| 60/65 [11:24<00:56, 11.38s/it]
2022-03-30 19:10:18,389 - INFO - tqdm - accuracy: 0.5922, batch_loss: 0.5663, loss: 0.6679 ||:  94%|#########3| 61/65 [11:35<00:45, 11.39s/it]
2022-03-30 19:10:29,789 - INFO - tqdm - accuracy: 0.5938, batch_loss: 0.5355, loss: 0.6658 ||:  95%|#########5| 62/65 [11:47<00:34, 11.39s/it]
2022-03-30 19:10:41,204 - INFO - tqdm - accuracy: 0.5938, batch_loss: 0.6330, loss: 0.6653 ||:  97%|#########6| 63/65 [11:58<00:22, 11.40s/it]
2022-03-30 19:10:52,259 - INFO - tqdm - accuracy: 0.5945, batch_loss: 0.6993, loss: 0.6658 ||:  98%|#########8| 64/65 [12:09<00:11, 11.30s/it]
2022-03-30 19:10:57,316 - INFO - tqdm - accuracy: 0.5949, batch_loss: 0.6243, loss: 0.6652 ||: 100%|##########| 65/65 [12:14<00:00,  9.42s/it]
2022-03-30 19:10:57,316 - INFO - tqdm - accuracy: 0.5949, batch_loss: 0.6243, loss: 0.6652 ||: 100%|##########| 65/65 [12:14<00:00, 11.30s/it]
2022-03-30 19:11:00,435 - INFO - allennlp.training.trainer - Validating
2022-03-30 19:11:00,436 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-03-30 19:11:10,598 - INFO - tqdm - accuracy: 0.6707, batch_loss: 0.5704, loss: 0.6414 ||:  36%|###5      | 41/115 [00:10<00:18,  3.99it/s]
2022-03-30 19:11:20,666 - INFO - tqdm - accuracy: 0.6605, batch_loss: 0.4262, loss: 0.6448 ||:  70%|#######   | 81/115 [00:20<00:08,  3.97it/s]
2022-03-30 19:11:29,156 - INFO - tqdm - accuracy: 0.6507, batch_loss: 0.7293, loss: 0.6622 ||: 100%|##########| 115/115 [00:28<00:00,  3.93it/s]
2022-03-30 19:11:29,156 - INFO - tqdm - accuracy: 0.6507, batch_loss: 0.7293, loss: 0.6622 ||: 100%|##########| 115/115 [00:28<00:00,  4.00it/s]
2022-03-30 19:11:29,156 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-03-30 19:11:29,157 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.595  |     0.651
2022-03-30 19:11:29,157 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-03-30 19:11:29,157 - INFO - allennlp.training.tensorboard_writer - loss               |     0.665  |     0.662
2022-03-30 19:11:29,157 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5793.102  |       N/A
2022-03-30 19:11:35,173 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to './output_ir-ora-d/best.th'.
2022-03-30 19:11:56,108 - INFO - allennlp.training.trainer - Epoch duration: 0:13:13.378496
2022-03-30 19:11:56,108 - INFO - allennlp.training.trainer - Estimated training time remaining: 2:49:24
2022-03-30 19:11:56,108 - INFO - allennlp.training.trainer - Epoch 2/14
2022-03-30 19:11:56,109 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-03-30 19:11:56,109 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-03-30 19:11:56,110 - INFO - allennlp.training.trainer - Training
2022-03-30 19:11:56,110 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-03-30 19:12:07,303 - INFO - tqdm - accuracy: 0.5938, batch_loss: 0.6379, loss: 0.6379 ||:   2%|1         | 1/65 [00:11<11:56, 11.19s/it]
2022-03-30 19:12:18,850 - INFO - tqdm - accuracy: 0.7188, batch_loss: 0.5051, loss: 0.5715 ||:   3%|3         | 2/65 [00:22<11:58, 11.40s/it]
2022-03-30 19:12:30,698 - INFO - tqdm - accuracy: 0.7292, batch_loss: 0.5547, loss: 0.5659 ||:   5%|4         | 3/65 [00:34<11:59, 11.61s/it]
2022-03-30 19:12:42,334 - INFO - tqdm - accuracy: 0.7344, batch_loss: 0.5077, loss: 0.5514 ||:   6%|6         | 4/65 [00:46<11:48, 11.62s/it]
2022-03-30 19:12:53,727 - INFO - tqdm - accuracy: 0.7250, batch_loss: 0.5124, loss: 0.5436 ||:   8%|7         | 5/65 [00:57<11:32, 11.54s/it]
2022-03-30 19:13:04,860 - INFO - tqdm - accuracy: 0.7448, batch_loss: 0.4889, loss: 0.5345 ||:   9%|9         | 6/65 [01:08<11:12, 11.40s/it]
2022-03-30 19:13:16,146 - INFO - tqdm - accuracy: 0.7455, batch_loss: 0.5576, loss: 0.5378 ||:  11%|#         | 7/65 [01:20<10:59, 11.36s/it]
2022-03-30 19:13:27,511 - INFO - tqdm - accuracy: 0.7383, batch_loss: 0.6106, loss: 0.5469 ||:  12%|#2        | 8/65 [01:31<10:47, 11.36s/it]
2022-03-30 19:13:38,987 - INFO - tqdm - accuracy: 0.7465, batch_loss: 0.4659, loss: 0.5379 ||:  14%|#3        | 9/65 [01:42<10:38, 11.40s/it]
2022-03-30 19:13:50,479 - INFO - tqdm - accuracy: 0.7438, batch_loss: 0.5337, loss: 0.5375 ||:  15%|#5        | 10/65 [01:54<10:28, 11.43s/it]
2022-03-30 19:14:01,933 - INFO - tqdm - accuracy: 0.7330, batch_loss: 0.5891, loss: 0.5422 ||:  17%|#6        | 11/65 [02:05<10:17, 11.44s/it]
2022-03-30 19:14:13,350 - INFO - tqdm - accuracy: 0.7318, batch_loss: 0.5224, loss: 0.5405 ||:  18%|#8        | 12/65 [02:17<10:05, 11.43s/it]
2022-03-30 19:14:24,380 - INFO - tqdm - accuracy: 0.7108, batch_loss: 0.7724, loss: 0.5583 ||:  20%|##        | 13/65 [02:28<09:48, 11.31s/it]
2022-03-30 19:14:35,762 - INFO - tqdm - accuracy: 0.7136, batch_loss: 0.5605, loss: 0.5585 ||:  22%|##1       | 14/65 [02:39<09:37, 11.33s/it]
2022-03-30 19:14:47,155 - INFO - tqdm - accuracy: 0.7077, batch_loss: 0.6703, loss: 0.5660 ||:  23%|##3       | 15/65 [02:51<09:27, 11.35s/it]
2022-03-30 19:14:58,592 - INFO - tqdm - accuracy: 0.7123, batch_loss: 0.4798, loss: 0.5606 ||:  25%|##4       | 16/65 [03:02<09:17, 11.38s/it]
2022-03-30 19:15:10,023 - INFO - tqdm - accuracy: 0.7053, batch_loss: 0.6373, loss: 0.5651 ||:  26%|##6       | 17/65 [03:13<09:06, 11.39s/it]
2022-03-30 19:15:21,482 - INFO - tqdm - accuracy: 0.6974, batch_loss: 0.8324, loss: 0.5799 ||:  28%|##7       | 18/65 [03:25<08:56, 11.41s/it]
2022-03-30 19:15:32,928 - INFO - tqdm - accuracy: 0.6985, batch_loss: 0.5892, loss: 0.5804 ||:  29%|##9       | 19/65 [03:36<08:45, 11.42s/it]
2022-03-30 19:15:44,384 - INFO - tqdm - accuracy: 0.6980, batch_loss: 0.5497, loss: 0.5789 ||:  31%|###       | 20/65 [03:48<08:34, 11.43s/it]
2022-03-30 19:15:55,853 - INFO - tqdm - accuracy: 0.6915, batch_loss: 0.6289, loss: 0.5813 ||:  32%|###2      | 21/65 [03:59<08:23, 11.44s/it]
2022-03-30 19:16:07,300 - INFO - tqdm - accuracy: 0.6913, batch_loss: 0.5566, loss: 0.5801 ||:  34%|###3      | 22/65 [04:11<08:12, 11.44s/it]
2022-03-30 19:16:18,743 - INFO - tqdm - accuracy: 0.6925, batch_loss: 0.5251, loss: 0.5778 ||:  35%|###5      | 23/65 [04:22<08:00, 11.44s/it]
2022-03-30 19:16:30,075 - INFO - tqdm - accuracy: 0.6936, batch_loss: 0.5825, loss: 0.5780 ||:  37%|###6      | 24/65 [04:33<07:47, 11.41s/it]
2022-03-30 19:16:41,509 - INFO - tqdm - accuracy: 0.6946, batch_loss: 0.5261, loss: 0.5759 ||:  38%|###8      | 25/65 [04:45<07:36, 11.42s/it]
2022-03-30 19:16:52,953 - INFO - tqdm - accuracy: 0.6980, batch_loss: 0.4886, loss: 0.5725 ||:  40%|####      | 26/65 [04:56<07:25, 11.43s/it]
2022-03-30 19:17:04,394 - INFO - tqdm - accuracy: 0.7022, batch_loss: 0.4461, loss: 0.5678 ||:  42%|####1     | 27/65 [05:08<07:14, 11.43s/it]
2022-03-30 19:17:15,837 - INFO - tqdm - accuracy: 0.7061, batch_loss: 0.4535, loss: 0.5638 ||:  43%|####3     | 28/65 [05:19<07:03, 11.43s/it]
2022-03-30 19:17:27,254 - INFO - tqdm - accuracy: 0.7066, batch_loss: 0.5798, loss: 0.5643 ||:  45%|####4     | 29/65 [05:31<06:51, 11.43s/it]
2022-03-30 19:17:38,675 - INFO - tqdm - accuracy: 0.7070, batch_loss: 0.4858, loss: 0.5617 ||:  46%|####6     | 30/65 [05:42<06:39, 11.43s/it]
2022-03-30 19:17:50,099 - INFO - tqdm - accuracy: 0.7043, batch_loss: 0.5388, loss: 0.5610 ||:  48%|####7     | 31/65 [05:53<06:28, 11.43s/it]
2022-03-30 19:18:01,509 - INFO - tqdm - accuracy: 0.7067, batch_loss: 0.5253, loss: 0.5598 ||:  49%|####9     | 32/65 [06:05<06:16, 11.42s/it]
2022-03-30 19:18:12,917 - INFO - tqdm - accuracy: 0.7043, batch_loss: 0.6422, loss: 0.5623 ||:  51%|#####     | 33/65 [06:16<06:05, 11.42s/it]
2022-03-30 19:18:24,343 - INFO - tqdm - accuracy: 0.7029, batch_loss: 0.6385, loss: 0.5646 ||:  52%|#####2    | 34/65 [06:28<05:54, 11.42s/it]
2022-03-30 19:18:35,763 - INFO - tqdm - accuracy: 0.7033, batch_loss: 0.4540, loss: 0.5614 ||:  54%|#####3    | 35/65 [06:39<05:42, 11.42s/it]
2022-03-30 19:18:47,175 - INFO - tqdm - accuracy: 0.7046, batch_loss: 0.5268, loss: 0.5605 ||:  55%|#####5    | 36/65 [06:51<05:31, 11.42s/it]
2022-03-30 19:18:58,589 - INFO - tqdm - accuracy: 0.7033, batch_loss: 0.5686, loss: 0.5607 ||:  57%|#####6    | 37/65 [07:02<05:19, 11.42s/it]
2022-03-30 19:19:09,998 - INFO - tqdm - accuracy: 0.7062, batch_loss: 0.3615, loss: 0.5554 ||:  58%|#####8    | 38/65 [07:13<05:08, 11.41s/it]
2022-03-30 19:19:21,419 - INFO - tqdm - accuracy: 0.7065, batch_loss: 0.5252, loss: 0.5547 ||:  60%|######    | 39/65 [07:25<04:56, 11.42s/it]
2022-03-30 19:19:32,828 - INFO - tqdm - accuracy: 0.7060, batch_loss: 0.6123, loss: 0.5561 ||:  62%|######1   | 40/65 [07:36<04:45, 11.41s/it]
2022-03-30 19:19:44,247 - INFO - tqdm - accuracy: 0.7071, batch_loss: 0.4322, loss: 0.5531 ||:  63%|######3   | 41/65 [07:48<04:33, 11.42s/it]
2022-03-30 19:19:55,541 - INFO - tqdm - accuracy: 0.7103, batch_loss: 0.4285, loss: 0.5501 ||:  65%|######4   | 42/65 [07:59<04:21, 11.38s/it]
2022-03-30 19:20:06,958 - INFO - tqdm - accuracy: 0.7098, batch_loss: 0.6191, loss: 0.5517 ||:  66%|######6   | 43/65 [08:10<04:10, 11.39s/it]
2022-03-30 19:20:18,367 - INFO - tqdm - accuracy: 0.7100, batch_loss: 0.4822, loss: 0.5501 ||:  68%|######7   | 44/65 [08:22<03:59, 11.40s/it]
2022-03-30 19:20:29,782 - INFO - tqdm - accuracy: 0.7116, batch_loss: 0.5395, loss: 0.5499 ||:  69%|######9   | 45/65 [08:33<03:48, 11.40s/it]
2022-03-30 19:20:41,190 - INFO - tqdm - accuracy: 0.7131, batch_loss: 0.4323, loss: 0.5473 ||:  71%|#######   | 46/65 [08:45<03:36, 11.40s/it]
2022-03-30 19:20:52,612 - INFO - tqdm - accuracy: 0.7139, batch_loss: 0.5924, loss: 0.5483 ||:  72%|#######2  | 47/65 [08:56<03:25, 11.41s/it]
2022-03-30 19:21:04,023 - INFO - tqdm - accuracy: 0.7134, batch_loss: 0.6904, loss: 0.5513 ||:  74%|#######3  | 48/65 [09:07<03:13, 11.41s/it]
2022-03-30 19:21:15,429 - INFO - tqdm - accuracy: 0.7128, batch_loss: 0.5316, loss: 0.5509 ||:  75%|#######5  | 49/65 [09:19<03:02, 11.41s/it]
2022-03-30 19:21:26,835 - INFO - tqdm - accuracy: 0.7123, batch_loss: 0.6069, loss: 0.5520 ||:  77%|#######6  | 50/65 [09:30<02:51, 11.41s/it]
2022-03-30 19:21:38,234 - INFO - tqdm - accuracy: 0.7100, batch_loss: 0.7046, loss: 0.5550 ||:  78%|#######8  | 51/65 [09:42<02:39, 11.41s/it]
2022-03-30 19:21:49,652 - INFO - tqdm - accuracy: 0.7090, batch_loss: 0.5001, loss: 0.5539 ||:  80%|########  | 52/65 [09:53<02:28, 11.41s/it]
2022-03-30 19:22:01,062 - INFO - tqdm - accuracy: 0.7115, batch_loss: 0.4725, loss: 0.5524 ||:  82%|########1 | 53/65 [10:04<02:16, 11.41s/it]
2022-03-30 19:22:12,485 - INFO - tqdm - accuracy: 0.7122, batch_loss: 0.5256, loss: 0.5519 ||:  83%|########3 | 54/65 [10:16<02:05, 11.41s/it]
2022-03-30 19:22:23,876 - INFO - tqdm - accuracy: 0.7123, batch_loss: 0.6559, loss: 0.5538 ||:  85%|########4 | 55/65 [10:27<01:54, 11.41s/it]
2022-03-30 19:22:35,265 - INFO - tqdm - accuracy: 0.7125, batch_loss: 0.5692, loss: 0.5541 ||:  86%|########6 | 56/65 [10:39<01:42, 11.40s/it]
2022-03-30 19:22:46,687 - INFO - tqdm - accuracy: 0.7104, batch_loss: 0.7267, loss: 0.5571 ||:  88%|########7 | 57/65 [10:50<01:31, 11.41s/it]
2022-03-30 19:22:57,836 - INFO - tqdm - accuracy: 0.7094, batch_loss: 0.5107, loss: 0.5563 ||:  89%|########9 | 58/65 [11:01<01:19, 11.33s/it]
2022-03-30 19:23:09,244 - INFO - tqdm - accuracy: 0.7117, batch_loss: 0.4356, loss: 0.5542 ||:  91%|######### | 59/65 [11:13<01:08, 11.35s/it]
2022-03-30 19:23:20,645 - INFO - tqdm - accuracy: 0.7097, batch_loss: 0.5992, loss: 0.5550 ||:  92%|#########2| 60/65 [11:24<00:56, 11.37s/it]
2022-03-30 19:23:32,052 - INFO - tqdm - accuracy: 0.7104, batch_loss: 0.4619, loss: 0.5535 ||:  94%|#########3| 61/65 [11:35<00:45, 11.38s/it]
2022-03-30 19:23:43,458 - INFO - tqdm - accuracy: 0.7080, batch_loss: 0.6688, loss: 0.5553 ||:  95%|#########5| 62/65 [11:47<00:34, 11.39s/it]
2022-03-30 19:23:54,863 - INFO - tqdm - accuracy: 0.7072, batch_loss: 0.5409, loss: 0.5551 ||:  97%|#########6| 63/65 [11:58<00:22, 11.39s/it]
2022-03-30 19:24:06,288 - INFO - tqdm - accuracy: 0.7049, batch_loss: 0.6793, loss: 0.5570 ||:  98%|#########8| 64/65 [12:10<00:11, 11.40s/it]
2022-03-30 19:24:11,362 - INFO - tqdm - accuracy: 0.7045, batch_loss: 0.6466, loss: 0.5584 ||: 100%|##########| 65/65 [12:15<00:00,  9.50s/it]
2022-03-30 19:24:11,362 - INFO - tqdm - accuracy: 0.7045, batch_loss: 0.6466, loss: 0.5584 ||: 100%|##########| 65/65 [12:15<00:00, 11.31s/it]
2022-03-30 19:24:14,474 - INFO - allennlp.training.trainer - Validating
2022-03-30 19:24:14,475 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-03-30 19:24:24,497 - INFO - tqdm - accuracy: 0.5875, batch_loss: 0.8752, loss: 0.7658 ||:  35%|###4      | 40/115 [00:10<00:18,  3.99it/s]
2022-03-30 19:24:34,626 - INFO - tqdm - accuracy: 0.6273, batch_loss: 0.2952, loss: 0.6786 ||:  70%|#######   | 81/115 [00:20<00:08,  4.03it/s]
2022-03-30 19:24:43,255 - INFO - tqdm - accuracy: 0.6419, batch_loss: 1.3150, loss: 0.6938 ||: 100%|##########| 115/115 [00:28<00:00,  3.93it/s]
2022-03-30 19:24:43,256 - INFO - tqdm - accuracy: 0.6419, batch_loss: 1.3150, loss: 0.6938 ||: 100%|##########| 115/115 [00:28<00:00,  4.00it/s]
2022-03-30 19:24:43,256 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-03-30 19:24:43,257 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.705  |     0.642
2022-03-30 19:24:43,257 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-03-30 19:24:43,257 - INFO - allennlp.training.tensorboard_writer - loss               |     0.558  |     0.694
2022-03-30 19:24:43,258 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5793.102  |       N/A
2022-03-30 19:24:49,890 - INFO - allennlp.training.trainer - Epoch duration: 0:12:53.781533
2022-03-30 19:24:49,890 - INFO - allennlp.training.trainer - Estimated training time remaining: 2:35:50
2022-03-30 19:24:49,890 - INFO - allennlp.training.trainer - Epoch 3/14
2022-03-30 19:24:49,890 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-03-30 19:24:49,891 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-03-30 19:24:49,892 - INFO - allennlp.training.trainer - Training
2022-03-30 19:24:49,892 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-03-30 19:25:01,238 - INFO - tqdm - accuracy: 0.8438, batch_loss: 0.4512, loss: 0.4512 ||:   2%|1         | 1/65 [00:11<12:06, 11.34s/it]
2022-03-30 19:25:12,825 - INFO - tqdm - accuracy: 0.7812, batch_loss: 0.5821, loss: 0.5166 ||:   3%|3         | 2/65 [00:22<12:03, 11.49s/it]
2022-03-30 19:25:24,520 - INFO - tqdm - accuracy: 0.8125, batch_loss: 0.4473, loss: 0.4935 ||:   5%|4         | 3/65 [00:34<11:58, 11.58s/it]
2022-03-30 19:25:35,979 - INFO - tqdm - accuracy: 0.8359, batch_loss: 0.3907, loss: 0.4678 ||:   6%|6         | 4/65 [00:46<11:43, 11.53s/it]
2022-03-30 19:25:47,758 - INFO - tqdm - accuracy: 0.8438, batch_loss: 0.3937, loss: 0.4530 ||:   8%|7         | 5/65 [00:57<11:37, 11.62s/it]
2022-03-30 19:25:59,027 - INFO - tqdm - accuracy: 0.8490, batch_loss: 0.3779, loss: 0.4405 ||:   9%|9         | 6/65 [01:09<11:18, 11.50s/it]
2022-03-30 19:26:10,365 - INFO - tqdm - accuracy: 0.8571, batch_loss: 0.3737, loss: 0.4309 ||:  11%|#         | 7/65 [01:20<11:04, 11.45s/it]
2022-03-30 19:26:21,677 - INFO - tqdm - accuracy: 0.8672, batch_loss: 0.2699, loss: 0.4108 ||:  12%|#2        | 8/65 [01:31<10:50, 11.40s/it]
2022-03-30 19:26:33,153 - INFO - tqdm - accuracy: 0.8576, batch_loss: 0.3872, loss: 0.4082 ||:  14%|#3        | 9/65 [01:43<10:39, 11.43s/it]
2022-03-30 19:26:44,609 - INFO - tqdm - accuracy: 0.8625, batch_loss: 0.2711, loss: 0.3945 ||:  15%|#5        | 10/65 [01:54<10:28, 11.44s/it]
2022-03-30 19:26:55,919 - INFO - tqdm - accuracy: 0.8636, batch_loss: 0.3579, loss: 0.3912 ||:  17%|#6        | 11/65 [02:06<10:15, 11.40s/it]
2022-03-30 19:27:07,319 - INFO - tqdm - accuracy: 0.8698, batch_loss: 0.2380, loss: 0.3784 ||:  18%|#8        | 12/65 [02:17<10:04, 11.40s/it]
2022-03-30 19:27:18,696 - INFO - tqdm - accuracy: 0.8750, batch_loss: 0.2195, loss: 0.3662 ||:  20%|##        | 13/65 [02:28<09:52, 11.39s/it]
2022-03-30 19:27:30,104 - INFO - tqdm - accuracy: 0.8705, batch_loss: 0.4140, loss: 0.3696 ||:  22%|##1       | 14/65 [02:40<09:41, 11.40s/it]
2022-03-30 19:27:41,546 - INFO - tqdm - accuracy: 0.8646, batch_loss: 0.4508, loss: 0.3750 ||:  23%|##3       | 15/65 [02:51<09:30, 11.41s/it]
2022-03-30 19:27:52,998 - INFO - tqdm - accuracy: 0.8633, batch_loss: 0.3730, loss: 0.3749 ||:  25%|##4       | 16/65 [03:03<09:19, 11.42s/it]
2022-03-30 19:28:04,444 - INFO - tqdm - accuracy: 0.8566, batch_loss: 0.5020, loss: 0.3823 ||:  26%|##6       | 17/65 [03:14<09:08, 11.43s/it]
2022-03-30 19:28:15,894 - INFO - tqdm - accuracy: 0.8628, batch_loss: 0.1806, loss: 0.3711 ||:  28%|##7       | 18/65 [03:26<08:57, 11.44s/it]
2022-03-30 19:28:27,363 - INFO - tqdm - accuracy: 0.8586, batch_loss: 0.4798, loss: 0.3769 ||:  29%|##9       | 19/65 [03:37<08:46, 11.45s/it]
2022-03-30 19:28:38,813 - INFO - tqdm - accuracy: 0.8500, batch_loss: 0.6432, loss: 0.3902 ||:  31%|###       | 20/65 [03:48<08:35, 11.45s/it]
2022-03-30 19:28:50,252 - INFO - tqdm - accuracy: 0.8542, batch_loss: 0.2623, loss: 0.3841 ||:  32%|###2      | 21/65 [04:00<08:23, 11.44s/it]
2022-03-30 19:29:01,705 - INFO - tqdm - accuracy: 0.8551, batch_loss: 0.3264, loss: 0.3815 ||:  34%|###3      | 22/65 [04:11<08:12, 11.45s/it]
2022-03-30 19:29:13,152 - INFO - tqdm - accuracy: 0.8560, batch_loss: 0.2422, loss: 0.3754 ||:  35%|###5      | 23/65 [04:23<08:00, 11.45s/it]
2022-03-30 19:29:24,602 - INFO - tqdm - accuracy: 0.8542, batch_loss: 0.3173, loss: 0.3730 ||:  37%|###6      | 24/65 [04:34<07:49, 11.45s/it]
2022-03-30 19:29:36,058 - INFO - tqdm - accuracy: 0.8562, batch_loss: 0.2609, loss: 0.3685 ||:  38%|###8      | 25/65 [04:46<07:38, 11.45s/it]
2022-03-30 19:29:47,510 - INFO - tqdm - accuracy: 0.8594, batch_loss: 0.2036, loss: 0.3622 ||:  40%|####      | 26/65 [04:57<07:26, 11.45s/it]
2022-03-30 19:29:58,952 - INFO - tqdm - accuracy: 0.8600, batch_loss: 0.3015, loss: 0.3599 ||:  42%|####1     | 27/65 [05:09<07:15, 11.45s/it]
2022-03-30 19:30:10,400 - INFO - tqdm - accuracy: 0.8605, batch_loss: 0.2875, loss: 0.3573 ||:  43%|####3     | 28/65 [05:20<07:03, 11.45s/it]
2022-03-30 19:30:21,849 - INFO - tqdm - accuracy: 0.8610, batch_loss: 0.2243, loss: 0.3527 ||:  45%|####4     | 29/65 [05:31<06:52, 11.45s/it]
2022-03-30 19:30:33,290 - INFO - tqdm - accuracy: 0.8604, batch_loss: 0.2658, loss: 0.3498 ||:  46%|####6     | 30/65 [05:43<06:40, 11.45s/it]
2022-03-30 19:30:44,733 - INFO - tqdm - accuracy: 0.8619, batch_loss: 0.4225, loss: 0.3522 ||:  48%|####7     | 31/65 [05:54<06:29, 11.45s/it]
2022-03-30 19:30:56,033 - INFO - tqdm - accuracy: 0.8623, batch_loss: 0.3362, loss: 0.3517 ||:  49%|####9     | 32/65 [06:06<06:16, 11.40s/it]
2022-03-30 19:31:07,478 - INFO - tqdm - accuracy: 0.8617, batch_loss: 0.3333, loss: 0.3511 ||:  51%|#####     | 33/65 [06:17<06:05, 11.41s/it]
2022-03-30 19:31:18,917 - INFO - tqdm - accuracy: 0.8612, batch_loss: 0.3763, loss: 0.3519 ||:  52%|#####2    | 34/65 [06:29<05:54, 11.42s/it]
2022-03-30 19:31:30,350 - INFO - tqdm - accuracy: 0.8598, batch_loss: 0.2999, loss: 0.3504 ||:  54%|#####3    | 35/65 [06:40<05:42, 11.43s/it]
2022-03-30 19:31:41,782 - INFO - tqdm - accuracy: 0.8585, batch_loss: 0.4619, loss: 0.3535 ||:  55%|#####5    | 36/65 [06:51<05:31, 11.43s/it]
2022-03-30 19:31:53,202 - INFO - tqdm - accuracy: 0.8590, batch_loss: 0.2344, loss: 0.3503 ||:  57%|#####6    | 37/65 [07:03<05:19, 11.42s/it]
2022-03-30 19:32:04,621 - INFO - tqdm - accuracy: 0.8577, batch_loss: 0.3620, loss: 0.3506 ||:  58%|#####8    | 38/65 [07:14<05:08, 11.42s/it]
2022-03-30 19:32:16,029 - INFO - tqdm - accuracy: 0.8566, batch_loss: 0.4563, loss: 0.3533 ||:  60%|######    | 39/65 [07:26<04:56, 11.42s/it]
2022-03-30 19:32:27,445 - INFO - tqdm - accuracy: 0.8562, batch_loss: 0.3545, loss: 0.3533 ||:  62%|######1   | 40/65 [07:37<04:45, 11.42s/it]
2022-03-30 19:32:38,860 - INFO - tqdm - accuracy: 0.8559, batch_loss: 0.2573, loss: 0.3510 ||:  63%|######3   | 41/65 [07:48<04:34, 11.42s/it]
2022-03-30 19:32:50,276 - INFO - tqdm - accuracy: 0.8564, batch_loss: 0.3010, loss: 0.3498 ||:  65%|######4   | 42/65 [08:00<04:22, 11.42s/it]
2022-03-30 19:33:01,676 - INFO - tqdm - accuracy: 0.8583, batch_loss: 0.2872, loss: 0.3483 ||:  66%|######6   | 43/65 [08:11<04:11, 11.41s/it]
2022-03-30 19:33:13,068 - INFO - tqdm - accuracy: 0.8565, batch_loss: 0.5007, loss: 0.3518 ||:  68%|######7   | 44/65 [08:23<03:59, 11.41s/it]
2022-03-30 19:33:24,485 - INFO - tqdm - accuracy: 0.8556, batch_loss: 0.4108, loss: 0.3531 ||:  69%|######9   | 45/65 [08:34<03:48, 11.41s/it]
2022-03-30 19:33:35,907 - INFO - tqdm - accuracy: 0.8539, batch_loss: 0.3413, loss: 0.3528 ||:  71%|#######   | 46/65 [08:46<03:36, 11.41s/it]
2022-03-30 19:33:47,338 - INFO - tqdm - accuracy: 0.8544, batch_loss: 0.3495, loss: 0.3528 ||:  72%|#######2  | 47/65 [08:57<03:25, 11.42s/it]
2022-03-30 19:33:58,757 - INFO - tqdm - accuracy: 0.8574, batch_loss: 0.1289, loss: 0.3481 ||:  74%|#######3  | 48/65 [09:08<03:14, 11.42s/it]
2022-03-30 19:34:10,167 - INFO - tqdm - accuracy: 0.8571, batch_loss: 0.3632, loss: 0.3484 ||:  75%|#######5  | 49/65 [09:20<03:02, 11.42s/it]
2022-03-30 19:34:21,583 - INFO - tqdm - accuracy: 0.8569, batch_loss: 0.4577, loss: 0.3506 ||:  77%|#######6  | 50/65 [09:31<02:51, 11.42s/it]
2022-03-30 19:34:33,009 - INFO - tqdm - accuracy: 0.8542, batch_loss: 0.5203, loss: 0.3539 ||:  78%|#######8  | 51/65 [09:43<02:39, 11.42s/it]
2022-03-30 19:34:44,173 - INFO - tqdm - accuracy: 0.8528, batch_loss: 0.4454, loss: 0.3557 ||:  80%|########  | 52/65 [09:54<02:27, 11.34s/it]
2022-03-30 19:34:55,558 - INFO - tqdm - accuracy: 0.8526, batch_loss: 0.3772, loss: 0.3561 ||:  82%|########1 | 53/65 [10:05<02:16, 11.36s/it]
2022-03-30 19:35:06,957 - INFO - tqdm - accuracy: 0.8507, batch_loss: 0.4230, loss: 0.3573 ||:  83%|########3 | 54/65 [10:17<02:05, 11.37s/it]
2022-03-30 19:35:18,368 - INFO - tqdm - accuracy: 0.8494, batch_loss: 0.3837, loss: 0.3578 ||:  85%|########4 | 55/65 [10:28<01:53, 11.38s/it]
2022-03-30 19:35:29,773 - INFO - tqdm - accuracy: 0.8499, batch_loss: 0.3800, loss: 0.3582 ||:  86%|########6 | 56/65 [10:39<01:42, 11.39s/it]
2022-03-30 19:35:41,166 - INFO - tqdm - accuracy: 0.8492, batch_loss: 0.3842, loss: 0.3587 ||:  88%|########7 | 57/65 [10:51<01:31, 11.39s/it]
2022-03-30 19:35:52,573 - INFO - tqdm - accuracy: 0.8508, batch_loss: 0.2319, loss: 0.3565 ||:  89%|########9 | 58/65 [11:02<01:19, 11.40s/it]
2022-03-30 19:36:03,630 - INFO - tqdm - accuracy: 0.8495, batch_loss: 0.4085, loss: 0.3574 ||:  91%|######### | 59/65 [11:13<01:07, 11.29s/it]
2022-03-30 19:36:15,031 - INFO - tqdm - accuracy: 0.8499, batch_loss: 0.2921, loss: 0.3563 ||:  92%|#########2| 60/65 [11:25<00:56, 11.33s/it]
2022-03-30 19:36:26,421 - INFO - tqdm - accuracy: 0.8503, batch_loss: 0.3057, loss: 0.3554 ||:  94%|#########3| 61/65 [11:36<00:45, 11.34s/it]
2022-03-30 19:36:37,832 - INFO - tqdm - accuracy: 0.8502, batch_loss: 0.3504, loss: 0.3554 ||:  95%|#########5| 62/65 [11:47<00:34, 11.36s/it]
2022-03-30 19:36:49,241 - INFO - tqdm - accuracy: 0.8496, batch_loss: 0.3492, loss: 0.3553 ||:  97%|#########6| 63/65 [11:59<00:22, 11.38s/it]
2022-03-30 19:37:00,645 - INFO - tqdm - accuracy: 0.8500, batch_loss: 0.2951, loss: 0.3543 ||:  98%|#########8| 64/65 [12:10<00:11, 11.39s/it]
2022-03-30 19:37:05,710 - INFO - tqdm - accuracy: 0.8496, batch_loss: 0.3633, loss: 0.3545 ||: 100%|##########| 65/65 [12:15<00:00,  9.49s/it]
2022-03-30 19:37:05,710 - INFO - tqdm - accuracy: 0.8496, batch_loss: 0.3633, loss: 0.3545 ||: 100%|##########| 65/65 [12:15<00:00, 11.32s/it]
2022-03-30 19:37:08,751 - INFO - allennlp.training.trainer - Validating
2022-03-30 19:37:08,753 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-03-30 19:37:18,995 - INFO - tqdm - accuracy: 0.6585, batch_loss: 1.4763, loss: 1.1410 ||:  36%|###5      | 41/115 [00:10<00:18,  3.98it/s]
2022-03-30 19:37:29,229 - INFO - tqdm - accuracy: 0.6829, batch_loss: 0.4508, loss: 0.9496 ||:  71%|#######1  | 82/115 [00:20<00:08,  3.97it/s]
2022-03-30 19:37:37,483 - INFO - tqdm - accuracy: 0.6681, batch_loss: 0.0264, loss: 0.9164 ||: 100%|##########| 115/115 [00:28<00:00,  4.00it/s]
2022-03-30 19:37:37,483 - INFO - tqdm - accuracy: 0.6681, batch_loss: 0.0264, loss: 0.9164 ||: 100%|##########| 115/115 [00:28<00:00,  4.00it/s]
2022-03-30 19:37:37,484 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-03-30 19:37:37,484 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.850  |     0.668
2022-03-30 19:37:37,485 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-03-30 19:37:37,486 - INFO - allennlp.training.tensorboard_writer - loss               |     0.354  |     0.916
2022-03-30 19:37:37,486 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5793.102  |       N/A
2022-03-30 19:37:43,542 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to './output_ir-ora-d/best.th'.
2022-03-30 19:37:57,769 - INFO - allennlp.training.trainer - Epoch duration: 0:13:07.878740
2022-03-30 19:37:57,769 - INFO - allennlp.training.trainer - Estimated training time remaining: 2:23:14
2022-03-30 19:37:57,770 - INFO - allennlp.training.trainer - Epoch 4/14
2022-03-30 19:37:57,770 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-03-30 19:37:57,770 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-03-30 19:37:57,771 - INFO - allennlp.training.trainer - Training
2022-03-30 19:37:57,772 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-03-30 19:38:08,962 - INFO - tqdm - accuracy: 0.9062, batch_loss: 0.2183, loss: 0.2183 ||:   2%|1         | 1/65 [00:11<11:56, 11.19s/it]
2022-03-30 19:38:20,520 - INFO - tqdm - accuracy: 0.9375, batch_loss: 0.1331, loss: 0.1757 ||:   3%|3         | 2/65 [00:22<11:58, 11.41s/it]
2022-03-30 19:38:32,392 - INFO - tqdm - accuracy: 0.9167, batch_loss: 0.3274, loss: 0.2263 ||:   5%|4         | 3/65 [00:34<12:00, 11.62s/it]
2022-03-30 19:38:44,070 - INFO - tqdm - accuracy: 0.9141, batch_loss: 0.2626, loss: 0.2354 ||:   6%|6         | 4/65 [00:46<11:50, 11.64s/it]
2022-03-30 19:38:55,469 - INFO - tqdm - accuracy: 0.9125, batch_loss: 0.2386, loss: 0.2360 ||:   8%|7         | 5/65 [00:57<11:33, 11.55s/it]
2022-03-30 19:39:06,708 - INFO - tqdm - accuracy: 0.9271, batch_loss: 0.1145, loss: 0.2158 ||:   9%|9         | 6/65 [01:08<11:15, 11.45s/it]
2022-03-30 19:39:17,936 - INFO - tqdm - accuracy: 0.9241, batch_loss: 0.1843, loss: 0.2113 ||:  11%|#         | 7/65 [01:20<10:59, 11.38s/it]
2022-03-30 19:39:29,277 - INFO - tqdm - accuracy: 0.9297, batch_loss: 0.1523, loss: 0.2039 ||:  12%|#2        | 8/65 [01:31<10:47, 11.36s/it]
2022-03-30 19:39:40,749 - INFO - tqdm - accuracy: 0.9271, batch_loss: 0.1958, loss: 0.2030 ||:  14%|#3        | 9/65 [01:42<10:38, 11.40s/it]
2022-03-30 19:39:52,274 - INFO - tqdm - accuracy: 0.9250, batch_loss: 0.1455, loss: 0.1972 ||:  15%|#5        | 10/65 [01:54<10:29, 11.44s/it]
2022-03-30 19:40:03,765 - INFO - tqdm - accuracy: 0.9261, batch_loss: 0.1591, loss: 0.1938 ||:  17%|#6        | 11/65 [02:05<10:18, 11.45s/it]
2022-03-30 19:40:15,164 - INFO - tqdm - accuracy: 0.9323, batch_loss: 0.1067, loss: 0.1865 ||:  18%|#8        | 12/65 [02:17<10:06, 11.44s/it]
2022-03-30 19:40:26,534 - INFO - tqdm - accuracy: 0.9303, batch_loss: 0.2298, loss: 0.1899 ||:  20%|##        | 13/65 [02:28<09:53, 11.42s/it]
2022-03-30 19:40:37,906 - INFO - tqdm - accuracy: 0.9330, batch_loss: 0.1014, loss: 0.1835 ||:  22%|##1       | 14/65 [02:40<09:41, 11.40s/it]
2022-03-30 19:40:49,321 - INFO - tqdm - accuracy: 0.9333, batch_loss: 0.1908, loss: 0.1840 ||:  23%|##3       | 15/65 [02:51<09:30, 11.41s/it]
2022-03-30 19:41:00,744 - INFO - tqdm - accuracy: 0.9336, batch_loss: 0.1309, loss: 0.1807 ||:  25%|##4       | 16/65 [03:02<09:19, 11.41s/it]
2022-03-30 19:41:12,198 - INFO - tqdm - accuracy: 0.9357, batch_loss: 0.1846, loss: 0.1809 ||:  26%|##6       | 17/65 [03:14<09:08, 11.42s/it]
2022-03-30 19:41:23,657 - INFO - tqdm - accuracy: 0.9358, batch_loss: 0.1903, loss: 0.1815 ||:  28%|##7       | 18/65 [03:25<08:57, 11.43s/it]
2022-03-30 19:41:35,101 - INFO - tqdm - accuracy: 0.9375, batch_loss: 0.1043, loss: 0.1774 ||:  29%|##9       | 19/65 [03:37<08:46, 11.44s/it]
2022-03-30 19:41:46,552 - INFO - tqdm - accuracy: 0.9328, batch_loss: 0.2726, loss: 0.1822 ||:  31%|###       | 20/65 [03:48<08:34, 11.44s/it]
2022-03-30 19:41:57,996 - INFO - tqdm - accuracy: 0.9360, batch_loss: 0.0677, loss: 0.1767 ||:  32%|###2      | 21/65 [04:00<08:23, 11.44s/it]
2022-03-30 19:42:09,407 - INFO - tqdm - accuracy: 0.9361, batch_loss: 0.1801, loss: 0.1769 ||:  34%|###3      | 22/65 [04:11<08:11, 11.43s/it]
2022-03-30 19:42:20,800 - INFO - tqdm - accuracy: 0.9293, batch_loss: 0.3364, loss: 0.1838 ||:  35%|###5      | 23/65 [04:23<07:59, 11.42s/it]
2022-03-30 19:42:32,195 - INFO - tqdm - accuracy: 0.9297, batch_loss: 0.1484, loss: 0.1823 ||:  37%|###6      | 24/65 [04:34<07:47, 11.41s/it]
2022-03-30 19:42:43,582 - INFO - tqdm - accuracy: 0.9250, batch_loss: 0.3094, loss: 0.1874 ||:  38%|###8      | 25/65 [04:45<07:36, 11.41s/it]
2022-03-30 19:42:54,966 - INFO - tqdm - accuracy: 0.9267, batch_loss: 0.0718, loss: 0.1830 ||:  40%|####      | 26/65 [04:57<07:24, 11.40s/it]
2022-03-30 19:43:06,354 - INFO - tqdm - accuracy: 0.9294, batch_loss: 0.0239, loss: 0.1771 ||:  42%|####1     | 27/65 [05:08<07:13, 11.40s/it]
2022-03-30 19:43:17,769 - INFO - tqdm - accuracy: 0.9286, batch_loss: 0.3077, loss: 0.1817 ||:  43%|####3     | 28/65 [05:19<07:01, 11.40s/it]
2022-03-30 19:43:29,158 - INFO - tqdm - accuracy: 0.9300, batch_loss: 0.1109, loss: 0.1793 ||:  45%|####4     | 29/65 [05:31<06:50, 11.40s/it]
2022-03-30 19:43:40,425 - INFO - tqdm - accuracy: 0.9313, batch_loss: 0.1343, loss: 0.1778 ||:  46%|####6     | 30/65 [05:42<06:37, 11.36s/it]
2022-03-30 19:43:51,826 - INFO - tqdm - accuracy: 0.9335, batch_loss: 0.0934, loss: 0.1751 ||:  48%|####7     | 31/65 [05:54<06:26, 11.37s/it]
2022-03-30 19:44:03,219 - INFO - tqdm - accuracy: 0.9336, batch_loss: 0.1120, loss: 0.1731 ||:  49%|####9     | 32/65 [06:05<06:15, 11.38s/it]
2022-03-30 19:44:14,614 - INFO - tqdm - accuracy: 0.9347, batch_loss: 0.1532, loss: 0.1725 ||:  51%|#####     | 33/65 [06:16<06:04, 11.38s/it]
2022-03-30 19:44:26,001 - INFO - tqdm - accuracy: 0.9347, batch_loss: 0.2827, loss: 0.1757 ||:  52%|#####2    | 34/65 [06:28<05:52, 11.38s/it]
2022-03-30 19:44:37,407 - INFO - tqdm - accuracy: 0.9339, batch_loss: 0.2542, loss: 0.1780 ||:  54%|#####3    | 35/65 [06:39<05:41, 11.39s/it]
2022-03-30 19:44:48,825 - INFO - tqdm - accuracy: 0.9340, batch_loss: 0.1604, loss: 0.1775 ||:  55%|#####5    | 36/65 [06:51<05:30, 11.40s/it]
2022-03-30 19:44:59,871 - INFO - tqdm - accuracy: 0.9341, batch_loss: 0.2359, loss: 0.1791 ||:  57%|#####6    | 37/65 [07:02<05:16, 11.29s/it]
2022-03-30 19:45:11,302 - INFO - tqdm - accuracy: 0.9350, batch_loss: 0.0787, loss: 0.1764 ||:  58%|#####8    | 38/65 [07:13<05:06, 11.33s/it]
2022-03-30 19:45:22,731 - INFO - tqdm - accuracy: 0.9358, batch_loss: 0.1412, loss: 0.1755 ||:  60%|######    | 39/65 [07:24<04:55, 11.36s/it]
2022-03-30 19:45:34,561 - INFO - tqdm - accuracy: 0.9335, batch_loss: 0.3153, loss: 0.1790 ||:  62%|######1   | 40/65 [07:36<04:47, 11.50s/it]
2022-03-30 19:45:45,929 - INFO - tqdm - accuracy: 0.9329, batch_loss: 0.1386, loss: 0.1780 ||:  63%|######3   | 41/65 [07:48<04:35, 11.46s/it]
2022-03-30 19:45:57,334 - INFO - tqdm - accuracy: 0.9322, batch_loss: 0.1962, loss: 0.1785 ||:  65%|######4   | 42/65 [07:59<04:23, 11.45s/it]
2022-03-30 19:46:08,732 - INFO - tqdm - accuracy: 0.9338, batch_loss: 0.0646, loss: 0.1758 ||:  66%|######6   | 43/65 [08:10<04:11, 11.43s/it]
2022-03-30 19:46:20,161 - INFO - tqdm - accuracy: 0.9325, batch_loss: 0.3157, loss: 0.1790 ||:  68%|######7   | 44/65 [08:22<04:00, 11.43s/it]
2022-03-30 19:46:31,435 - INFO - tqdm - accuracy: 0.9333, batch_loss: 0.1514, loss: 0.1784 ||:  69%|######9   | 45/65 [08:33<03:47, 11.38s/it]
2022-03-30 19:46:42,829 - INFO - tqdm - accuracy: 0.9313, batch_loss: 0.4065, loss: 0.1833 ||:  71%|#######   | 46/65 [08:45<03:36, 11.39s/it]
2022-03-30 19:46:54,237 - INFO - tqdm - accuracy: 0.9321, batch_loss: 0.1010, loss: 0.1816 ||:  72%|#######2  | 47/65 [08:56<03:25, 11.39s/it]
2022-03-30 19:47:05,646 - INFO - tqdm - accuracy: 0.9316, batch_loss: 0.1588, loss: 0.1811 ||:  74%|#######3  | 48/65 [09:07<03:13, 11.40s/it]
2022-03-30 19:47:17,079 - INFO - tqdm - accuracy: 0.9324, batch_loss: 0.0833, loss: 0.1791 ||:  75%|#######5  | 49/65 [09:19<03:02, 11.41s/it]
2022-03-30 19:47:28,499 - INFO - tqdm - accuracy: 0.9318, batch_loss: 0.1759, loss: 0.1791 ||:  77%|#######6  | 50/65 [09:30<02:51, 11.41s/it]
2022-03-30 19:47:39,900 - INFO - tqdm - accuracy: 0.9326, batch_loss: 0.0980, loss: 0.1775 ||:  78%|#######8  | 51/65 [09:42<02:39, 11.41s/it]
2022-03-30 19:47:51,314 - INFO - tqdm - accuracy: 0.9327, batch_loss: 0.1416, loss: 0.1768 ||:  80%|########  | 52/65 [09:53<02:28, 11.41s/it]
2022-03-30 19:48:02,708 - INFO - tqdm - accuracy: 0.9316, batch_loss: 0.2499, loss: 0.1782 ||:  82%|########1 | 53/65 [10:04<02:16, 11.41s/it]
2022-03-30 19:48:14,106 - INFO - tqdm - accuracy: 0.9299, batch_loss: 0.3157, loss: 0.1807 ||:  83%|########3 | 54/65 [10:16<02:05, 11.40s/it]
2022-03-30 19:48:25,526 - INFO - tqdm - accuracy: 0.9295, batch_loss: 0.2559, loss: 0.1821 ||:  85%|########4 | 55/65 [10:27<01:54, 11.41s/it]
2022-03-30 19:48:36,947 - INFO - tqdm - accuracy: 0.9296, batch_loss: 0.1365, loss: 0.1813 ||:  86%|########6 | 56/65 [10:39<01:42, 11.41s/it]
2022-03-30 19:48:48,368 - INFO - tqdm - accuracy: 0.9287, batch_loss: 0.2897, loss: 0.1832 ||:  88%|########7 | 57/65 [10:50<01:31, 11.41s/it]
2022-03-30 19:48:59,761 - INFO - tqdm - accuracy: 0.9294, batch_loss: 0.2080, loss: 0.1836 ||:  89%|########9 | 58/65 [11:01<01:19, 11.41s/it]
2022-03-30 19:49:11,178 - INFO - tqdm - accuracy: 0.9290, batch_loss: 0.1466, loss: 0.1830 ||:  91%|######### | 59/65 [11:13<01:08, 11.41s/it]
2022-03-30 19:49:22,603 - INFO - tqdm - accuracy: 0.9286, batch_loss: 0.2346, loss: 0.1838 ||:  92%|#########2| 60/65 [11:24<00:57, 11.42s/it]
2022-03-30 19:49:34,026 - INFO - tqdm - accuracy: 0.9288, batch_loss: 0.1494, loss: 0.1833 ||:  94%|#########3| 61/65 [11:36<00:45, 11.42s/it]
2022-03-30 19:49:45,219 - INFO - tqdm - accuracy: 0.9289, batch_loss: 0.1131, loss: 0.1821 ||:  95%|#########5| 62/65 [11:47<00:34, 11.35s/it]
2022-03-30 19:49:56,626 - INFO - tqdm - accuracy: 0.9300, batch_loss: 0.0691, loss: 0.1803 ||:  97%|#########6| 63/65 [11:58<00:22, 11.37s/it]
2022-03-30 19:50:08,052 - INFO - tqdm - accuracy: 0.9297, batch_loss: 0.2009, loss: 0.1807 ||:  98%|#########8| 64/65 [12:10<00:11, 11.38s/it]
2022-03-30 19:50:13,122 - INFO - tqdm - accuracy: 0.9292, batch_loss: 0.2405, loss: 0.1816 ||: 100%|##########| 65/65 [12:15<00:00,  9.49s/it]
2022-03-30 19:50:13,123 - INFO - tqdm - accuracy: 0.9292, batch_loss: 0.2405, loss: 0.1816 ||: 100%|##########| 65/65 [12:15<00:00, 11.31s/it]
2022-03-30 19:50:15,979 - INFO - allennlp.training.trainer - Validating
2022-03-30 19:50:15,981 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-03-30 19:50:25,995 - INFO - tqdm - accuracy: 0.5875, batch_loss: 0.0006, loss: 1.3439 ||:  35%|###4      | 40/115 [00:10<00:18,  3.98it/s]
2022-03-30 19:50:36,083 - INFO - tqdm - accuracy: 0.5938, batch_loss: 0.0022, loss: 1.3326 ||:  70%|######9   | 80/115 [00:20<00:08,  3.97it/s]
2022-03-30 19:50:44,754 - INFO - tqdm - accuracy: 0.5939, batch_loss: 0.0031, loss: 1.3322 ||: 100%|##########| 115/115 [00:28<00:00,  3.94it/s]
2022-03-30 19:50:44,754 - INFO - tqdm - accuracy: 0.5939, batch_loss: 0.0031, loss: 1.3322 ||: 100%|##########| 115/115 [00:28<00:00,  4.00it/s]
2022-03-30 19:50:44,754 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-03-30 19:50:44,755 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.929  |     0.594
2022-03-30 19:50:44,755 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-03-30 19:50:44,755 - INFO - allennlp.training.tensorboard_writer - loss               |     0.182  |     1.332
2022-03-30 19:50:44,755 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5793.102  |       N/A
2022-03-30 19:50:50,762 - INFO - allennlp.training.trainer - Epoch duration: 0:12:52.992699
2022-03-30 19:50:50,763 - INFO - allennlp.training.trainer - Estimated training time remaining: 2:09:56
2022-03-30 19:50:50,763 - INFO - allennlp.training.trainer - Epoch 5/14
2022-03-30 19:50:50,763 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-03-30 19:50:50,763 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-03-30 19:50:50,765 - INFO - allennlp.training.trainer - Training
2022-03-30 19:50:50,765 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-03-30 19:51:02,004 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.0701, loss: 0.0701 ||:   2%|1         | 1/65 [00:11<11:59, 11.24s/it]
2022-03-30 19:51:13,556 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.0936, loss: 0.0818 ||:   3%|3         | 2/65 [00:22<11:59, 11.42s/it]
2022-03-30 19:51:25,179 - INFO - tqdm - accuracy: 0.9583, batch_loss: 0.1720, loss: 0.1119 ||:   5%|4         | 3/65 [00:34<11:53, 11.51s/it]
2022-03-30 19:51:36,622 - INFO - tqdm - accuracy: 0.9609, batch_loss: 0.0882, loss: 0.1060 ||:   6%|6         | 4/65 [00:45<11:40, 11.49s/it]
2022-03-30 19:51:47,988 - INFO - tqdm - accuracy: 0.9563, batch_loss: 0.1567, loss: 0.1161 ||:   8%|7         | 5/65 [00:57<11:26, 11.44s/it]
2022-03-30 19:51:59,335 - INFO - tqdm - accuracy: 0.9635, batch_loss: 0.0704, loss: 0.1085 ||:   9%|9         | 6/65 [01:08<11:13, 11.41s/it]
2022-03-30 19:52:10,614 - INFO - tqdm - accuracy: 0.9643, batch_loss: 0.0759, loss: 0.1039 ||:  11%|#         | 7/65 [01:19<10:59, 11.37s/it]
2022-03-30 19:52:22,065 - INFO - tqdm - accuracy: 0.9609, batch_loss: 0.1588, loss: 0.1107 ||:  12%|#2        | 8/65 [01:31<10:49, 11.39s/it]
2022-03-30 19:52:33,525 - INFO - tqdm - accuracy: 0.9653, batch_loss: 0.0303, loss: 0.1018 ||:  14%|#3        | 9/65 [01:42<10:39, 11.41s/it]
2022-03-30 19:52:44,996 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.0295, loss: 0.0946 ||:  15%|#5        | 10/65 [01:54<10:28, 11.43s/it]
2022-03-30 19:52:56,445 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.0612, loss: 0.0915 ||:  17%|#6        | 11/65 [02:05<10:17, 11.44s/it]
2022-03-30 19:53:07,874 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.0625, loss: 0.0891 ||:  18%|#8        | 12/65 [02:17<10:06, 11.43s/it]
2022-03-30 19:53:19,283 - INFO - tqdm - accuracy: 0.9663, batch_loss: 0.1295, loss: 0.0922 ||:  20%|##        | 13/65 [02:28<09:54, 11.43s/it]
2022-03-30 19:53:30,675 - INFO - tqdm - accuracy: 0.9621, batch_loss: 0.2971, loss: 0.1069 ||:  22%|##1       | 14/65 [02:39<09:42, 11.42s/it]
2022-03-30 19:53:42,051 - INFO - tqdm - accuracy: 0.9646, batch_loss: 0.0168, loss: 0.1008 ||:  23%|##3       | 15/65 [02:51<09:30, 11.40s/it]
2022-03-30 19:53:53,125 - INFO - tqdm - accuracy: 0.9667, batch_loss: 0.0705, loss: 0.0989 ||:  25%|##4       | 16/65 [03:02<09:13, 11.30s/it]
2022-03-30 19:54:04,563 - INFO - tqdm - accuracy: 0.9687, batch_loss: 0.0532, loss: 0.0963 ||:  26%|##6       | 17/65 [03:13<09:04, 11.34s/it]
2022-03-30 19:54:16,007 - INFO - tqdm - accuracy: 0.9652, batch_loss: 0.2174, loss: 0.1030 ||:  28%|##7       | 18/65 [03:25<08:54, 11.37s/it]
2022-03-30 19:54:27,453 - INFO - tqdm - accuracy: 0.9621, batch_loss: 0.3037, loss: 0.1135 ||:  29%|##9       | 19/65 [03:36<08:44, 11.40s/it]
2022-03-30 19:54:38,922 - INFO - tqdm - accuracy: 0.9640, batch_loss: 0.0725, loss: 0.1115 ||:  31%|###       | 20/65 [03:48<08:33, 11.42s/it]
2022-03-30 19:54:50,380 - INFO - tqdm - accuracy: 0.9627, batch_loss: 0.1787, loss: 0.1147 ||:  32%|###2      | 21/65 [03:59<08:22, 11.43s/it]
2022-03-30 19:55:01,835 - INFO - tqdm - accuracy: 0.9616, batch_loss: 0.1443, loss: 0.1160 ||:  34%|###3      | 22/65 [04:11<08:11, 11.44s/it]
2022-03-30 19:55:13,284 - INFO - tqdm - accuracy: 0.9578, batch_loss: 0.2353, loss: 0.1212 ||:  35%|###5      | 23/65 [04:22<08:00, 11.44s/it]
2022-03-30 19:55:24,735 - INFO - tqdm - accuracy: 0.9583, batch_loss: 0.1439, loss: 0.1222 ||:  37%|###6      | 24/65 [04:33<07:49, 11.44s/it]
2022-03-30 19:55:36,178 - INFO - tqdm - accuracy: 0.9574, batch_loss: 0.1498, loss: 0.1233 ||:  38%|###8      | 25/65 [04:45<07:37, 11.44s/it]
2022-03-30 19:55:47,623 - INFO - tqdm - accuracy: 0.9567, batch_loss: 0.0701, loss: 0.1212 ||:  40%|####      | 26/65 [04:56<07:26, 11.44s/it]
2022-03-30 19:55:59,068 - INFO - tqdm - accuracy: 0.9571, batch_loss: 0.1110, loss: 0.1208 ||:  42%|####1     | 27/65 [05:08<07:14, 11.44s/it]
2022-03-30 19:56:10,520 - INFO - tqdm - accuracy: 0.9587, batch_loss: 0.0329, loss: 0.1177 ||:  43%|####3     | 28/65 [05:19<07:03, 11.45s/it]
2022-03-30 19:56:21,961 - INFO - tqdm - accuracy: 0.9601, batch_loss: 0.0472, loss: 0.1153 ||:  45%|####4     | 29/65 [05:31<06:52, 11.44s/it]
2022-03-30 19:56:33,398 - INFO - tqdm - accuracy: 0.9593, batch_loss: 0.1856, loss: 0.1176 ||:  46%|####6     | 30/65 [05:42<06:40, 11.44s/it]
2022-03-30 19:56:44,827 - INFO - tqdm - accuracy: 0.9596, batch_loss: 0.0470, loss: 0.1153 ||:  48%|####7     | 31/65 [05:54<06:28, 11.44s/it]
2022-03-30 19:56:56,247 - INFO - tqdm - accuracy: 0.9609, batch_loss: 0.0232, loss: 0.1125 ||:  49%|####9     | 32/65 [06:05<06:17, 11.43s/it]
2022-03-30 19:57:07,665 - INFO - tqdm - accuracy: 0.9611, batch_loss: 0.0937, loss: 0.1119 ||:  51%|#####     | 33/65 [06:16<06:05, 11.43s/it]
2022-03-30 19:57:19,065 - INFO - tqdm - accuracy: 0.9623, batch_loss: 0.0584, loss: 0.1103 ||:  52%|#####2    | 34/65 [06:28<05:54, 11.42s/it]
2022-03-30 19:57:30,483 - INFO - tqdm - accuracy: 0.9625, batch_loss: 0.0632, loss: 0.1090 ||:  54%|#####3    | 35/65 [06:39<05:42, 11.42s/it]
2022-03-30 19:57:41,886 - INFO - tqdm - accuracy: 0.9635, batch_loss: 0.0173, loss: 0.1064 ||:  55%|#####5    | 36/65 [06:51<05:31, 11.41s/it]
2022-03-30 19:57:53,321 - INFO - tqdm - accuracy: 0.9645, batch_loss: 0.0431, loss: 0.1047 ||:  57%|#####6    | 37/65 [07:02<05:19, 11.42s/it]
2022-03-30 19:58:04,757 - INFO - tqdm - accuracy: 0.9646, batch_loss: 0.0876, loss: 0.1043 ||:  58%|#####8    | 38/65 [07:13<05:08, 11.43s/it]
2022-03-30 19:58:16,164 - INFO - tqdm - accuracy: 0.9647, batch_loss: 0.0635, loss: 0.1032 ||:  60%|######    | 39/65 [07:25<04:56, 11.42s/it]
2022-03-30 19:58:27,588 - INFO - tqdm - accuracy: 0.9633, batch_loss: 0.3174, loss: 0.1086 ||:  62%|######1   | 40/65 [07:36<04:45, 11.42s/it]
2022-03-30 19:58:38,999 - INFO - tqdm - accuracy: 0.9634, batch_loss: 0.0596, loss: 0.1074 ||:  63%|######3   | 41/65 [07:48<04:34, 11.42s/it]
2022-03-30 19:58:50,440 - INFO - tqdm - accuracy: 0.9635, batch_loss: 0.0670, loss: 0.1064 ||:  65%|######4   | 42/65 [07:59<04:22, 11.42s/it]
2022-03-30 19:59:01,862 - INFO - tqdm - accuracy: 0.9629, batch_loss: 0.2314, loss: 0.1093 ||:  66%|######6   | 43/65 [08:11<04:11, 11.42s/it]
2022-03-30 19:59:13,279 - INFO - tqdm - accuracy: 0.9616, batch_loss: 0.2787, loss: 0.1132 ||:  68%|######7   | 44/65 [08:22<03:59, 11.42s/it]
2022-03-30 19:59:24,451 - INFO - tqdm - accuracy: 0.9618, batch_loss: 0.0638, loss: 0.1121 ||:  69%|######9   | 45/65 [08:33<03:46, 11.35s/it]
2022-03-30 19:59:35,864 - INFO - tqdm - accuracy: 0.9613, batch_loss: 0.1358, loss: 0.1126 ||:  71%|#######   | 46/65 [08:45<03:35, 11.37s/it]
2022-03-30 19:59:47,274 - INFO - tqdm - accuracy: 0.9607, batch_loss: 0.1326, loss: 0.1130 ||:  72%|#######2  | 47/65 [08:56<03:24, 11.38s/it]
2022-03-30 19:59:58,680 - INFO - tqdm - accuracy: 0.9609, batch_loss: 0.0789, loss: 0.1123 ||:  74%|#######3  | 48/65 [09:07<03:13, 11.39s/it]
2022-03-30 20:00:10,090 - INFO - tqdm - accuracy: 0.9617, batch_loss: 0.0489, loss: 0.1110 ||:  75%|#######5  | 49/65 [09:19<03:02, 11.39s/it]
2022-03-30 20:00:21,508 - INFO - tqdm - accuracy: 0.9625, batch_loss: 0.0419, loss: 0.1096 ||:  77%|#######6  | 50/65 [09:30<02:51, 11.40s/it]
2022-03-30 20:00:32,916 - INFO - tqdm - accuracy: 0.9632, batch_loss: 0.0357, loss: 0.1082 ||:  78%|#######8  | 51/65 [09:42<02:39, 11.40s/it]
2022-03-30 20:00:44,333 - INFO - tqdm - accuracy: 0.9621, batch_loss: 0.1327, loss: 0.1087 ||:  80%|########  | 52/65 [09:53<02:28, 11.41s/it]
2022-03-30 20:00:55,750 - INFO - tqdm - accuracy: 0.9622, batch_loss: 0.0681, loss: 0.1079 ||:  82%|########1 | 53/65 [10:04<02:16, 11.41s/it]
2022-03-30 20:01:07,173 - INFO - tqdm - accuracy: 0.9624, batch_loss: 0.0693, loss: 0.1072 ||:  83%|########3 | 54/65 [10:16<02:05, 11.41s/it]
2022-03-30 20:01:18,595 - INFO - tqdm - accuracy: 0.9619, batch_loss: 0.1191, loss: 0.1074 ||:  85%|########4 | 55/65 [10:27<01:54, 11.42s/it]
2022-03-30 20:01:30,024 - INFO - tqdm - accuracy: 0.9615, batch_loss: 0.0998, loss: 0.1073 ||:  86%|########6 | 56/65 [10:39<01:42, 11.42s/it]
2022-03-30 20:01:41,436 - INFO - tqdm - accuracy: 0.9616, batch_loss: 0.1026, loss: 0.1072 ||:  88%|########7 | 57/65 [10:50<01:31, 11.42s/it]
2022-03-30 20:01:52,863 - INFO - tqdm - accuracy: 0.9606, batch_loss: 0.2535, loss: 0.1097 ||:  89%|########9 | 58/65 [11:02<01:19, 11.42s/it]
2022-03-30 20:02:04,166 - INFO - tqdm - accuracy: 0.9608, batch_loss: 0.1072, loss: 0.1097 ||:  91%|######### | 59/65 [11:13<01:08, 11.39s/it]
2022-03-30 20:02:15,598 - INFO - tqdm - accuracy: 0.9614, batch_loss: 0.0852, loss: 0.1092 ||:  92%|#########2| 60/65 [11:24<00:56, 11.40s/it]
2022-03-30 20:02:27,024 - INFO - tqdm - accuracy: 0.9605, batch_loss: 0.1542, loss: 0.1100 ||:  94%|#########3| 61/65 [11:36<00:45, 11.41s/it]
2022-03-30 20:02:38,442 - INFO - tqdm - accuracy: 0.9607, batch_loss: 0.0703, loss: 0.1093 ||:  95%|#########5| 62/65 [11:47<00:34, 11.41s/it]
2022-03-30 20:02:49,885 - INFO - tqdm - accuracy: 0.9598, batch_loss: 0.1509, loss: 0.1100 ||:  97%|#########6| 63/65 [11:59<00:22, 11.42s/it]
2022-03-30 20:03:01,295 - INFO - tqdm - accuracy: 0.9599, batch_loss: 0.1158, loss: 0.1101 ||:  98%|#########8| 64/65 [12:10<00:11, 11.42s/it]
2022-03-30 20:03:06,359 - INFO - tqdm - accuracy: 0.9597, batch_loss: 0.2060, loss: 0.1116 ||: 100%|##########| 65/65 [12:15<00:00,  9.51s/it]
2022-03-30 20:03:06,360 - INFO - tqdm - accuracy: 0.9597, batch_loss: 0.2060, loss: 0.1116 ||: 100%|##########| 65/65 [12:15<00:00, 11.32s/it]
2022-03-30 20:03:09,201 - INFO - allennlp.training.trainer - Validating
2022-03-30 20:03:09,203 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-03-30 20:03:19,220 - INFO - tqdm - accuracy: 0.5875, batch_loss: 7.4316, loss: 2.0667 ||:  35%|###4      | 40/115 [00:10<00:18,  3.99it/s]
2022-03-30 20:03:29,356 - INFO - tqdm - accuracy: 0.6335, batch_loss: 0.4144, loss: 1.6417 ||:  70%|#######   | 81/115 [00:20<00:08,  4.11it/s]
2022-03-30 20:03:37,962 - INFO - tqdm - accuracy: 0.6114, batch_loss: 0.0015, loss: 1.6972 ||: 100%|##########| 115/115 [00:28<00:00,  3.94it/s]
2022-03-30 20:03:37,962 - INFO - tqdm - accuracy: 0.6114, batch_loss: 0.0015, loss: 1.6972 ||: 100%|##########| 115/115 [00:28<00:00,  4.00it/s]
2022-03-30 20:03:37,962 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-03-30 20:03:37,963 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.960  |     0.611
2022-03-30 20:03:37,964 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-03-30 20:03:37,964 - INFO - allennlp.training.tensorboard_writer - loss               |     0.112  |     1.697
2022-03-30 20:03:37,965 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5793.102  |       N/A
2022-03-30 20:03:44,116 - INFO - allennlp.training.trainer - Epoch duration: 0:12:53.353574
2022-03-30 20:03:44,117 - INFO - allennlp.training.trainer - Estimated training time remaining: 1:56:47
2022-03-30 20:03:44,117 - INFO - allennlp.training.trainer - Epoch 6/14
2022-03-30 20:03:44,117 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-03-30 20:03:44,117 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-03-30 20:03:44,118 - INFO - allennlp.training.trainer - Training
2022-03-30 20:03:44,119 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-03-30 20:03:55,449 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0446, loss: 0.0446 ||:   2%|1         | 1/65 [00:11<12:05, 11.33s/it]
2022-03-30 20:04:07,019 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0415, loss: 0.0431 ||:   3%|3         | 2/65 [00:22<12:02, 11.47s/it]
2022-03-30 20:04:18,666 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.0952, loss: 0.0605 ||:   5%|4         | 3/65 [00:34<11:56, 11.55s/it]
2022-03-30 20:04:30,132 - INFO - tqdm - accuracy: 0.9766, batch_loss: 0.1129, loss: 0.0736 ||:   6%|6         | 4/65 [00:46<11:42, 11.52s/it]
2022-03-30 20:04:41,525 - INFO - tqdm - accuracy: 0.9812, batch_loss: 0.0368, loss: 0.0662 ||:   8%|7         | 5/65 [00:57<11:28, 11.47s/it]
2022-03-30 20:04:52,872 - INFO - tqdm - accuracy: 0.9792, batch_loss: 0.0653, loss: 0.0661 ||:   9%|9         | 6/65 [01:08<11:14, 11.43s/it]
2022-03-30 20:05:04,241 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.2496, loss: 0.0923 ||:  11%|#         | 7/65 [01:20<11:01, 11.41s/it]
2022-03-30 20:05:15,678 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.0444, loss: 0.0863 ||:  12%|#2        | 8/65 [01:31<10:50, 11.42s/it]
2022-03-30 20:05:27,131 - INFO - tqdm - accuracy: 0.9653, batch_loss: 0.1306, loss: 0.0912 ||:  14%|#3        | 9/65 [01:43<10:40, 11.43s/it]
2022-03-30 20:05:38,996 - INFO - tqdm - accuracy: 0.9531, batch_loss: 0.2941, loss: 0.1115 ||:  15%|#5        | 10/65 [01:54<10:36, 11.56s/it]
2022-03-30 20:05:50,398 - INFO - tqdm - accuracy: 0.9517, batch_loss: 0.1132, loss: 0.1117 ||:  17%|#6        | 11/65 [02:06<10:21, 11.51s/it]
2022-03-30 20:06:01,837 - INFO - tqdm - accuracy: 0.9505, batch_loss: 0.1001, loss: 0.1107 ||:  18%|#8        | 12/65 [02:17<10:09, 11.49s/it]
2022-03-30 20:06:13,282 - INFO - tqdm - accuracy: 0.9519, batch_loss: 0.1075, loss: 0.1104 ||:  20%|##        | 13/65 [02:29<09:56, 11.48s/it]
2022-03-30 20:06:24,679 - INFO - tqdm - accuracy: 0.9531, batch_loss: 0.1492, loss: 0.1132 ||:  22%|##1       | 14/65 [02:40<09:44, 11.45s/it]
2022-03-30 20:06:36,068 - INFO - tqdm - accuracy: 0.9542, batch_loss: 0.0771, loss: 0.1108 ||:  23%|##3       | 15/65 [02:51<09:31, 11.43s/it]
2022-03-30 20:06:47,454 - INFO - tqdm - accuracy: 0.9570, batch_loss: 0.0257, loss: 0.1055 ||:  25%|##4       | 16/65 [03:03<09:19, 11.42s/it]
2022-03-30 20:06:58,827 - INFO - tqdm - accuracy: 0.9596, batch_loss: 0.0135, loss: 0.1001 ||:  26%|##6       | 17/65 [03:14<09:07, 11.41s/it]
2022-03-30 20:07:10,211 - INFO - tqdm - accuracy: 0.9601, batch_loss: 0.0888, loss: 0.0995 ||:  28%|##7       | 18/65 [03:26<08:55, 11.40s/it]
2022-03-30 20:07:21,610 - INFO - tqdm - accuracy: 0.9622, batch_loss: 0.0237, loss: 0.0955 ||:  29%|##9       | 19/65 [03:37<08:44, 11.40s/it]
2022-03-30 20:07:33,035 - INFO - tqdm - accuracy: 0.9625, batch_loss: 0.0392, loss: 0.0927 ||:  31%|###       | 20/65 [03:48<08:33, 11.41s/it]
2022-03-30 20:07:44,451 - INFO - tqdm - accuracy: 0.9628, batch_loss: 0.0458, loss: 0.0904 ||:  32%|###2      | 21/65 [04:00<08:22, 11.41s/it]
2022-03-30 20:07:55,873 - INFO - tqdm - accuracy: 0.9645, batch_loss: 0.0181, loss: 0.0871 ||:  34%|###3      | 22/65 [04:11<08:10, 11.41s/it]
2022-03-30 20:08:07,310 - INFO - tqdm - accuracy: 0.9660, batch_loss: 0.0209, loss: 0.0843 ||:  35%|###5      | 23/65 [04:23<07:59, 11.42s/it]
2022-03-30 20:08:18,731 - INFO - tqdm - accuracy: 0.9648, batch_loss: 0.1552, loss: 0.0872 ||:  37%|###6      | 24/65 [04:34<07:48, 11.42s/it]
2022-03-30 20:08:30,156 - INFO - tqdm - accuracy: 0.9663, batch_loss: 0.0509, loss: 0.0858 ||:  38%|###8      | 25/65 [04:46<07:36, 11.42s/it]
2022-03-30 20:08:41,573 - INFO - tqdm - accuracy: 0.9675, batch_loss: 0.0309, loss: 0.0837 ||:  40%|####      | 26/65 [04:57<07:25, 11.42s/it]
2022-03-30 20:08:52,988 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.0240, loss: 0.0814 ||:  42%|####1     | 27/65 [05:08<07:13, 11.42s/it]
2022-03-30 20:09:04,407 - INFO - tqdm - accuracy: 0.9699, batch_loss: 0.0336, loss: 0.0797 ||:  43%|####3     | 28/65 [05:20<07:02, 11.42s/it]
2022-03-30 20:09:15,853 - INFO - tqdm - accuracy: 0.9698, batch_loss: 0.0538, loss: 0.0788 ||:  45%|####4     | 29/65 [05:31<06:51, 11.43s/it]
2022-03-30 20:09:27,157 - INFO - tqdm - accuracy: 0.9677, batch_loss: 0.1652, loss: 0.0817 ||:  46%|####6     | 30/65 [05:43<06:38, 11.39s/it]
2022-03-30 20:09:38,587 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.0118, loss: 0.0795 ||:  48%|####7     | 31/65 [05:54<06:27, 11.40s/it]
2022-03-30 20:09:50,039 - INFO - tqdm - accuracy: 0.9697, batch_loss: 0.0075, loss: 0.0772 ||:  49%|####9     | 32/65 [06:05<06:16, 11.42s/it]
2022-03-30 20:10:01,462 - INFO - tqdm - accuracy: 0.9706, batch_loss: 0.0624, loss: 0.0768 ||:  51%|#####     | 33/65 [06:17<06:05, 11.42s/it]
2022-03-30 20:10:12,889 - INFO - tqdm - accuracy: 0.9715, batch_loss: 0.0105, loss: 0.0748 ||:  52%|#####2    | 34/65 [06:28<05:54, 11.42s/it]
2022-03-30 20:10:24,329 - INFO - tqdm - accuracy: 0.9723, batch_loss: 0.0143, loss: 0.0731 ||:  54%|#####3    | 35/65 [06:40<05:42, 11.43s/it]
2022-03-30 20:10:35,778 - INFO - tqdm - accuracy: 0.9731, batch_loss: 0.0322, loss: 0.0719 ||:  55%|#####5    | 36/65 [06:51<05:31, 11.43s/it]
2022-03-30 20:10:47,228 - INFO - tqdm - accuracy: 0.9738, batch_loss: 0.0228, loss: 0.0706 ||:  57%|#####6    | 37/65 [07:03<05:20, 11.44s/it]
2022-03-30 20:10:58,665 - INFO - tqdm - accuracy: 0.9745, batch_loss: 0.0780, loss: 0.0708 ||:  58%|#####8    | 38/65 [07:14<05:08, 11.44s/it]
2022-03-30 20:11:10,087 - INFO - tqdm - accuracy: 0.9744, batch_loss: 0.1133, loss: 0.0719 ||:  60%|######    | 39/65 [07:25<04:57, 11.43s/it]
2022-03-30 20:11:21,532 - INFO - tqdm - accuracy: 0.9727, batch_loss: 0.2547, loss: 0.0765 ||:  62%|######1   | 40/65 [07:37<04:45, 11.44s/it]
2022-03-30 20:11:32,626 - INFO - tqdm - accuracy: 0.9733, batch_loss: 0.0340, loss: 0.0754 ||:  63%|######3   | 41/65 [07:48<04:32, 11.33s/it]
2022-03-30 20:11:44,076 - INFO - tqdm - accuracy: 0.9739, batch_loss: 0.0063, loss: 0.0738 ||:  65%|######4   | 42/65 [07:59<04:21, 11.37s/it]
2022-03-30 20:11:55,277 - INFO - tqdm - accuracy: 0.9738, batch_loss: 0.1061, loss: 0.0745 ||:  66%|######6   | 43/65 [08:11<04:09, 11.32s/it]
2022-03-30 20:12:06,727 - INFO - tqdm - accuracy: 0.9737, batch_loss: 0.0358, loss: 0.0737 ||:  68%|######7   | 44/65 [08:22<03:58, 11.36s/it]
2022-03-30 20:12:18,172 - INFO - tqdm - accuracy: 0.9729, batch_loss: 0.1528, loss: 0.0754 ||:  69%|######9   | 45/65 [08:34<03:47, 11.38s/it]
2022-03-30 20:12:29,625 - INFO - tqdm - accuracy: 0.9728, batch_loss: 0.0669, loss: 0.0752 ||:  71%|#######   | 46/65 [08:45<03:36, 11.40s/it]
2022-03-30 20:12:41,069 - INFO - tqdm - accuracy: 0.9734, batch_loss: 0.0332, loss: 0.0743 ||:  72%|#######2  | 47/65 [08:56<03:25, 11.42s/it]
2022-03-30 20:12:52,516 - INFO - tqdm - accuracy: 0.9733, batch_loss: 0.0748, loss: 0.0744 ||:  74%|#######3  | 48/65 [09:08<03:14, 11.43s/it]
2022-03-30 20:13:03,956 - INFO - tqdm - accuracy: 0.9738, batch_loss: 0.0194, loss: 0.0732 ||:  75%|#######5  | 49/65 [09:19<03:02, 11.43s/it]
2022-03-30 20:13:15,403 - INFO - tqdm - accuracy: 0.9744, batch_loss: 0.0034, loss: 0.0718 ||:  77%|#######6  | 50/65 [09:31<02:51, 11.43s/it]
2022-03-30 20:13:26,833 - INFO - tqdm - accuracy: 0.9749, batch_loss: 0.0228, loss: 0.0709 ||:  78%|#######8  | 51/65 [09:42<02:40, 11.43s/it]
2022-03-30 20:13:38,143 - INFO - tqdm - accuracy: 0.9753, batch_loss: 0.0257, loss: 0.0700 ||:  80%|########  | 52/65 [09:54<02:28, 11.40s/it]
2022-03-30 20:13:49,600 - INFO - tqdm - accuracy: 0.9752, batch_loss: 0.0402, loss: 0.0694 ||:  82%|########1 | 53/65 [10:05<02:16, 11.41s/it]
2022-03-30 20:14:01,035 - INFO - tqdm - accuracy: 0.9757, batch_loss: 0.0209, loss: 0.0685 ||:  83%|########3 | 54/65 [10:16<02:05, 11.42s/it]
2022-03-30 20:14:12,486 - INFO - tqdm - accuracy: 0.9756, batch_loss: 0.0365, loss: 0.0680 ||:  85%|########4 | 55/65 [10:28<01:54, 11.43s/it]
2022-03-30 20:14:23,816 - INFO - tqdm - accuracy: 0.9749, batch_loss: 0.2636, loss: 0.0715 ||:  86%|########6 | 56/65 [10:39<01:42, 11.40s/it]
2022-03-30 20:14:35,264 - INFO - tqdm - accuracy: 0.9748, batch_loss: 0.0960, loss: 0.0719 ||:  88%|########7 | 57/65 [10:51<01:31, 11.41s/it]
2022-03-30 20:14:46,703 - INFO - tqdm - accuracy: 0.9752, batch_loss: 0.0056, loss: 0.0707 ||:  89%|########9 | 58/65 [11:02<01:19, 11.42s/it]
2022-03-30 20:14:58,136 - INFO - tqdm - accuracy: 0.9756, batch_loss: 0.0205, loss: 0.0699 ||:  91%|######### | 59/65 [11:14<01:08, 11.43s/it]
2022-03-30 20:15:09,579 - INFO - tqdm - accuracy: 0.9760, batch_loss: 0.0110, loss: 0.0689 ||:  92%|#########2| 60/65 [11:25<00:57, 11.43s/it]
2022-03-30 20:15:21,026 - INFO - tqdm - accuracy: 0.9764, batch_loss: 0.0206, loss: 0.0681 ||:  94%|#########3| 61/65 [11:36<00:45, 11.44s/it]
2022-03-30 20:15:32,478 - INFO - tqdm - accuracy: 0.9763, batch_loss: 0.0812, loss: 0.0683 ||:  95%|#########5| 62/65 [11:48<00:34, 11.44s/it]
2022-03-30 20:15:43,899 - INFO - tqdm - accuracy: 0.9762, batch_loss: 0.1010, loss: 0.0688 ||:  97%|#########6| 63/65 [11:59<00:22, 11.43s/it]
2022-03-30 20:15:55,341 - INFO - tqdm - accuracy: 0.9761, batch_loss: 0.0869, loss: 0.0691 ||:  98%|#########8| 64/65 [12:11<00:11, 11.44s/it]
2022-03-30 20:16:00,419 - INFO - tqdm - accuracy: 0.9757, batch_loss: 0.1098, loss: 0.0698 ||: 100%|##########| 65/65 [12:16<00:00,  9.53s/it]
2022-03-30 20:16:00,420 - INFO - tqdm - accuracy: 0.9757, batch_loss: 0.1098, loss: 0.0698 ||: 100%|##########| 65/65 [12:16<00:00, 11.33s/it]
2022-03-30 20:16:03,236 - INFO - allennlp.training.trainer - Validating
2022-03-30 20:16:03,237 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-03-30 20:16:13,382 - INFO - tqdm - accuracy: 0.6049, batch_loss: 3.9373, loss: 1.7633 ||:  36%|###5      | 41/115 [00:10<00:18,  3.96it/s]
2022-03-30 20:16:23,395 - INFO - tqdm - accuracy: 0.6460, batch_loss: 0.0206, loss: 1.7298 ||:  70%|#######   | 81/115 [00:20<00:07,  4.40it/s]
2022-03-30 20:16:32,009 - INFO - tqdm - accuracy: 0.6463, batch_loss: 1.7822, loss: 1.7202 ||: 100%|##########| 115/115 [00:28<00:00,  3.94it/s]
2022-03-30 20:16:32,009 - INFO - tqdm - accuracy: 0.6463, batch_loss: 1.7822, loss: 1.7202 ||: 100%|##########| 115/115 [00:28<00:00,  4.00it/s]
2022-03-30 20:16:32,010 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-03-30 20:16:32,010 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.976  |     0.646
2022-03-30 20:16:32,011 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-03-30 20:16:32,012 - INFO - allennlp.training.tensorboard_writer - loss               |     0.070  |     1.720
2022-03-30 20:16:32,012 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5793.102  |       N/A
2022-03-30 20:16:38,225 - INFO - allennlp.training.trainer - Epoch duration: 0:12:54.108322
2022-03-30 20:16:38,225 - INFO - allennlp.training.trainer - Estimated training time remaining: 1:43:43
2022-03-30 20:16:38,225 - INFO - allennlp.training.trainer - Epoch 7/14
2022-03-30 20:16:38,226 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-03-30 20:16:38,226 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-03-30 20:16:38,227 - INFO - allennlp.training.trainer - Training
2022-03-30 20:16:38,227 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-03-30 20:16:49,552 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0182, loss: 0.0182 ||:   2%|1         | 1/65 [00:11<12:04, 11.32s/it]
2022-03-30 20:17:01,104 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0289, loss: 0.0235 ||:   3%|3         | 2/65 [00:22<12:01, 11.46s/it]
2022-03-30 20:17:12,752 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.1325, loss: 0.0599 ||:   5%|4         | 3/65 [00:34<11:55, 11.54s/it]
2022-03-30 20:17:24,220 - INFO - tqdm - accuracy: 0.9922, batch_loss: 0.0373, loss: 0.0542 ||:   6%|6         | 4/65 [00:45<11:42, 11.51s/it]
2022-03-30 20:17:35,606 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.2266, loss: 0.0887 ||:   8%|7         | 5/65 [00:57<11:28, 11.47s/it]
2022-03-30 20:17:46,944 - INFO - tqdm - accuracy: 0.9740, batch_loss: 0.0061, loss: 0.0749 ||:   9%|9         | 6/65 [01:08<11:14, 11.42s/it]
2022-03-30 20:17:58,337 - INFO - tqdm - accuracy: 0.9732, batch_loss: 0.0313, loss: 0.0687 ||:  11%|#         | 7/65 [01:20<11:01, 11.41s/it]
2022-03-30 20:18:09,753 - INFO - tqdm - accuracy: 0.9766, batch_loss: 0.0206, loss: 0.0627 ||:  12%|#2        | 8/65 [01:31<10:50, 11.41s/it]
2022-03-30 20:18:21,195 - INFO - tqdm - accuracy: 0.9792, batch_loss: 0.0133, loss: 0.0572 ||:  14%|#3        | 9/65 [01:42<10:39, 11.42s/it]
2022-03-30 20:18:32,646 - INFO - tqdm - accuracy: 0.9812, batch_loss: 0.0252, loss: 0.0540 ||:  15%|#5        | 10/65 [01:54<10:28, 11.43s/it]
2022-03-30 20:18:44,099 - INFO - tqdm - accuracy: 0.9830, batch_loss: 0.0104, loss: 0.0500 ||:  17%|#6        | 11/65 [02:05<10:17, 11.44s/it]
2022-03-30 20:18:55,582 - INFO - tqdm - accuracy: 0.9792, batch_loss: 0.0789, loss: 0.0524 ||:  18%|#8        | 12/65 [02:17<10:06, 11.45s/it]
2022-03-30 20:19:07,024 - INFO - tqdm - accuracy: 0.9760, batch_loss: 0.0951, loss: 0.0557 ||:  20%|##        | 13/65 [02:28<09:55, 11.45s/it]
2022-03-30 20:19:18,461 - INFO - tqdm - accuracy: 0.9754, batch_loss: 0.0323, loss: 0.0540 ||:  22%|##1       | 14/65 [02:40<09:43, 11.45s/it]
2022-03-30 20:19:29,754 - INFO - tqdm - accuracy: 0.9771, batch_loss: 0.0093, loss: 0.0511 ||:  23%|##3       | 15/65 [02:51<09:29, 11.40s/it]
2022-03-30 20:19:41,165 - INFO - tqdm - accuracy: 0.9766, batch_loss: 0.0501, loss: 0.0510 ||:  25%|##4       | 16/65 [03:02<09:18, 11.40s/it]
2022-03-30 20:19:52,430 - INFO - tqdm - accuracy: 0.9779, batch_loss: 0.0202, loss: 0.0492 ||:  26%|##6       | 17/65 [03:14<09:05, 11.36s/it]
2022-03-30 20:20:03,815 - INFO - tqdm - accuracy: 0.9774, batch_loss: 0.0452, loss: 0.0490 ||:  28%|##7       | 18/65 [03:25<08:54, 11.37s/it]
2022-03-30 20:20:15,182 - INFO - tqdm - accuracy: 0.9786, batch_loss: 0.0059, loss: 0.0467 ||:  29%|##9       | 19/65 [03:36<08:42, 11.37s/it]
2022-03-30 20:20:26,448 - INFO - tqdm - accuracy: 0.9797, batch_loss: 0.0112, loss: 0.0449 ||:  31%|###       | 20/65 [03:48<08:30, 11.34s/it]
2022-03-30 20:20:37,840 - INFO - tqdm - accuracy: 0.9807, batch_loss: 0.0363, loss: 0.0445 ||:  32%|###2      | 21/65 [03:59<08:19, 11.35s/it]
2022-03-30 20:20:49,227 - INFO - tqdm - accuracy: 0.9801, batch_loss: 0.0383, loss: 0.0442 ||:  34%|###3      | 22/65 [04:10<08:08, 11.36s/it]
2022-03-30 20:21:00,645 - INFO - tqdm - accuracy: 0.9810, batch_loss: 0.0093, loss: 0.0427 ||:  35%|###5      | 23/65 [04:22<07:57, 11.38s/it]
2022-03-30 20:21:12,040 - INFO - tqdm - accuracy: 0.9818, batch_loss: 0.0109, loss: 0.0414 ||:  37%|###6      | 24/65 [04:33<07:46, 11.38s/it]
2022-03-30 20:21:23,447 - INFO - tqdm - accuracy: 0.9825, batch_loss: 0.0061, loss: 0.0400 ||:  38%|###8      | 25/65 [04:45<07:35, 11.39s/it]
2022-03-30 20:21:34,864 - INFO - tqdm - accuracy: 0.9832, batch_loss: 0.0137, loss: 0.0390 ||:  40%|####      | 26/65 [04:56<07:24, 11.40s/it]
2022-03-30 20:21:46,275 - INFO - tqdm - accuracy: 0.9838, batch_loss: 0.0138, loss: 0.0380 ||:  42%|####1     | 27/65 [05:08<07:13, 11.40s/it]
2022-03-30 20:21:57,695 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0017, loss: 0.0367 ||:  43%|####3     | 28/65 [05:19<07:02, 11.41s/it]
2022-03-30 20:22:09,122 - INFO - tqdm - accuracy: 0.9828, batch_loss: 0.1591, loss: 0.0410 ||:  45%|####4     | 29/65 [05:30<06:50, 11.41s/it]
2022-03-30 20:22:20,540 - INFO - tqdm - accuracy: 0.9833, batch_loss: 0.0161, loss: 0.0401 ||:  46%|####6     | 30/65 [05:42<06:39, 11.41s/it]
2022-03-30 20:22:31,955 - INFO - tqdm - accuracy: 0.9839, batch_loss: 0.0179, loss: 0.0394 ||:  48%|####7     | 31/65 [05:53<06:28, 11.41s/it]
2022-03-30 20:22:43,381 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0181, loss: 0.0387 ||:  49%|####9     | 32/65 [06:05<06:16, 11.42s/it]
2022-03-30 20:22:54,815 - INFO - tqdm - accuracy: 0.9848, batch_loss: 0.0129, loss: 0.0380 ||:  51%|#####     | 33/65 [06:16<06:05, 11.42s/it]
2022-03-30 20:23:06,248 - INFO - tqdm - accuracy: 0.9835, batch_loss: 0.1974, loss: 0.0427 ||:  52%|#####2    | 34/65 [06:28<05:54, 11.43s/it]
2022-03-30 20:23:17,679 - INFO - tqdm - accuracy: 0.9830, batch_loss: 0.0836, loss: 0.0438 ||:  54%|#####3    | 35/65 [06:39<05:42, 11.43s/it]
2022-03-30 20:23:29,117 - INFO - tqdm - accuracy: 0.9835, batch_loss: 0.0055, loss: 0.0428 ||:  55%|#####5    | 36/65 [06:50<05:31, 11.43s/it]
2022-03-30 20:23:40,561 - INFO - tqdm - accuracy: 0.9831, batch_loss: 0.0418, loss: 0.0427 ||:  57%|#####6    | 37/65 [07:02<05:20, 11.43s/it]
2022-03-30 20:23:52,008 - INFO - tqdm - accuracy: 0.9836, batch_loss: 0.0192, loss: 0.0421 ||:  58%|#####8    | 38/65 [07:13<05:08, 11.44s/it]
2022-03-30 20:24:03,439 - INFO - tqdm - accuracy: 0.9832, batch_loss: 0.0427, loss: 0.0421 ||:  60%|######    | 39/65 [07:25<04:57, 11.44s/it]
2022-03-30 20:24:14,879 - INFO - tqdm - accuracy: 0.9828, batch_loss: 0.0897, loss: 0.0433 ||:  62%|######1   | 40/65 [07:36<04:45, 11.44s/it]
2022-03-30 20:24:26,307 - INFO - tqdm - accuracy: 0.9832, batch_loss: 0.0063, loss: 0.0424 ||:  63%|######3   | 41/65 [07:48<04:34, 11.43s/it]
2022-03-30 20:24:37,738 - INFO - tqdm - accuracy: 0.9829, batch_loss: 0.0454, loss: 0.0425 ||:  65%|######4   | 42/65 [07:59<04:22, 11.43s/it]
2022-03-30 20:24:49,156 - INFO - tqdm - accuracy: 0.9833, batch_loss: 0.0095, loss: 0.0417 ||:  66%|######6   | 43/65 [08:10<04:11, 11.43s/it]
2022-03-30 20:25:00,583 - INFO - tqdm - accuracy: 0.9837, batch_loss: 0.0230, loss: 0.0413 ||:  68%|######7   | 44/65 [08:22<03:59, 11.43s/it]
2022-03-30 20:25:12,415 - INFO - tqdm - accuracy: 0.9840, batch_loss: 0.0179, loss: 0.0408 ||:  69%|######9   | 45/65 [08:34<03:50, 11.55s/it]
2022-03-30 20:25:23,809 - INFO - tqdm - accuracy: 0.9837, batch_loss: 0.0320, loss: 0.0406 ||:  71%|#######   | 46/65 [08:45<03:38, 11.50s/it]
2022-03-30 20:25:35,252 - INFO - tqdm - accuracy: 0.9840, batch_loss: 0.0056, loss: 0.0398 ||:  72%|#######2  | 47/65 [08:57<03:26, 11.49s/it]
2022-03-30 20:25:46,696 - INFO - tqdm - accuracy: 0.9837, batch_loss: 0.0417, loss: 0.0399 ||:  74%|#######3  | 48/65 [09:08<03:15, 11.47s/it]
2022-03-30 20:25:58,125 - INFO - tqdm - accuracy: 0.9841, batch_loss: 0.0084, loss: 0.0392 ||:  75%|#######5  | 49/65 [09:19<03:03, 11.46s/it]
2022-03-30 20:26:09,564 - INFO - tqdm - accuracy: 0.9838, batch_loss: 0.0704, loss: 0.0399 ||:  77%|#######6  | 50/65 [09:31<02:51, 11.45s/it]
2022-03-30 20:26:20,999 - INFO - tqdm - accuracy: 0.9841, batch_loss: 0.0047, loss: 0.0392 ||:  78%|#######8  | 51/65 [09:42<02:40, 11.45s/it]
2022-03-30 20:26:32,427 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0108, loss: 0.0386 ||:  80%|########  | 52/65 [09:54<02:28, 11.44s/it]
2022-03-30 20:26:43,870 - INFO - tqdm - accuracy: 0.9841, batch_loss: 0.0764, loss: 0.0393 ||:  82%|########1 | 53/65 [10:05<02:17, 11.44s/it]
2022-03-30 20:26:55,318 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0133, loss: 0.0389 ||:  83%|########3 | 54/65 [10:17<02:05, 11.44s/it]
2022-03-30 20:27:06,753 - INFO - tqdm - accuracy: 0.9841, batch_loss: 0.1908, loss: 0.0416 ||:  85%|########4 | 55/65 [10:28<01:54, 11.44s/it]
2022-03-30 20:27:18,190 - INFO - tqdm - accuracy: 0.9838, batch_loss: 0.0462, loss: 0.0417 ||:  86%|########6 | 56/65 [10:39<01:42, 11.44s/it]
2022-03-30 20:27:29,636 - INFO - tqdm - accuracy: 0.9836, batch_loss: 0.0713, loss: 0.0422 ||:  88%|########7 | 57/65 [10:51<01:31, 11.44s/it]
2022-03-30 20:27:41,082 - INFO - tqdm - accuracy: 0.9838, batch_loss: 0.0153, loss: 0.0418 ||:  89%|########9 | 58/65 [11:02<01:20, 11.44s/it]
2022-03-30 20:27:51,937 - INFO - tqdm - accuracy: 0.9841, batch_loss: 0.0037, loss: 0.0411 ||:  91%|######### | 59/65 [11:13<01:07, 11.27s/it]
2022-03-30 20:28:03,385 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0386, loss: 0.0411 ||:  92%|#########2| 60/65 [11:25<00:56, 11.32s/it]
2022-03-30 20:28:14,826 - INFO - tqdm - accuracy: 0.9846, batch_loss: 0.0261, loss: 0.0408 ||:  94%|#########3| 61/65 [11:36<00:45, 11.36s/it]
2022-03-30 20:28:26,256 - INFO - tqdm - accuracy: 0.9849, batch_loss: 0.0334, loss: 0.0407 ||:  95%|#########5| 62/65 [11:48<00:34, 11.38s/it]
2022-03-30 20:28:37,709 - INFO - tqdm - accuracy: 0.9851, batch_loss: 0.0024, loss: 0.0401 ||:  97%|#########6| 63/65 [11:59<00:22, 11.40s/it]
2022-03-30 20:28:49,146 - INFO - tqdm - accuracy: 0.9849, batch_loss: 0.1173, loss: 0.0413 ||:  98%|#########8| 64/65 [12:10<00:11, 11.41s/it]
2022-03-30 20:28:54,224 - INFO - tqdm - accuracy: 0.9850, batch_loss: 0.0019, loss: 0.0407 ||: 100%|##########| 65/65 [12:15<00:00,  9.51s/it]
2022-03-30 20:28:54,225 - INFO - tqdm - accuracy: 0.9850, batch_loss: 0.0019, loss: 0.0407 ||: 100%|##########| 65/65 [12:15<00:00, 11.32s/it]
2022-03-30 20:28:57,061 - INFO - allennlp.training.trainer - Validating
2022-03-30 20:28:57,063 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-03-30 20:29:07,281 - INFO - tqdm - accuracy: 0.6829, batch_loss: 4.0202, loss: 1.6535 ||:  36%|###5      | 41/115 [00:10<00:18,  4.02it/s]
2022-03-30 20:29:17,491 - INFO - tqdm - accuracy: 0.6442, batch_loss: 0.0057, loss: 1.7859 ||:  71%|#######1  | 82/115 [00:20<00:08,  3.95it/s]
2022-03-30 20:29:25,836 - INFO - tqdm - accuracy: 0.6288, batch_loss: 4.1007, loss: 1.8879 ||: 100%|##########| 115/115 [00:28<00:00,  3.95it/s]
2022-03-30 20:29:25,836 - INFO - tqdm - accuracy: 0.6288, batch_loss: 4.1007, loss: 1.8879 ||: 100%|##########| 115/115 [00:28<00:00,  4.00it/s]
2022-03-30 20:29:25,836 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-03-30 20:29:25,837 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.985  |     0.629
2022-03-30 20:29:25,838 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-03-30 20:29:25,838 - INFO - allennlp.training.tensorboard_writer - loss               |     0.041  |     1.888
2022-03-30 20:29:25,839 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5793.102  |       N/A
2022-03-30 20:29:31,969 - INFO - allennlp.training.trainer - Epoch duration: 0:12:53.743479
2022-03-30 20:29:31,969 - INFO - allennlp.training.trainer - Estimated training time remaining: 1:30:42
2022-03-30 20:29:31,969 - INFO - allennlp.training.trainer - Epoch 8/14
2022-03-30 20:29:31,970 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-03-30 20:29:31,970 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-03-30 20:29:31,971 - INFO - allennlp.training.trainer - Training
2022-03-30 20:29:31,972 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-03-30 20:29:43,301 - INFO - tqdm - accuracy: 0.9375, batch_loss: 0.0966, loss: 0.0966 ||:   2%|1         | 1/65 [00:11<12:05, 11.33s/it]
2022-03-30 20:29:54,814 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.0054, loss: 0.0510 ||:   3%|3         | 2/65 [00:22<12:00, 11.44s/it]
2022-03-30 20:30:06,429 - INFO - tqdm - accuracy: 0.9792, batch_loss: 0.0042, loss: 0.0354 ||:   5%|4         | 3/65 [00:34<11:54, 11.52s/it]
2022-03-30 20:30:17,880 - INFO - tqdm - accuracy: 0.9766, batch_loss: 0.0350, loss: 0.0353 ||:   6%|6         | 4/65 [00:45<11:41, 11.49s/it]
2022-03-30 20:30:29,252 - INFO - tqdm - accuracy: 0.9812, batch_loss: 0.0263, loss: 0.0335 ||:   8%|7         | 5/65 [00:57<11:26, 11.45s/it]
2022-03-30 20:30:40,586 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0140, loss: 0.0302 ||:   9%|9         | 6/65 [01:08<11:13, 11.41s/it]
2022-03-30 20:30:51,970 - INFO - tqdm - accuracy: 0.9866, batch_loss: 0.0107, loss: 0.0274 ||:  11%|#         | 7/65 [01:19<11:01, 11.40s/it]
2022-03-30 20:31:03,409 - INFO - tqdm - accuracy: 0.9883, batch_loss: 0.0279, loss: 0.0275 ||:  12%|#2        | 8/65 [01:31<10:50, 11.41s/it]
2022-03-30 20:31:14,866 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.0299, loss: 0.0278 ||:  14%|#3        | 9/65 [01:42<10:39, 11.43s/it]
2022-03-30 20:31:26,313 - INFO - tqdm - accuracy: 0.9906, batch_loss: 0.0072, loss: 0.0257 ||:  15%|#5        | 10/65 [01:54<10:28, 11.43s/it]
2022-03-30 20:31:37,762 - INFO - tqdm - accuracy: 0.9886, batch_loss: 0.0506, loss: 0.0280 ||:  17%|#6        | 11/65 [02:05<10:17, 11.44s/it]
2022-03-30 20:31:49,176 - INFO - tqdm - accuracy: 0.9870, batch_loss: 0.0354, loss: 0.0286 ||:  18%|#8        | 12/65 [02:17<10:05, 11.43s/it]
2022-03-30 20:32:00,573 - INFO - tqdm - accuracy: 0.9880, batch_loss: 0.0141, loss: 0.0275 ||:  20%|##        | 13/65 [02:28<09:53, 11.42s/it]
2022-03-30 20:32:11,953 - INFO - tqdm - accuracy: 0.9888, batch_loss: 0.0030, loss: 0.0257 ||:  22%|##1       | 14/65 [02:39<09:41, 11.41s/it]
2022-03-30 20:32:23,337 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.0034, loss: 0.0242 ||:  23%|##3       | 15/65 [02:51<09:30, 11.40s/it]
2022-03-30 20:32:34,724 - INFO - tqdm - accuracy: 0.9883, batch_loss: 0.0438, loss: 0.0255 ||:  25%|##4       | 16/65 [03:02<09:18, 11.40s/it]
2022-03-30 20:32:46,076 - INFO - tqdm - accuracy: 0.9871, batch_loss: 0.0761, loss: 0.0284 ||:  26%|##6       | 17/65 [03:14<09:06, 11.38s/it]
2022-03-30 20:32:57,447 - INFO - tqdm - accuracy: 0.9861, batch_loss: 0.0345, loss: 0.0288 ||:  28%|##7       | 18/65 [03:25<08:54, 11.38s/it]
2022-03-30 20:33:08,854 - INFO - tqdm - accuracy: 0.9868, batch_loss: 0.0178, loss: 0.0282 ||:  29%|##9       | 19/65 [03:36<08:43, 11.39s/it]
2022-03-30 20:33:20,280 - INFO - tqdm - accuracy: 0.9859, batch_loss: 0.1886, loss: 0.0362 ||:  31%|###       | 20/65 [03:48<08:32, 11.40s/it]
2022-03-30 20:33:31,701 - INFO - tqdm - accuracy: 0.9866, batch_loss: 0.0236, loss: 0.0356 ||:  32%|###2      | 21/65 [03:59<08:21, 11.41s/it]
2022-03-30 20:33:43,137 - INFO - tqdm - accuracy: 0.9858, batch_loss: 0.0273, loss: 0.0352 ||:  34%|###3      | 22/65 [04:11<08:10, 11.41s/it]
2022-03-30 20:33:54,544 - INFO - tqdm - accuracy: 0.9864, batch_loss: 0.0240, loss: 0.0348 ||:  35%|###5      | 23/65 [04:22<07:59, 11.41s/it]
2022-03-30 20:34:05,963 - INFO - tqdm - accuracy: 0.9857, batch_loss: 0.0787, loss: 0.0366 ||:  37%|###6      | 24/65 [04:33<07:47, 11.41s/it]
2022-03-30 20:34:17,405 - INFO - tqdm - accuracy: 0.9862, batch_loss: 0.0110, loss: 0.0356 ||:  38%|###8      | 25/65 [04:45<07:36, 11.42s/it]
2022-03-30 20:34:28,839 - INFO - tqdm - accuracy: 0.9856, batch_loss: 0.0687, loss: 0.0368 ||:  40%|####      | 26/65 [04:56<07:25, 11.43s/it]
2022-03-30 20:34:40,017 - INFO - tqdm - accuracy: 0.9850, batch_loss: 0.1116, loss: 0.0396 ||:  42%|####1     | 27/65 [05:08<07:11, 11.35s/it]
2022-03-30 20:34:51,443 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0917, loss: 0.0415 ||:  43%|####3     | 28/65 [05:19<07:00, 11.37s/it]
2022-03-30 20:35:02,864 - INFO - tqdm - accuracy: 0.9849, batch_loss: 0.0026, loss: 0.0401 ||:  45%|####4     | 29/65 [05:30<06:49, 11.39s/it]
2022-03-30 20:35:14,287 - INFO - tqdm - accuracy: 0.9854, batch_loss: 0.0040, loss: 0.0389 ||:  46%|####6     | 30/65 [05:42<06:38, 11.40s/it]
2022-03-30 20:35:25,732 - INFO - tqdm - accuracy: 0.9859, batch_loss: 0.0091, loss: 0.0380 ||:  48%|####7     | 31/65 [05:53<06:28, 11.41s/it]
2022-03-30 20:35:37,175 - INFO - tqdm - accuracy: 0.9863, batch_loss: 0.0054, loss: 0.0369 ||:  49%|####9     | 32/65 [06:05<06:16, 11.42s/it]
2022-03-30 20:35:48,611 - INFO - tqdm - accuracy: 0.9858, batch_loss: 0.0540, loss: 0.0375 ||:  51%|#####     | 33/65 [06:16<06:05, 11.43s/it]
2022-03-30 20:36:00,047 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.1096, loss: 0.0396 ||:  52%|#####2    | 34/65 [06:28<05:54, 11.43s/it]
2022-03-30 20:36:11,490 - INFO - tqdm - accuracy: 0.9848, batch_loss: 0.0023, loss: 0.0385 ||:  54%|#####3    | 35/65 [06:39<05:42, 11.43s/it]
2022-03-30 20:36:22,942 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0564, loss: 0.0390 ||:  55%|#####5    | 36/65 [06:50<05:31, 11.44s/it]
2022-03-30 20:36:34,376 - INFO - tqdm - accuracy: 0.9848, batch_loss: 0.0216, loss: 0.0385 ||:  57%|#####6    | 37/65 [07:02<05:20, 11.44s/it]
2022-03-30 20:36:45,809 - INFO - tqdm - accuracy: 0.9852, batch_loss: 0.0016, loss: 0.0376 ||:  58%|#####8    | 38/65 [07:13<05:08, 11.44s/it]
2022-03-30 20:36:56,986 - INFO - tqdm - accuracy: 0.9856, batch_loss: 0.0105, loss: 0.0369 ||:  60%|######    | 39/65 [07:25<04:55, 11.36s/it]
2022-03-30 20:37:08,433 - INFO - tqdm - accuracy: 0.9852, batch_loss: 0.1123, loss: 0.0388 ||:  62%|######1   | 40/65 [07:36<04:44, 11.38s/it]
2022-03-30 20:37:19,876 - INFO - tqdm - accuracy: 0.9855, batch_loss: 0.0117, loss: 0.0381 ||:  63%|######3   | 41/65 [07:47<04:33, 11.40s/it]
2022-03-30 20:37:31,312 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.1345, loss: 0.0404 ||:  65%|######4   | 42/65 [07:59<04:22, 11.41s/it]
2022-03-30 20:37:42,754 - INFO - tqdm - accuracy: 0.9840, batch_loss: 0.0758, loss: 0.0412 ||:  66%|######6   | 43/65 [08:10<04:11, 11.42s/it]
2022-03-30 20:37:54,184 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0291, loss: 0.0409 ||:  68%|######7   | 44/65 [08:22<03:59, 11.42s/it]
2022-03-30 20:38:05,614 - INFO - tqdm - accuracy: 0.9847, batch_loss: 0.0144, loss: 0.0404 ||:  69%|######9   | 45/65 [08:33<03:48, 11.43s/it]
2022-03-30 20:38:17,060 - INFO - tqdm - accuracy: 0.9851, batch_loss: 0.0031, loss: 0.0395 ||:  71%|#######   | 46/65 [08:45<03:37, 11.43s/it]
2022-03-30 20:38:28,510 - INFO - tqdm - accuracy: 0.9854, batch_loss: 0.0023, loss: 0.0387 ||:  72%|#######2  | 47/65 [08:56<03:25, 11.44s/it]
2022-03-30 20:38:39,935 - INFO - tqdm - accuracy: 0.9857, batch_loss: 0.0082, loss: 0.0381 ||:  74%|#######3  | 48/65 [09:07<03:14, 11.43s/it]
2022-03-30 20:38:51,384 - INFO - tqdm - accuracy: 0.9860, batch_loss: 0.0028, loss: 0.0374 ||:  75%|#######5  | 49/65 [09:19<03:03, 11.44s/it]
2022-03-30 20:39:02,825 - INFO - tqdm - accuracy: 0.9862, batch_loss: 0.0040, loss: 0.0367 ||:  77%|#######6  | 50/65 [09:30<02:51, 11.44s/it]
2022-03-30 20:39:14,277 - INFO - tqdm - accuracy: 0.9865, batch_loss: 0.0288, loss: 0.0366 ||:  78%|#######8  | 51/65 [09:42<02:40, 11.44s/it]
2022-03-30 20:39:25,722 - INFO - tqdm - accuracy: 0.9868, batch_loss: 0.0031, loss: 0.0359 ||:  80%|########  | 52/65 [09:53<02:28, 11.44s/it]
2022-03-30 20:39:37,190 - INFO - tqdm - accuracy: 0.9870, batch_loss: 0.0047, loss: 0.0353 ||:  82%|########1 | 53/65 [10:05<02:17, 11.45s/it]
2022-03-30 20:39:48,637 - INFO - tqdm - accuracy: 0.9873, batch_loss: 0.0029, loss: 0.0347 ||:  83%|########3 | 54/65 [10:16<02:05, 11.45s/it]
2022-03-30 20:40:00,083 - INFO - tqdm - accuracy: 0.9875, batch_loss: 0.0032, loss: 0.0342 ||:  85%|########4 | 55/65 [10:28<01:54, 11.45s/it]
2022-03-30 20:40:11,530 - INFO - tqdm - accuracy: 0.9866, batch_loss: 0.1876, loss: 0.0369 ||:  86%|########6 | 56/65 [10:39<01:43, 11.45s/it]
2022-03-30 20:40:22,985 - INFO - tqdm - accuracy: 0.9868, batch_loss: 0.0094, loss: 0.0364 ||:  88%|########7 | 57/65 [10:51<01:31, 11.45s/it]
2022-03-30 20:40:34,437 - INFO - tqdm - accuracy: 0.9871, batch_loss: 0.0035, loss: 0.0359 ||:  89%|########9 | 58/65 [11:02<01:20, 11.45s/it]
2022-03-30 20:40:45,896 - INFO - tqdm - accuracy: 0.9873, batch_loss: 0.0079, loss: 0.0354 ||:  91%|######### | 59/65 [11:13<01:08, 11.45s/it]
2022-03-30 20:40:57,005 - INFO - tqdm - accuracy: 0.9875, batch_loss: 0.0072, loss: 0.0349 ||:  92%|#########2| 60/65 [11:25<00:56, 11.35s/it]
2022-03-30 20:41:08,466 - INFO - tqdm - accuracy: 0.9872, batch_loss: 0.0366, loss: 0.0349 ||:  94%|#########3| 61/65 [11:36<00:45, 11.38s/it]
2022-03-30 20:41:19,908 - INFO - tqdm - accuracy: 0.9874, batch_loss: 0.0210, loss: 0.0347 ||:  95%|#########5| 62/65 [11:47<00:34, 11.40s/it]
2022-03-30 20:41:31,354 - INFO - tqdm - accuracy: 0.9876, batch_loss: 0.0035, loss: 0.0342 ||:  97%|#########6| 63/65 [11:59<00:22, 11.41s/it]
2022-03-30 20:41:42,805 - INFO - tqdm - accuracy: 0.9878, batch_loss: 0.0167, loss: 0.0339 ||:  98%|#########8| 64/65 [12:10<00:11, 11.43s/it]
2022-03-30 20:41:47,766 - INFO - tqdm - accuracy: 0.9879, batch_loss: 0.0024, loss: 0.0335 ||: 100%|##########| 65/65 [12:15<00:00,  9.49s/it]
2022-03-30 20:41:47,766 - INFO - tqdm - accuracy: 0.9879, batch_loss: 0.0024, loss: 0.0335 ||: 100%|##########| 65/65 [12:15<00:00, 11.32s/it]
2022-03-30 20:41:50,606 - INFO - allennlp.training.trainer - Validating
2022-03-30 20:41:50,607 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-03-30 20:42:00,629 - INFO - tqdm - accuracy: 0.6125, batch_loss: 0.6167, loss: 2.0844 ||:  35%|###4      | 40/115 [00:10<00:18,  3.97it/s]
2022-03-30 20:42:10,730 - INFO - tqdm - accuracy: 0.6125, batch_loss: 2.7512, loss: 2.1024 ||:  70%|######9   | 80/115 [00:20<00:08,  3.95it/s]
2022-03-30 20:42:19,395 - INFO - tqdm - accuracy: 0.6201, batch_loss: 0.0010, loss: 1.9834 ||: 100%|##########| 115/115 [00:28<00:00,  4.01it/s]
2022-03-30 20:42:19,396 - INFO - tqdm - accuracy: 0.6201, batch_loss: 0.0010, loss: 1.9834 ||: 100%|##########| 115/115 [00:28<00:00,  3.99it/s]
2022-03-30 20:42:19,396 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-03-30 20:42:19,396 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.988  |     0.620
2022-03-30 20:42:19,396 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-03-30 20:42:19,397 - INFO - allennlp.training.tensorboard_writer - loss               |     0.033  |     1.983
2022-03-30 20:42:19,397 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5793.102  |       N/A
2022-03-30 20:42:25,640 - INFO - allennlp.training.trainer - Epoch duration: 0:12:53.670619
2022-03-30 20:42:25,640 - INFO - allennlp.training.trainer - Estimated training time remaining: 1:17:42
2022-03-30 20:42:25,640 - INFO - allennlp.training.trainer - Epoch 9/14
2022-03-30 20:42:25,641 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-03-30 20:42:25,641 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-03-30 20:42:25,642 - INFO - allennlp.training.trainer - Training
2022-03-30 20:42:25,642 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-03-30 20:42:36,973 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0029, loss: 0.0029 ||:   2%|1         | 1/65 [00:11<12:05, 11.33s/it]
2022-03-30 20:42:48,521 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0062, loss: 0.0045 ||:   3%|3         | 2/65 [00:22<12:01, 11.46s/it]
2022-03-30 20:43:00,149 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0070, loss: 0.0053 ||:   5%|4         | 3/65 [00:34<11:55, 11.54s/it]
2022-03-30 20:43:11,490 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0027, loss: 0.0047 ||:   6%|6         | 4/65 [00:45<11:38, 11.46s/it]
2022-03-30 20:43:22,890 - INFO - tqdm - accuracy: 0.9938, batch_loss: 0.0625, loss: 0.0163 ||:   8%|7         | 5/65 [00:57<11:26, 11.44s/it]
2022-03-30 20:43:34,242 - INFO - tqdm - accuracy: 0.9948, batch_loss: 0.0020, loss: 0.0139 ||:   9%|9         | 6/65 [01:08<11:13, 11.41s/it]
2022-03-30 20:43:45,618 - INFO - tqdm - accuracy: 0.9955, batch_loss: 0.0018, loss: 0.0122 ||:  11%|#         | 7/65 [01:19<11:01, 11.40s/it]
2022-03-30 20:43:57,038 - INFO - tqdm - accuracy: 0.9961, batch_loss: 0.0027, loss: 0.0110 ||:  12%|#2        | 8/65 [01:31<10:50, 11.41s/it]
2022-03-30 20:44:08,504 - INFO - tqdm - accuracy: 0.9965, batch_loss: 0.0014, loss: 0.0099 ||:  14%|#3        | 9/65 [01:42<10:39, 11.42s/it]
2022-03-30 20:44:19,985 - INFO - tqdm - accuracy: 0.9969, batch_loss: 0.0016, loss: 0.0091 ||:  15%|#5        | 10/65 [01:54<10:29, 11.44s/it]
2022-03-30 20:44:31,441 - INFO - tqdm - accuracy: 0.9972, batch_loss: 0.0211, loss: 0.0102 ||:  17%|#6        | 11/65 [02:05<10:18, 11.45s/it]
2022-03-30 20:44:42,879 - INFO - tqdm - accuracy: 0.9948, batch_loss: 0.0414, loss: 0.0128 ||:  18%|#8        | 12/65 [02:17<10:06, 11.44s/it]
2022-03-30 20:44:54,310 - INFO - tqdm - accuracy: 0.9952, batch_loss: 0.0014, loss: 0.0119 ||:  20%|##        | 13/65 [02:28<09:54, 11.44s/it]
2022-03-30 20:45:05,716 - INFO - tqdm - accuracy: 0.9955, batch_loss: 0.0027, loss: 0.0113 ||:  22%|##1       | 14/65 [02:40<09:42, 11.43s/it]
2022-03-30 20:45:17,586 - INFO - tqdm - accuracy: 0.9958, batch_loss: 0.0030, loss: 0.0107 ||:  23%|##3       | 15/65 [02:51<09:38, 11.56s/it]
2022-03-30 20:45:28,934 - INFO - tqdm - accuracy: 0.9961, batch_loss: 0.0022, loss: 0.0102 ||:  25%|##4       | 16/65 [03:03<09:23, 11.50s/it]
2022-03-30 20:45:40,344 - INFO - tqdm - accuracy: 0.9963, batch_loss: 0.0023, loss: 0.0097 ||:  26%|##6       | 17/65 [03:14<09:10, 11.47s/it]
2022-03-30 20:45:51,749 - INFO - tqdm - accuracy: 0.9965, batch_loss: 0.0091, loss: 0.0097 ||:  28%|##7       | 18/65 [03:26<08:58, 11.45s/it]
2022-03-30 20:46:03,171 - INFO - tqdm - accuracy: 0.9967, batch_loss: 0.0093, loss: 0.0097 ||:  29%|##9       | 19/65 [03:37<08:46, 11.44s/it]
2022-03-30 20:46:14,592 - INFO - tqdm - accuracy: 0.9953, batch_loss: 0.1202, loss: 0.0152 ||:  31%|###       | 20/65 [03:48<08:34, 11.44s/it]
2022-03-30 20:46:26,022 - INFO - tqdm - accuracy: 0.9955, batch_loss: 0.0026, loss: 0.0146 ||:  32%|###2      | 21/65 [04:00<08:23, 11.43s/it]
2022-03-30 20:46:37,461 - INFO - tqdm - accuracy: 0.9957, batch_loss: 0.0220, loss: 0.0149 ||:  34%|###3      | 22/65 [04:11<08:11, 11.44s/it]
2022-03-30 20:46:48,904 - INFO - tqdm - accuracy: 0.9959, batch_loss: 0.0015, loss: 0.0143 ||:  35%|###5      | 23/65 [04:23<08:00, 11.44s/it]
2022-03-30 20:47:00,350 - INFO - tqdm - accuracy: 0.9948, batch_loss: 0.0598, loss: 0.0162 ||:  37%|###6      | 24/65 [04:34<07:49, 11.44s/it]
2022-03-30 20:47:11,795 - INFO - tqdm - accuracy: 0.9950, batch_loss: 0.0065, loss: 0.0158 ||:  38%|###8      | 25/65 [04:46<07:37, 11.44s/it]
2022-03-30 20:47:23,233 - INFO - tqdm - accuracy: 0.9952, batch_loss: 0.0025, loss: 0.0153 ||:  40%|####      | 26/65 [04:57<07:26, 11.44s/it]
2022-03-30 20:47:34,678 - INFO - tqdm - accuracy: 0.9954, batch_loss: 0.0020, loss: 0.0148 ||:  42%|####1     | 27/65 [05:09<07:14, 11.44s/it]
2022-03-30 20:47:46,125 - INFO - tqdm - accuracy: 0.9944, batch_loss: 0.0269, loss: 0.0153 ||:  43%|####3     | 28/65 [05:20<07:03, 11.44s/it]
2022-03-30 20:47:57,576 - INFO - tqdm - accuracy: 0.9946, batch_loss: 0.0071, loss: 0.0150 ||:  45%|####4     | 29/65 [05:31<06:52, 11.45s/it]
2022-03-30 20:48:08,902 - INFO - tqdm - accuracy: 0.9938, batch_loss: 0.0376, loss: 0.0157 ||:  46%|####6     | 30/65 [05:43<06:39, 11.41s/it]
2022-03-30 20:48:20,357 - INFO - tqdm - accuracy: 0.9940, batch_loss: 0.0157, loss: 0.0157 ||:  48%|####7     | 31/65 [05:54<06:28, 11.42s/it]
2022-03-30 20:48:31,815 - INFO - tqdm - accuracy: 0.9941, batch_loss: 0.0025, loss: 0.0153 ||:  49%|####9     | 32/65 [06:06<06:17, 11.43s/it]
2022-03-30 20:48:43,259 - INFO - tqdm - accuracy: 0.9943, batch_loss: 0.0214, loss: 0.0155 ||:  51%|#####     | 33/65 [06:17<06:05, 11.44s/it]
2022-03-30 20:48:54,700 - INFO - tqdm - accuracy: 0.9945, batch_loss: 0.0148, loss: 0.0155 ||:  52%|#####2    | 34/65 [06:29<05:54, 11.44s/it]
2022-03-30 20:49:05,820 - INFO - tqdm - accuracy: 0.9946, batch_loss: 0.0024, loss: 0.0151 ||:  54%|#####3    | 35/65 [06:40<05:40, 11.34s/it]
2022-03-30 20:49:17,275 - INFO - tqdm - accuracy: 0.9948, batch_loss: 0.0022, loss: 0.0148 ||:  55%|#####5    | 36/65 [06:51<05:29, 11.38s/it]
2022-03-30 20:49:28,714 - INFO - tqdm - accuracy: 0.9949, batch_loss: 0.0254, loss: 0.0150 ||:  57%|#####6    | 37/65 [07:03<05:19, 11.40s/it]
2022-03-30 20:49:40,176 - INFO - tqdm - accuracy: 0.9951, batch_loss: 0.0085, loss: 0.0149 ||:  58%|#####8    | 38/65 [07:14<05:08, 11.42s/it]
2022-03-30 20:49:51,629 - INFO - tqdm - accuracy: 0.9952, batch_loss: 0.0031, loss: 0.0146 ||:  60%|######    | 39/65 [07:25<04:57, 11.43s/it]
2022-03-30 20:50:03,091 - INFO - tqdm - accuracy: 0.9953, batch_loss: 0.0016, loss: 0.0142 ||:  62%|######1   | 40/65 [07:37<04:45, 11.44s/it]
2022-03-30 20:50:14,540 - INFO - tqdm - accuracy: 0.9954, batch_loss: 0.0037, loss: 0.0140 ||:  63%|######3   | 41/65 [07:48<04:34, 11.44s/it]
2022-03-30 20:50:26,005 - INFO - tqdm - accuracy: 0.9955, batch_loss: 0.0119, loss: 0.0139 ||:  65%|######4   | 42/65 [08:00<04:23, 11.45s/it]
2022-03-30 20:50:37,492 - INFO - tqdm - accuracy: 0.9956, batch_loss: 0.0009, loss: 0.0136 ||:  66%|######6   | 43/65 [08:11<04:12, 11.46s/it]
2022-03-30 20:50:48,974 - INFO - tqdm - accuracy: 0.9957, batch_loss: 0.0040, loss: 0.0134 ||:  68%|######7   | 44/65 [08:23<04:00, 11.47s/it]
2022-03-30 20:51:00,433 - INFO - tqdm - accuracy: 0.9958, batch_loss: 0.0019, loss: 0.0132 ||:  69%|######9   | 45/65 [08:34<03:49, 11.46s/it]
2022-03-30 20:51:11,906 - INFO - tqdm - accuracy: 0.9952, batch_loss: 0.0425, loss: 0.0138 ||:  71%|#######   | 46/65 [08:46<03:37, 11.47s/it]
2022-03-30 20:51:23,366 - INFO - tqdm - accuracy: 0.9953, batch_loss: 0.0096, loss: 0.0137 ||:  72%|#######2  | 47/65 [08:57<03:26, 11.46s/it]
2022-03-30 20:51:34,839 - INFO - tqdm - accuracy: 0.9948, batch_loss: 0.0499, loss: 0.0145 ||:  74%|#######3  | 48/65 [09:09<03:14, 11.47s/it]
2022-03-30 20:51:46,313 - INFO - tqdm - accuracy: 0.9949, batch_loss: 0.0039, loss: 0.0142 ||:  75%|#######5  | 49/65 [09:20<03:03, 11.47s/it]
2022-03-30 20:51:57,537 - INFO - tqdm - accuracy: 0.9950, batch_loss: 0.0028, loss: 0.0140 ||:  77%|#######6  | 50/65 [09:31<02:50, 11.40s/it]
2022-03-30 20:52:09,011 - INFO - tqdm - accuracy: 0.9951, batch_loss: 0.0039, loss: 0.0138 ||:  78%|#######8  | 51/65 [09:43<02:39, 11.42s/it]
2022-03-30 20:52:20,498 - INFO - tqdm - accuracy: 0.9952, batch_loss: 0.0340, loss: 0.0142 ||:  80%|########  | 52/65 [09:54<02:28, 11.44s/it]
2022-03-30 20:52:31,968 - INFO - tqdm - accuracy: 0.9953, batch_loss: 0.0049, loss: 0.0140 ||:  82%|########1 | 53/65 [10:06<02:17, 11.45s/it]
2022-03-30 20:52:43,436 - INFO - tqdm - accuracy: 0.9954, batch_loss: 0.0017, loss: 0.0138 ||:  83%|########3 | 54/65 [10:17<02:05, 11.45s/it]
2022-03-30 20:52:54,896 - INFO - tqdm - accuracy: 0.9955, batch_loss: 0.0016, loss: 0.0136 ||:  85%|########4 | 55/65 [10:29<01:54, 11.46s/it]
2022-03-30 20:53:06,359 - INFO - tqdm - accuracy: 0.9950, batch_loss: 0.0371, loss: 0.0140 ||:  86%|########6 | 56/65 [10:40<01:43, 11.46s/it]
2022-03-30 20:53:17,811 - INFO - tqdm - accuracy: 0.9951, batch_loss: 0.0040, loss: 0.0138 ||:  88%|########7 | 57/65 [10:52<01:31, 11.46s/it]
2022-03-30 20:53:29,257 - INFO - tqdm - accuracy: 0.9951, batch_loss: 0.0107, loss: 0.0138 ||:  89%|########9 | 58/65 [11:03<01:20, 11.45s/it]
2022-03-30 20:53:40,694 - INFO - tqdm - accuracy: 0.9952, batch_loss: 0.0050, loss: 0.0136 ||:  91%|######### | 59/65 [11:15<01:08, 11.45s/it]
2022-03-30 20:53:52,148 - INFO - tqdm - accuracy: 0.9953, batch_loss: 0.0033, loss: 0.0135 ||:  92%|#########2| 60/65 [11:26<00:57, 11.45s/it]
2022-03-30 20:54:03,599 - INFO - tqdm - accuracy: 0.9954, batch_loss: 0.0243, loss: 0.0136 ||:  94%|#########3| 61/65 [11:37<00:45, 11.45s/it]
2022-03-30 20:54:15,022 - INFO - tqdm - accuracy: 0.9955, batch_loss: 0.0091, loss: 0.0136 ||:  95%|#########5| 62/65 [11:49<00:34, 11.44s/it]
2022-03-30 20:54:26,443 - INFO - tqdm - accuracy: 0.9955, batch_loss: 0.0020, loss: 0.0134 ||:  97%|#########6| 63/65 [12:00<00:22, 11.44s/it]
2022-03-30 20:54:37,866 - INFO - tqdm - accuracy: 0.9956, batch_loss: 0.0017, loss: 0.0132 ||:  98%|#########8| 64/65 [12:12<00:11, 11.43s/it]
2022-03-30 20:54:42,937 - INFO - tqdm - accuracy: 0.9956, batch_loss: 0.0071, loss: 0.0131 ||: 100%|##########| 65/65 [12:17<00:00,  9.52s/it]
2022-03-30 20:54:42,937 - INFO - tqdm - accuracy: 0.9956, batch_loss: 0.0071, loss: 0.0131 ||: 100%|##########| 65/65 [12:17<00:00, 11.34s/it]
2022-03-30 20:54:45,827 - INFO - allennlp.training.trainer - Validating
2022-03-30 20:54:45,829 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-03-30 20:54:55,883 - INFO - tqdm - accuracy: 0.6049, batch_loss: 3.5413, loss: 2.4929 ||:  36%|###5      | 41/115 [00:10<00:18,  4.00it/s]
2022-03-30 20:55:05,956 - INFO - tqdm - accuracy: 0.6149, batch_loss: 2.3159, loss: 2.3092 ||:  70%|#######   | 81/115 [00:20<00:08,  3.98it/s]
2022-03-30 20:55:14,567 - INFO - tqdm - accuracy: 0.6026, batch_loss: 0.0155, loss: 2.2365 ||: 100%|##########| 115/115 [00:28<00:00,  3.94it/s]
2022-03-30 20:55:14,568 - INFO - tqdm - accuracy: 0.6026, batch_loss: 0.0155, loss: 2.2365 ||: 100%|##########| 115/115 [00:28<00:00,  4.00it/s]
2022-03-30 20:55:14,568 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-03-30 20:55:14,568 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.996  |     0.603
2022-03-30 20:55:14,569 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-03-30 20:55:14,570 - INFO - allennlp.training.tensorboard_writer - loss               |     0.013  |     2.237
2022-03-30 20:55:14,571 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5793.102  |       N/A
2022-03-30 20:55:20,754 - INFO - allennlp.training.trainer - Epoch duration: 0:12:55.113671
2022-03-30 20:55:20,754 - INFO - allennlp.training.trainer - Estimated training time remaining: 1:04:44
2022-03-30 20:55:20,754 - INFO - allennlp.training.trainer - Epoch 10/14
2022-03-30 20:55:20,755 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-03-30 20:55:20,755 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-03-30 20:55:20,756 - INFO - allennlp.training.trainer - Training
2022-03-30 20:55:20,756 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-03-30 20:55:32,119 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0070, loss: 0.0070 ||:   2%|1         | 1/65 [00:11<12:07, 11.36s/it]
2022-03-30 20:55:43,429 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0037, loss: 0.0054 ||:   3%|3         | 2/65 [00:22<11:53, 11.33s/it]
2022-03-30 20:55:55,076 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0018, loss: 0.0042 ||:   5%|4         | 3/65 [00:34<11:51, 11.48s/it]
2022-03-30 20:56:06,579 - INFO - tqdm - accuracy: 0.9922, batch_loss: 0.0635, loss: 0.0190 ||:   6%|6         | 4/65 [00:45<11:40, 11.49s/it]
2022-03-30 20:56:17,833 - INFO - tqdm - accuracy: 0.9938, batch_loss: 0.0008, loss: 0.0154 ||:   8%|7         | 5/65 [00:57<11:24, 11.40s/it]
2022-03-30 20:56:29,196 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.1031, loss: 0.0300 ||:   9%|9         | 6/65 [01:08<11:11, 11.39s/it]
2022-03-30 20:56:40,612 - INFO - tqdm - accuracy: 0.9911, batch_loss: 0.0027, loss: 0.0261 ||:  11%|#         | 7/65 [01:19<11:01, 11.40s/it]
2022-03-30 20:56:52,076 - INFO - tqdm - accuracy: 0.9922, batch_loss: 0.0011, loss: 0.0230 ||:  12%|#2        | 8/65 [01:31<10:50, 11.42s/it]
2022-03-30 20:57:03,571 - INFO - tqdm - accuracy: 0.9931, batch_loss: 0.0015, loss: 0.0206 ||:  14%|#3        | 9/65 [01:42<10:40, 11.44s/it]
2022-03-30 20:57:15,054 - INFO - tqdm - accuracy: 0.9938, batch_loss: 0.0019, loss: 0.0187 ||:  15%|#5        | 10/65 [01:54<10:30, 11.46s/it]
2022-03-30 20:57:26,488 - INFO - tqdm - accuracy: 0.9915, batch_loss: 0.0846, loss: 0.0247 ||:  17%|#6        | 11/65 [02:05<10:18, 11.45s/it]
2022-03-30 20:57:37,908 - INFO - tqdm - accuracy: 0.9922, batch_loss: 0.0183, loss: 0.0242 ||:  18%|#8        | 12/65 [02:17<10:06, 11.44s/it]
2022-03-30 20:57:49,325 - INFO - tqdm - accuracy: 0.9928, batch_loss: 0.0034, loss: 0.0226 ||:  20%|##        | 13/65 [02:28<09:54, 11.43s/it]
2022-03-30 20:58:00,717 - INFO - tqdm - accuracy: 0.9911, batch_loss: 0.0284, loss: 0.0230 ||:  22%|##1       | 14/65 [02:39<09:42, 11.42s/it]
2022-03-30 20:58:12,122 - INFO - tqdm - accuracy: 0.9917, batch_loss: 0.0011, loss: 0.0215 ||:  23%|##3       | 15/65 [02:51<09:30, 11.42s/it]
2022-03-30 20:58:23,532 - INFO - tqdm - accuracy: 0.9922, batch_loss: 0.0019, loss: 0.0203 ||:  25%|##4       | 16/65 [03:02<09:19, 11.41s/it]
2022-03-30 20:58:34,940 - INFO - tqdm - accuracy: 0.9926, batch_loss: 0.0007, loss: 0.0191 ||:  26%|##6       | 17/65 [03:14<09:07, 11.41s/it]
2022-03-30 20:58:46,354 - INFO - tqdm - accuracy: 0.9931, batch_loss: 0.0190, loss: 0.0191 ||:  28%|##7       | 18/65 [03:25<08:56, 11.41s/it]
2022-03-30 20:58:57,800 - INFO - tqdm - accuracy: 0.9934, batch_loss: 0.0047, loss: 0.0184 ||:  29%|##9       | 19/65 [03:37<08:45, 11.42s/it]
2022-03-30 20:59:09,265 - INFO - tqdm - accuracy: 0.9938, batch_loss: 0.0119, loss: 0.0181 ||:  31%|###       | 20/65 [03:48<08:34, 11.44s/it]
2022-03-30 20:59:20,712 - INFO - tqdm - accuracy: 0.9940, batch_loss: 0.0022, loss: 0.0173 ||:  32%|###2      | 21/65 [03:59<08:23, 11.44s/it]
2022-03-30 20:59:32,175 - INFO - tqdm - accuracy: 0.9943, batch_loss: 0.0039, loss: 0.0167 ||:  34%|###3      | 22/65 [04:11<08:12, 11.45s/it]
2022-03-30 20:59:43,279 - INFO - tqdm - accuracy: 0.9946, batch_loss: 0.0055, loss: 0.0162 ||:  35%|###5      | 23/65 [04:22<07:56, 11.34s/it]
2022-03-30 20:59:54,695 - INFO - tqdm - accuracy: 0.9948, batch_loss: 0.0015, loss: 0.0156 ||:  37%|###6      | 24/65 [04:33<07:45, 11.37s/it]
2022-03-30 21:00:06,139 - INFO - tqdm - accuracy: 0.9950, batch_loss: 0.0111, loss: 0.0154 ||:  38%|###8      | 25/65 [04:45<07:35, 11.39s/it]
2022-03-30 21:00:17,466 - INFO - tqdm - accuracy: 0.9940, batch_loss: 0.1043, loss: 0.0188 ||:  40%|####      | 26/65 [04:56<07:23, 11.37s/it]
2022-03-30 21:00:28,893 - INFO - tqdm - accuracy: 0.9942, batch_loss: 0.0164, loss: 0.0187 ||:  42%|####1     | 27/65 [05:08<07:12, 11.39s/it]
2022-03-30 21:00:40,340 - INFO - tqdm - accuracy: 0.9944, batch_loss: 0.0042, loss: 0.0182 ||:  43%|####3     | 28/65 [05:19<07:01, 11.41s/it]
2022-03-30 21:00:51,789 - INFO - tqdm - accuracy: 0.9946, batch_loss: 0.0018, loss: 0.0176 ||:  45%|####4     | 29/65 [05:31<06:51, 11.42s/it]
2022-03-30 21:01:03,221 - INFO - tqdm - accuracy: 0.9948, batch_loss: 0.0039, loss: 0.0172 ||:  46%|####6     | 30/65 [05:42<06:39, 11.42s/it]
2022-03-30 21:01:14,672 - INFO - tqdm - accuracy: 0.9950, batch_loss: 0.0013, loss: 0.0167 ||:  48%|####7     | 31/65 [05:53<06:28, 11.43s/it]
2022-03-30 21:01:26,115 - INFO - tqdm - accuracy: 0.9951, batch_loss: 0.0022, loss: 0.0162 ||:  49%|####9     | 32/65 [06:05<06:17, 11.43s/it]
2022-03-30 21:01:37,543 - INFO - tqdm - accuracy: 0.9953, batch_loss: 0.0028, loss: 0.0158 ||:  51%|#####     | 33/65 [06:16<06:05, 11.43s/it]
2022-03-30 21:01:48,973 - INFO - tqdm - accuracy: 0.9945, batch_loss: 0.1908, loss: 0.0210 ||:  52%|#####2    | 34/65 [06:28<05:54, 11.43s/it]
2022-03-30 21:02:00,379 - INFO - tqdm - accuracy: 0.9946, batch_loss: 0.0013, loss: 0.0204 ||:  54%|#####3    | 35/65 [06:39<05:42, 11.42s/it]
2022-03-30 21:02:11,804 - INFO - tqdm - accuracy: 0.9948, batch_loss: 0.0008, loss: 0.0199 ||:  55%|#####5    | 36/65 [06:51<05:31, 11.42s/it]
2022-03-30 21:02:23,234 - INFO - tqdm - accuracy: 0.9949, batch_loss: 0.0008, loss: 0.0193 ||:  57%|#####6    | 37/65 [07:02<05:19, 11.43s/it]
2022-03-30 21:02:34,664 - INFO - tqdm - accuracy: 0.9951, batch_loss: 0.0040, loss: 0.0189 ||:  58%|#####8    | 38/65 [07:13<05:08, 11.43s/it]
2022-03-30 21:02:46,093 - INFO - tqdm - accuracy: 0.9952, batch_loss: 0.0013, loss: 0.0185 ||:  60%|######    | 39/65 [07:25<04:57, 11.43s/it]
2022-03-30 21:02:57,491 - INFO - tqdm - accuracy: 0.9953, batch_loss: 0.0035, loss: 0.0181 ||:  62%|######1   | 40/65 [07:36<04:45, 11.42s/it]
2022-03-30 21:03:08,933 - INFO - tqdm - accuracy: 0.9954, batch_loss: 0.0044, loss: 0.0178 ||:  63%|######3   | 41/65 [07:48<04:34, 11.43s/it]
2022-03-30 21:03:20,372 - INFO - tqdm - accuracy: 0.9955, batch_loss: 0.0048, loss: 0.0175 ||:  65%|######4   | 42/65 [07:59<04:22, 11.43s/it]
2022-03-30 21:03:31,818 - INFO - tqdm - accuracy: 0.9956, batch_loss: 0.0046, loss: 0.0172 ||:  66%|######6   | 43/65 [08:11<04:11, 11.43s/it]
2022-03-30 21:03:43,268 - INFO - tqdm - accuracy: 0.9957, batch_loss: 0.0010, loss: 0.0168 ||:  68%|######7   | 44/65 [08:22<04:00, 11.44s/it]
2022-03-30 21:03:54,731 - INFO - tqdm - accuracy: 0.9958, batch_loss: 0.0027, loss: 0.0165 ||:  69%|######9   | 45/65 [08:33<03:48, 11.45s/it]
2022-03-30 21:04:06,179 - INFO - tqdm - accuracy: 0.9959, batch_loss: 0.0016, loss: 0.0162 ||:  71%|#######   | 46/65 [08:45<03:37, 11.45s/it]
2022-03-30 21:04:17,646 - INFO - tqdm - accuracy: 0.9960, batch_loss: 0.0030, loss: 0.0159 ||:  72%|#######2  | 47/65 [08:56<03:26, 11.45s/it]
2022-03-30 21:04:29,114 - INFO - tqdm - accuracy: 0.9954, batch_loss: 0.0669, loss: 0.0169 ||:  74%|#######3  | 48/65 [09:08<03:14, 11.46s/it]
2022-03-30 21:04:40,558 - INFO - tqdm - accuracy: 0.9955, batch_loss: 0.0014, loss: 0.0166 ||:  75%|#######5  | 49/65 [09:19<03:03, 11.45s/it]
2022-03-30 21:04:52,448 - INFO - tqdm - accuracy: 0.9956, batch_loss: 0.0028, loss: 0.0164 ||:  77%|#######6  | 50/65 [09:31<02:53, 11.58s/it]
2022-03-30 21:05:03,850 - INFO - tqdm - accuracy: 0.9957, batch_loss: 0.0010, loss: 0.0161 ||:  78%|#######8  | 51/65 [09:43<02:41, 11.53s/it]
2022-03-30 21:05:15,276 - INFO - tqdm - accuracy: 0.9958, batch_loss: 0.0019, loss: 0.0158 ||:  80%|########  | 52/65 [09:54<02:29, 11.50s/it]
2022-03-30 21:05:26,717 - INFO - tqdm - accuracy: 0.9959, batch_loss: 0.0027, loss: 0.0155 ||:  82%|########1 | 53/65 [10:05<02:17, 11.48s/it]
2022-03-30 21:05:38,160 - INFO - tqdm - accuracy: 0.9959, batch_loss: 0.0021, loss: 0.0153 ||:  83%|########3 | 54/65 [10:17<02:06, 11.47s/it]
2022-03-30 21:05:49,587 - INFO - tqdm - accuracy: 0.9955, batch_loss: 0.0465, loss: 0.0159 ||:  85%|########4 | 55/65 [10:28<01:54, 11.46s/it]
2022-03-30 21:06:01,002 - INFO - tqdm - accuracy: 0.9955, batch_loss: 0.0019, loss: 0.0156 ||:  86%|########6 | 56/65 [10:40<01:42, 11.44s/it]
2022-03-30 21:06:12,422 - INFO - tqdm - accuracy: 0.9956, batch_loss: 0.0191, loss: 0.0157 ||:  88%|########7 | 57/65 [10:51<01:31, 11.44s/it]
2022-03-30 21:06:23,827 - INFO - tqdm - accuracy: 0.9957, batch_loss: 0.0038, loss: 0.0155 ||:  89%|########9 | 58/65 [11:03<01:19, 11.43s/it]
2022-03-30 21:06:35,249 - INFO - tqdm - accuracy: 0.9952, batch_loss: 0.0933, loss: 0.0168 ||:  91%|######### | 59/65 [11:14<01:08, 11.43s/it]
2022-03-30 21:06:46,665 - INFO - tqdm - accuracy: 0.9953, batch_loss: 0.0071, loss: 0.0166 ||:  92%|#########2| 60/65 [11:25<00:57, 11.42s/it]
2022-03-30 21:06:58,066 - INFO - tqdm - accuracy: 0.9954, batch_loss: 0.0010, loss: 0.0164 ||:  94%|#########3| 61/65 [11:37<00:45, 11.42s/it]
2022-03-30 21:07:09,510 - INFO - tqdm - accuracy: 0.9955, batch_loss: 0.0021, loss: 0.0161 ||:  95%|#########5| 62/65 [11:48<00:34, 11.42s/it]
2022-03-30 21:07:20,924 - INFO - tqdm - accuracy: 0.9955, batch_loss: 0.0011, loss: 0.0159 ||:  97%|#########6| 63/65 [12:00<00:22, 11.42s/it]
2022-03-30 21:07:32,351 - INFO - tqdm - accuracy: 0.9951, batch_loss: 0.0975, loss: 0.0172 ||:  98%|#########8| 64/65 [12:11<00:11, 11.42s/it]
2022-03-30 21:07:37,434 - INFO - tqdm - accuracy: 0.9951, batch_loss: 0.0008, loss: 0.0169 ||: 100%|##########| 65/65 [12:16<00:00,  9.52s/it]
2022-03-30 21:07:37,435 - INFO - tqdm - accuracy: 0.9951, batch_loss: 0.0008, loss: 0.0169 ||: 100%|##########| 65/65 [12:16<00:00, 11.33s/it]
2022-03-30 21:07:40,269 - INFO - allennlp.training.trainer - Validating
2022-03-30 21:07:40,270 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-03-30 21:07:50,318 - INFO - tqdm - accuracy: 0.6125, batch_loss: 0.0001, loss: 1.9607 ||:  35%|###4      | 40/115 [00:10<00:18,  4.00it/s]
2022-03-30 21:08:00,461 - INFO - tqdm - accuracy: 0.5466, batch_loss: 0.5135, loss: 2.2928 ||:  70%|#######   | 81/115 [00:20<00:08,  3.97it/s]
2022-03-30 21:08:09,076 - INFO - tqdm - accuracy: 0.5721, batch_loss: 4.6604, loss: 2.3304 ||: 100%|##########| 115/115 [00:28<00:00,  3.94it/s]
2022-03-30 21:08:09,077 - INFO - tqdm - accuracy: 0.5721, batch_loss: 4.6604, loss: 2.3304 ||: 100%|##########| 115/115 [00:28<00:00,  3.99it/s]
2022-03-30 21:08:09,077 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-03-30 21:08:09,077 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.995  |     0.572
2022-03-30 21:08:09,078 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-03-30 21:08:09,079 - INFO - allennlp.training.tensorboard_writer - loss               |     0.017  |     2.330
2022-03-30 21:08:09,080 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5793.102  |       N/A
2022-03-30 21:08:15,261 - INFO - allennlp.training.trainer - Epoch duration: 0:12:54.506328
2022-03-30 21:08:15,261 - INFO - allennlp.training.trainer - Estimated training time remaining: 0:51:46
2022-03-30 21:08:15,261 - INFO - allennlp.training.trainer - Epoch 11/14
2022-03-30 21:08:15,261 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-03-30 21:08:15,262 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-03-30 21:08:15,263 - INFO - allennlp.training.trainer - Training
2022-03-30 21:08:15,263 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-03-30 21:08:26,621 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0015, loss: 0.0015 ||:   2%|1         | 1/65 [00:11<12:06, 11.36s/it]
2022-03-30 21:08:38,204 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0793, loss: 0.0404 ||:   3%|3         | 2/65 [00:22<12:03, 11.49s/it]
2022-03-30 21:08:49,830 - INFO - tqdm - accuracy: 0.9792, batch_loss: 0.1580, loss: 0.0796 ||:   5%|4         | 3/65 [00:34<11:56, 11.55s/it]
2022-03-30 21:09:01,296 - INFO - tqdm - accuracy: 0.9766, batch_loss: 0.2027, loss: 0.1104 ||:   6%|6         | 4/65 [00:46<11:42, 11.52s/it]
2022-03-30 21:09:12,667 - INFO - tqdm - accuracy: 0.9812, batch_loss: 0.0078, loss: 0.0899 ||:   8%|7         | 5/65 [00:57<11:27, 11.47s/it]
2022-03-30 21:09:24,021 - INFO - tqdm - accuracy: 0.9792, batch_loss: 0.0364, loss: 0.0810 ||:   9%|9         | 6/65 [01:08<11:14, 11.43s/it]
2022-03-30 21:09:35,413 - INFO - tqdm - accuracy: 0.9821, batch_loss: 0.0025, loss: 0.0697 ||:  11%|#         | 7/65 [01:20<11:02, 11.42s/it]
2022-03-30 21:09:46,865 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0014, loss: 0.0612 ||:  12%|#2        | 8/65 [01:31<10:51, 11.43s/it]
2022-03-30 21:09:58,350 - INFO - tqdm - accuracy: 0.9861, batch_loss: 0.0011, loss: 0.0545 ||:  14%|#3        | 9/65 [01:43<10:40, 11.45s/it]
2022-03-30 21:10:09,819 - INFO - tqdm - accuracy: 0.9875, batch_loss: 0.0012, loss: 0.0492 ||:  15%|#5        | 10/65 [01:54<10:29, 11.45s/it]
2022-03-30 21:10:21,291 - INFO - tqdm - accuracy: 0.9858, batch_loss: 0.0669, loss: 0.0508 ||:  17%|#6        | 11/65 [02:06<10:18, 11.46s/it]
2022-03-30 21:10:32,737 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0549, loss: 0.0511 ||:  18%|#8        | 12/65 [02:17<10:07, 11.45s/it]
2022-03-30 21:10:44,146 - INFO - tqdm - accuracy: 0.9856, batch_loss: 0.0056, loss: 0.0476 ||:  20%|##        | 13/65 [02:28<09:54, 11.44s/it]
2022-03-30 21:10:55,582 - INFO - tqdm - accuracy: 0.9866, batch_loss: 0.0026, loss: 0.0444 ||:  22%|##1       | 14/65 [02:40<09:43, 11.44s/it]
2022-03-30 21:11:06,973 - INFO - tqdm - accuracy: 0.9875, batch_loss: 0.0016, loss: 0.0416 ||:  23%|##3       | 15/65 [02:51<09:31, 11.42s/it]
2022-03-30 21:11:18,248 - INFO - tqdm - accuracy: 0.9883, batch_loss: 0.0008, loss: 0.0390 ||:  25%|##4       | 16/65 [03:02<09:17, 11.38s/it]
2022-03-30 21:11:29,639 - INFO - tqdm - accuracy: 0.9890, batch_loss: 0.0020, loss: 0.0368 ||:  26%|##6       | 17/65 [03:14<09:06, 11.38s/it]
2022-03-30 21:11:41,025 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.0046, loss: 0.0350 ||:  28%|##7       | 18/65 [03:25<08:55, 11.38s/it]
2022-03-30 21:11:52,423 - INFO - tqdm - accuracy: 0.9901, batch_loss: 0.0021, loss: 0.0333 ||:  29%|##9       | 19/65 [03:37<08:43, 11.39s/it]
2022-03-30 21:12:03,820 - INFO - tqdm - accuracy: 0.9906, batch_loss: 0.0045, loss: 0.0319 ||:  31%|###       | 20/65 [03:48<08:32, 11.39s/it]
2022-03-30 21:12:15,240 - INFO - tqdm - accuracy: 0.9911, batch_loss: 0.0101, loss: 0.0308 ||:  32%|###2      | 21/65 [03:59<08:21, 11.40s/it]
2022-03-30 21:12:26,672 - INFO - tqdm - accuracy: 0.9915, batch_loss: 0.0113, loss: 0.0300 ||:  34%|###3      | 22/65 [04:11<08:10, 11.41s/it]
2022-03-30 21:12:38,107 - INFO - tqdm - accuracy: 0.9918, batch_loss: 0.0100, loss: 0.0291 ||:  35%|###5      | 23/65 [04:22<07:59, 11.42s/it]
2022-03-30 21:12:49,527 - INFO - tqdm - accuracy: 0.9922, batch_loss: 0.0059, loss: 0.0281 ||:  37%|###6      | 24/65 [04:34<07:48, 11.42s/it]
2022-03-30 21:13:00,949 - INFO - tqdm - accuracy: 0.9912, batch_loss: 0.0340, loss: 0.0284 ||:  38%|###8      | 25/65 [04:45<07:36, 11.42s/it]
2022-03-30 21:13:12,388 - INFO - tqdm - accuracy: 0.9916, batch_loss: 0.0099, loss: 0.0276 ||:  40%|####      | 26/65 [04:57<07:25, 11.43s/it]
2022-03-30 21:13:23,808 - INFO - tqdm - accuracy: 0.9919, batch_loss: 0.0168, loss: 0.0272 ||:  42%|####1     | 27/65 [05:08<07:14, 11.42s/it]
2022-03-30 21:13:35,234 - INFO - tqdm - accuracy: 0.9922, batch_loss: 0.0011, loss: 0.0263 ||:  43%|####3     | 28/65 [05:19<07:02, 11.42s/it]
2022-03-30 21:13:46,681 - INFO - tqdm - accuracy: 0.9925, batch_loss: 0.0023, loss: 0.0255 ||:  45%|####4     | 29/65 [05:31<06:51, 11.43s/it]
2022-03-30 21:13:58,112 - INFO - tqdm - accuracy: 0.9927, batch_loss: 0.0070, loss: 0.0249 ||:  46%|####6     | 30/65 [05:42<06:40, 11.43s/it]
2022-03-30 21:14:09,532 - INFO - tqdm - accuracy: 0.9929, batch_loss: 0.0011, loss: 0.0241 ||:  48%|####7     | 31/65 [05:54<06:28, 11.43s/it]
2022-03-30 21:14:20,987 - INFO - tqdm - accuracy: 0.9932, batch_loss: 0.0013, loss: 0.0234 ||:  49%|####9     | 32/65 [06:05<06:17, 11.44s/it]
2022-03-30 21:14:32,444 - INFO - tqdm - accuracy: 0.9934, batch_loss: 0.0017, loss: 0.0227 ||:  51%|#####     | 33/65 [06:17<06:06, 11.44s/it]
2022-03-30 21:14:43,893 - INFO - tqdm - accuracy: 0.9926, batch_loss: 0.0842, loss: 0.0245 ||:  52%|#####2    | 34/65 [06:28<05:54, 11.44s/it]
2022-03-30 21:14:55,327 - INFO - tqdm - accuracy: 0.9929, batch_loss: 0.0012, loss: 0.0239 ||:  54%|#####3    | 35/65 [06:40<05:43, 11.44s/it]
2022-03-30 21:15:06,769 - INFO - tqdm - accuracy: 0.9931, batch_loss: 0.0012, loss: 0.0232 ||:  55%|#####5    | 36/65 [06:51<05:31, 11.44s/it]
2022-03-30 21:15:18,187 - INFO - tqdm - accuracy: 0.9932, batch_loss: 0.0015, loss: 0.0227 ||:  57%|#####6    | 37/65 [07:02<05:20, 11.43s/it]
2022-03-30 21:15:29,625 - INFO - tqdm - accuracy: 0.9934, batch_loss: 0.0079, loss: 0.0223 ||:  58%|#####8    | 38/65 [07:14<05:08, 11.44s/it]
2022-03-30 21:15:41,041 - INFO - tqdm - accuracy: 0.9936, batch_loss: 0.0172, loss: 0.0221 ||:  60%|######    | 39/65 [07:25<04:57, 11.43s/it]
2022-03-30 21:15:52,483 - INFO - tqdm - accuracy: 0.9938, batch_loss: 0.0059, loss: 0.0217 ||:  62%|######1   | 40/65 [07:37<04:45, 11.43s/it]
2022-03-30 21:16:03,908 - INFO - tqdm - accuracy: 0.9939, batch_loss: 0.0058, loss: 0.0213 ||:  63%|######3   | 41/65 [07:48<04:34, 11.43s/it]
2022-03-30 21:16:15,345 - INFO - tqdm - accuracy: 0.9940, batch_loss: 0.0012, loss: 0.0209 ||:  65%|######4   | 42/65 [08:00<04:22, 11.43s/it]
2022-03-30 21:16:26,655 - INFO - tqdm - accuracy: 0.9935, batch_loss: 0.0381, loss: 0.0213 ||:  66%|######6   | 43/65 [08:11<04:10, 11.40s/it]
2022-03-30 21:16:38,086 - INFO - tqdm - accuracy: 0.9936, batch_loss: 0.0017, loss: 0.0208 ||:  68%|######7   | 44/65 [08:22<03:59, 11.41s/it]
2022-03-30 21:16:49,166 - INFO - tqdm - accuracy: 0.9937, batch_loss: 0.0010, loss: 0.0204 ||:  69%|######9   | 45/65 [08:33<03:46, 11.31s/it]
2022-03-30 21:17:00,604 - INFO - tqdm - accuracy: 0.9932, batch_loss: 0.0748, loss: 0.0216 ||:  71%|#######   | 46/65 [08:45<03:35, 11.35s/it]
2022-03-30 21:17:12,053 - INFO - tqdm - accuracy: 0.9933, batch_loss: 0.0229, loss: 0.0216 ||:  72%|#######2  | 47/65 [08:56<03:24, 11.38s/it]
2022-03-30 21:17:23,501 - INFO - tqdm - accuracy: 0.9935, batch_loss: 0.0014, loss: 0.0212 ||:  74%|#######3  | 48/65 [09:08<03:13, 11.40s/it]
2022-03-30 21:17:34,946 - INFO - tqdm - accuracy: 0.9936, batch_loss: 0.0014, loss: 0.0208 ||:  75%|#######5  | 49/65 [09:19<03:02, 11.41s/it]
2022-03-30 21:17:46,403 - INFO - tqdm - accuracy: 0.9937, batch_loss: 0.0068, loss: 0.0205 ||:  77%|#######6  | 50/65 [09:31<02:51, 11.43s/it]
2022-03-30 21:17:57,842 - INFO - tqdm - accuracy: 0.9939, batch_loss: 0.0160, loss: 0.0204 ||:  78%|#######8  | 51/65 [09:42<02:40, 11.43s/it]
2022-03-30 21:18:09,285 - INFO - tqdm - accuracy: 0.9940, batch_loss: 0.0009, loss: 0.0200 ||:  80%|########  | 52/65 [09:54<02:28, 11.43s/it]
2022-03-30 21:18:20,711 - INFO - tqdm - accuracy: 0.9935, batch_loss: 0.0372, loss: 0.0203 ||:  82%|########1 | 53/65 [10:05<02:17, 11.43s/it]
2022-03-30 21:18:32,159 - INFO - tqdm - accuracy: 0.9936, batch_loss: 0.0014, loss: 0.0200 ||:  83%|########3 | 54/65 [10:16<02:05, 11.44s/it]
2022-03-30 21:18:43,606 - INFO - tqdm - accuracy: 0.9937, batch_loss: 0.0007, loss: 0.0196 ||:  85%|########4 | 55/65 [10:28<01:54, 11.44s/it]
2022-03-30 21:18:55,032 - INFO - tqdm - accuracy: 0.9939, batch_loss: 0.0050, loss: 0.0194 ||:  86%|########6 | 56/65 [10:39<01:42, 11.44s/it]
2022-03-30 21:19:06,217 - INFO - tqdm - accuracy: 0.9940, batch_loss: 0.0028, loss: 0.0191 ||:  88%|########7 | 57/65 [10:50<01:30, 11.36s/it]
2022-03-30 21:19:17,659 - INFO - tqdm - accuracy: 0.9941, batch_loss: 0.0213, loss: 0.0191 ||:  89%|########9 | 58/65 [11:02<01:19, 11.38s/it]
2022-03-30 21:19:29,092 - INFO - tqdm - accuracy: 0.9942, batch_loss: 0.0011, loss: 0.0188 ||:  91%|######### | 59/65 [11:13<01:08, 11.40s/it]
2022-03-30 21:19:40,537 - INFO - tqdm - accuracy: 0.9943, batch_loss: 0.0084, loss: 0.0187 ||:  92%|#########2| 60/65 [11:25<00:57, 11.41s/it]
2022-03-30 21:19:51,981 - INFO - tqdm - accuracy: 0.9944, batch_loss: 0.0011, loss: 0.0184 ||:  94%|#########3| 61/65 [11:36<00:45, 11.42s/it]
2022-03-30 21:20:03,430 - INFO - tqdm - accuracy: 0.9945, batch_loss: 0.0016, loss: 0.0181 ||:  95%|#########5| 62/65 [11:48<00:34, 11.43s/it]
2022-03-30 21:20:14,867 - INFO - tqdm - accuracy: 0.9945, batch_loss: 0.0008, loss: 0.0178 ||:  97%|#########6| 63/65 [11:59<00:22, 11.43s/it]
2022-03-30 21:20:26,301 - INFO - tqdm - accuracy: 0.9946, batch_loss: 0.0015, loss: 0.0176 ||:  98%|#########8| 64/65 [12:11<00:11, 11.43s/it]
2022-03-30 21:20:31,365 - INFO - tqdm - accuracy: 0.9947, batch_loss: 0.0190, loss: 0.0176 ||: 100%|##########| 65/65 [12:16<00:00,  9.52s/it]
2022-03-30 21:20:31,365 - INFO - tqdm - accuracy: 0.9947, batch_loss: 0.0190, loss: 0.0176 ||: 100%|##########| 65/65 [12:16<00:00, 11.32s/it]
2022-03-30 21:20:34,348 - INFO - allennlp.training.trainer - Validating
2022-03-30 21:20:34,349 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-03-30 21:20:44,385 - INFO - tqdm - accuracy: 0.5750, batch_loss: 0.0146, loss: 2.7912 ||:  35%|###4      | 40/115 [00:10<00:18,  4.00it/s]
2022-03-30 21:20:54,621 - INFO - tqdm - accuracy: 0.5963, batch_loss: 7.1496, loss: 2.5529 ||:  70%|#######   | 81/115 [00:20<00:07,  4.41it/s]
2022-03-30 21:21:03,171 - INFO - tqdm - accuracy: 0.6157, batch_loss: 0.0122, loss: 2.3513 ||: 100%|##########| 115/115 [00:28<00:00,  3.93it/s]
2022-03-30 21:21:03,171 - INFO - tqdm - accuracy: 0.6157, batch_loss: 0.0122, loss: 2.3513 ||: 100%|##########| 115/115 [00:28<00:00,  3.99it/s]
2022-03-30 21:21:03,171 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-03-30 21:21:03,172 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.995  |     0.616
2022-03-30 21:21:03,173 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-03-30 21:21:03,173 - INFO - allennlp.training.tensorboard_writer - loss               |     0.018  |     2.351
2022-03-30 21:21:03,173 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5793.102  |       N/A
2022-03-30 21:21:09,538 - INFO - allennlp.training.trainer - Epoch duration: 0:12:54.277245
2022-03-30 21:21:09,539 - INFO - allennlp.training.trainer - Estimated training time remaining: 0:38:49
2022-03-30 21:21:09,539 - INFO - allennlp.training.trainer - Epoch 12/14
2022-03-30 21:21:09,539 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-03-30 21:21:09,539 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-03-30 21:21:09,541 - INFO - allennlp.training.trainer - Training
2022-03-30 21:21:09,541 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-03-30 21:21:20,880 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0044, loss: 0.0044 ||:   2%|1         | 1/65 [00:11<12:05, 11.34s/it]
2022-03-30 21:21:32,489 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0020, loss: 0.0032 ||:   3%|3         | 2/65 [00:22<12:04, 11.50s/it]
2022-03-30 21:21:44,041 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0007, loss: 0.0024 ||:   5%|4         | 3/65 [00:34<11:54, 11.52s/it]
2022-03-30 21:21:55,492 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0009, loss: 0.0020 ||:   6%|6         | 4/65 [00:45<11:41, 11.49s/it]
2022-03-30 21:22:06,832 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0044, loss: 0.0025 ||:   8%|7         | 5/65 [00:57<11:26, 11.44s/it]
2022-03-30 21:22:18,181 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0075, loss: 0.0033 ||:   9%|9         | 6/65 [01:08<11:13, 11.41s/it]
2022-03-30 21:22:29,578 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0006, loss: 0.0029 ||:  11%|#         | 7/65 [01:20<11:01, 11.40s/it]
2022-03-30 21:22:41,029 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0011, loss: 0.0027 ||:  12%|#2        | 8/65 [01:31<10:50, 11.42s/it]
2022-03-30 21:22:52,516 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0007, loss: 0.0025 ||:  14%|#3        | 9/65 [01:42<10:40, 11.44s/it]
2022-03-30 21:23:03,990 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0006, loss: 0.0023 ||:  15%|#5        | 10/65 [01:54<10:29, 11.45s/it]
2022-03-30 21:23:15,432 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0025, loss: 0.0023 ||:  17%|#6        | 11/65 [02:05<10:18, 11.45s/it]
2022-03-30 21:23:26,851 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0031, loss: 0.0024 ||:  18%|#8        | 12/65 [02:17<10:06, 11.44s/it]
2022-03-30 21:23:37,925 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0006, loss: 0.0022 ||:  20%|##        | 13/65 [02:28<09:49, 11.33s/it]
2022-03-30 21:23:49,336 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0019, loss: 0.0022 ||:  22%|##1       | 14/65 [02:39<09:39, 11.35s/it]
2022-03-30 21:24:00,747 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0011, loss: 0.0021 ||:  23%|##3       | 15/65 [02:51<09:28, 11.37s/it]
2022-03-30 21:24:12,145 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0030, loss: 0.0022 ||:  25%|##4       | 16/65 [03:02<09:17, 11.38s/it]
2022-03-30 21:24:23,543 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0008, loss: 0.0021 ||:  26%|##6       | 17/65 [03:14<09:06, 11.38s/it]
2022-03-30 21:24:34,934 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0018, loss: 0.0021 ||:  28%|##7       | 18/65 [03:25<08:55, 11.39s/it]
2022-03-30 21:24:46,320 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0058, loss: 0.0023 ||:  29%|##9       | 19/65 [03:36<08:43, 11.39s/it]
2022-03-30 21:24:58,096 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0023, loss: 0.0023 ||:  31%|###       | 20/65 [03:48<08:37, 11.50s/it]
2022-03-30 21:25:09,452 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0020, loss: 0.0023 ||:  32%|###2      | 21/65 [03:59<08:24, 11.46s/it]
2022-03-30 21:25:20,863 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0012, loss: 0.0022 ||:  34%|###3      | 22/65 [04:11<08:12, 11.44s/it]
2022-03-30 21:25:32,265 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0019, loss: 0.0022 ||:  35%|###5      | 23/65 [04:22<08:00, 11.43s/it]
2022-03-30 21:25:43,671 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0011, loss: 0.0022 ||:  37%|###6      | 24/65 [04:34<07:48, 11.42s/it]
2022-03-30 21:25:54,830 - INFO - tqdm - accuracy: 0.9987, batch_loss: 0.0314, loss: 0.0033 ||:  38%|###8      | 25/65 [04:45<07:33, 11.34s/it]
2022-03-30 21:26:06,274 - INFO - tqdm - accuracy: 0.9988, batch_loss: 0.0011, loss: 0.0033 ||:  40%|####      | 26/65 [04:56<07:23, 11.37s/it]
2022-03-30 21:26:17,680 - INFO - tqdm - accuracy: 0.9988, batch_loss: 0.0025, loss: 0.0032 ||:  42%|####1     | 27/65 [05:08<07:12, 11.38s/it]
2022-03-30 21:26:29,087 - INFO - tqdm - accuracy: 0.9989, batch_loss: 0.0009, loss: 0.0031 ||:  43%|####3     | 28/65 [05:19<07:01, 11.39s/it]
2022-03-30 21:26:40,493 - INFO - tqdm - accuracy: 0.9989, batch_loss: 0.0011, loss: 0.0031 ||:  45%|####4     | 29/65 [05:30<06:50, 11.40s/it]
2022-03-30 21:26:51,884 - INFO - tqdm - accuracy: 0.9990, batch_loss: 0.0025, loss: 0.0031 ||:  46%|####6     | 30/65 [05:42<06:38, 11.39s/it]
2022-03-30 21:27:03,297 - INFO - tqdm - accuracy: 0.9990, batch_loss: 0.0004, loss: 0.0030 ||:  48%|####7     | 31/65 [05:53<06:27, 11.40s/it]
2022-03-30 21:27:14,619 - INFO - tqdm - accuracy: 0.9990, batch_loss: 0.0159, loss: 0.0034 ||:  49%|####9     | 32/65 [06:05<06:15, 11.38s/it]
2022-03-30 21:27:26,041 - INFO - tqdm - accuracy: 0.9991, batch_loss: 0.0024, loss: 0.0033 ||:  51%|#####     | 33/65 [06:16<06:04, 11.39s/it]
2022-03-30 21:27:37,452 - INFO - tqdm - accuracy: 0.9991, batch_loss: 0.0013, loss: 0.0033 ||:  52%|#####2    | 34/65 [06:27<05:53, 11.40s/it]
2022-03-30 21:27:48,863 - INFO - tqdm - accuracy: 0.9991, batch_loss: 0.0081, loss: 0.0034 ||:  54%|#####3    | 35/65 [06:39<05:42, 11.40s/it]
2022-03-30 21:28:00,276 - INFO - tqdm - accuracy: 0.9991, batch_loss: 0.0013, loss: 0.0034 ||:  55%|#####5    | 36/65 [06:50<05:30, 11.40s/it]
2022-03-30 21:28:11,711 - INFO - tqdm - accuracy: 0.9992, batch_loss: 0.0086, loss: 0.0035 ||:  57%|#####6    | 37/65 [07:02<05:19, 11.41s/it]
2022-03-30 21:28:23,134 - INFO - tqdm - accuracy: 0.9992, batch_loss: 0.0104, loss: 0.0037 ||:  58%|#####8    | 38/65 [07:13<05:08, 11.42s/it]
2022-03-30 21:28:34,573 - INFO - tqdm - accuracy: 0.9992, batch_loss: 0.0026, loss: 0.0037 ||:  60%|######    | 39/65 [07:25<04:57, 11.42s/it]
2022-03-30 21:28:46,003 - INFO - tqdm - accuracy: 0.9992, batch_loss: 0.0005, loss: 0.0036 ||:  62%|######1   | 40/65 [07:36<04:45, 11.43s/it]
2022-03-30 21:28:57,438 - INFO - tqdm - accuracy: 0.9992, batch_loss: 0.0017, loss: 0.0035 ||:  63%|######3   | 41/65 [07:47<04:34, 11.43s/it]
2022-03-30 21:29:08,893 - INFO - tqdm - accuracy: 0.9993, batch_loss: 0.0043, loss: 0.0036 ||:  65%|######4   | 42/65 [07:59<04:23, 11.44s/it]
2022-03-30 21:29:20,344 - INFO - tqdm - accuracy: 0.9993, batch_loss: 0.0007, loss: 0.0035 ||:  66%|######6   | 43/65 [08:10<04:11, 11.44s/it]
2022-03-30 21:29:31,784 - INFO - tqdm - accuracy: 0.9993, batch_loss: 0.0006, loss: 0.0034 ||:  68%|######7   | 44/65 [08:22<04:00, 11.44s/it]
2022-03-30 21:29:43,221 - INFO - tqdm - accuracy: 0.9993, batch_loss: 0.0007, loss: 0.0034 ||:  69%|######9   | 45/65 [08:33<03:48, 11.44s/it]
2022-03-30 21:29:54,602 - INFO - tqdm - accuracy: 0.9993, batch_loss: 0.0008, loss: 0.0033 ||:  71%|#######   | 46/65 [08:45<03:37, 11.42s/it]
2022-03-30 21:30:05,986 - INFO - tqdm - accuracy: 0.9993, batch_loss: 0.0007, loss: 0.0032 ||:  72%|#######2  | 47/65 [08:56<03:25, 11.41s/it]
2022-03-30 21:30:17,403 - INFO - tqdm - accuracy: 0.9993, batch_loss: 0.0008, loss: 0.0032 ||:  74%|#######3  | 48/65 [09:07<03:14, 11.41s/it]
2022-03-30 21:30:28,866 - INFO - tqdm - accuracy: 0.9994, batch_loss: 0.0003, loss: 0.0031 ||:  75%|#######5  | 49/65 [09:19<03:02, 11.43s/it]
2022-03-30 21:30:40,359 - INFO - tqdm - accuracy: 0.9994, batch_loss: 0.0006, loss: 0.0031 ||:  77%|#######6  | 50/65 [09:30<02:51, 11.45s/it]
2022-03-30 21:30:51,841 - INFO - tqdm - accuracy: 0.9994, batch_loss: 0.0013, loss: 0.0031 ||:  78%|#######8  | 51/65 [09:42<02:40, 11.46s/it]
2022-03-30 21:31:03,247 - INFO - tqdm - accuracy: 0.9994, batch_loss: 0.0104, loss: 0.0032 ||:  80%|########  | 52/65 [09:53<02:28, 11.44s/it]
2022-03-30 21:31:14,618 - INFO - tqdm - accuracy: 0.9994, batch_loss: 0.0007, loss: 0.0031 ||:  82%|########1 | 53/65 [10:05<02:17, 11.42s/it]
2022-03-30 21:31:25,990 - INFO - tqdm - accuracy: 0.9994, batch_loss: 0.0008, loss: 0.0031 ||:  83%|########3 | 54/65 [10:16<02:05, 11.41s/it]
2022-03-30 21:31:37,428 - INFO - tqdm - accuracy: 0.9994, batch_loss: 0.0030, loss: 0.0031 ||:  85%|########4 | 55/65 [10:27<01:54, 11.42s/it]
2022-03-30 21:31:48,878 - INFO - tqdm - accuracy: 0.9994, batch_loss: 0.0006, loss: 0.0031 ||:  86%|########6 | 56/65 [10:39<01:42, 11.43s/it]
2022-03-30 21:32:00,326 - INFO - tqdm - accuracy: 0.9995, batch_loss: 0.0053, loss: 0.0031 ||:  88%|########7 | 57/65 [10:50<01:31, 11.43s/it]
2022-03-30 21:32:11,781 - INFO - tqdm - accuracy: 0.9989, batch_loss: 0.0418, loss: 0.0038 ||:  89%|########9 | 58/65 [11:02<01:20, 11.44s/it]
2022-03-30 21:32:23,239 - INFO - tqdm - accuracy: 0.9989, batch_loss: 0.0171, loss: 0.0040 ||:  91%|######### | 59/65 [11:13<01:08, 11.45s/it]
2022-03-30 21:32:34,694 - INFO - tqdm - accuracy: 0.9990, batch_loss: 0.0062, loss: 0.0040 ||:  92%|#########2| 60/65 [11:25<00:57, 11.45s/it]
2022-03-30 21:32:46,147 - INFO - tqdm - accuracy: 0.9990, batch_loss: 0.0007, loss: 0.0040 ||:  94%|#########3| 61/65 [11:36<00:45, 11.45s/it]
2022-03-30 21:32:57,595 - INFO - tqdm - accuracy: 0.9990, batch_loss: 0.0007, loss: 0.0039 ||:  95%|#########5| 62/65 [11:48<00:34, 11.45s/it]
2022-03-30 21:33:09,025 - INFO - tqdm - accuracy: 0.9990, batch_loss: 0.0108, loss: 0.0040 ||:  97%|#########6| 63/65 [11:59<00:22, 11.44s/it]
2022-03-30 21:33:20,355 - INFO - tqdm - accuracy: 0.9990, batch_loss: 0.0019, loss: 0.0040 ||:  98%|#########8| 64/65 [12:10<00:11, 11.41s/it]
2022-03-30 21:33:25,441 - INFO - tqdm - accuracy: 0.9990, batch_loss: 0.0004, loss: 0.0039 ||: 100%|##########| 65/65 [12:15<00:00,  9.51s/it]
2022-03-30 21:33:25,442 - INFO - tqdm - accuracy: 0.9990, batch_loss: 0.0004, loss: 0.0039 ||: 100%|##########| 65/65 [12:15<00:00, 11.32s/it]
2022-03-30 21:33:28,361 - INFO - allennlp.training.trainer - Validating
2022-03-30 21:33:28,363 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-03-30 21:33:38,450 - INFO - tqdm - accuracy: 0.6543, batch_loss: 3.6413, loss: 1.9738 ||:  36%|###5      | 41/115 [00:10<00:18,  4.01it/s]
2022-03-30 21:33:48,550 - INFO - tqdm - accuracy: 0.6211, batch_loss: 0.0447, loss: 2.3429 ||:  70%|#######   | 81/115 [00:20<00:08,  3.95it/s]
2022-03-30 21:33:57,154 - INFO - tqdm - accuracy: 0.6114, batch_loss: 0.3427, loss: 2.4921 ||: 100%|##########| 115/115 [00:28<00:00,  3.94it/s]
2022-03-30 21:33:57,155 - INFO - tqdm - accuracy: 0.6114, batch_loss: 0.3427, loss: 2.4921 ||: 100%|##########| 115/115 [00:28<00:00,  3.99it/s]
2022-03-30 21:33:57,155 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-03-30 21:33:57,155 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.999  |     0.611
2022-03-30 21:33:57,156 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-03-30 21:33:57,157 - INFO - allennlp.training.tensorboard_writer - loss               |     0.004  |     2.492
2022-03-30 21:33:57,157 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5793.102  |       N/A
2022-03-30 21:34:03,425 - INFO - allennlp.training.trainer - Epoch duration: 0:12:53.886084
2022-03-30 21:34:03,425 - INFO - allennlp.training.trainer - Estimated training time remaining: 0:25:52
2022-03-30 21:34:03,425 - INFO - allennlp.training.trainer - Epoch 13/14
2022-03-30 21:34:03,426 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-03-30 21:34:03,426 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-03-30 21:34:03,427 - INFO - allennlp.training.trainer - Training
2022-03-30 21:34:03,427 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-03-30 21:34:14,749 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0005, loss: 0.0005 ||:   2%|1         | 1/65 [00:11<12:04, 11.32s/it]
2022-03-30 21:34:26,309 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0011, loss: 0.0008 ||:   3%|3         | 2/65 [00:22<12:02, 11.46s/it]
2022-03-30 21:34:38,024 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0006, loss: 0.0007 ||:   5%|4         | 3/65 [00:34<11:57, 11.58s/it]
2022-03-30 21:34:49,478 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0006, loss: 0.0007 ||:   6%|6         | 4/65 [00:46<11:43, 11.53s/it]
2022-03-30 21:35:00,841 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0050, loss: 0.0016 ||:   8%|7         | 5/65 [00:57<11:28, 11.47s/it]
2022-03-30 21:35:12,172 - INFO - tqdm - accuracy: 0.9948, batch_loss: 0.0349, loss: 0.0071 ||:   9%|9         | 6/65 [01:08<11:13, 11.42s/it]
2022-03-30 21:35:23,533 - INFO - tqdm - accuracy: 0.9955, batch_loss: 0.0011, loss: 0.0063 ||:  11%|#         | 7/65 [01:20<11:01, 11.40s/it]
2022-03-30 21:35:34,955 - INFO - tqdm - accuracy: 0.9961, batch_loss: 0.0067, loss: 0.0063 ||:  12%|#2        | 8/65 [01:31<10:50, 11.41s/it]
2022-03-30 21:35:46,425 - INFO - tqdm - accuracy: 0.9965, batch_loss: 0.0008, loss: 0.0057 ||:  14%|#3        | 9/65 [01:42<10:39, 11.43s/it]
2022-03-30 21:35:57,882 - INFO - tqdm - accuracy: 0.9969, batch_loss: 0.0138, loss: 0.0065 ||:  15%|#5        | 10/65 [01:54<10:29, 11.44s/it]
2022-03-30 21:36:09,326 - INFO - tqdm - accuracy: 0.9972, batch_loss: 0.0026, loss: 0.0062 ||:  17%|#6        | 11/65 [02:05<10:17, 11.44s/it]
2022-03-30 21:36:20,739 - INFO - tqdm - accuracy: 0.9974, batch_loss: 0.0008, loss: 0.0057 ||:  18%|#8        | 12/65 [02:17<10:05, 11.43s/it]
2022-03-30 21:36:32,111 - INFO - tqdm - accuracy: 0.9976, batch_loss: 0.0074, loss: 0.0058 ||:  20%|##        | 13/65 [02:28<09:53, 11.41s/it]
2022-03-30 21:36:43,480 - INFO - tqdm - accuracy: 0.9955, batch_loss: 0.0305, loss: 0.0076 ||:  22%|##1       | 14/65 [02:40<09:41, 11.40s/it]
2022-03-30 21:36:54,882 - INFO - tqdm - accuracy: 0.9958, batch_loss: 0.0004, loss: 0.0071 ||:  23%|##3       | 15/65 [02:51<09:30, 11.40s/it]
2022-03-30 21:37:06,317 - INFO - tqdm - accuracy: 0.9941, batch_loss: 0.2417, loss: 0.0218 ||:  25%|##4       | 16/65 [03:02<09:19, 11.41s/it]
2022-03-30 21:37:17,766 - INFO - tqdm - accuracy: 0.9945, batch_loss: 0.0007, loss: 0.0205 ||:  26%|##6       | 17/65 [03:14<09:08, 11.42s/it]
2022-03-30 21:37:28,873 - INFO - tqdm - accuracy: 0.9948, batch_loss: 0.0026, loss: 0.0196 ||:  28%|##7       | 18/65 [03:25<08:52, 11.33s/it]
2022-03-30 21:37:40,324 - INFO - tqdm - accuracy: 0.9951, batch_loss: 0.0008, loss: 0.0186 ||:  29%|##9       | 19/65 [03:36<08:42, 11.36s/it]
2022-03-30 21:37:51,785 - INFO - tqdm - accuracy: 0.9937, batch_loss: 0.0448, loss: 0.0199 ||:  31%|###       | 20/65 [03:48<08:32, 11.39s/it]
2022-03-30 21:38:03,235 - INFO - tqdm - accuracy: 0.9940, batch_loss: 0.0018, loss: 0.0190 ||:  32%|###2      | 21/65 [03:59<08:22, 11.41s/it]
2022-03-30 21:38:14,682 - INFO - tqdm - accuracy: 0.9929, batch_loss: 0.0816, loss: 0.0219 ||:  34%|###3      | 22/65 [04:11<08:11, 11.42s/it]
2022-03-30 21:38:26,104 - INFO - tqdm - accuracy: 0.9932, batch_loss: 0.0021, loss: 0.0210 ||:  35%|###5      | 23/65 [04:22<07:59, 11.42s/it]
2022-03-30 21:38:37,469 - INFO - tqdm - accuracy: 0.9935, batch_loss: 0.0003, loss: 0.0201 ||:  37%|###6      | 24/65 [04:34<07:47, 11.40s/it]
2022-03-30 21:38:48,862 - INFO - tqdm - accuracy: 0.9937, batch_loss: 0.0016, loss: 0.0194 ||:  38%|###8      | 25/65 [04:45<07:36, 11.40s/it]
2022-03-30 21:39:00,246 - INFO - tqdm - accuracy: 0.9940, batch_loss: 0.0010, loss: 0.0187 ||:  40%|####      | 26/65 [04:56<07:24, 11.40s/it]
2022-03-30 21:39:11,634 - INFO - tqdm - accuracy: 0.9930, batch_loss: 0.0607, loss: 0.0202 ||:  42%|####1     | 27/65 [05:08<07:12, 11.39s/it]
2022-03-30 21:39:23,029 - INFO - tqdm - accuracy: 0.9933, batch_loss: 0.0036, loss: 0.0196 ||:  43%|####3     | 28/65 [05:19<07:01, 11.39s/it]
2022-03-30 21:39:34,450 - INFO - tqdm - accuracy: 0.9935, batch_loss: 0.0013, loss: 0.0190 ||:  45%|####4     | 29/65 [05:31<06:50, 11.40s/it]
2022-03-30 21:39:45,855 - INFO - tqdm - accuracy: 0.9937, batch_loss: 0.0018, loss: 0.0184 ||:  46%|####6     | 30/65 [05:42<06:39, 11.40s/it]
2022-03-30 21:39:57,267 - INFO - tqdm - accuracy: 0.9939, batch_loss: 0.0004, loss: 0.0179 ||:  48%|####7     | 31/65 [05:53<06:27, 11.41s/it]
2022-03-30 21:40:08,671 - INFO - tqdm - accuracy: 0.9941, batch_loss: 0.0013, loss: 0.0173 ||:  49%|####9     | 32/65 [06:05<06:16, 11.41s/it]
2022-03-30 21:40:20,084 - INFO - tqdm - accuracy: 0.9943, batch_loss: 0.0004, loss: 0.0168 ||:  51%|#####     | 33/65 [06:16<06:05, 11.41s/it]
2022-03-30 21:40:31,361 - INFO - tqdm - accuracy: 0.9945, batch_loss: 0.0008, loss: 0.0164 ||:  52%|#####2    | 34/65 [06:27<05:52, 11.37s/it]
2022-03-30 21:40:42,768 - INFO - tqdm - accuracy: 0.9937, batch_loss: 0.0676, loss: 0.0178 ||:  54%|#####3    | 35/65 [06:39<05:41, 11.38s/it]
2022-03-30 21:40:54,191 - INFO - tqdm - accuracy: 0.9939, batch_loss: 0.0016, loss: 0.0174 ||:  55%|#####5    | 36/65 [06:50<05:30, 11.39s/it]
2022-03-30 21:41:05,611 - INFO - tqdm - accuracy: 0.9941, batch_loss: 0.0004, loss: 0.0169 ||:  57%|#####6    | 37/65 [07:02<05:19, 11.40s/it]
2022-03-30 21:41:17,039 - INFO - tqdm - accuracy: 0.9942, batch_loss: 0.0015, loss: 0.0165 ||:  58%|#####8    | 38/65 [07:13<05:08, 11.41s/it]
2022-03-30 21:41:28,464 - INFO - tqdm - accuracy: 0.9944, batch_loss: 0.0041, loss: 0.0162 ||:  60%|######    | 39/65 [07:25<04:56, 11.41s/it]
2022-03-30 21:41:39,886 - INFO - tqdm - accuracy: 0.9945, batch_loss: 0.0008, loss: 0.0158 ||:  62%|######1   | 40/65 [07:36<04:45, 11.42s/it]
2022-03-30 21:41:51,313 - INFO - tqdm - accuracy: 0.9947, batch_loss: 0.0003, loss: 0.0154 ||:  63%|######3   | 41/65 [07:47<04:34, 11.42s/it]
2022-03-30 21:42:02,740 - INFO - tqdm - accuracy: 0.9948, batch_loss: 0.0007, loss: 0.0151 ||:  65%|######4   | 42/65 [07:59<04:22, 11.42s/it]
2022-03-30 21:42:14,148 - INFO - tqdm - accuracy: 0.9949, batch_loss: 0.0007, loss: 0.0147 ||:  66%|######6   | 43/65 [08:10<04:11, 11.42s/it]
2022-03-30 21:42:25,569 - INFO - tqdm - accuracy: 0.9950, batch_loss: 0.0010, loss: 0.0144 ||:  68%|######7   | 44/65 [08:22<03:59, 11.42s/it]
2022-03-30 21:42:36,991 - INFO - tqdm - accuracy: 0.9951, batch_loss: 0.0020, loss: 0.0142 ||:  69%|######9   | 45/65 [08:33<03:48, 11.42s/it]
2022-03-30 21:42:48,129 - INFO - tqdm - accuracy: 0.9952, batch_loss: 0.0136, loss: 0.0141 ||:  71%|#######   | 46/65 [08:44<03:35, 11.34s/it]
2022-03-30 21:42:59,509 - INFO - tqdm - accuracy: 0.9953, batch_loss: 0.0010, loss: 0.0139 ||:  72%|#######2  | 47/65 [08:56<03:24, 11.35s/it]
2022-03-30 21:43:10,884 - INFO - tqdm - accuracy: 0.9954, batch_loss: 0.0141, loss: 0.0139 ||:  74%|#######3  | 48/65 [09:07<03:13, 11.36s/it]
2022-03-30 21:43:22,261 - INFO - tqdm - accuracy: 0.9955, batch_loss: 0.0012, loss: 0.0136 ||:  75%|#######5  | 49/65 [09:18<03:01, 11.36s/it]
2022-03-30 21:43:33,497 - INFO - tqdm - accuracy: 0.9956, batch_loss: 0.0004, loss: 0.0133 ||:  77%|#######6  | 50/65 [09:30<02:49, 11.32s/it]
2022-03-30 21:43:44,889 - INFO - tqdm - accuracy: 0.9957, batch_loss: 0.0058, loss: 0.0132 ||:  78%|#######8  | 51/65 [09:41<02:38, 11.34s/it]
2022-03-30 21:43:56,297 - INFO - tqdm - accuracy: 0.9958, batch_loss: 0.0023, loss: 0.0130 ||:  80%|########  | 52/65 [09:52<02:27, 11.36s/it]
2022-03-30 21:44:07,692 - INFO - tqdm - accuracy: 0.9959, batch_loss: 0.0117, loss: 0.0130 ||:  82%|########1 | 53/65 [10:04<02:16, 11.37s/it]
2022-03-30 21:44:19,105 - INFO - tqdm - accuracy: 0.9959, batch_loss: 0.0005, loss: 0.0127 ||:  83%|########3 | 54/65 [10:15<02:05, 11.38s/it]
2022-03-30 21:44:31,001 - INFO - tqdm - accuracy: 0.9960, batch_loss: 0.0045, loss: 0.0126 ||:  85%|########4 | 55/65 [10:27<01:55, 11.54s/it]
2022-03-30 21:44:42,360 - INFO - tqdm - accuracy: 0.9961, batch_loss: 0.0023, loss: 0.0124 ||:  86%|########6 | 56/65 [10:38<01:43, 11.48s/it]
2022-03-30 21:44:53,740 - INFO - tqdm - accuracy: 0.9962, batch_loss: 0.0009, loss: 0.0122 ||:  88%|########7 | 57/65 [10:50<01:31, 11.45s/it]
2022-03-30 21:45:05,142 - INFO - tqdm - accuracy: 0.9962, batch_loss: 0.0003, loss: 0.0120 ||:  89%|########9 | 58/65 [11:01<01:20, 11.44s/it]
2022-03-30 21:45:16,549 - INFO - tqdm - accuracy: 0.9963, batch_loss: 0.0010, loss: 0.0118 ||:  91%|######### | 59/65 [11:13<01:08, 11.43s/it]
2022-03-30 21:45:27,974 - INFO - tqdm - accuracy: 0.9964, batch_loss: 0.0024, loss: 0.0117 ||:  92%|#########2| 60/65 [11:24<00:57, 11.43s/it]
2022-03-30 21:45:39,419 - INFO - tqdm - accuracy: 0.9964, batch_loss: 0.0004, loss: 0.0115 ||:  94%|#########3| 61/65 [11:35<00:45, 11.43s/it]
2022-03-30 21:45:50,882 - INFO - tqdm - accuracy: 0.9965, batch_loss: 0.0017, loss: 0.0113 ||:  95%|#########5| 62/65 [11:47<00:34, 11.44s/it]
2022-03-30 21:46:02,338 - INFO - tqdm - accuracy: 0.9965, batch_loss: 0.0005, loss: 0.0111 ||:  97%|#########6| 63/65 [11:58<00:22, 11.45s/it]
2022-03-30 21:46:13,764 - INFO - tqdm - accuracy: 0.9966, batch_loss: 0.0004, loss: 0.0110 ||:  98%|#########8| 64/65 [12:10<00:11, 11.44s/it]
2022-03-30 21:46:18,827 - INFO - tqdm - accuracy: 0.9966, batch_loss: 0.0052, loss: 0.0109 ||: 100%|##########| 65/65 [12:15<00:00,  9.53s/it]
2022-03-30 21:46:18,829 - INFO - tqdm - accuracy: 0.9966, batch_loss: 0.0052, loss: 0.0109 ||: 100%|##########| 65/65 [12:15<00:00, 11.31s/it]
2022-03-30 21:46:21,781 - INFO - allennlp.training.trainer - Validating
2022-03-30 21:46:21,783 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-03-30 21:46:31,916 - INFO - tqdm - accuracy: 0.7037, batch_loss: 0.0001, loss: 2.0654 ||:  36%|###5      | 41/115 [00:10<00:18,  4.01it/s]
2022-03-30 21:46:41,999 - INFO - tqdm - accuracy: 0.6087, batch_loss: 2.9397, loss: 2.6802 ||:  70%|#######   | 81/115 [00:20<00:08,  3.95it/s]
2022-03-30 21:46:50,511 - INFO - tqdm - accuracy: 0.6070, batch_loss: 0.0004, loss: 2.5257 ||: 100%|##########| 115/115 [00:28<00:00,  3.95it/s]
2022-03-30 21:46:50,511 - INFO - tqdm - accuracy: 0.6070, batch_loss: 0.0004, loss: 2.5257 ||: 100%|##########| 115/115 [00:28<00:00,  4.00it/s]
2022-03-30 21:46:50,512 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-03-30 21:46:50,512 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.997  |     0.607
2022-03-30 21:46:50,512 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-03-30 21:46:50,512 - INFO - allennlp.training.tensorboard_writer - loss               |     0.011  |     2.526
2022-03-30 21:46:50,514 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5793.102  |       N/A
2022-03-30 21:46:56,967 - INFO - allennlp.training.trainer - Epoch duration: 0:12:53.541997
2022-03-30 21:46:56,968 - INFO - allennlp.training.trainer - Estimated training time remaining: 0:12:56
2022-03-30 21:46:56,968 - INFO - allennlp.training.trainer - Epoch 14/14
2022-03-30 21:46:56,968 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-03-30 21:46:56,968 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-03-30 21:46:56,970 - INFO - allennlp.training.trainer - Training
2022-03-30 21:46:56,970 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-03-30 21:47:08,268 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0003, loss: 0.0003 ||:   2%|1         | 1/65 [00:11<12:03, 11.30s/it]
2022-03-30 21:47:19,797 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0013, loss: 0.0008 ||:   3%|3         | 2/65 [00:22<12:00, 11.43s/it]
2022-03-30 21:47:31,497 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0012, loss: 0.0009 ||:   5%|4         | 3/65 [00:34<11:56, 11.56s/it]
2022-03-30 21:47:42,945 - INFO - tqdm - accuracy: 0.9922, batch_loss: 0.0723, loss: 0.0188 ||:   6%|6         | 4/65 [00:45<11:42, 11.51s/it]
2022-03-30 21:47:54,298 - INFO - tqdm - accuracy: 0.9938, batch_loss: 0.0013, loss: 0.0153 ||:   8%|7         | 5/65 [00:57<11:27, 11.46s/it]
2022-03-30 21:48:05,628 - INFO - tqdm - accuracy: 0.9948, batch_loss: 0.0005, loss: 0.0128 ||:   9%|9         | 6/65 [01:08<11:13, 11.41s/it]
2022-03-30 21:48:16,990 - INFO - tqdm - accuracy: 0.9911, batch_loss: 0.0441, loss: 0.0173 ||:  11%|#         | 7/65 [01:20<11:00, 11.40s/it]
2022-03-30 21:48:28,436 - INFO - tqdm - accuracy: 0.9922, batch_loss: 0.0005, loss: 0.0152 ||:  12%|#2        | 8/65 [01:31<10:50, 11.41s/it]
2022-03-30 21:48:39,887 - INFO - tqdm - accuracy: 0.9931, batch_loss: 0.0011, loss: 0.0136 ||:  14%|#3        | 9/65 [01:42<10:39, 11.42s/it]
2022-03-30 21:48:51,349 - INFO - tqdm - accuracy: 0.9938, batch_loss: 0.0006, loss: 0.0123 ||:  15%|#5        | 10/65 [01:54<10:28, 11.44s/it]
2022-03-30 21:49:02,775 - INFO - tqdm - accuracy: 0.9943, batch_loss: 0.0016, loss: 0.0113 ||:  17%|#6        | 11/65 [02:05<10:17, 11.43s/it]
2022-03-30 21:49:14,168 - INFO - tqdm - accuracy: 0.9948, batch_loss: 0.0007, loss: 0.0105 ||:  18%|#8        | 12/65 [02:17<10:05, 11.42s/it]
2022-03-30 21:49:25,545 - INFO - tqdm - accuracy: 0.9952, batch_loss: 0.0004, loss: 0.0097 ||:  20%|##        | 13/65 [02:28<09:53, 11.41s/it]
2022-03-30 21:49:36,930 - INFO - tqdm - accuracy: 0.9955, batch_loss: 0.0031, loss: 0.0092 ||:  22%|##1       | 14/65 [02:39<09:41, 11.40s/it]
2022-03-30 21:49:48,303 - INFO - tqdm - accuracy: 0.9958, batch_loss: 0.0021, loss: 0.0087 ||:  23%|##3       | 15/65 [02:51<09:29, 11.39s/it]
2022-03-30 21:49:59,705 - INFO - tqdm - accuracy: 0.9961, batch_loss: 0.0007, loss: 0.0082 ||:  25%|##4       | 16/65 [03:02<09:18, 11.40s/it]
2022-03-30 21:50:11,107 - INFO - tqdm - accuracy: 0.9963, batch_loss: 0.0055, loss: 0.0081 ||:  26%|##6       | 17/65 [03:14<09:07, 11.40s/it]
2022-03-30 21:50:22,523 - INFO - tqdm - accuracy: 0.9965, batch_loss: 0.0006, loss: 0.0077 ||:  28%|##7       | 18/65 [03:25<08:55, 11.40s/it]
2022-03-30 21:50:33,926 - INFO - tqdm - accuracy: 0.9967, batch_loss: 0.0003, loss: 0.0073 ||:  29%|##9       | 19/65 [03:36<08:44, 11.40s/it]
2022-03-30 21:50:45,348 - INFO - tqdm - accuracy: 0.9969, batch_loss: 0.0006, loss: 0.0069 ||:  31%|###       | 20/65 [03:48<08:33, 11.41s/it]
2022-03-30 21:50:56,743 - INFO - tqdm - accuracy: 0.9970, batch_loss: 0.0003, loss: 0.0066 ||:  32%|###2      | 21/65 [03:59<08:21, 11.40s/it]
2022-03-30 21:51:08,012 - INFO - tqdm - accuracy: 0.9972, batch_loss: 0.0005, loss: 0.0063 ||:  34%|###3      | 22/65 [04:11<08:08, 11.36s/it]
2022-03-30 21:51:19,423 - INFO - tqdm - accuracy: 0.9973, batch_loss: 0.0004, loss: 0.0061 ||:  35%|###5      | 23/65 [04:22<07:57, 11.38s/it]
2022-03-30 21:51:30,838 - INFO - tqdm - accuracy: 0.9974, batch_loss: 0.0007, loss: 0.0059 ||:  37%|###6      | 24/65 [04:33<07:46, 11.39s/it]
2022-03-30 21:51:42,255 - INFO - tqdm - accuracy: 0.9975, batch_loss: 0.0117, loss: 0.0061 ||:  38%|###8      | 25/65 [04:45<07:35, 11.40s/it]
2022-03-30 21:51:53,683 - INFO - tqdm - accuracy: 0.9976, batch_loss: 0.0008, loss: 0.0059 ||:  40%|####      | 26/65 [04:56<07:24, 11.41s/it]
2022-03-30 21:52:05,097 - INFO - tqdm - accuracy: 0.9977, batch_loss: 0.0022, loss: 0.0057 ||:  42%|####1     | 27/65 [05:08<07:13, 11.41s/it]
2022-03-30 21:52:16,516 - INFO - tqdm - accuracy: 0.9978, batch_loss: 0.0003, loss: 0.0056 ||:  43%|####3     | 28/65 [05:19<07:02, 11.41s/it]
2022-03-30 21:52:27,947 - INFO - tqdm - accuracy: 0.9978, batch_loss: 0.0198, loss: 0.0060 ||:  45%|####4     | 29/65 [05:30<06:51, 11.42s/it]
2022-03-30 21:52:39,366 - INFO - tqdm - accuracy: 0.9979, batch_loss: 0.0009, loss: 0.0059 ||:  46%|####6     | 30/65 [05:42<06:39, 11.42s/it]
2022-03-30 21:52:50,818 - INFO - tqdm - accuracy: 0.9980, batch_loss: 0.0004, loss: 0.0057 ||:  48%|####7     | 31/65 [05:53<06:28, 11.43s/it]
2022-03-30 21:53:02,225 - INFO - tqdm - accuracy: 0.9980, batch_loss: 0.0004, loss: 0.0055 ||:  49%|####9     | 32/65 [06:05<06:16, 11.42s/it]
2022-03-30 21:53:13,669 - INFO - tqdm - accuracy: 0.9981, batch_loss: 0.0006, loss: 0.0054 ||:  51%|#####     | 33/65 [06:16<06:05, 11.43s/it]
2022-03-30 21:53:25,115 - INFO - tqdm - accuracy: 0.9982, batch_loss: 0.0037, loss: 0.0053 ||:  52%|#####2    | 34/65 [06:28<05:54, 11.43s/it]
2022-03-30 21:53:36,565 - INFO - tqdm - accuracy: 0.9982, batch_loss: 0.0004, loss: 0.0052 ||:  54%|#####3    | 35/65 [06:39<05:43, 11.44s/it]
2022-03-30 21:53:47,999 - INFO - tqdm - accuracy: 0.9983, batch_loss: 0.0017, loss: 0.0051 ||:  55%|#####5    | 36/65 [06:51<05:31, 11.44s/it]
2022-03-30 21:53:59,436 - INFO - tqdm - accuracy: 0.9975, batch_loss: 0.0778, loss: 0.0071 ||:  57%|#####6    | 37/65 [07:02<05:20, 11.44s/it]
2022-03-30 21:54:10,864 - INFO - tqdm - accuracy: 0.9975, batch_loss: 0.0013, loss: 0.0069 ||:  58%|#####8    | 38/65 [07:13<05:08, 11.43s/it]
2022-03-30 21:54:22,311 - INFO - tqdm - accuracy: 0.9976, batch_loss: 0.0006, loss: 0.0067 ||:  60%|######    | 39/65 [07:25<04:57, 11.44s/it]
2022-03-30 21:54:33,741 - INFO - tqdm - accuracy: 0.9977, batch_loss: 0.0019, loss: 0.0066 ||:  62%|######1   | 40/65 [07:36<04:45, 11.44s/it]
2022-03-30 21:54:45,153 - INFO - tqdm - accuracy: 0.9977, batch_loss: 0.0007, loss: 0.0065 ||:  63%|######3   | 41/65 [07:48<04:34, 11.43s/it]
2022-03-30 21:54:56,595 - INFO - tqdm - accuracy: 0.9978, batch_loss: 0.0005, loss: 0.0063 ||:  65%|######4   | 42/65 [07:59<04:22, 11.43s/it]
2022-03-30 21:55:08,029 - INFO - tqdm - accuracy: 0.9978, batch_loss: 0.0011, loss: 0.0062 ||:  66%|######6   | 43/65 [08:11<04:11, 11.43s/it]
2022-03-30 21:55:19,479 - INFO - tqdm - accuracy: 0.9979, batch_loss: 0.0020, loss: 0.0061 ||:  68%|######7   | 44/65 [08:22<04:00, 11.44s/it]
2022-03-30 21:55:30,927 - INFO - tqdm - accuracy: 0.9979, batch_loss: 0.0005, loss: 0.0060 ||:  69%|######9   | 45/65 [08:33<03:48, 11.44s/it]
2022-03-30 21:55:42,363 - INFO - tqdm - accuracy: 0.9980, batch_loss: 0.0005, loss: 0.0059 ||:  71%|#######   | 46/65 [08:45<03:37, 11.44s/it]
2022-03-30 21:55:53,802 - INFO - tqdm - accuracy: 0.9980, batch_loss: 0.0005, loss: 0.0058 ||:  72%|#######2  | 47/65 [08:56<03:25, 11.44s/it]
2022-03-30 21:56:05,266 - INFO - tqdm - accuracy: 0.9974, batch_loss: 0.1196, loss: 0.0081 ||:  74%|#######3  | 48/65 [09:08<03:14, 11.45s/it]
2022-03-30 21:56:16,711 - INFO - tqdm - accuracy: 0.9974, batch_loss: 0.0006, loss: 0.0080 ||:  75%|#######5  | 49/65 [09:19<03:03, 11.45s/it]
2022-03-30 21:56:28,161 - INFO - tqdm - accuracy: 0.9975, batch_loss: 0.0007, loss: 0.0078 ||:  77%|#######6  | 50/65 [09:31<02:51, 11.45s/it]
2022-03-30 21:56:39,611 - INFO - tqdm - accuracy: 0.9975, batch_loss: 0.0032, loss: 0.0077 ||:  78%|#######8  | 51/65 [09:42<02:40, 11.45s/it]
2022-03-30 21:56:50,598 - INFO - tqdm - accuracy: 0.9976, batch_loss: 0.0159, loss: 0.0079 ||:  80%|########  | 52/65 [09:53<02:27, 11.31s/it]
2022-03-30 21:57:02,053 - INFO - tqdm - accuracy: 0.9976, batch_loss: 0.0004, loss: 0.0078 ||:  82%|########1 | 53/65 [10:05<02:16, 11.35s/it]
2022-03-30 21:57:13,494 - INFO - tqdm - accuracy: 0.9971, batch_loss: 0.1209, loss: 0.0099 ||:  83%|########3 | 54/65 [10:16<02:05, 11.38s/it]
2022-03-30 21:57:24,915 - INFO - tqdm - accuracy: 0.9972, batch_loss: 0.0029, loss: 0.0097 ||:  85%|########4 | 55/65 [10:27<01:53, 11.39s/it]
2022-03-30 21:57:36,091 - INFO - tqdm - accuracy: 0.9972, batch_loss: 0.0105, loss: 0.0097 ||:  86%|########6 | 56/65 [10:39<01:41, 11.33s/it]
2022-03-30 21:57:47,458 - INFO - tqdm - accuracy: 0.9973, batch_loss: 0.0140, loss: 0.0098 ||:  88%|########7 | 57/65 [10:50<01:30, 11.34s/it]
2022-03-30 21:57:58,877 - INFO - tqdm - accuracy: 0.9973, batch_loss: 0.0039, loss: 0.0097 ||:  89%|########9 | 58/65 [11:01<01:19, 11.36s/it]
2022-03-30 21:58:10,317 - INFO - tqdm - accuracy: 0.9974, batch_loss: 0.0007, loss: 0.0096 ||:  91%|######### | 59/65 [11:13<01:08, 11.39s/it]
2022-03-30 21:58:21,750 - INFO - tqdm - accuracy: 0.9974, batch_loss: 0.0011, loss: 0.0094 ||:  92%|#########2| 60/65 [11:24<00:57, 11.40s/it]
2022-03-30 21:58:33,180 - INFO - tqdm - accuracy: 0.9974, batch_loss: 0.0004, loss: 0.0093 ||:  94%|#########3| 61/65 [11:36<00:45, 11.41s/it]
2022-03-30 21:58:44,609 - INFO - tqdm - accuracy: 0.9975, batch_loss: 0.0008, loss: 0.0091 ||:  95%|#########5| 62/65 [11:47<00:34, 11.42s/it]
2022-03-30 21:58:56,050 - INFO - tqdm - accuracy: 0.9975, batch_loss: 0.0008, loss: 0.0090 ||:  97%|#########6| 63/65 [11:59<00:22, 11.42s/it]
2022-03-30 21:59:07,479 - INFO - tqdm - accuracy: 0.9976, batch_loss: 0.0006, loss: 0.0089 ||:  98%|#########8| 64/65 [12:10<00:11, 11.42s/it]
2022-03-30 21:59:12,553 - INFO - tqdm - accuracy: 0.9976, batch_loss: 0.0004, loss: 0.0087 ||: 100%|##########| 65/65 [12:15<00:00,  9.52s/it]
2022-03-30 21:59:12,554 - INFO - tqdm - accuracy: 0.9976, batch_loss: 0.0004, loss: 0.0087 ||: 100%|##########| 65/65 [12:15<00:00, 11.32s/it]
2022-03-30 21:59:15,443 - INFO - allennlp.training.trainer - Validating
2022-03-30 21:59:15,444 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-03-30 21:59:25,462 - INFO - tqdm - accuracy: 0.6375, batch_loss: 2.2751, loss: 2.4360 ||:  35%|###4      | 40/115 [00:10<00:18,  3.98it/s]
2022-03-30 21:59:35,560 - INFO - tqdm - accuracy: 0.6438, batch_loss: 4.8191, loss: 2.3699 ||:  70%|######9   | 80/115 [00:20<00:08,  3.98it/s]
2022-03-30 21:59:44,237 - INFO - tqdm - accuracy: 0.6201, batch_loss: 3.1588, loss: 2.5100 ||: 100%|##########| 115/115 [00:28<00:00,  3.93it/s]
2022-03-30 21:59:44,238 - INFO - tqdm - accuracy: 0.6201, batch_loss: 3.1588, loss: 2.5100 ||: 100%|##########| 115/115 [00:28<00:00,  3.99it/s]
2022-03-30 21:59:44,238 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-03-30 21:59:44,238 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.998  |     0.620
2022-03-30 21:59:44,239 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-03-30 21:59:44,240 - INFO - allennlp.training.tensorboard_writer - loss               |     0.009  |     2.510
2022-03-30 21:59:44,241 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5793.102  |       N/A
2022-03-30 21:59:50,392 - INFO - allennlp.training.trainer - Epoch duration: 0:12:53.423774
2022-03-30 21:59:50,392 - INFO - allennlp.training.checkpointer - loading best weights
2022-03-30 21:59:51,318 - INFO - allennlp.common.util - Metrics: {
  "best_epoch": 3,
  "peak_worker_0_memory_MB": 5793.1015625,
  "peak_gpu_0_memory_MB": 9580.5712890625,
  "training_duration": "3:13:51.882069",
  "training_start_epoch": 0,
  "training_epochs": 14,
  "epoch": 14,
  "training_accuracy": 0.997573993207181,
  "training_loss": 0.00873826548805972,
  "training_worker_0_memory_MB": 5793.1015625,
  "training_gpu_0_memory_MB": 9580.5712890625,
  "validation_accuracy": 0.6200873362445415,
  "validation_loss": 2.51002111101375,
  "best_validation_accuracy": 0.6681222707423581,
  "best_validation_loss": 0.9163556621531429
}
2022-03-30 21:59:51,319 - INFO - allennlp.models.archival - archiving weights and vocabulary to ./output_ir-ora-d/model.tar.gz
