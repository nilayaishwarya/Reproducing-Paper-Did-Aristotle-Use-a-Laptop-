2022-04-13 17:40:27,208 - INFO - allennlp.common.params - random_seed = 42
2022-04-13 17:40:27,209 - INFO - allennlp.common.params - numpy_seed = 42
2022-04-13 17:40:27,209 - INFO - allennlp.common.params - pytorch_seed = 42
2022-04-13 17:40:27,886 - INFO - allennlp.common.checks - Pytorch version: 1.7.1
2022-04-13 17:40:27,888 - INFO - allennlp.common.params - type = default
2022-04-13 17:40:27,891 - INFO - allennlp.common.params - dataset_reader.type = strategy_qa_reader
2022-04-13 17:40:27,891 - INFO - allennlp.common.params - dataset_reader.lazy = False
2022-04-13 17:40:27,891 - INFO - allennlp.common.params - dataset_reader.cache_directory = None
2022-04-13 17:40:27,891 - INFO - allennlp.common.params - dataset_reader.max_instances = None
2022-04-13 17:40:27,892 - INFO - allennlp.common.params - dataset_reader.manual_distributed_sharding = False
2022-04-13 17:40:27,892 - INFO - allennlp.common.params - dataset_reader.manual_multi_process_sharding = False
2022-04-13 17:40:27,892 - INFO - allennlp.common.params - dataset_reader.tokenizer_wrapper.type = default
2022-04-13 17:40:27,892 - INFO - allennlp.common.params - dataset_reader.tokenizer_wrapper.pretrained_model = roberta-large
2022-04-13 17:40:27,892 - INFO - allennlp.common.params - dataset_reader.tokenizer_wrapper.call_kwargs.return_offsets_mapping = True
2022-04-13 17:40:27,892 - INFO - allennlp.common.params - dataset_reader.tokenizer_wrapper.call_kwargs.return_special_tokens_mask = True
2022-04-13 17:40:27,892 - INFO - allennlp.common.params - dataset_reader.tokenizer_wrapper.call_kwargs.truncation = only_first
2022-04-13 17:40:29,103 - INFO - allennlp.common.params - dataset_reader.save_tokenizer = True
2022-04-13 17:40:29,104 - INFO - allennlp.common.params - dataset_reader.is_training = True
2022-04-13 17:40:29,104 - INFO - allennlp.common.params - dataset_reader.pickle.action = None
2022-04-13 17:40:29,104 - INFO - allennlp.common.params - dataset_reader.pickle.file_name = None
2022-04-13 17:40:29,104 - INFO - allennlp.common.params - dataset_reader.pickle.path = ../pickle
2022-04-13 17:40:29,104 - INFO - allennlp.common.params - dataset_reader.pickle.save_even_when_max_instances = False
2022-04-13 17:40:29,104 - INFO - allennlp.common.params - dataset_reader.paragraphs_source = IR-ORA-D
2022-04-13 17:40:29,104 - INFO - allennlp.common.params - dataset_reader.answer_last_decomposition_step = False
2022-04-13 17:40:29,104 - INFO - allennlp.common.params - dataset_reader.skip_if_context_missing = True
2022-04-13 17:40:29,104 - INFO - allennlp.common.params - dataset_reader.paragraphs_limit = 10
2022-04-13 17:40:29,104 - INFO - allennlp.common.params - dataset_reader.generated_decompositions_paths = None
2022-04-13 17:40:29,104 - INFO - allennlp.common.params - dataset_reader.save_elasticsearch_cache = True
2022-04-13 17:40:29,548 - INFO - filelock - Lock 139992087176144 acquired on cache.lock
2022-04-13 17:40:30,221 - INFO - filelock - Lock 139992087176144 released on cache.lock
2022-04-13 17:40:30,221 - INFO - allennlp.common.params - train_data_path = data/strategyqa/train.json
2022-04-13 17:40:30,222 - INFO - allennlp.common.params - vocabulary = <allennlp.common.lazy.Lazy object at 0x7f5272b97fd0>
2022-04-13 17:40:30,222 - INFO - allennlp.common.params - datasets_for_vocab_creation = None
2022-04-13 17:40:30,222 - INFO - allennlp.common.params - validation_dataset_reader.type = strategy_qa_reader
2022-04-13 17:40:30,222 - INFO - allennlp.common.params - validation_dataset_reader.lazy = False
2022-04-13 17:40:30,222 - INFO - allennlp.common.params - validation_dataset_reader.cache_directory = None
2022-04-13 17:40:30,222 - INFO - allennlp.common.params - validation_dataset_reader.max_instances = None
2022-04-13 17:40:30,223 - INFO - allennlp.common.params - validation_dataset_reader.manual_distributed_sharding = False
2022-04-13 17:40:30,223 - INFO - allennlp.common.params - validation_dataset_reader.manual_multi_process_sharding = False
2022-04-13 17:40:30,223 - INFO - allennlp.common.params - validation_dataset_reader.tokenizer_wrapper.type = default
2022-04-13 17:40:30,223 - INFO - allennlp.common.params - validation_dataset_reader.tokenizer_wrapper.pretrained_model = roberta-large
2022-04-13 17:40:30,223 - INFO - allennlp.common.params - validation_dataset_reader.tokenizer_wrapper.call_kwargs.return_offsets_mapping = True
2022-04-13 17:40:30,223 - INFO - allennlp.common.params - validation_dataset_reader.tokenizer_wrapper.call_kwargs.return_special_tokens_mask = True
2022-04-13 17:40:30,223 - INFO - allennlp.common.params - validation_dataset_reader.tokenizer_wrapper.call_kwargs.truncation = only_first
2022-04-13 17:40:31,441 - INFO - allennlp.common.params - validation_dataset_reader.save_tokenizer = False
2022-04-13 17:40:31,441 - INFO - allennlp.common.params - validation_dataset_reader.is_training = False
2022-04-13 17:40:31,441 - INFO - allennlp.common.params - validation_dataset_reader.pickle.action = None
2022-04-13 17:40:31,441 - INFO - allennlp.common.params - validation_dataset_reader.pickle.file_name = None
2022-04-13 17:40:31,442 - INFO - allennlp.common.params - validation_dataset_reader.pickle.path = ../pickle
2022-04-13 17:40:31,442 - INFO - allennlp.common.params - validation_dataset_reader.pickle.save_even_when_max_instances = False
2022-04-13 17:40:31,442 - INFO - allennlp.common.params - validation_dataset_reader.paragraphs_source = IR-ORA-D
2022-04-13 17:40:31,442 - INFO - allennlp.common.params - validation_dataset_reader.answer_last_decomposition_step = False
2022-04-13 17:40:31,442 - INFO - allennlp.common.params - validation_dataset_reader.skip_if_context_missing = True
2022-04-13 17:40:31,442 - INFO - allennlp.common.params - validation_dataset_reader.paragraphs_limit = 10
2022-04-13 17:40:31,442 - INFO - allennlp.common.params - validation_dataset_reader.generated_decompositions_paths = None
2022-04-13 17:40:31,442 - INFO - allennlp.common.params - validation_dataset_reader.save_elasticsearch_cache = True
2022-04-13 17:40:31,442 - INFO - filelock - Lock 139992087834320 acquired on cache.lock
2022-04-13 17:40:32,270 - INFO - filelock - Lock 139992087834320 released on cache.lock
2022-04-13 17:40:32,270 - INFO - allennlp.common.params - validation_data_path = data/strategyqa/dev.json
2022-04-13 17:40:32,270 - INFO - allennlp.common.params - validation_data_loader = None
2022-04-13 17:40:32,270 - INFO - allennlp.common.params - test_data_path = None
2022-04-13 17:40:32,270 - INFO - allennlp.common.params - evaluate_on_test = True
2022-04-13 17:40:32,271 - INFO - allennlp.common.params - batch_weight_key = 
2022-04-13 17:40:32,271 - INFO - allennlp.training.util - Reading training data from data/strategyqa/train.json
2022-04-13 17:40:32,271 - INFO - tqdm - reading instances: 0it [00:00, ?it/s]
2022-04-13 17:40:32,271 - INFO - src.data.dataset_readers.base.base_dataset_reader - Reading the dataset from scratch
2022-04-13 17:40:32,271 - INFO - src.data.dataset_readers.strategy_qa_reader - Reading the dataset:
2022-04-13 17:40:32,271 - INFO - src.data.dataset_readers.strategy_qa_reader - Reading file at data/strategyqa/train.json
2022-04-13 17:40:39,553 - INFO - filelock - Lock 139991266064208 acquired on cache.lock
2022-04-13 17:40:40,347 - INFO - filelock - Lock 139991266064208 released on cache.lock
2022-04-13 17:40:40,383 - INFO - allennlp.training.util - Reading validation data from data/strategyqa/dev.json
2022-04-13 17:40:40,383 - INFO - tqdm - reading instances: 0it [00:00, ?it/s]
2022-04-13 17:40:40,383 - INFO - src.data.dataset_readers.base.base_dataset_reader - Reading the dataset from scratch
2022-04-13 17:40:40,383 - INFO - src.data.dataset_readers.strategy_qa_reader - Reading the dataset:
2022-04-13 17:40:40,384 - INFO - src.data.dataset_readers.strategy_qa_reader - Reading file at data/strategyqa/dev.json
2022-04-13 17:40:40,827 - INFO - src.data.dataset_readers.utils.elasticsearch_utils - Empty results from Elasticsearch for: actions commercial pilots take concerning engine noise arriving departing Orange County, California?
2022-04-13 17:40:41,151 - INFO - filelock - Lock 139991273481808 acquired on cache.lock
2022-04-13 17:40:41,939 - INFO - filelock - Lock 139991273481808 released on cache.lock
2022-04-13 17:40:41,975 - INFO - allennlp.common.params - type = from_instances
2022-04-13 17:40:41,975 - INFO - allennlp.common.params - min_count = None
2022-04-13 17:40:41,975 - INFO - allennlp.common.params - max_vocab_size = None
2022-04-13 17:40:41,975 - INFO - allennlp.common.params - non_padded_namespaces = ('*tags', '*labels')
2022-04-13 17:40:41,975 - INFO - allennlp.common.params - pretrained_files = None
2022-04-13 17:40:41,976 - INFO - allennlp.common.params - only_include_pretrained_words = False
2022-04-13 17:40:41,976 - INFO - allennlp.common.params - tokens_to_add = None
2022-04-13 17:40:41,976 - INFO - allennlp.common.params - min_pretrained_embeddings = None
2022-04-13 17:40:41,976 - INFO - allennlp.common.params - padding_token = @@PADDING@@
2022-04-13 17:40:41,976 - INFO - allennlp.common.params - oov_token = @@UNKNOWN@@
2022-04-13 17:40:41,976 - INFO - allennlp.data.vocabulary - Fitting token dictionary from dataset.
2022-04-13 17:40:41,976 - INFO - tqdm - building vocab: 0it [00:00, ?it/s]
2022-04-13 17:40:41,982 - INFO - allennlp.common.params - model.type = hf_classifier
2022-04-13 17:40:41,983 - INFO - allennlp.common.params - model.regularizer = None
2022-04-13 17:40:41,983 - INFO - allennlp.common.params - model.pretrained_model = roberta-large
2022-04-13 17:40:41,983 - INFO - allennlp.common.params - model.tokenizer_wrapper.type = default
2022-04-13 17:40:41,983 - INFO - allennlp.common.params - model.tokenizer_wrapper.pretrained_model = roberta-large
2022-04-13 17:40:41,984 - INFO - allennlp.common.params - model.tokenizer_wrapper.call_kwargs.return_offsets_mapping = True
2022-04-13 17:40:41,984 - INFO - allennlp.common.params - model.tokenizer_wrapper.call_kwargs.return_special_tokens_mask = True
2022-04-13 17:40:41,984 - INFO - allennlp.common.params - model.tokenizer_wrapper.call_kwargs.truncation = only_first
2022-04-13 17:40:43,167 - INFO - allennlp.common.params - model.num_labels = 2
2022-04-13 17:40:43,167 - INFO - allennlp.common.params - model.label_namespace = labels
2022-04-13 17:40:43,169 - INFO - allennlp.common.params - model.transformer_weights_path = None
2022-04-13 17:40:43,169 - INFO - allennlp.common.params - model.initializer.regexes.0.1.type = pretrained
2022-04-13 17:40:43,169 - INFO - allennlp.common.params - model.initializer.regexes.0.1.weights_file_path = output_roberta-star/model.tar.gz
2022-04-13 17:40:43,171 - INFO - allennlp.models.archival - extracting archive file output_roberta-star/model.tar.gz to temp dir /tmp/tmpwc6sv973
2022-04-13 17:40:55,285 - INFO - allennlp.models.archival - removing temporary unarchived model dir at /tmp/tmpwc6sv973
2022-04-13 17:40:55,461 - INFO - allennlp.common.params - model.initializer.prevent_regexes = None
2022-04-13 17:41:19,664 - INFO - allennlp.nn.initializers - Initializing parameters
2022-04-13 17:41:19,665 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.embeddings.word_embeddings.weight using .* initializer
2022-04-13 17:41:19,674 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.embeddings.position_embeddings.weight using .* initializer
2022-04-13 17:41:19,674 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.embeddings.token_type_embeddings.weight using .* initializer
2022-04-13 17:41:19,675 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.embeddings.LayerNorm.weight using .* initializer
2022-04-13 17:41:19,675 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.embeddings.LayerNorm.bias using .* initializer
2022-04-13 17:41:19,675 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.self.query.weight using .* initializer
2022-04-13 17:41:19,675 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.self.query.bias using .* initializer
2022-04-13 17:41:19,676 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.self.key.weight using .* initializer
2022-04-13 17:41:19,676 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.self.key.bias using .* initializer
2022-04-13 17:41:19,676 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.self.value.weight using .* initializer
2022-04-13 17:41:19,676 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.self.value.bias using .* initializer
2022-04-13 17:41:19,677 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.output.dense.weight using .* initializer
2022-04-13 17:41:19,677 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.output.dense.bias using .* initializer
2022-04-13 17:41:19,677 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.output.LayerNorm.weight using .* initializer
2022-04-13 17:41:19,677 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.output.LayerNorm.bias using .* initializer
2022-04-13 17:41:19,677 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.intermediate.dense.weight using .* initializer
2022-04-13 17:41:19,678 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.intermediate.dense.bias using .* initializer
2022-04-13 17:41:19,679 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.output.dense.weight using .* initializer
2022-04-13 17:41:19,680 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.output.dense.bias using .* initializer
2022-04-13 17:41:19,680 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.output.LayerNorm.weight using .* initializer
2022-04-13 17:41:19,680 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.output.LayerNorm.bias using .* initializer
2022-04-13 17:41:19,680 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.self.query.weight using .* initializer
2022-04-13 17:41:19,680 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.self.query.bias using .* initializer
2022-04-13 17:41:19,681 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.self.key.weight using .* initializer
2022-04-13 17:41:19,681 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.self.key.bias using .* initializer
2022-04-13 17:41:19,681 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.self.value.weight using .* initializer
2022-04-13 17:41:19,681 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.self.value.bias using .* initializer
2022-04-13 17:41:19,682 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.output.dense.weight using .* initializer
2022-04-13 17:41:19,682 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.output.dense.bias using .* initializer
2022-04-13 17:41:19,682 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.output.LayerNorm.weight using .* initializer
2022-04-13 17:41:19,682 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.output.LayerNorm.bias using .* initializer
2022-04-13 17:41:19,682 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.intermediate.dense.weight using .* initializer
2022-04-13 17:41:19,683 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.intermediate.dense.bias using .* initializer
2022-04-13 17:41:19,683 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.output.dense.weight using .* initializer
2022-04-13 17:41:19,684 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.output.dense.bias using .* initializer
2022-04-13 17:41:19,685 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.output.LayerNorm.weight using .* initializer
2022-04-13 17:41:19,685 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.output.LayerNorm.bias using .* initializer
2022-04-13 17:41:19,685 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.self.query.weight using .* initializer
2022-04-13 17:41:19,685 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.self.query.bias using .* initializer
2022-04-13 17:41:19,685 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.self.key.weight using .* initializer
2022-04-13 17:41:19,686 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.self.key.bias using .* initializer
2022-04-13 17:41:19,686 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.self.value.weight using .* initializer
2022-04-13 17:41:19,686 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.self.value.bias using .* initializer
2022-04-13 17:41:19,686 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.output.dense.weight using .* initializer
2022-04-13 17:41:19,687 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.output.dense.bias using .* initializer
2022-04-13 17:41:19,687 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.output.LayerNorm.weight using .* initializer
2022-04-13 17:41:19,687 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.output.LayerNorm.bias using .* initializer
2022-04-13 17:41:19,687 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.intermediate.dense.weight using .* initializer
2022-04-13 17:41:19,688 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.intermediate.dense.bias using .* initializer
2022-04-13 17:41:19,688 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.output.dense.weight using .* initializer
2022-04-13 17:41:19,689 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.output.dense.bias using .* initializer
2022-04-13 17:41:19,690 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.output.LayerNorm.weight using .* initializer
2022-04-13 17:41:19,690 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.output.LayerNorm.bias using .* initializer
2022-04-13 17:41:19,690 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.self.query.weight using .* initializer
2022-04-13 17:41:19,690 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.self.query.bias using .* initializer
2022-04-13 17:41:19,691 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.self.key.weight using .* initializer
2022-04-13 17:41:19,691 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.self.key.bias using .* initializer
2022-04-13 17:41:19,691 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.self.value.weight using .* initializer
2022-04-13 17:41:19,692 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.self.value.bias using .* initializer
2022-04-13 17:41:19,692 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.output.dense.weight using .* initializer
2022-04-13 17:41:19,692 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.output.dense.bias using .* initializer
2022-04-13 17:41:19,692 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.output.LayerNorm.weight using .* initializer
2022-04-13 17:41:19,692 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.output.LayerNorm.bias using .* initializer
2022-04-13 17:41:19,693 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.intermediate.dense.weight using .* initializer
2022-04-13 17:41:19,694 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.intermediate.dense.bias using .* initializer
2022-04-13 17:41:19,694 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.output.dense.weight using .* initializer
2022-04-13 17:41:19,695 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.output.dense.bias using .* initializer
2022-04-13 17:41:19,695 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.output.LayerNorm.weight using .* initializer
2022-04-13 17:41:19,695 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.output.LayerNorm.bias using .* initializer
2022-04-13 17:41:19,695 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.self.query.weight using .* initializer
2022-04-13 17:41:19,696 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.self.query.bias using .* initializer
2022-04-13 17:41:19,696 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.self.key.weight using .* initializer
2022-04-13 17:41:19,696 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.self.key.bias using .* initializer
2022-04-13 17:41:19,696 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.self.value.weight using .* initializer
2022-04-13 17:41:19,697 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.self.value.bias using .* initializer
2022-04-13 17:41:19,697 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.output.dense.weight using .* initializer
2022-04-13 17:41:19,697 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.output.dense.bias using .* initializer
2022-04-13 17:41:19,698 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.output.LayerNorm.weight using .* initializer
2022-04-13 17:41:19,698 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.output.LayerNorm.bias using .* initializer
2022-04-13 17:41:19,698 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.intermediate.dense.weight using .* initializer
2022-04-13 17:41:19,699 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.intermediate.dense.bias using .* initializer
2022-04-13 17:41:19,699 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.output.dense.weight using .* initializer
2022-04-13 17:41:19,700 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.output.dense.bias using .* initializer
2022-04-13 17:41:19,700 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.output.LayerNorm.weight using .* initializer
2022-04-13 17:41:19,700 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.output.LayerNorm.bias using .* initializer
2022-04-13 17:41:19,701 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.self.query.weight using .* initializer
2022-04-13 17:41:19,701 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.self.query.bias using .* initializer
2022-04-13 17:41:19,701 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.self.key.weight using .* initializer
2022-04-13 17:41:19,702 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.self.key.bias using .* initializer
2022-04-13 17:41:19,702 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.self.value.weight using .* initializer
2022-04-13 17:41:19,702 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.self.value.bias using .* initializer
2022-04-13 17:41:19,702 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.output.dense.weight using .* initializer
2022-04-13 17:41:19,703 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.output.dense.bias using .* initializer
2022-04-13 17:41:19,703 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.output.LayerNorm.weight using .* initializer
2022-04-13 17:41:19,703 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.output.LayerNorm.bias using .* initializer
2022-04-13 17:41:19,703 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.intermediate.dense.weight using .* initializer
2022-04-13 17:41:19,704 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.intermediate.dense.bias using .* initializer
2022-04-13 17:41:19,704 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.output.dense.weight using .* initializer
2022-04-13 17:41:19,705 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.output.dense.bias using .* initializer
2022-04-13 17:41:19,706 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.output.LayerNorm.weight using .* initializer
2022-04-13 17:41:19,706 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.output.LayerNorm.bias using .* initializer
2022-04-13 17:41:19,706 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.self.query.weight using .* initializer
2022-04-13 17:41:19,706 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.self.query.bias using .* initializer
2022-04-13 17:41:19,707 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.self.key.weight using .* initializer
2022-04-13 17:41:19,707 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.self.key.bias using .* initializer
2022-04-13 17:41:19,707 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.self.value.weight using .* initializer
2022-04-13 17:41:19,707 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.self.value.bias using .* initializer
2022-04-13 17:41:19,708 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.output.dense.weight using .* initializer
2022-04-13 17:41:19,708 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.output.dense.bias using .* initializer
2022-04-13 17:41:19,708 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.output.LayerNorm.weight using .* initializer
2022-04-13 17:41:19,708 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.output.LayerNorm.bias using .* initializer
2022-04-13 17:41:19,709 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.intermediate.dense.weight using .* initializer
2022-04-13 17:41:19,710 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.intermediate.dense.bias using .* initializer
2022-04-13 17:41:19,710 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.output.dense.weight using .* initializer
2022-04-13 17:41:19,711 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.output.dense.bias using .* initializer
2022-04-13 17:41:19,711 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.output.LayerNorm.weight using .* initializer
2022-04-13 17:41:19,711 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.output.LayerNorm.bias using .* initializer
2022-04-13 17:41:19,711 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.self.query.weight using .* initializer
2022-04-13 17:41:19,711 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.self.query.bias using .* initializer
2022-04-13 17:41:19,712 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.self.key.weight using .* initializer
2022-04-13 17:41:19,712 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.self.key.bias using .* initializer
2022-04-13 17:41:19,712 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.self.value.weight using .* initializer
2022-04-13 17:41:19,712 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.self.value.bias using .* initializer
2022-04-13 17:41:19,713 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.output.dense.weight using .* initializer
2022-04-13 17:41:19,713 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.output.dense.bias using .* initializer
2022-04-13 17:41:19,713 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.output.LayerNorm.weight using .* initializer
2022-04-13 17:41:19,713 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.output.LayerNorm.bias using .* initializer
2022-04-13 17:41:19,713 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.intermediate.dense.weight using .* initializer
2022-04-13 17:41:19,714 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.intermediate.dense.bias using .* initializer
2022-04-13 17:41:19,714 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.output.dense.weight using .* initializer
2022-04-13 17:41:19,715 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.output.dense.bias using .* initializer
2022-04-13 17:41:19,715 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.output.LayerNorm.weight using .* initializer
2022-04-13 17:41:19,716 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.output.LayerNorm.bias using .* initializer
2022-04-13 17:41:19,716 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.self.query.weight using .* initializer
2022-04-13 17:41:19,716 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.self.query.bias using .* initializer
2022-04-13 17:41:19,716 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.self.key.weight using .* initializer
2022-04-13 17:41:19,716 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.self.key.bias using .* initializer
2022-04-13 17:41:19,717 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.self.value.weight using .* initializer
2022-04-13 17:41:19,717 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.self.value.bias using .* initializer
2022-04-13 17:41:19,717 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.output.dense.weight using .* initializer
2022-04-13 17:41:19,717 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.output.dense.bias using .* initializer
2022-04-13 17:41:19,718 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.output.LayerNorm.weight using .* initializer
2022-04-13 17:41:19,718 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.output.LayerNorm.bias using .* initializer
2022-04-13 17:41:19,718 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.intermediate.dense.weight using .* initializer
2022-04-13 17:41:19,719 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.intermediate.dense.bias using .* initializer
2022-04-13 17:41:19,719 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.output.dense.weight using .* initializer
2022-04-13 17:41:19,720 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.output.dense.bias using .* initializer
2022-04-13 17:41:19,720 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.output.LayerNorm.weight using .* initializer
2022-04-13 17:41:19,720 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.output.LayerNorm.bias using .* initializer
2022-04-13 17:41:19,720 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.self.query.weight using .* initializer
2022-04-13 17:41:19,721 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.self.query.bias using .* initializer
2022-04-13 17:41:19,721 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.self.key.weight using .* initializer
2022-04-13 17:41:19,721 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.self.key.bias using .* initializer
2022-04-13 17:41:19,721 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.self.value.weight using .* initializer
2022-04-13 17:41:19,722 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.self.value.bias using .* initializer
2022-04-13 17:41:19,722 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.output.dense.weight using .* initializer
2022-04-13 17:41:19,722 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.output.dense.bias using .* initializer
2022-04-13 17:41:19,722 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.output.LayerNorm.weight using .* initializer
2022-04-13 17:41:19,722 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.output.LayerNorm.bias using .* initializer
2022-04-13 17:41:19,723 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.intermediate.dense.weight using .* initializer
2022-04-13 17:41:19,724 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.intermediate.dense.bias using .* initializer
2022-04-13 17:41:19,724 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.output.dense.weight using .* initializer
2022-04-13 17:41:19,725 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.output.dense.bias using .* initializer
2022-04-13 17:41:19,725 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.output.LayerNorm.weight using .* initializer
2022-04-13 17:41:19,725 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.output.LayerNorm.bias using .* initializer
2022-04-13 17:41:19,725 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.self.query.weight using .* initializer
2022-04-13 17:41:19,725 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.self.query.bias using .* initializer
2022-04-13 17:41:19,726 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.self.key.weight using .* initializer
2022-04-13 17:41:19,726 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.self.key.bias using .* initializer
2022-04-13 17:41:19,726 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.self.value.weight using .* initializer
2022-04-13 17:41:19,726 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.self.value.bias using .* initializer
2022-04-13 17:41:19,726 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.output.dense.weight using .* initializer
2022-04-13 17:41:19,727 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.output.dense.bias using .* initializer
2022-04-13 17:41:19,727 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.output.LayerNorm.weight using .* initializer
2022-04-13 17:41:19,727 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.output.LayerNorm.bias using .* initializer
2022-04-13 17:41:19,727 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.intermediate.dense.weight using .* initializer
2022-04-13 17:41:19,728 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.intermediate.dense.bias using .* initializer
2022-04-13 17:41:19,728 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.output.dense.weight using .* initializer
2022-04-13 17:41:19,729 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.output.dense.bias using .* initializer
2022-04-13 17:41:19,730 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.output.LayerNorm.weight using .* initializer
2022-04-13 17:41:19,730 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.output.LayerNorm.bias using .* initializer
2022-04-13 17:41:19,730 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.self.query.weight using .* initializer
2022-04-13 17:41:19,730 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.self.query.bias using .* initializer
2022-04-13 17:41:19,730 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.self.key.weight using .* initializer
2022-04-13 17:41:19,731 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.self.key.bias using .* initializer
2022-04-13 17:41:19,731 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.self.value.weight using .* initializer
2022-04-13 17:41:19,731 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.self.value.bias using .* initializer
2022-04-13 17:41:19,731 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.output.dense.weight using .* initializer
2022-04-13 17:41:19,732 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.output.dense.bias using .* initializer
2022-04-13 17:41:19,732 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.output.LayerNorm.weight using .* initializer
2022-04-13 17:41:19,732 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.output.LayerNorm.bias using .* initializer
2022-04-13 17:41:19,732 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.intermediate.dense.weight using .* initializer
2022-04-13 17:41:19,733 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.intermediate.dense.bias using .* initializer
2022-04-13 17:41:19,733 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.output.dense.weight using .* initializer
2022-04-13 17:41:19,734 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.output.dense.bias using .* initializer
2022-04-13 17:41:19,734 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.output.LayerNorm.weight using .* initializer
2022-04-13 17:41:19,734 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.output.LayerNorm.bias using .* initializer
2022-04-13 17:41:19,734 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.self.query.weight using .* initializer
2022-04-13 17:41:19,735 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.self.query.bias using .* initializer
2022-04-13 17:41:19,735 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.self.key.weight using .* initializer
2022-04-13 17:41:19,735 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.self.key.bias using .* initializer
2022-04-13 17:41:19,735 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.self.value.weight using .* initializer
2022-04-13 17:41:19,736 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.self.value.bias using .* initializer
2022-04-13 17:41:19,736 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.output.dense.weight using .* initializer
2022-04-13 17:41:19,736 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.output.dense.bias using .* initializer
2022-04-13 17:41:19,736 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.output.LayerNorm.weight using .* initializer
2022-04-13 17:41:19,737 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.output.LayerNorm.bias using .* initializer
2022-04-13 17:41:19,737 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.intermediate.dense.weight using .* initializer
2022-04-13 17:41:19,738 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.intermediate.dense.bias using .* initializer
2022-04-13 17:41:19,738 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.output.dense.weight using .* initializer
2022-04-13 17:41:19,739 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.output.dense.bias using .* initializer
2022-04-13 17:41:19,739 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.output.LayerNorm.weight using .* initializer
2022-04-13 17:41:19,739 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.output.LayerNorm.bias using .* initializer
2022-04-13 17:41:19,739 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.self.query.weight using .* initializer
2022-04-13 17:41:19,740 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.self.query.bias using .* initializer
2022-04-13 17:41:19,740 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.self.key.weight using .* initializer
2022-04-13 17:41:19,740 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.self.key.bias using .* initializer
2022-04-13 17:41:19,740 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.self.value.weight using .* initializer
2022-04-13 17:41:19,741 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.self.value.bias using .* initializer
2022-04-13 17:41:19,741 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.output.dense.weight using .* initializer
2022-04-13 17:41:19,741 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.output.dense.bias using .* initializer
2022-04-13 17:41:19,741 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.output.LayerNorm.weight using .* initializer
2022-04-13 17:41:19,741 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.output.LayerNorm.bias using .* initializer
2022-04-13 17:41:19,741 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.intermediate.dense.weight using .* initializer
2022-04-13 17:41:19,742 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.intermediate.dense.bias using .* initializer
2022-04-13 17:41:19,743 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.output.dense.weight using .* initializer
2022-04-13 17:41:19,744 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.output.dense.bias using .* initializer
2022-04-13 17:41:19,744 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.output.LayerNorm.weight using .* initializer
2022-04-13 17:41:19,744 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.output.LayerNorm.bias using .* initializer
2022-04-13 17:41:19,744 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.self.query.weight using .* initializer
2022-04-13 17:41:19,744 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.self.query.bias using .* initializer
2022-04-13 17:41:19,744 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.self.key.weight using .* initializer
2022-04-13 17:41:19,745 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.self.key.bias using .* initializer
2022-04-13 17:41:19,745 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.self.value.weight using .* initializer
2022-04-13 17:41:19,745 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.self.value.bias using .* initializer
2022-04-13 17:41:19,745 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.output.dense.weight using .* initializer
2022-04-13 17:41:19,746 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.output.dense.bias using .* initializer
2022-04-13 17:41:19,746 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.output.LayerNorm.weight using .* initializer
2022-04-13 17:41:19,746 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.output.LayerNorm.bias using .* initializer
2022-04-13 17:41:19,746 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.intermediate.dense.weight using .* initializer
2022-04-13 17:41:19,747 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.intermediate.dense.bias using .* initializer
2022-04-13 17:41:19,747 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.output.dense.weight using .* initializer
2022-04-13 17:41:19,748 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.output.dense.bias using .* initializer
2022-04-13 17:41:19,748 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.output.LayerNorm.weight using .* initializer
2022-04-13 17:41:19,749 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.output.LayerNorm.bias using .* initializer
2022-04-13 17:41:19,749 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.self.query.weight using .* initializer
2022-04-13 17:41:19,749 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.self.query.bias using .* initializer
2022-04-13 17:41:19,749 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.self.key.weight using .* initializer
2022-04-13 17:41:19,750 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.self.key.bias using .* initializer
2022-04-13 17:41:19,750 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.self.value.weight using .* initializer
2022-04-13 17:41:19,750 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.self.value.bias using .* initializer
2022-04-13 17:41:19,750 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.output.dense.weight using .* initializer
2022-04-13 17:41:19,751 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.output.dense.bias using .* initializer
2022-04-13 17:41:19,751 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.output.LayerNorm.weight using .* initializer
2022-04-13 17:41:19,751 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.output.LayerNorm.bias using .* initializer
2022-04-13 17:41:19,751 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.intermediate.dense.weight using .* initializer
2022-04-13 17:41:19,752 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.intermediate.dense.bias using .* initializer
2022-04-13 17:41:19,752 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.output.dense.weight using .* initializer
2022-04-13 17:41:19,753 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.output.dense.bias using .* initializer
2022-04-13 17:41:19,753 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.output.LayerNorm.weight using .* initializer
2022-04-13 17:41:19,753 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.output.LayerNorm.bias using .* initializer
2022-04-13 17:41:19,753 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.self.query.weight using .* initializer
2022-04-13 17:41:19,754 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.self.query.bias using .* initializer
2022-04-13 17:41:19,754 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.self.key.weight using .* initializer
2022-04-13 17:41:19,754 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.self.key.bias using .* initializer
2022-04-13 17:41:19,754 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.self.value.weight using .* initializer
2022-04-13 17:41:19,755 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.self.value.bias using .* initializer
2022-04-13 17:41:19,755 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.output.dense.weight using .* initializer
2022-04-13 17:41:19,755 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.output.dense.bias using .* initializer
2022-04-13 17:41:19,755 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.output.LayerNorm.weight using .* initializer
2022-04-13 17:41:19,756 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.output.LayerNorm.bias using .* initializer
2022-04-13 17:41:19,756 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.intermediate.dense.weight using .* initializer
2022-04-13 17:41:19,757 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.intermediate.dense.bias using .* initializer
2022-04-13 17:41:19,757 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.output.dense.weight using .* initializer
2022-04-13 17:41:19,758 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.output.dense.bias using .* initializer
2022-04-13 17:41:19,758 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.output.LayerNorm.weight using .* initializer
2022-04-13 17:41:19,758 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.output.LayerNorm.bias using .* initializer
2022-04-13 17:41:19,758 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.self.query.weight using .* initializer
2022-04-13 17:41:19,759 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.self.query.bias using .* initializer
2022-04-13 17:41:19,759 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.self.key.weight using .* initializer
2022-04-13 17:41:19,759 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.self.key.bias using .* initializer
2022-04-13 17:41:19,759 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.self.value.weight using .* initializer
2022-04-13 17:41:19,760 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.self.value.bias using .* initializer
2022-04-13 17:41:19,760 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.output.dense.weight using .* initializer
2022-04-13 17:41:19,760 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.output.dense.bias using .* initializer
2022-04-13 17:41:19,760 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.output.LayerNorm.weight using .* initializer
2022-04-13 17:41:19,760 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.output.LayerNorm.bias using .* initializer
2022-04-13 17:41:19,761 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.intermediate.dense.weight using .* initializer
2022-04-13 17:41:19,761 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.intermediate.dense.bias using .* initializer
2022-04-13 17:41:19,762 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.output.dense.weight using .* initializer
2022-04-13 17:41:19,763 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.output.dense.bias using .* initializer
2022-04-13 17:41:19,763 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.output.LayerNorm.weight using .* initializer
2022-04-13 17:41:19,763 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.output.LayerNorm.bias using .* initializer
2022-04-13 17:41:19,763 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.self.query.weight using .* initializer
2022-04-13 17:41:19,763 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.self.query.bias using .* initializer
2022-04-13 17:41:19,764 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.self.key.weight using .* initializer
2022-04-13 17:41:19,764 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.self.key.bias using .* initializer
2022-04-13 17:41:19,764 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.self.value.weight using .* initializer
2022-04-13 17:41:19,764 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.self.value.bias using .* initializer
2022-04-13 17:41:19,765 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.output.dense.weight using .* initializer
2022-04-13 17:41:19,765 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.output.dense.bias using .* initializer
2022-04-13 17:41:19,765 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.output.LayerNorm.weight using .* initializer
2022-04-13 17:41:19,765 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.output.LayerNorm.bias using .* initializer
2022-04-13 17:41:19,765 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.intermediate.dense.weight using .* initializer
2022-04-13 17:41:19,766 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.intermediate.dense.bias using .* initializer
2022-04-13 17:41:19,766 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.output.dense.weight using .* initializer
2022-04-13 17:41:19,767 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.output.dense.bias using .* initializer
2022-04-13 17:41:19,768 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.output.LayerNorm.weight using .* initializer
2022-04-13 17:41:19,768 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.output.LayerNorm.bias using .* initializer
2022-04-13 17:41:19,768 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.self.query.weight using .* initializer
2022-04-13 17:41:19,768 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.self.query.bias using .* initializer
2022-04-13 17:41:19,768 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.self.key.weight using .* initializer
2022-04-13 17:41:19,769 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.self.key.bias using .* initializer
2022-04-13 17:41:19,769 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.self.value.weight using .* initializer
2022-04-13 17:41:19,769 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.self.value.bias using .* initializer
2022-04-13 17:41:19,769 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.output.dense.weight using .* initializer
2022-04-13 17:41:19,770 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.output.dense.bias using .* initializer
2022-04-13 17:41:19,770 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.output.LayerNorm.weight using .* initializer
2022-04-13 17:41:19,770 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.output.LayerNorm.bias using .* initializer
2022-04-13 17:41:19,770 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.intermediate.dense.weight using .* initializer
2022-04-13 17:41:19,771 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.intermediate.dense.bias using .* initializer
2022-04-13 17:41:19,771 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.output.dense.weight using .* initializer
2022-04-13 17:41:19,772 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.output.dense.bias using .* initializer
2022-04-13 17:41:19,772 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.output.LayerNorm.weight using .* initializer
2022-04-13 17:41:19,773 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.output.LayerNorm.bias using .* initializer
2022-04-13 17:41:19,773 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.self.query.weight using .* initializer
2022-04-13 17:41:19,773 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.self.query.bias using .* initializer
2022-04-13 17:41:19,773 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.self.key.weight using .* initializer
2022-04-13 17:41:19,774 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.self.key.bias using .* initializer
2022-04-13 17:41:19,774 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.self.value.weight using .* initializer
2022-04-13 17:41:19,774 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.self.value.bias using .* initializer
2022-04-13 17:41:19,774 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.output.dense.weight using .* initializer
2022-04-13 17:41:19,774 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.output.dense.bias using .* initializer
2022-04-13 17:41:19,775 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.output.LayerNorm.weight using .* initializer
2022-04-13 17:41:19,775 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.output.LayerNorm.bias using .* initializer
2022-04-13 17:41:19,775 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.intermediate.dense.weight using .* initializer
2022-04-13 17:41:19,776 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.intermediate.dense.bias using .* initializer
2022-04-13 17:41:19,776 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.output.dense.weight using .* initializer
2022-04-13 17:41:19,777 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.output.dense.bias using .* initializer
2022-04-13 17:41:19,777 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.output.LayerNorm.weight using .* initializer
2022-04-13 17:41:19,777 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.output.LayerNorm.bias using .* initializer
2022-04-13 17:41:19,777 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.self.query.weight using .* initializer
2022-04-13 17:41:19,778 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.self.query.bias using .* initializer
2022-04-13 17:41:19,778 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.self.key.weight using .* initializer
2022-04-13 17:41:19,778 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.self.key.bias using .* initializer
2022-04-13 17:41:19,778 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.self.value.weight using .* initializer
2022-04-13 17:41:19,779 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.self.value.bias using .* initializer
2022-04-13 17:41:19,779 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.output.dense.weight using .* initializer
2022-04-13 17:41:19,779 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.output.dense.bias using .* initializer
2022-04-13 17:41:19,779 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.output.LayerNorm.weight using .* initializer
2022-04-13 17:41:19,780 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.output.LayerNorm.bias using .* initializer
2022-04-13 17:41:19,780 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.intermediate.dense.weight using .* initializer
2022-04-13 17:41:19,781 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.intermediate.dense.bias using .* initializer
2022-04-13 17:41:19,781 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.output.dense.weight using .* initializer
2022-04-13 17:41:19,782 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.output.dense.bias using .* initializer
2022-04-13 17:41:19,782 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.output.LayerNorm.weight using .* initializer
2022-04-13 17:41:19,782 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.output.LayerNorm.bias using .* initializer
2022-04-13 17:41:19,782 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.self.query.weight using .* initializer
2022-04-13 17:41:19,782 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.self.query.bias using .* initializer
2022-04-13 17:41:19,783 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.self.key.weight using .* initializer
2022-04-13 17:41:19,783 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.self.key.bias using .* initializer
2022-04-13 17:41:19,783 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.self.value.weight using .* initializer
2022-04-13 17:41:19,783 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.self.value.bias using .* initializer
2022-04-13 17:41:19,784 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.output.dense.weight using .* initializer
2022-04-13 17:41:19,784 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.output.dense.bias using .* initializer
2022-04-13 17:41:19,784 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.output.LayerNorm.weight using .* initializer
2022-04-13 17:41:19,784 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.output.LayerNorm.bias using .* initializer
2022-04-13 17:41:19,784 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.intermediate.dense.weight using .* initializer
2022-04-13 17:41:19,785 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.intermediate.dense.bias using .* initializer
2022-04-13 17:41:19,785 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.output.dense.weight using .* initializer
2022-04-13 17:41:19,786 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.output.dense.bias using .* initializer
2022-04-13 17:41:19,787 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.output.LayerNorm.weight using .* initializer
2022-04-13 17:41:19,787 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.output.LayerNorm.bias using .* initializer
2022-04-13 17:41:19,787 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.self.query.weight using .* initializer
2022-04-13 17:41:19,787 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.self.query.bias using .* initializer
2022-04-13 17:41:19,787 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.self.key.weight using .* initializer
2022-04-13 17:41:19,788 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.self.key.bias using .* initializer
2022-04-13 17:41:19,788 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.self.value.weight using .* initializer
2022-04-13 17:41:19,788 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.self.value.bias using .* initializer
2022-04-13 17:41:19,788 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.output.dense.weight using .* initializer
2022-04-13 17:41:19,789 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.output.dense.bias using .* initializer
2022-04-13 17:41:19,789 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.output.LayerNorm.weight using .* initializer
2022-04-13 17:41:19,789 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.output.LayerNorm.bias using .* initializer
2022-04-13 17:41:19,789 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.intermediate.dense.weight using .* initializer
2022-04-13 17:41:19,790 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.intermediate.dense.bias using .* initializer
2022-04-13 17:41:19,790 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.output.dense.weight using .* initializer
2022-04-13 17:41:19,791 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.output.dense.bias using .* initializer
2022-04-13 17:41:19,791 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.output.LayerNorm.weight using .* initializer
2022-04-13 17:41:19,791 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.output.LayerNorm.bias using .* initializer
2022-04-13 17:41:19,792 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.pooler.dense.weight using .* initializer
2022-04-13 17:41:19,792 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.pooler.dense.bias using .* initializer
2022-04-13 17:41:19,792 - INFO - allennlp.nn.initializers - Initializing _classifier.classifier.dense.weight using .* initializer
2022-04-13 17:41:19,792 - INFO - allennlp.nn.initializers - Initializing _classifier.classifier.dense.bias using .* initializer
2022-04-13 17:41:19,793 - INFO - allennlp.nn.initializers - Initializing _classifier.classifier.out_proj.weight using .* initializer
2022-04-13 17:41:19,793 - INFO - allennlp.nn.initializers - Initializing _classifier.classifier.out_proj.bias using .* initializer
2022-04-13 17:41:19,793 - INFO - allennlp.nn.initializers - Done initializing parameters; the following parameters are using their default initialization from their code
2022-04-13 17:41:20,338 - INFO - filelock - Lock 139991266063312 acquired on ./output_ir-ora-d_hyper0/vocabulary/.lock
2022-04-13 17:41:20,338 - INFO - filelock - Lock 139991266063312 released on ./output_ir-ora-d_hyper0/vocabulary/.lock
2022-04-13 17:41:20,338 - INFO - allennlp.common.params - data_loader.type = pytorch_dataloader
2022-04-13 17:41:20,339 - INFO - allennlp.common.params - data_loader.batch_size = 1
2022-04-13 17:41:20,339 - INFO - allennlp.common.params - data_loader.shuffle = False
2022-04-13 17:41:20,339 - INFO - allennlp.common.params - data_loader.sampler = None
2022-04-13 17:41:20,339 - INFO - allennlp.common.params - data_loader.num_workers = 0
2022-04-13 17:41:20,339 - INFO - allennlp.common.params - data_loader.pin_memory = False
2022-04-13 17:41:20,339 - INFO - allennlp.common.params - data_loader.drop_last = False
2022-04-13 17:41:20,339 - INFO - allennlp.common.params - data_loader.timeout = 0
2022-04-13 17:41:20,340 - INFO - allennlp.common.params - data_loader.worker_init_fn = None
2022-04-13 17:41:20,340 - INFO - allennlp.common.params - data_loader.multiprocessing_context = None
2022-04-13 17:41:20,340 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None
2022-04-13 17:41:20,340 - INFO - allennlp.common.params - data_loader.batch_sampler.type = bucket
2022-04-13 17:41:20,340 - INFO - allennlp.common.params - data_loader.batch_sampler.batch_size = 2
2022-04-13 17:41:20,340 - INFO - allennlp.common.params - data_loader.batch_sampler.sorting_keys = None
2022-04-13 17:41:20,340 - INFO - allennlp.common.params - data_loader.batch_sampler.padding_noise = 0.1
2022-04-13 17:41:20,340 - INFO - allennlp.common.params - data_loader.batch_sampler.drop_last = False
2022-04-13 17:41:20,343 - INFO - allennlp.common.params - data_loader.type = pytorch_dataloader
2022-04-13 17:41:20,343 - INFO - allennlp.common.params - data_loader.batch_size = 1
2022-04-13 17:41:20,344 - INFO - allennlp.common.params - data_loader.shuffle = False
2022-04-13 17:41:20,344 - INFO - allennlp.common.params - data_loader.sampler = None
2022-04-13 17:41:20,344 - INFO - allennlp.common.params - data_loader.num_workers = 0
2022-04-13 17:41:20,344 - INFO - allennlp.common.params - data_loader.pin_memory = False
2022-04-13 17:41:20,344 - INFO - allennlp.common.params - data_loader.drop_last = False
2022-04-13 17:41:20,344 - INFO - allennlp.common.params - data_loader.timeout = 0
2022-04-13 17:41:20,344 - INFO - allennlp.common.params - data_loader.worker_init_fn = None
2022-04-13 17:41:20,344 - INFO - allennlp.common.params - data_loader.multiprocessing_context = None
2022-04-13 17:41:20,344 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None
2022-04-13 17:41:20,345 - INFO - allennlp.common.params - data_loader.batch_sampler.type = bucket
2022-04-13 17:41:20,345 - INFO - allennlp.common.params - data_loader.batch_sampler.batch_size = 2
2022-04-13 17:41:20,345 - INFO - allennlp.common.params - data_loader.batch_sampler.sorting_keys = None
2022-04-13 17:41:20,345 - INFO - allennlp.common.params - data_loader.batch_sampler.padding_noise = 0.1
2022-04-13 17:41:20,345 - INFO - allennlp.common.params - data_loader.batch_sampler.drop_last = False
2022-04-13 17:41:20,345 - INFO - allennlp.common.params - trainer.type = gradient_descent
2022-04-13 17:41:20,346 - INFO - allennlp.common.params - trainer.patience = None
2022-04-13 17:41:20,346 - INFO - allennlp.common.params - trainer.validation_metric = +accuracy
2022-04-13 17:41:20,346 - INFO - allennlp.common.params - trainer.num_epochs = 15
2022-04-13 17:41:20,346 - INFO - allennlp.common.params - trainer.cuda_device = 0
2022-04-13 17:41:20,346 - INFO - allennlp.common.params - trainer.grad_norm = None
2022-04-13 17:41:20,346 - INFO - allennlp.common.params - trainer.grad_clipping = None
2022-04-13 17:41:20,347 - INFO - allennlp.common.params - trainer.distributed = False
2022-04-13 17:41:20,347 - INFO - allennlp.common.params - trainer.world_size = 1
2022-04-13 17:41:20,347 - INFO - allennlp.common.params - trainer.num_gradient_accumulation_steps = 16
2022-04-13 17:41:20,347 - INFO - allennlp.common.params - trainer.use_amp = False
2022-04-13 17:41:20,347 - INFO - allennlp.common.params - trainer.no_grad = None
2022-04-13 17:41:20,347 - INFO - allennlp.common.params - trainer.momentum_scheduler = None
2022-04-13 17:41:20,347 - INFO - allennlp.common.params - trainer.tensorboard_writer = <allennlp.common.lazy.Lazy object at 0x7f526874c350>
2022-04-13 17:41:20,347 - INFO - allennlp.common.params - trainer.moving_average = None
2022-04-13 17:41:20,348 - INFO - allennlp.common.params - trainer.checkpointer = <allennlp.common.lazy.Lazy object at 0x7f526874c3d0>
2022-04-13 17:41:20,348 - INFO - allennlp.common.params - trainer.batch_callbacks = None
2022-04-13 17:41:20,348 - INFO - allennlp.common.params - trainer.epoch_callbacks = None
2022-04-13 17:41:20,348 - INFO - allennlp.common.params - trainer.end_callbacks = None
2022-04-13 17:41:20,348 - INFO - allennlp.common.params - trainer.trainer_callbacks = None
2022-04-13 17:41:24,718 - INFO - allennlp.common.params - trainer.optimizer.type = huggingface_adamw
2022-04-13 17:41:24,720 - INFO - allennlp.common.params - trainer.optimizer.parameter_groups = None
2022-04-13 17:41:24,720 - INFO - allennlp.common.params - trainer.optimizer.lr = 1e-05
2022-04-13 17:41:24,720 - INFO - allennlp.common.params - trainer.optimizer.betas = [0.9, 0.98]
2022-04-13 17:41:24,720 - INFO - allennlp.common.params - trainer.optimizer.eps = 1e-06
2022-04-13 17:41:24,720 - INFO - allennlp.common.params - trainer.optimizer.weight_decay = 0.2
2022-04-13 17:41:24,720 - INFO - allennlp.common.params - trainer.optimizer.correct_bias = True
2022-04-13 17:41:24,721 - INFO - allennlp.training.optimizers - Number of trainable parameters: 356411394
2022-04-13 17:41:24,723 - INFO - allennlp.common.util - The following parameters are Frozen (without gradient):
2022-04-13 17:41:24,725 - INFO - allennlp.common.util - The following parameters are Tunable (with gradient):
2022-04-13 17:41:24,725 - INFO - allennlp.common.util - _classifier.roberta.embeddings.word_embeddings.weight
2022-04-13 17:41:24,725 - INFO - allennlp.common.util - _classifier.roberta.embeddings.position_embeddings.weight
2022-04-13 17:41:24,726 - INFO - allennlp.common.util - _classifier.roberta.embeddings.token_type_embeddings.weight
2022-04-13 17:41:24,726 - INFO - allennlp.common.util - _classifier.roberta.embeddings.LayerNorm.weight
2022-04-13 17:41:24,726 - INFO - allennlp.common.util - _classifier.roberta.embeddings.LayerNorm.bias
2022-04-13 17:41:24,726 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.self.query.weight
2022-04-13 17:41:24,726 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.self.query.bias
2022-04-13 17:41:24,726 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.self.key.weight
2022-04-13 17:41:24,726 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.self.key.bias
2022-04-13 17:41:24,726 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.self.value.weight
2022-04-13 17:41:24,726 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.self.value.bias
2022-04-13 17:41:24,726 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.output.dense.weight
2022-04-13 17:41:24,727 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.output.dense.bias
2022-04-13 17:41:24,727 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.output.LayerNorm.weight
2022-04-13 17:41:24,727 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.output.LayerNorm.bias
2022-04-13 17:41:24,727 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.intermediate.dense.weight
2022-04-13 17:41:24,727 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.intermediate.dense.bias
2022-04-13 17:41:24,727 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.output.dense.weight
2022-04-13 17:41:24,727 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.output.dense.bias
2022-04-13 17:41:24,727 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.output.LayerNorm.weight
2022-04-13 17:41:24,727 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.output.LayerNorm.bias
2022-04-13 17:41:24,727 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.self.query.weight
2022-04-13 17:41:24,728 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.self.query.bias
2022-04-13 17:41:24,728 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.self.key.weight
2022-04-13 17:41:24,728 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.self.key.bias
2022-04-13 17:41:24,728 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.self.value.weight
2022-04-13 17:41:24,728 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.self.value.bias
2022-04-13 17:41:24,728 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.output.dense.weight
2022-04-13 17:41:24,728 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.output.dense.bias
2022-04-13 17:41:24,728 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.output.LayerNorm.weight
2022-04-13 17:41:24,728 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.output.LayerNorm.bias
2022-04-13 17:41:24,728 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.intermediate.dense.weight
2022-04-13 17:41:24,728 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.intermediate.dense.bias
2022-04-13 17:41:24,729 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.output.dense.weight
2022-04-13 17:41:24,729 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.output.dense.bias
2022-04-13 17:41:24,729 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.output.LayerNorm.weight
2022-04-13 17:41:24,729 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.output.LayerNorm.bias
2022-04-13 17:41:24,729 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.self.query.weight
2022-04-13 17:41:24,729 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.self.query.bias
2022-04-13 17:41:24,729 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.self.key.weight
2022-04-13 17:41:24,729 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.self.key.bias
2022-04-13 17:41:24,729 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.self.value.weight
2022-04-13 17:41:24,729 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.self.value.bias
2022-04-13 17:41:24,729 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.output.dense.weight
2022-04-13 17:41:24,730 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.output.dense.bias
2022-04-13 17:41:24,730 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.output.LayerNorm.weight
2022-04-13 17:41:24,730 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.output.LayerNorm.bias
2022-04-13 17:41:24,730 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.intermediate.dense.weight
2022-04-13 17:41:24,730 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.intermediate.dense.bias
2022-04-13 17:41:24,730 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.output.dense.weight
2022-04-13 17:41:24,730 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.output.dense.bias
2022-04-13 17:41:24,730 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.output.LayerNorm.weight
2022-04-13 17:41:24,730 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.output.LayerNorm.bias
2022-04-13 17:41:24,730 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.self.query.weight
2022-04-13 17:41:24,730 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.self.query.bias
2022-04-13 17:41:24,731 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.self.key.weight
2022-04-13 17:41:24,731 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.self.key.bias
2022-04-13 17:41:24,731 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.self.value.weight
2022-04-13 17:41:24,731 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.self.value.bias
2022-04-13 17:41:24,731 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.output.dense.weight
2022-04-13 17:41:24,731 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.output.dense.bias
2022-04-13 17:41:24,731 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.output.LayerNorm.weight
2022-04-13 17:41:24,731 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.output.LayerNorm.bias
2022-04-13 17:41:24,731 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.intermediate.dense.weight
2022-04-13 17:41:24,731 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.intermediate.dense.bias
2022-04-13 17:41:24,731 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.output.dense.weight
2022-04-13 17:41:24,732 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.output.dense.bias
2022-04-13 17:41:24,732 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.output.LayerNorm.weight
2022-04-13 17:41:24,732 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.output.LayerNorm.bias
2022-04-13 17:41:24,732 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.self.query.weight
2022-04-13 17:41:24,732 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.self.query.bias
2022-04-13 17:41:24,732 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.self.key.weight
2022-04-13 17:41:24,732 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.self.key.bias
2022-04-13 17:41:24,732 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.self.value.weight
2022-04-13 17:41:24,732 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.self.value.bias
2022-04-13 17:41:24,732 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.output.dense.weight
2022-04-13 17:41:24,732 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.output.dense.bias
2022-04-13 17:41:24,732 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.output.LayerNorm.weight
2022-04-13 17:41:24,733 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.output.LayerNorm.bias
2022-04-13 17:41:24,733 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.intermediate.dense.weight
2022-04-13 17:41:24,733 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.intermediate.dense.bias
2022-04-13 17:41:24,733 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.output.dense.weight
2022-04-13 17:41:24,733 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.output.dense.bias
2022-04-13 17:41:24,733 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.output.LayerNorm.weight
2022-04-13 17:41:24,733 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.output.LayerNorm.bias
2022-04-13 17:41:24,733 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.self.query.weight
2022-04-13 17:41:24,733 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.self.query.bias
2022-04-13 17:41:24,733 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.self.key.weight
2022-04-13 17:41:24,733 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.self.key.bias
2022-04-13 17:41:24,734 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.self.value.weight
2022-04-13 17:41:24,734 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.self.value.bias
2022-04-13 17:41:24,734 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.output.dense.weight
2022-04-13 17:41:24,734 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.output.dense.bias
2022-04-13 17:41:24,734 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.output.LayerNorm.weight
2022-04-13 17:41:24,734 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.output.LayerNorm.bias
2022-04-13 17:41:24,734 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.intermediate.dense.weight
2022-04-13 17:41:24,734 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.intermediate.dense.bias
2022-04-13 17:41:24,734 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.output.dense.weight
2022-04-13 17:41:24,734 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.output.dense.bias
2022-04-13 17:41:24,734 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.output.LayerNorm.weight
2022-04-13 17:41:24,735 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.output.LayerNorm.bias
2022-04-13 17:41:24,735 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.self.query.weight
2022-04-13 17:41:24,735 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.self.query.bias
2022-04-13 17:41:24,735 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.self.key.weight
2022-04-13 17:41:24,735 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.self.key.bias
2022-04-13 17:41:24,735 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.self.value.weight
2022-04-13 17:41:24,735 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.self.value.bias
2022-04-13 17:41:24,735 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.output.dense.weight
2022-04-13 17:41:24,735 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.output.dense.bias
2022-04-13 17:41:24,735 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.output.LayerNorm.weight
2022-04-13 17:41:24,736 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.output.LayerNorm.bias
2022-04-13 17:41:24,736 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.intermediate.dense.weight
2022-04-13 17:41:24,736 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.intermediate.dense.bias
2022-04-13 17:41:24,736 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.output.dense.weight
2022-04-13 17:41:24,736 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.output.dense.bias
2022-04-13 17:41:24,736 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.output.LayerNorm.weight
2022-04-13 17:41:24,736 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.output.LayerNorm.bias
2022-04-13 17:41:24,736 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.self.query.weight
2022-04-13 17:41:24,736 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.self.query.bias
2022-04-13 17:41:24,736 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.self.key.weight
2022-04-13 17:41:24,736 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.self.key.bias
2022-04-13 17:41:24,737 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.self.value.weight
2022-04-13 17:41:24,737 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.self.value.bias
2022-04-13 17:41:24,737 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.output.dense.weight
2022-04-13 17:41:24,737 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.output.dense.bias
2022-04-13 17:41:24,737 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.output.LayerNorm.weight
2022-04-13 17:41:24,737 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.output.LayerNorm.bias
2022-04-13 17:41:24,737 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.intermediate.dense.weight
2022-04-13 17:41:24,737 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.intermediate.dense.bias
2022-04-13 17:41:24,737 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.output.dense.weight
2022-04-13 17:41:24,737 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.output.dense.bias
2022-04-13 17:41:24,737 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.output.LayerNorm.weight
2022-04-13 17:41:24,738 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.output.LayerNorm.bias
2022-04-13 17:41:24,738 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.self.query.weight
2022-04-13 17:41:24,738 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.self.query.bias
2022-04-13 17:41:24,738 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.self.key.weight
2022-04-13 17:41:24,738 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.self.key.bias
2022-04-13 17:41:24,738 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.self.value.weight
2022-04-13 17:41:24,738 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.self.value.bias
2022-04-13 17:41:24,738 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.output.dense.weight
2022-04-13 17:41:24,738 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.output.dense.bias
2022-04-13 17:41:24,738 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.output.LayerNorm.weight
2022-04-13 17:41:24,738 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.output.LayerNorm.bias
2022-04-13 17:41:24,738 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.intermediate.dense.weight
2022-04-13 17:41:24,739 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.intermediate.dense.bias
2022-04-13 17:41:24,739 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.output.dense.weight
2022-04-13 17:41:24,739 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.output.dense.bias
2022-04-13 17:41:24,739 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.output.LayerNorm.weight
2022-04-13 17:41:24,739 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.output.LayerNorm.bias
2022-04-13 17:41:24,739 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.self.query.weight
2022-04-13 17:41:24,739 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.self.query.bias
2022-04-13 17:41:24,739 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.self.key.weight
2022-04-13 17:41:24,739 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.self.key.bias
2022-04-13 17:41:24,739 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.self.value.weight
2022-04-13 17:41:24,739 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.self.value.bias
2022-04-13 17:41:24,740 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.output.dense.weight
2022-04-13 17:41:24,740 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.output.dense.bias
2022-04-13 17:41:24,740 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.output.LayerNorm.weight
2022-04-13 17:41:24,740 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.output.LayerNorm.bias
2022-04-13 17:41:24,740 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.intermediate.dense.weight
2022-04-13 17:41:24,740 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.intermediate.dense.bias
2022-04-13 17:41:24,740 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.output.dense.weight
2022-04-13 17:41:24,740 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.output.dense.bias
2022-04-13 17:41:24,740 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.output.LayerNorm.weight
2022-04-13 17:41:24,740 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.output.LayerNorm.bias
2022-04-13 17:41:24,741 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.self.query.weight
2022-04-13 17:41:24,741 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.self.query.bias
2022-04-13 17:41:24,741 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.self.key.weight
2022-04-13 17:41:24,741 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.self.key.bias
2022-04-13 17:41:24,741 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.self.value.weight
2022-04-13 17:41:24,741 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.self.value.bias
2022-04-13 17:41:24,741 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.output.dense.weight
2022-04-13 17:41:24,741 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.output.dense.bias
2022-04-13 17:41:24,741 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.output.LayerNorm.weight
2022-04-13 17:41:24,741 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.output.LayerNorm.bias
2022-04-13 17:41:24,741 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.intermediate.dense.weight
2022-04-13 17:41:24,742 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.intermediate.dense.bias
2022-04-13 17:41:24,742 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.output.dense.weight
2022-04-13 17:41:24,742 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.output.dense.bias
2022-04-13 17:41:24,742 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.output.LayerNorm.weight
2022-04-13 17:41:24,742 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.output.LayerNorm.bias
2022-04-13 17:41:24,742 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.self.query.weight
2022-04-13 17:41:24,742 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.self.query.bias
2022-04-13 17:41:24,742 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.self.key.weight
2022-04-13 17:41:24,742 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.self.key.bias
2022-04-13 17:41:24,742 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.self.value.weight
2022-04-13 17:41:24,742 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.self.value.bias
2022-04-13 17:41:24,743 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.output.dense.weight
2022-04-13 17:41:24,743 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.output.dense.bias
2022-04-13 17:41:24,743 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.output.LayerNorm.weight
2022-04-13 17:41:24,743 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.output.LayerNorm.bias
2022-04-13 17:41:24,743 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.intermediate.dense.weight
2022-04-13 17:41:24,743 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.intermediate.dense.bias
2022-04-13 17:41:24,743 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.output.dense.weight
2022-04-13 17:41:24,743 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.output.dense.bias
2022-04-13 17:41:24,743 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.output.LayerNorm.weight
2022-04-13 17:41:24,743 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.output.LayerNorm.bias
2022-04-13 17:41:24,743 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.self.query.weight
2022-04-13 17:41:24,744 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.self.query.bias
2022-04-13 17:41:24,744 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.self.key.weight
2022-04-13 17:41:24,744 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.self.key.bias
2022-04-13 17:41:24,744 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.self.value.weight
2022-04-13 17:41:24,744 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.self.value.bias
2022-04-13 17:41:24,744 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.output.dense.weight
2022-04-13 17:41:24,744 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.output.dense.bias
2022-04-13 17:41:24,744 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.output.LayerNorm.weight
2022-04-13 17:41:24,744 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.output.LayerNorm.bias
2022-04-13 17:41:24,744 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.intermediate.dense.weight
2022-04-13 17:41:24,744 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.intermediate.dense.bias
2022-04-13 17:41:24,745 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.output.dense.weight
2022-04-13 17:41:24,745 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.output.dense.bias
2022-04-13 17:41:24,745 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.output.LayerNorm.weight
2022-04-13 17:41:24,745 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.output.LayerNorm.bias
2022-04-13 17:41:24,745 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.self.query.weight
2022-04-13 17:41:24,745 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.self.query.bias
2022-04-13 17:41:24,745 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.self.key.weight
2022-04-13 17:41:24,745 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.self.key.bias
2022-04-13 17:41:24,745 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.self.value.weight
2022-04-13 17:41:24,745 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.self.value.bias
2022-04-13 17:41:24,745 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.output.dense.weight
2022-04-13 17:41:24,746 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.output.dense.bias
2022-04-13 17:41:24,746 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.output.LayerNorm.weight
2022-04-13 17:41:24,746 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.output.LayerNorm.bias
2022-04-13 17:41:24,746 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.intermediate.dense.weight
2022-04-13 17:41:24,746 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.intermediate.dense.bias
2022-04-13 17:41:24,746 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.output.dense.weight
2022-04-13 17:41:24,746 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.output.dense.bias
2022-04-13 17:41:24,746 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.output.LayerNorm.weight
2022-04-13 17:41:24,746 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.output.LayerNorm.bias
2022-04-13 17:41:24,746 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.self.query.weight
2022-04-13 17:41:24,746 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.self.query.bias
2022-04-13 17:41:24,747 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.self.key.weight
2022-04-13 17:41:24,747 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.self.key.bias
2022-04-13 17:41:24,747 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.self.value.weight
2022-04-13 17:41:24,747 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.self.value.bias
2022-04-13 17:41:24,747 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.output.dense.weight
2022-04-13 17:41:24,747 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.output.dense.bias
2022-04-13 17:41:24,747 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.output.LayerNorm.weight
2022-04-13 17:41:24,747 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.output.LayerNorm.bias
2022-04-13 17:41:24,747 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.intermediate.dense.weight
2022-04-13 17:41:24,747 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.intermediate.dense.bias
2022-04-13 17:41:24,747 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.output.dense.weight
2022-04-13 17:41:24,748 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.output.dense.bias
2022-04-13 17:41:24,748 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.output.LayerNorm.weight
2022-04-13 17:41:24,748 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.output.LayerNorm.bias
2022-04-13 17:41:24,748 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.self.query.weight
2022-04-13 17:41:24,748 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.self.query.bias
2022-04-13 17:41:24,748 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.self.key.weight
2022-04-13 17:41:24,748 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.self.key.bias
2022-04-13 17:41:24,748 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.self.value.weight
2022-04-13 17:41:24,748 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.self.value.bias
2022-04-13 17:41:24,748 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.output.dense.weight
2022-04-13 17:41:24,748 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.output.dense.bias
2022-04-13 17:41:24,749 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.output.LayerNorm.weight
2022-04-13 17:41:24,749 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.output.LayerNorm.bias
2022-04-13 17:41:24,749 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.intermediate.dense.weight
2022-04-13 17:41:24,749 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.intermediate.dense.bias
2022-04-13 17:41:24,749 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.output.dense.weight
2022-04-13 17:41:24,749 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.output.dense.bias
2022-04-13 17:41:24,749 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.output.LayerNorm.weight
2022-04-13 17:41:24,749 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.output.LayerNorm.bias
2022-04-13 17:41:24,749 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.self.query.weight
2022-04-13 17:41:24,749 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.self.query.bias
2022-04-13 17:41:24,749 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.self.key.weight
2022-04-13 17:41:24,750 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.self.key.bias
2022-04-13 17:41:24,750 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.self.value.weight
2022-04-13 17:41:24,750 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.self.value.bias
2022-04-13 17:41:24,750 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.output.dense.weight
2022-04-13 17:41:24,750 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.output.dense.bias
2022-04-13 17:41:24,750 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.output.LayerNorm.weight
2022-04-13 17:41:24,750 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.output.LayerNorm.bias
2022-04-13 17:41:24,750 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.intermediate.dense.weight
2022-04-13 17:41:24,750 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.intermediate.dense.bias
2022-04-13 17:41:24,750 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.output.dense.weight
2022-04-13 17:41:24,750 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.output.dense.bias
2022-04-13 17:41:24,751 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.output.LayerNorm.weight
2022-04-13 17:41:24,751 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.output.LayerNorm.bias
2022-04-13 17:41:24,751 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.self.query.weight
2022-04-13 17:41:24,751 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.self.query.bias
2022-04-13 17:41:24,751 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.self.key.weight
2022-04-13 17:41:24,751 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.self.key.bias
2022-04-13 17:41:24,751 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.self.value.weight
2022-04-13 17:41:24,751 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.self.value.bias
2022-04-13 17:41:24,751 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.output.dense.weight
2022-04-13 17:41:24,751 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.output.dense.bias
2022-04-13 17:41:24,751 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.output.LayerNorm.weight
2022-04-13 17:41:24,752 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.output.LayerNorm.bias
2022-04-13 17:41:24,752 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.intermediate.dense.weight
2022-04-13 17:41:24,752 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.intermediate.dense.bias
2022-04-13 17:41:24,752 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.output.dense.weight
2022-04-13 17:41:24,752 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.output.dense.bias
2022-04-13 17:41:24,752 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.output.LayerNorm.weight
2022-04-13 17:41:24,752 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.output.LayerNorm.bias
2022-04-13 17:41:24,752 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.self.query.weight
2022-04-13 17:41:24,752 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.self.query.bias
2022-04-13 17:41:24,752 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.self.key.weight
2022-04-13 17:41:24,753 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.self.key.bias
2022-04-13 17:41:24,753 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.self.value.weight
2022-04-13 17:41:24,753 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.self.value.bias
2022-04-13 17:41:24,753 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.output.dense.weight
2022-04-13 17:41:24,753 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.output.dense.bias
2022-04-13 17:41:24,753 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.output.LayerNorm.weight
2022-04-13 17:41:24,753 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.output.LayerNorm.bias
2022-04-13 17:41:24,753 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.intermediate.dense.weight
2022-04-13 17:41:24,753 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.intermediate.dense.bias
2022-04-13 17:41:24,753 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.output.dense.weight
2022-04-13 17:41:24,753 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.output.dense.bias
2022-04-13 17:41:24,754 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.output.LayerNorm.weight
2022-04-13 17:41:24,754 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.output.LayerNorm.bias
2022-04-13 17:41:24,754 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.self.query.weight
2022-04-13 17:41:24,754 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.self.query.bias
2022-04-13 17:41:24,754 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.self.key.weight
2022-04-13 17:41:24,754 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.self.key.bias
2022-04-13 17:41:24,754 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.self.value.weight
2022-04-13 17:41:24,754 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.self.value.bias
2022-04-13 17:41:24,754 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.output.dense.weight
2022-04-13 17:41:24,754 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.output.dense.bias
2022-04-13 17:41:24,754 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.output.LayerNorm.weight
2022-04-13 17:41:24,755 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.output.LayerNorm.bias
2022-04-13 17:41:24,755 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.intermediate.dense.weight
2022-04-13 17:41:24,755 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.intermediate.dense.bias
2022-04-13 17:41:24,755 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.output.dense.weight
2022-04-13 17:41:24,755 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.output.dense.bias
2022-04-13 17:41:24,755 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.output.LayerNorm.weight
2022-04-13 17:41:24,755 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.output.LayerNorm.bias
2022-04-13 17:41:24,755 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.self.query.weight
2022-04-13 17:41:24,755 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.self.query.bias
2022-04-13 17:41:24,755 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.self.key.weight
2022-04-13 17:41:24,756 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.self.key.bias
2022-04-13 17:41:24,756 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.self.value.weight
2022-04-13 17:41:24,756 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.self.value.bias
2022-04-13 17:41:24,756 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.output.dense.weight
2022-04-13 17:41:24,756 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.output.dense.bias
2022-04-13 17:41:24,756 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.output.LayerNorm.weight
2022-04-13 17:41:24,756 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.output.LayerNorm.bias
2022-04-13 17:41:24,756 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.intermediate.dense.weight
2022-04-13 17:41:24,756 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.intermediate.dense.bias
2022-04-13 17:41:24,756 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.output.dense.weight
2022-04-13 17:41:24,756 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.output.dense.bias
2022-04-13 17:41:24,756 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.output.LayerNorm.weight
2022-04-13 17:41:24,757 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.output.LayerNorm.bias
2022-04-13 17:41:24,757 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.self.query.weight
2022-04-13 17:41:24,757 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.self.query.bias
2022-04-13 17:41:24,757 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.self.key.weight
2022-04-13 17:41:24,757 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.self.key.bias
2022-04-13 17:41:24,757 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.self.value.weight
2022-04-13 17:41:24,757 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.self.value.bias
2022-04-13 17:41:24,757 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.output.dense.weight
2022-04-13 17:41:24,757 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.output.dense.bias
2022-04-13 17:41:24,757 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.output.LayerNorm.weight
2022-04-13 17:41:24,757 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.output.LayerNorm.bias
2022-04-13 17:41:24,758 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.intermediate.dense.weight
2022-04-13 17:41:24,758 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.intermediate.dense.bias
2022-04-13 17:41:24,758 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.output.dense.weight
2022-04-13 17:41:24,758 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.output.dense.bias
2022-04-13 17:41:24,758 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.output.LayerNorm.weight
2022-04-13 17:41:24,758 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.output.LayerNorm.bias
2022-04-13 17:41:24,758 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.self.query.weight
2022-04-13 17:41:24,758 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.self.query.bias
2022-04-13 17:41:24,758 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.self.key.weight
2022-04-13 17:41:24,758 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.self.key.bias
2022-04-13 17:41:24,758 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.self.value.weight
2022-04-13 17:41:24,759 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.self.value.bias
2022-04-13 17:41:24,759 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.output.dense.weight
2022-04-13 17:41:24,759 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.output.dense.bias
2022-04-13 17:41:24,759 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.output.LayerNorm.weight
2022-04-13 17:41:24,759 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.output.LayerNorm.bias
2022-04-13 17:41:24,759 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.intermediate.dense.weight
2022-04-13 17:41:24,759 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.intermediate.dense.bias
2022-04-13 17:41:24,759 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.output.dense.weight
2022-04-13 17:41:24,759 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.output.dense.bias
2022-04-13 17:41:24,759 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.output.LayerNorm.weight
2022-04-13 17:41:24,760 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.output.LayerNorm.bias
2022-04-13 17:41:24,760 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.self.query.weight
2022-04-13 17:41:24,760 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.self.query.bias
2022-04-13 17:41:24,760 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.self.key.weight
2022-04-13 17:41:24,760 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.self.key.bias
2022-04-13 17:41:24,760 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.self.value.weight
2022-04-13 17:41:24,760 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.self.value.bias
2022-04-13 17:41:24,760 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.output.dense.weight
2022-04-13 17:41:24,760 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.output.dense.bias
2022-04-13 17:41:24,760 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.output.LayerNorm.weight
2022-04-13 17:41:24,761 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.output.LayerNorm.bias
2022-04-13 17:41:24,761 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.intermediate.dense.weight
2022-04-13 17:41:24,761 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.intermediate.dense.bias
2022-04-13 17:41:24,761 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.output.dense.weight
2022-04-13 17:41:24,761 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.output.dense.bias
2022-04-13 17:41:24,761 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.output.LayerNorm.weight
2022-04-13 17:41:24,761 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.output.LayerNorm.bias
2022-04-13 17:41:24,761 - INFO - allennlp.common.util - _classifier.roberta.pooler.dense.weight
2022-04-13 17:41:24,761 - INFO - allennlp.common.util - _classifier.roberta.pooler.dense.bias
2022-04-13 17:41:24,761 - INFO - allennlp.common.util - _classifier.classifier.dense.weight
2022-04-13 17:41:24,761 - INFO - allennlp.common.util - _classifier.classifier.dense.bias
2022-04-13 17:41:24,762 - INFO - allennlp.common.util - _classifier.classifier.out_proj.weight
2022-04-13 17:41:24,762 - INFO - allennlp.common.util - _classifier.classifier.out_proj.bias
2022-04-13 17:41:24,762 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.type = slanted_triangular
2022-04-13 17:41:24,762 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.cut_frac = 0.06
2022-04-13 17:41:24,762 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.ratio = 32
2022-04-13 17:41:24,762 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.last_epoch = -1
2022-04-13 17:41:24,763 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.gradual_unfreezing = False
2022-04-13 17:41:24,763 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.discriminative_fine_tuning = False
2022-04-13 17:41:24,763 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.decay_factor = 0.38
2022-04-13 17:41:24,763 - INFO - allennlp.common.params - type = default
2022-04-13 17:41:24,763 - INFO - allennlp.common.params - keep_serialized_model_every_num_seconds = None
2022-04-13 17:41:24,763 - INFO - allennlp.common.params - num_serialized_models_to_keep = 2
2022-04-13 17:41:24,763 - INFO - allennlp.common.params - model_save_interval = None
2022-04-13 17:41:24,764 - INFO - allennlp.common.params - summary_interval = 100
2022-04-13 17:41:24,764 - INFO - allennlp.common.params - histogram_interval = None
2022-04-13 17:41:24,764 - INFO - allennlp.common.params - batch_size_interval = None
2022-04-13 17:41:24,764 - INFO - allennlp.common.params - should_log_parameter_statistics = True
2022-04-13 17:41:24,764 - INFO - allennlp.common.params - should_log_learning_rate = False
2022-04-13 17:41:24,764 - INFO - allennlp.common.params - get_batch_num_total = None
2022-04-13 17:41:24,768 - WARNING - allennlp.training.trainer - You provided a validation dataset but patience was set to None, meaning that early stopping is disabled
2022-04-13 17:41:24,860 - INFO - allennlp.training.trainer - Beginning training.
2022-04-13 17:41:24,860 - INFO - allennlp.training.trainer - Epoch 0/14
2022-04-13 17:41:24,860 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-13 17:41:24,860 - INFO - allennlp.training.trainer - GPU 0 memory usage: 1.3G
2022-04-13 17:41:24,862 - INFO - allennlp.training.trainer - Training
2022-04-13 17:41:24,862 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-13 17:41:24,862 - INFO - allennlp.data.samplers.bucket_batch_sampler - No sorting keys given; trying to guess a good one
2022-04-13 17:41:24,863 - INFO - allennlp.data.samplers.bucket_batch_sampler - Using ['tokens'] as the sorting keys
2022-04-13 17:41:44,799 - INFO - tqdm - accuracy: 0.5156, batch_loss: 1.5481, loss: 1.5485 ||:   3%|3         | 2/65 [00:19<10:27,  9.96s/it]
2022-04-13 17:41:54,829 - INFO - tqdm - accuracy: 0.5208, batch_loss: 2.0186, loss: 1.7052 ||:   5%|4         | 3/65 [00:29<10:19,  9.99s/it]
2022-04-13 17:42:04,935 - INFO - tqdm - accuracy: 0.5469, batch_loss: 1.3413, loss: 1.6143 ||:   6%|6         | 4/65 [00:40<10:12, 10.04s/it]
2022-04-13 17:42:15,165 - INFO - tqdm - accuracy: 0.5312, batch_loss: 1.2133, loss: 1.5341 ||:   8%|7         | 5/65 [00:50<10:06, 10.11s/it]
2022-04-13 17:42:25,523 - INFO - tqdm - accuracy: 0.5156, batch_loss: 1.6996, loss: 1.5617 ||:   9%|9         | 6/65 [01:00<10:01, 10.19s/it]
2022-04-13 17:42:36,010 - INFO - tqdm - accuracy: 0.5223, batch_loss: 1.4528, loss: 1.5461 ||:  11%|#         | 7/65 [01:11<09:56, 10.29s/it]
2022-04-13 17:42:46,515 - INFO - tqdm - accuracy: 0.5156, batch_loss: 1.5003, loss: 1.5404 ||:  12%|#2        | 8/65 [01:21<09:50, 10.36s/it]
2022-04-13 17:42:57,056 - INFO - tqdm - accuracy: 0.5104, batch_loss: 1.6758, loss: 1.5554 ||:  14%|#3        | 9/65 [01:32<09:43, 10.41s/it]
2022-04-13 17:43:07,588 - INFO - tqdm - accuracy: 0.4969, batch_loss: 1.5000, loss: 1.5499 ||:  15%|#5        | 10/65 [01:42<09:34, 10.45s/it]
2022-04-13 17:43:18,360 - INFO - tqdm - accuracy: 0.4943, batch_loss: 1.4277, loss: 1.5388 ||:  17%|#6        | 11/65 [01:53<09:29, 10.55s/it]
2022-04-13 17:43:29,213 - INFO - tqdm - accuracy: 0.5104, batch_loss: 0.9454, loss: 1.4893 ||:  18%|#8        | 12/65 [02:04<09:24, 10.64s/it]
2022-04-13 17:43:40,149 - INFO - tqdm - accuracy: 0.5144, batch_loss: 0.8646, loss: 1.4413 ||:  20%|##        | 13/65 [02:15<09:18, 10.73s/it]
2022-04-13 17:43:51,069 - INFO - tqdm - accuracy: 0.5134, batch_loss: 1.3310, loss: 1.4334 ||:  22%|##1       | 14/65 [02:26<09:10, 10.79s/it]
2022-04-13 17:44:02,185 - INFO - tqdm - accuracy: 0.5167, batch_loss: 1.0014, loss: 1.4046 ||:  23%|##3       | 15/65 [02:37<09:04, 10.89s/it]
2022-04-13 17:44:13,379 - INFO - tqdm - accuracy: 0.5059, batch_loss: 1.0313, loss: 1.3813 ||:  25%|##4       | 16/65 [02:48<08:57, 10.98s/it]
2022-04-13 17:44:24,602 - INFO - tqdm - accuracy: 0.5110, batch_loss: 0.5861, loss: 1.3345 ||:  26%|##6       | 17/65 [02:59<08:50, 11.05s/it]
2022-04-13 17:44:35,749 - INFO - tqdm - accuracy: 0.5156, batch_loss: 0.8126, loss: 1.3055 ||:  28%|##7       | 18/65 [03:10<08:40, 11.08s/it]
2022-04-13 17:44:46,939 - INFO - tqdm - accuracy: 0.5181, batch_loss: 0.8275, loss: 1.2803 ||:  29%|##9       | 19/65 [03:22<08:31, 11.11s/it]
2022-04-13 17:44:57,932 - INFO - tqdm - accuracy: 0.5250, batch_loss: 0.6315, loss: 1.2479 ||:  31%|###       | 20/65 [03:33<08:18, 11.08s/it]
2022-04-13 17:45:09,237 - INFO - tqdm - accuracy: 0.5193, batch_loss: 0.9865, loss: 1.2355 ||:  32%|###2      | 21/65 [03:44<08:10, 11.15s/it]
2022-04-13 17:45:20,494 - INFO - tqdm - accuracy: 0.5213, batch_loss: 0.7502, loss: 1.2134 ||:  34%|###3      | 22/65 [03:55<08:00, 11.18s/it]
2022-04-13 17:45:31,928 - INFO - tqdm - accuracy: 0.5204, batch_loss: 0.7884, loss: 1.1949 ||:  35%|###5      | 23/65 [04:07<07:52, 11.26s/it]
2022-04-13 17:45:43,409 - INFO - tqdm - accuracy: 0.5182, batch_loss: 0.7611, loss: 1.1768 ||:  37%|###6      | 24/65 [04:18<07:44, 11.32s/it]
2022-04-13 17:45:54,969 - INFO - tqdm - accuracy: 0.5138, batch_loss: 0.8459, loss: 1.1636 ||:  38%|###8      | 25/65 [04:30<07:35, 11.39s/it]
2022-04-13 17:46:06,570 - INFO - tqdm - accuracy: 0.5096, batch_loss: 0.7295, loss: 1.1469 ||:  40%|####      | 26/65 [04:41<07:26, 11.46s/it]
2022-04-13 17:46:18,194 - INFO - tqdm - accuracy: 0.5127, batch_loss: 0.6513, loss: 1.1286 ||:  42%|####1     | 27/65 [04:53<07:17, 11.51s/it]
2022-04-13 17:46:29,803 - INFO - tqdm - accuracy: 0.5123, batch_loss: 0.7321, loss: 1.1144 ||:  43%|####3     | 28/65 [05:04<07:06, 11.54s/it]
2022-04-13 17:46:41,376 - INFO - tqdm - accuracy: 0.5097, batch_loss: 0.7302, loss: 1.1011 ||:  45%|####4     | 29/65 [05:16<06:55, 11.55s/it]
2022-04-13 17:46:52,959 - INFO - tqdm - accuracy: 0.5073, batch_loss: 0.7443, loss: 1.0892 ||:  46%|####6     | 30/65 [05:28<06:44, 11.56s/it]
2022-04-13 17:47:04,517 - INFO - tqdm - accuracy: 0.5101, batch_loss: 0.6686, loss: 1.0757 ||:  48%|####7     | 31/65 [05:39<06:32, 11.56s/it]
2022-04-13 17:47:16,089 - INFO - tqdm - accuracy: 0.5088, batch_loss: 0.7437, loss: 1.0653 ||:  49%|####9     | 32/65 [05:51<06:21, 11.56s/it]
2022-04-13 17:47:27,669 - INFO - tqdm - accuracy: 0.5076, batch_loss: 0.7010, loss: 1.0543 ||:  51%|#####     | 33/65 [06:02<06:10, 11.57s/it]
2022-04-13 17:47:39,211 - INFO - tqdm - accuracy: 0.5074, batch_loss: 0.6995, loss: 1.0438 ||:  52%|#####2    | 34/65 [06:14<05:58, 11.56s/it]
2022-04-13 17:47:50,777 - INFO - tqdm - accuracy: 0.5062, batch_loss: 0.7117, loss: 1.0343 ||:  54%|#####3    | 35/65 [06:25<05:46, 11.56s/it]
2022-04-13 17:48:02,349 - INFO - tqdm - accuracy: 0.5052, batch_loss: 0.6847, loss: 1.0246 ||:  55%|#####5    | 36/65 [06:37<05:35, 11.56s/it]
2022-04-13 17:48:13,920 - INFO - tqdm - accuracy: 0.5017, batch_loss: 0.7110, loss: 1.0162 ||:  57%|#####6    | 37/65 [06:49<05:23, 11.57s/it]
2022-04-13 17:48:25,485 - INFO - tqdm - accuracy: 0.5033, batch_loss: 0.6743, loss: 1.0072 ||:  58%|#####8    | 38/65 [07:00<05:12, 11.57s/it]
2022-04-13 17:48:37,062 - INFO - tqdm - accuracy: 0.5040, batch_loss: 0.6898, loss: 0.9990 ||:  60%|######    | 39/65 [07:12<05:00, 11.57s/it]
2022-04-13 17:48:48,639 - INFO - tqdm - accuracy: 0.5070, batch_loss: 0.6657, loss: 0.9907 ||:  62%|######1   | 40/65 [07:23<04:49, 11.57s/it]
2022-04-13 17:49:00,224 - INFO - tqdm - accuracy: 0.5038, batch_loss: 0.7381, loss: 0.9845 ||:  63%|######3   | 41/65 [07:35<04:37, 11.58s/it]
2022-04-13 17:49:11,808 - INFO - tqdm - accuracy: 0.5060, batch_loss: 0.6604, loss: 0.9768 ||:  65%|######4   | 42/65 [07:46<04:26, 11.58s/it]
2022-04-13 17:49:23,403 - INFO - tqdm - accuracy: 0.5022, batch_loss: 0.7346, loss: 0.9712 ||:  66%|######6   | 43/65 [07:58<04:14, 11.58s/it]
2022-04-13 17:49:34,985 - INFO - tqdm - accuracy: 0.5050, batch_loss: 0.6490, loss: 0.9639 ||:  68%|######7   | 44/65 [08:10<04:03, 11.58s/it]
2022-04-13 17:49:46,581 - INFO - tqdm - accuracy: 0.5056, batch_loss: 0.6978, loss: 0.9579 ||:  69%|######9   | 45/65 [08:21<03:51, 11.59s/it]
2022-04-13 17:49:58,193 - INFO - tqdm - accuracy: 0.5061, batch_loss: 0.6829, loss: 0.9520 ||:  71%|#######   | 46/65 [08:33<03:40, 11.59s/it]
2022-04-13 17:50:09,769 - INFO - tqdm - accuracy: 0.5080, batch_loss: 0.6778, loss: 0.9461 ||:  72%|#######2  | 47/65 [08:44<03:28, 11.59s/it]
2022-04-13 17:50:21,354 - INFO - tqdm - accuracy: 0.5065, batch_loss: 0.7286, loss: 0.9416 ||:  74%|#######3  | 48/65 [08:56<03:16, 11.59s/it]
2022-04-13 17:50:32,969 - INFO - tqdm - accuracy: 0.5070, batch_loss: 0.6880, loss: 0.9364 ||:  75%|#######5  | 49/65 [09:08<03:05, 11.60s/it]
2022-04-13 17:50:44,538 - INFO - tqdm - accuracy: 0.5081, batch_loss: 0.6868, loss: 0.9314 ||:  77%|#######6  | 50/65 [09:19<02:53, 11.59s/it]
2022-04-13 17:50:56,135 - INFO - tqdm - accuracy: 0.5067, batch_loss: 0.7243, loss: 0.9274 ||:  78%|#######8  | 51/65 [09:31<02:42, 11.59s/it]
2022-04-13 17:51:07,693 - INFO - tqdm - accuracy: 0.5096, batch_loss: 0.6441, loss: 0.9219 ||:  80%|########  | 52/65 [09:42<02:30, 11.58s/it]
2022-04-13 17:51:19,233 - INFO - tqdm - accuracy: 0.5106, batch_loss: 0.6867, loss: 0.9175 ||:  82%|########1 | 53/65 [09:54<02:18, 11.57s/it]
2022-04-13 17:51:30,390 - INFO - tqdm - accuracy: 0.5153, batch_loss: 0.6164, loss: 0.9119 ||:  83%|########3 | 54/65 [10:05<02:05, 11.45s/it]
2022-04-13 17:51:41,907 - INFO - tqdm - accuracy: 0.5128, batch_loss: 0.7293, loss: 0.9086 ||:  85%|########4 | 55/65 [10:17<01:54, 11.47s/it]
2022-04-13 17:51:53,432 - INFO - tqdm - accuracy: 0.5131, batch_loss: 0.6940, loss: 0.9048 ||:  86%|########6 | 56/65 [10:28<01:43, 11.48s/it]
2022-04-13 17:52:04,940 - INFO - tqdm - accuracy: 0.5085, batch_loss: 0.7981, loss: 0.9029 ||:  88%|########7 | 57/65 [10:40<01:31, 11.49s/it]
2022-04-13 17:52:16,460 - INFO - tqdm - accuracy: 0.5084, batch_loss: 0.6782, loss: 0.8990 ||:  89%|########9 | 58/65 [10:51<01:20, 11.50s/it]
2022-04-13 17:52:27,982 - INFO - tqdm - accuracy: 0.5077, batch_loss: 0.7120, loss: 0.8958 ||:  91%|######### | 59/65 [11:03<01:09, 11.51s/it]
2022-04-13 17:52:39,487 - INFO - tqdm - accuracy: 0.5091, batch_loss: 0.6733, loss: 0.8921 ||:  92%|#########2| 60/65 [11:14<00:57, 11.51s/it]
2022-04-13 17:52:50,993 - INFO - tqdm - accuracy: 0.5090, batch_loss: 0.6858, loss: 0.8887 ||:  94%|#########3| 61/65 [11:26<00:46, 11.51s/it]
2022-04-13 17:53:02,489 - INFO - tqdm - accuracy: 0.5098, batch_loss: 0.6822, loss: 0.8854 ||:  95%|#########5| 62/65 [11:37<00:34, 11.50s/it]
2022-04-13 17:53:14,009 - INFO - tqdm - accuracy: 0.5087, batch_loss: 0.7115, loss: 0.8827 ||:  97%|#########6| 63/65 [11:49<00:23, 11.51s/it]
2022-04-13 17:53:25,526 - INFO - tqdm - accuracy: 0.5100, batch_loss: 0.6989, loss: 0.8798 ||:  98%|#########8| 64/65 [12:00<00:11, 11.51s/it]
2022-04-13 17:53:30,648 - INFO - tqdm - accuracy: 0.5095, batch_loss: 0.7214, loss: 0.8774 ||: 100%|##########| 65/65 [12:05<00:00,  9.59s/it]
2022-04-13 17:53:30,649 - INFO - tqdm - accuracy: 0.5095, batch_loss: 0.7214, loss: 0.8774 ||: 100%|##########| 65/65 [12:05<00:00, 11.17s/it]
2022-04-13 17:53:33,524 - INFO - allennlp.training.trainer - Validating
2022-04-13 17:53:33,526 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-13 17:53:33,526 - INFO - allennlp.data.samplers.bucket_batch_sampler - No sorting keys given; trying to guess a good one
2022-04-13 17:53:33,527 - INFO - allennlp.data.samplers.bucket_batch_sampler - Using ['tokens'] as the sorting keys
2022-04-13 17:53:43,666 - INFO - tqdm - accuracy: 0.5125, batch_loss: 0.6382, loss: 0.6962 ||:  35%|###4      | 40/115 [00:10<00:19,  3.92it/s]
2022-04-13 17:53:53,796 - INFO - tqdm - accuracy: 0.4688, batch_loss: 0.6469, loss: 0.6991 ||:  70%|######9   | 80/115 [00:20<00:08,  3.97it/s]
2022-04-13 17:54:02,679 - INFO - tqdm - accuracy: 0.4760, batch_loss: 0.7934, loss: 0.6982 ||: 100%|##########| 115/115 [00:29<00:00,  3.89it/s]
2022-04-13 17:54:02,679 - INFO - tqdm - accuracy: 0.4760, batch_loss: 0.7934, loss: 0.6982 ||: 100%|##########| 115/115 [00:29<00:00,  3.94it/s]
2022-04-13 17:54:02,679 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-13 17:54:02,681 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.509  |     0.476
2022-04-13 17:54:02,681 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  1359.606  |       N/A
2022-04-13 17:54:02,682 - INFO - allennlp.training.tensorboard_writer - loss               |     0.877  |     0.698
2022-04-13 17:54:02,682 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5802.840  |       N/A
2022-04-13 17:54:08,303 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to './output_ir-ora-d_hyper0/best.th'.
2022-04-13 17:54:09,685 - INFO - allennlp.training.trainer - Epoch duration: 0:12:44.825727
2022-04-13 17:54:09,686 - INFO - allennlp.training.trainer - Estimated training time remaining: 2:58:27
2022-04-13 17:54:09,686 - INFO - allennlp.training.trainer - Epoch 1/14
2022-04-13 17:54:09,686 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-13 17:54:09,687 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-13 17:54:09,688 - INFO - allennlp.training.trainer - Training
2022-04-13 17:54:09,688 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-13 17:54:21,169 - INFO - tqdm - accuracy: 0.3438, batch_loss: 0.7163, loss: 0.7163 ||:   2%|1         | 1/65 [00:11<12:14, 11.48s/it]
2022-04-13 17:54:32,957 - INFO - tqdm - accuracy: 0.4375, batch_loss: 0.6838, loss: 0.7001 ||:   3%|3         | 2/65 [00:23<12:14, 11.66s/it]
2022-04-13 17:54:44,790 - INFO - tqdm - accuracy: 0.4271, batch_loss: 0.7154, loss: 0.7052 ||:   5%|4         | 3/65 [00:35<12:07, 11.74s/it]
2022-04-13 17:54:56,352 - INFO - tqdm - accuracy: 0.4688, batch_loss: 0.6794, loss: 0.6987 ||:   6%|6         | 4/65 [00:46<11:51, 11.67s/it]
2022-04-13 17:55:07,812 - INFO - tqdm - accuracy: 0.4688, batch_loss: 0.6864, loss: 0.6963 ||:   8%|7         | 5/65 [00:58<11:35, 11.59s/it]
2022-04-13 17:55:19,260 - INFO - tqdm - accuracy: 0.5052, batch_loss: 0.6830, loss: 0.6941 ||:   9%|9         | 6/65 [01:09<11:21, 11.54s/it]
2022-04-13 17:55:30,758 - INFO - tqdm - accuracy: 0.5089, batch_loss: 0.6953, loss: 0.6942 ||:  11%|#         | 7/65 [01:21<11:08, 11.53s/it]
2022-04-13 17:55:42,319 - INFO - tqdm - accuracy: 0.5195, batch_loss: 0.6806, loss: 0.6925 ||:  12%|#2        | 8/65 [01:32<10:57, 11.54s/it]
2022-04-13 17:55:53,911 - INFO - tqdm - accuracy: 0.5312, batch_loss: 0.6647, loss: 0.6894 ||:  14%|#3        | 9/65 [01:44<10:47, 11.56s/it]
2022-04-13 17:56:05,532 - INFO - tqdm - accuracy: 0.5250, batch_loss: 0.6692, loss: 0.6874 ||:  15%|#5        | 10/65 [01:55<10:36, 11.58s/it]
2022-04-13 17:56:17,136 - INFO - tqdm - accuracy: 0.5199, batch_loss: 0.7296, loss: 0.6913 ||:  17%|#6        | 11/65 [02:07<10:25, 11.58s/it]
2022-04-13 17:56:28,724 - INFO - tqdm - accuracy: 0.5234, batch_loss: 0.6789, loss: 0.6902 ||:  18%|#8        | 12/65 [02:19<10:14, 11.59s/it]
2022-04-13 17:56:40,315 - INFO - tqdm - accuracy: 0.5312, batch_loss: 0.6704, loss: 0.6887 ||:  20%|##        | 13/65 [02:30<10:02, 11.59s/it]
2022-04-13 17:56:51,780 - INFO - tqdm - accuracy: 0.5290, batch_loss: 0.7195, loss: 0.6909 ||:  22%|##1       | 14/65 [02:42<09:49, 11.55s/it]
2022-04-13 17:57:03,349 - INFO - tqdm - accuracy: 0.5271, batch_loss: 0.6926, loss: 0.6910 ||:  23%|##3       | 15/65 [02:53<09:37, 11.56s/it]
2022-04-13 17:57:14,942 - INFO - tqdm - accuracy: 0.5293, batch_loss: 0.6830, loss: 0.6905 ||:  25%|##4       | 16/65 [03:05<09:26, 11.57s/it]
2022-04-13 17:57:26,560 - INFO - tqdm - accuracy: 0.5312, batch_loss: 0.7175, loss: 0.6921 ||:  26%|##6       | 17/65 [03:16<09:15, 11.58s/it]
2022-04-13 17:57:38,161 - INFO - tqdm - accuracy: 0.5365, batch_loss: 0.6306, loss: 0.6887 ||:  28%|##7       | 18/65 [03:28<09:04, 11.59s/it]
2022-04-13 17:57:49,774 - INFO - tqdm - accuracy: 0.5362, batch_loss: 0.6847, loss: 0.6885 ||:  29%|##9       | 19/65 [03:40<08:53, 11.60s/it]
2022-04-13 17:58:01,362 - INFO - tqdm - accuracy: 0.5422, batch_loss: 0.6678, loss: 0.6874 ||:  31%|###       | 20/65 [03:51<08:41, 11.59s/it]
2022-04-13 17:58:12,973 - INFO - tqdm - accuracy: 0.5446, batch_loss: 0.6495, loss: 0.6856 ||:  32%|###2      | 21/65 [04:03<08:30, 11.60s/it]
2022-04-13 17:58:24,456 - INFO - tqdm - accuracy: 0.5455, batch_loss: 0.6802, loss: 0.6854 ||:  34%|###3      | 22/65 [04:14<08:17, 11.56s/it]
2022-04-13 17:58:36,050 - INFO - tqdm - accuracy: 0.5476, batch_loss: 0.6647, loss: 0.6845 ||:  35%|###5      | 23/65 [04:26<08:06, 11.57s/it]
2022-04-13 17:58:47,663 - INFO - tqdm - accuracy: 0.5430, batch_loss: 0.7028, loss: 0.6852 ||:  37%|###6      | 24/65 [04:37<07:54, 11.59s/it]
2022-04-13 17:58:59,257 - INFO - tqdm - accuracy: 0.5475, batch_loss: 0.6418, loss: 0.6835 ||:  38%|###8      | 25/65 [04:49<07:43, 11.59s/it]
2022-04-13 17:59:10,866 - INFO - tqdm - accuracy: 0.5493, batch_loss: 0.6695, loss: 0.6830 ||:  40%|####      | 26/65 [05:01<07:32, 11.59s/it]
2022-04-13 17:59:22,469 - INFO - tqdm - accuracy: 0.5498, batch_loss: 0.6618, loss: 0.6822 ||:  42%|####1     | 27/65 [05:12<07:20, 11.60s/it]
2022-04-13 17:59:34,065 - INFO - tqdm - accuracy: 0.5547, batch_loss: 0.6591, loss: 0.6814 ||:  43%|####3     | 28/65 [05:24<07:09, 11.60s/it]
2022-04-13 17:59:45,655 - INFO - tqdm - accuracy: 0.5571, batch_loss: 0.6647, loss: 0.6808 ||:  45%|####4     | 29/65 [05:35<06:57, 11.59s/it]
2022-04-13 17:59:57,246 - INFO - tqdm - accuracy: 0.5573, batch_loss: 0.7019, loss: 0.6815 ||:  46%|####6     | 30/65 [05:47<06:45, 11.59s/it]
2022-04-13 18:00:08,841 - INFO - tqdm - accuracy: 0.5595, batch_loss: 0.6619, loss: 0.6809 ||:  48%|####7     | 31/65 [05:59<06:34, 11.59s/it]
2022-04-13 18:00:20,409 - INFO - tqdm - accuracy: 0.5566, batch_loss: 0.6479, loss: 0.6798 ||:  49%|####9     | 32/65 [06:10<06:22, 11.59s/it]
2022-04-13 18:00:31,983 - INFO - tqdm - accuracy: 0.5568, batch_loss: 0.6903, loss: 0.6801 ||:  51%|#####     | 33/65 [06:22<06:10, 11.58s/it]
2022-04-13 18:00:43,549 - INFO - tqdm - accuracy: 0.5597, batch_loss: 0.6640, loss: 0.6797 ||:  52%|#####2    | 34/65 [06:33<05:58, 11.58s/it]
2022-04-13 18:00:55,398 - INFO - tqdm - accuracy: 0.5598, batch_loss: 0.6700, loss: 0.6794 ||:  54%|#####3    | 35/65 [06:45<05:49, 11.66s/it]
2022-04-13 18:01:06,932 - INFO - tqdm - accuracy: 0.5660, batch_loss: 0.6221, loss: 0.6778 ||:  55%|#####5    | 36/65 [06:57<05:37, 11.62s/it]
2022-04-13 18:01:18,516 - INFO - tqdm - accuracy: 0.5684, batch_loss: 0.5847, loss: 0.6753 ||:  57%|#####6    | 37/65 [07:08<05:25, 11.61s/it]
2022-04-13 18:01:30,089 - INFO - tqdm - accuracy: 0.5691, batch_loss: 0.6829, loss: 0.6755 ||:  58%|#####8    | 38/65 [07:20<05:13, 11.60s/it]
2022-04-13 18:01:41,659 - INFO - tqdm - accuracy: 0.5689, batch_loss: 0.6550, loss: 0.6750 ||:  60%|######    | 39/65 [07:31<05:01, 11.59s/it]
2022-04-13 18:01:53,223 - INFO - tqdm - accuracy: 0.5727, batch_loss: 0.6215, loss: 0.6736 ||:  62%|######1   | 40/65 [07:43<04:49, 11.58s/it]
2022-04-13 18:02:04,788 - INFO - tqdm - accuracy: 0.5732, batch_loss: 0.6291, loss: 0.6725 ||:  63%|######3   | 41/65 [07:55<04:37, 11.58s/it]
2022-04-13 18:02:16,351 - INFO - tqdm - accuracy: 0.5774, batch_loss: 0.6342, loss: 0.6716 ||:  65%|######4   | 42/65 [08:06<04:26, 11.57s/it]
2022-04-13 18:02:27,923 - INFO - tqdm - accuracy: 0.5785, batch_loss: 0.6802, loss: 0.6718 ||:  66%|######6   | 43/65 [08:18<04:14, 11.57s/it]
2022-04-13 18:02:39,500 - INFO - tqdm - accuracy: 0.5795, batch_loss: 0.6562, loss: 0.6715 ||:  68%|######7   | 44/65 [08:29<04:03, 11.57s/it]
2022-04-13 18:02:51,076 - INFO - tqdm - accuracy: 0.5806, batch_loss: 0.5838, loss: 0.6695 ||:  69%|######9   | 45/65 [08:41<03:51, 11.57s/it]
2022-04-13 18:03:02,631 - INFO - tqdm - accuracy: 0.5836, batch_loss: 0.6494, loss: 0.6691 ||:  71%|#######   | 46/65 [08:52<03:39, 11.57s/it]
2022-04-13 18:03:14,206 - INFO - tqdm - accuracy: 0.5864, batch_loss: 0.6477, loss: 0.6686 ||:  72%|#######2  | 47/65 [09:04<03:28, 11.57s/it]
2022-04-13 18:03:25,763 - INFO - tqdm - accuracy: 0.5853, batch_loss: 0.6868, loss: 0.6690 ||:  74%|#######3  | 48/65 [09:16<03:16, 11.57s/it]
2022-04-13 18:03:37,332 - INFO - tqdm - accuracy: 0.5861, batch_loss: 0.7466, loss: 0.6706 ||:  75%|#######5  | 49/65 [09:27<03:05, 11.57s/it]
2022-04-13 18:03:48,901 - INFO - tqdm - accuracy: 0.5875, batch_loss: 0.6371, loss: 0.6699 ||:  77%|#######6  | 50/65 [09:39<02:53, 11.57s/it]
2022-04-13 18:04:00,477 - INFO - tqdm - accuracy: 0.5839, batch_loss: 0.7998, loss: 0.6725 ||:  78%|#######8  | 51/65 [09:50<02:41, 11.57s/it]
2022-04-13 18:04:12,041 - INFO - tqdm - accuracy: 0.5829, batch_loss: 0.6876, loss: 0.6728 ||:  80%|########  | 52/65 [10:02<02:30, 11.57s/it]
2022-04-13 18:04:23,631 - INFO - tqdm - accuracy: 0.5849, batch_loss: 0.6049, loss: 0.6715 ||:  82%|########1 | 53/65 [10:13<02:18, 11.58s/it]
2022-04-13 18:04:35,192 - INFO - tqdm - accuracy: 0.5839, batch_loss: 0.7221, loss: 0.6724 ||:  83%|########3 | 54/65 [10:25<02:07, 11.57s/it]
2022-04-13 18:04:46,768 - INFO - tqdm - accuracy: 0.5864, batch_loss: 0.5341, loss: 0.6699 ||:  85%|########4 | 55/65 [10:37<01:55, 11.57s/it]
2022-04-13 18:04:58,351 - INFO - tqdm - accuracy: 0.5887, batch_loss: 0.5676, loss: 0.6681 ||:  86%|########6 | 56/65 [10:48<01:44, 11.58s/it]
2022-04-13 18:05:09,662 - INFO - tqdm - accuracy: 0.5894, batch_loss: 0.6350, loss: 0.6675 ||:  88%|########7 | 57/65 [10:59<01:31, 11.50s/it]
2022-04-13 18:05:21,241 - INFO - tqdm - accuracy: 0.5884, batch_loss: 0.7386, loss: 0.6687 ||:  89%|########9 | 58/65 [11:11<01:20, 11.52s/it]
2022-04-13 18:05:32,807 - INFO - tqdm - accuracy: 0.5900, batch_loss: 0.6195, loss: 0.6679 ||:  91%|######### | 59/65 [11:23<01:09, 11.53s/it]
2022-04-13 18:05:44,379 - INFO - tqdm - accuracy: 0.5896, batch_loss: 0.7634, loss: 0.6695 ||:  92%|#########2| 60/65 [11:34<00:57, 11.55s/it]
2022-04-13 18:05:55,929 - INFO - tqdm - accuracy: 0.5922, batch_loss: 0.5644, loss: 0.6678 ||:  94%|#########3| 61/65 [11:46<00:46, 11.55s/it]
2022-04-13 18:06:07,491 - INFO - tqdm - accuracy: 0.5938, batch_loss: 0.5346, loss: 0.6656 ||:  95%|#########5| 62/65 [11:57<00:34, 11.55s/it]
2022-04-13 18:06:19,060 - INFO - tqdm - accuracy: 0.5933, batch_loss: 0.6385, loss: 0.6652 ||:  97%|#########6| 63/65 [12:09<00:23, 11.56s/it]
2022-04-13 18:06:30,252 - INFO - tqdm - accuracy: 0.5945, batch_loss: 0.6883, loss: 0.6655 ||:  98%|#########8| 64/65 [12:20<00:11, 11.45s/it]
2022-04-13 18:06:35,388 - INFO - tqdm - accuracy: 0.5949, batch_loss: 0.6238, loss: 0.6649 ||: 100%|##########| 65/65 [12:25<00:00,  9.55s/it]
2022-04-13 18:06:35,389 - INFO - tqdm - accuracy: 0.5949, batch_loss: 0.6238, loss: 0.6649 ||: 100%|##########| 65/65 [12:25<00:00, 11.47s/it]
2022-04-13 18:06:38,254 - INFO - allennlp.training.trainer - Validating
2022-04-13 18:06:38,256 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-13 18:06:48,323 - INFO - tqdm - accuracy: 0.7000, batch_loss: 0.6472, loss: 0.6482 ||:  35%|###4      | 40/115 [00:10<00:19,  3.91it/s]
2022-04-13 18:06:58,544 - INFO - tqdm - accuracy: 0.6750, batch_loss: 0.7455, loss: 0.6488 ||:  70%|######9   | 80/115 [00:20<00:08,  3.89it/s]
2022-04-13 18:07:07,409 - INFO - tqdm - accuracy: 0.6638, batch_loss: 0.7389, loss: 0.6646 ||: 100%|##########| 115/115 [00:29<00:00,  3.90it/s]
2022-04-13 18:07:07,409 - INFO - tqdm - accuracy: 0.6638, batch_loss: 0.7389, loss: 0.6646 ||: 100%|##########| 115/115 [00:29<00:00,  3.94it/s]
2022-04-13 18:07:07,409 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-13 18:07:07,410 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.595  |     0.664
2022-04-13 18:07:07,411 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-13 18:07:07,411 - INFO - allennlp.training.tensorboard_writer - loss               |     0.665  |     0.665
2022-04-13 18:07:07,412 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5802.840  |       N/A
2022-04-13 18:07:13,006 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to './output_ir-ora-d_hyper0/best.th'.
2022-04-13 18:07:28,848 - INFO - allennlp.training.trainer - Epoch duration: 0:13:19.162137
2022-04-13 18:07:28,848 - INFO - allennlp.training.trainer - Estimated training time remaining: 2:49:25
2022-04-13 18:07:28,849 - INFO - allennlp.training.trainer - Epoch 2/14
2022-04-13 18:07:28,849 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-13 18:07:28,849 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-13 18:07:28,850 - INFO - allennlp.training.trainer - Training
2022-04-13 18:07:28,851 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-13 18:07:40,226 - INFO - tqdm - accuracy: 0.6250, batch_loss: 0.6499, loss: 0.6499 ||:   2%|1         | 1/65 [00:11<12:08, 11.38s/it]
2022-04-13 18:07:51,908 - INFO - tqdm - accuracy: 0.7344, batch_loss: 0.5025, loss: 0.5762 ||:   3%|3         | 2/65 [00:23<12:08, 11.56s/it]
2022-04-13 18:08:03,505 - INFO - tqdm - accuracy: 0.7292, batch_loss: 0.5536, loss: 0.5687 ||:   5%|4         | 3/65 [00:34<11:57, 11.57s/it]
2022-04-13 18:08:15,059 - INFO - tqdm - accuracy: 0.7422, batch_loss: 0.5055, loss: 0.5529 ||:   6%|6         | 4/65 [00:46<11:45, 11.57s/it]
2022-04-13 18:08:26,631 - INFO - tqdm - accuracy: 0.7375, batch_loss: 0.5072, loss: 0.5437 ||:   8%|7         | 5/65 [00:57<11:34, 11.57s/it]
2022-04-13 18:08:38,056 - INFO - tqdm - accuracy: 0.7552, batch_loss: 0.4905, loss: 0.5349 ||:   9%|9         | 6/65 [01:09<11:19, 11.52s/it]
2022-04-13 18:08:49,585 - INFO - tqdm - accuracy: 0.7455, batch_loss: 0.5612, loss: 0.5386 ||:  11%|#         | 7/65 [01:20<11:08, 11.52s/it]
2022-04-13 18:09:01,093 - INFO - tqdm - accuracy: 0.7344, batch_loss: 0.6006, loss: 0.5464 ||:  12%|#2        | 8/65 [01:32<10:56, 11.52s/it]
2022-04-13 18:09:12,584 - INFO - tqdm - accuracy: 0.7465, batch_loss: 0.4597, loss: 0.5367 ||:  14%|#3        | 9/65 [01:43<10:44, 11.51s/it]
2022-04-13 18:09:24,101 - INFO - tqdm - accuracy: 0.7438, batch_loss: 0.5201, loss: 0.5351 ||:  15%|#5        | 10/65 [01:55<10:33, 11.51s/it]
2022-04-13 18:09:35,618 - INFO - tqdm - accuracy: 0.7358, batch_loss: 0.6054, loss: 0.5415 ||:  17%|#6        | 11/65 [02:06<10:21, 11.51s/it]
2022-04-13 18:09:47,142 - INFO - tqdm - accuracy: 0.7318, batch_loss: 0.5443, loss: 0.5417 ||:  18%|#8        | 12/65 [02:18<10:10, 11.52s/it]
2022-04-13 18:09:58,357 - INFO - tqdm - accuracy: 0.7084, batch_loss: 0.7646, loss: 0.5589 ||:  20%|##        | 13/65 [02:29<09:54, 11.43s/it]
2022-04-13 18:10:09,918 - INFO - tqdm - accuracy: 0.7136, batch_loss: 0.5533, loss: 0.5585 ||:  22%|##1       | 14/65 [02:41<09:44, 11.47s/it]
2022-04-13 18:10:21,475 - INFO - tqdm - accuracy: 0.7077, batch_loss: 0.6843, loss: 0.5669 ||:  23%|##3       | 15/65 [02:52<09:34, 11.49s/it]
2022-04-13 18:10:33,040 - INFO - tqdm - accuracy: 0.7123, batch_loss: 0.4810, loss: 0.5615 ||:  25%|##4       | 16/65 [03:04<09:24, 11.52s/it]
2022-04-13 18:10:44,620 - INFO - tqdm - accuracy: 0.7072, batch_loss: 0.6230, loss: 0.5651 ||:  26%|##6       | 17/65 [03:15<09:13, 11.53s/it]
2022-04-13 18:10:56,196 - INFO - tqdm - accuracy: 0.6991, batch_loss: 0.8281, loss: 0.5797 ||:  28%|##7       | 18/65 [03:27<09:02, 11.55s/it]
2022-04-13 18:11:07,793 - INFO - tqdm - accuracy: 0.7002, batch_loss: 0.5860, loss: 0.5800 ||:  29%|##9       | 19/65 [03:38<08:51, 11.56s/it]
2022-04-13 18:11:19,412 - INFO - tqdm - accuracy: 0.6995, batch_loss: 0.5409, loss: 0.5781 ||:  31%|###       | 20/65 [03:50<08:41, 11.58s/it]
2022-04-13 18:11:31,004 - INFO - tqdm - accuracy: 0.6945, batch_loss: 0.6371, loss: 0.5809 ||:  32%|###2      | 21/65 [04:02<08:29, 11.58s/it]
2022-04-13 18:11:42,582 - INFO - tqdm - accuracy: 0.6942, batch_loss: 0.5546, loss: 0.5797 ||:  34%|###3      | 22/65 [04:13<08:18, 11.58s/it]
2022-04-13 18:11:54,189 - INFO - tqdm - accuracy: 0.6939, batch_loss: 0.5342, loss: 0.5777 ||:  35%|###5      | 23/65 [04:25<08:06, 11.59s/it]
2022-04-13 18:12:05,652 - INFO - tqdm - accuracy: 0.6949, batch_loss: 0.5891, loss: 0.5782 ||:  37%|###6      | 24/65 [04:36<07:53, 11.55s/it]
2022-04-13 18:12:17,237 - INFO - tqdm - accuracy: 0.6971, batch_loss: 0.5253, loss: 0.5761 ||:  38%|###8      | 25/65 [04:48<07:42, 11.56s/it]
2022-04-13 18:12:28,816 - INFO - tqdm - accuracy: 0.7016, batch_loss: 0.4873, loss: 0.5727 ||:  40%|####      | 26/65 [04:59<07:31, 11.57s/it]
2022-04-13 18:12:40,415 - INFO - tqdm - accuracy: 0.7057, batch_loss: 0.4542, loss: 0.5683 ||:  42%|####1     | 27/65 [05:11<07:19, 11.58s/it]
2022-04-13 18:12:51,984 - INFO - tqdm - accuracy: 0.7106, batch_loss: 0.4524, loss: 0.5641 ||:  43%|####3     | 28/65 [05:23<07:08, 11.57s/it]
2022-04-13 18:13:03,562 - INFO - tqdm - accuracy: 0.7098, batch_loss: 0.5838, loss: 0.5648 ||:  45%|####4     | 29/65 [05:34<06:56, 11.58s/it]
2022-04-13 18:13:15,148 - INFO - tqdm - accuracy: 0.7101, batch_loss: 0.4829, loss: 0.5621 ||:  46%|####6     | 30/65 [05:46<06:45, 11.58s/it]
2022-04-13 18:13:26,739 - INFO - tqdm - accuracy: 0.7084, batch_loss: 0.5552, loss: 0.5619 ||:  48%|####7     | 31/65 [05:57<06:33, 11.58s/it]
2022-04-13 18:13:38,321 - INFO - tqdm - accuracy: 0.7107, batch_loss: 0.5226, loss: 0.5606 ||:  49%|####9     | 32/65 [06:09<06:22, 11.58s/it]
2022-04-13 18:13:49,893 - INFO - tqdm - accuracy: 0.7081, batch_loss: 0.6280, loss: 0.5627 ||:  51%|#####     | 33/65 [06:21<06:10, 11.58s/it]
2022-04-13 18:14:01,488 - INFO - tqdm - accuracy: 0.7065, batch_loss: 0.6367, loss: 0.5649 ||:  52%|#####2    | 34/65 [06:32<05:59, 11.58s/it]
2022-04-13 18:14:13,069 - INFO - tqdm - accuracy: 0.7078, batch_loss: 0.4576, loss: 0.5618 ||:  54%|#####3    | 35/65 [06:44<05:47, 11.58s/it]
2022-04-13 18:14:24,644 - INFO - tqdm - accuracy: 0.7089, batch_loss: 0.5263, loss: 0.5608 ||:  55%|#####5    | 36/65 [06:55<05:35, 11.58s/it]
2022-04-13 18:14:36,233 - INFO - tqdm - accuracy: 0.7075, batch_loss: 0.5745, loss: 0.5612 ||:  57%|#####6    | 37/65 [07:07<05:24, 11.58s/it]
2022-04-13 18:14:47,822 - INFO - tqdm - accuracy: 0.7103, batch_loss: 0.3489, loss: 0.5556 ||:  58%|#####8    | 38/65 [07:18<05:12, 11.58s/it]
2022-04-13 18:14:59,388 - INFO - tqdm - accuracy: 0.7105, batch_loss: 0.5079, loss: 0.5544 ||:  60%|######    | 39/65 [07:30<05:01, 11.58s/it]
2022-04-13 18:15:10,969 - INFO - tqdm - accuracy: 0.7099, batch_loss: 0.5989, loss: 0.5555 ||:  62%|######1   | 40/65 [07:42<04:49, 11.58s/it]
2022-04-13 18:15:22,553 - INFO - tqdm - accuracy: 0.7101, batch_loss: 0.4436, loss: 0.5528 ||:  63%|######3   | 41/65 [07:53<04:37, 11.58s/it]
2022-04-13 18:15:34,002 - INFO - tqdm - accuracy: 0.7133, batch_loss: 0.4441, loss: 0.5502 ||:  65%|######4   | 42/65 [08:05<04:25, 11.54s/it]
2022-04-13 18:15:45,578 - INFO - tqdm - accuracy: 0.7127, batch_loss: 0.6045, loss: 0.5514 ||:  66%|######6   | 43/65 [08:16<04:14, 11.55s/it]
2022-04-13 18:15:57,176 - INFO - tqdm - accuracy: 0.7129, batch_loss: 0.4743, loss: 0.5497 ||:  68%|######7   | 44/65 [08:28<04:02, 11.57s/it]
2022-04-13 18:16:08,755 - INFO - tqdm - accuracy: 0.7144, batch_loss: 0.5446, loss: 0.5496 ||:  69%|######9   | 45/65 [08:39<03:51, 11.57s/it]
2022-04-13 18:16:20,350 - INFO - tqdm - accuracy: 0.7165, batch_loss: 0.4180, loss: 0.5467 ||:  71%|#######   | 46/65 [08:51<03:39, 11.58s/it]
2022-04-13 18:16:31,919 - INFO - tqdm - accuracy: 0.7166, batch_loss: 0.5950, loss: 0.5477 ||:  72%|#######2  | 47/65 [09:03<03:28, 11.57s/it]
2022-04-13 18:16:43,509 - INFO - tqdm - accuracy: 0.7160, batch_loss: 0.6868, loss: 0.5506 ||:  74%|#######3  | 48/65 [09:14<03:16, 11.58s/it]
2022-04-13 18:16:55,108 - INFO - tqdm - accuracy: 0.7154, batch_loss: 0.5302, loss: 0.5502 ||:  75%|#######5  | 49/65 [09:26<03:05, 11.59s/it]
2022-04-13 18:17:06,701 - INFO - tqdm - accuracy: 0.7154, batch_loss: 0.6123, loss: 0.5515 ||:  77%|#######6  | 50/65 [09:37<02:53, 11.59s/it]
2022-04-13 18:17:18,274 - INFO - tqdm - accuracy: 0.7137, batch_loss: 0.7158, loss: 0.5547 ||:  78%|#######8  | 51/65 [09:49<02:42, 11.58s/it]
2022-04-13 18:17:29,872 - INFO - tqdm - accuracy: 0.7132, batch_loss: 0.5168, loss: 0.5539 ||:  80%|########  | 52/65 [10:01<02:30, 11.59s/it]
2022-04-13 18:17:41,469 - INFO - tqdm - accuracy: 0.7162, batch_loss: 0.4487, loss: 0.5520 ||:  82%|########1 | 53/65 [10:12<02:19, 11.59s/it]
2022-04-13 18:17:53,042 - INFO - tqdm - accuracy: 0.7163, batch_loss: 0.5296, loss: 0.5515 ||:  83%|########3 | 54/65 [10:24<02:07, 11.59s/it]
2022-04-13 18:18:04,595 - INFO - tqdm - accuracy: 0.7157, batch_loss: 0.6552, loss: 0.5534 ||:  85%|########4 | 55/65 [10:35<01:55, 11.58s/it]
2022-04-13 18:18:16,168 - INFO - tqdm - accuracy: 0.7152, batch_loss: 0.5641, loss: 0.5536 ||:  86%|########6 | 56/65 [10:47<01:44, 11.57s/it]
2022-04-13 18:18:27,748 - INFO - tqdm - accuracy: 0.7137, batch_loss: 0.7160, loss: 0.5565 ||:  88%|########7 | 57/65 [10:58<01:32, 11.58s/it]
2022-04-13 18:18:39,076 - INFO - tqdm - accuracy: 0.7127, batch_loss: 0.5083, loss: 0.5556 ||:  89%|########9 | 58/65 [11:10<01:20, 11.50s/it]
2022-04-13 18:18:50,658 - INFO - tqdm - accuracy: 0.7154, batch_loss: 0.4362, loss: 0.5536 ||:  91%|######### | 59/65 [11:21<01:09, 11.53s/it]
2022-04-13 18:19:02,240 - INFO - tqdm - accuracy: 0.7134, batch_loss: 0.6024, loss: 0.5544 ||:  92%|#########2| 60/65 [11:33<00:57, 11.54s/it]
2022-04-13 18:19:13,839 - INFO - tqdm - accuracy: 0.7130, batch_loss: 0.4682, loss: 0.5530 ||:  94%|#########3| 61/65 [11:44<00:46, 11.56s/it]
2022-04-13 18:19:25,412 - INFO - tqdm - accuracy: 0.7100, batch_loss: 0.6819, loss: 0.5551 ||:  95%|#########5| 62/65 [11:56<00:34, 11.56s/it]
2022-04-13 18:19:36,992 - INFO - tqdm - accuracy: 0.7097, batch_loss: 0.5475, loss: 0.5550 ||:  97%|#########6| 63/65 [12:08<00:23, 11.57s/it]
2022-04-13 18:19:48,559 - INFO - tqdm - accuracy: 0.7074, batch_loss: 0.6753, loss: 0.5569 ||:  98%|#########8| 64/65 [12:19<00:11, 11.57s/it]
2022-04-13 18:19:53,696 - INFO - tqdm - accuracy: 0.7074, batch_loss: 0.6448, loss: 0.5582 ||: 100%|##########| 65/65 [12:24<00:00,  9.64s/it]
2022-04-13 18:19:53,696 - INFO - tqdm - accuracy: 0.7074, batch_loss: 0.6448, loss: 0.5582 ||: 100%|##########| 65/65 [12:24<00:00, 11.46s/it]
2022-04-13 18:19:56,517 - INFO - allennlp.training.trainer - Validating
2022-04-13 18:19:56,519 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-13 18:20:06,682 - INFO - tqdm - accuracy: 0.5750, batch_loss: 0.8529, loss: 0.7594 ||:  35%|###4      | 40/115 [00:10<00:19,  3.92it/s]
2022-04-13 18:20:16,743 - INFO - tqdm - accuracy: 0.6101, batch_loss: 0.9296, loss: 0.6863 ||:  70%|######9   | 80/115 [00:20<00:08,  3.98it/s]
2022-04-13 18:20:25,749 - INFO - tqdm - accuracy: 0.6376, batch_loss: 1.3006, loss: 0.6953 ||: 100%|##########| 115/115 [00:29<00:00,  3.88it/s]
2022-04-13 18:20:25,749 - INFO - tqdm - accuracy: 0.6376, batch_loss: 1.3006, loss: 0.6953 ||: 100%|##########| 115/115 [00:29<00:00,  3.93it/s]
2022-04-13 18:20:25,749 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-13 18:20:25,750 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.707  |     0.638
2022-04-13 18:20:25,751 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-13 18:20:25,752 - INFO - allennlp.training.tensorboard_writer - loss               |     0.558  |     0.695
2022-04-13 18:20:25,752 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5802.840  |       N/A
2022-04-13 18:20:31,872 - INFO - allennlp.training.trainer - Epoch duration: 0:13:03.023762
2022-04-13 18:20:31,873 - INFO - allennlp.training.trainer - Estimated training time remaining: 2:36:28
2022-04-13 18:20:31,873 - INFO - allennlp.training.trainer - Epoch 3/14
2022-04-13 18:20:31,873 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-13 18:20:31,873 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-13 18:20:31,875 - INFO - allennlp.training.trainer - Training
2022-04-13 18:20:31,875 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-13 18:20:43,297 - INFO - tqdm - accuracy: 0.8750, batch_loss: 0.4469, loss: 0.4469 ||:   2%|1         | 1/65 [00:11<12:10, 11.42s/it]
2022-04-13 18:20:55,010 - INFO - tqdm - accuracy: 0.7969, batch_loss: 0.5779, loss: 0.5124 ||:   3%|3         | 2/65 [00:23<12:10, 11.59s/it]
2022-04-13 18:21:06,847 - INFO - tqdm - accuracy: 0.8125, batch_loss: 0.4619, loss: 0.4956 ||:   5%|4         | 3/65 [00:34<12:05, 11.70s/it]
2022-04-13 18:21:18,438 - INFO - tqdm - accuracy: 0.8359, batch_loss: 0.3807, loss: 0.4669 ||:   6%|6         | 4/65 [00:46<11:51, 11.66s/it]
2022-04-13 18:21:30,312 - INFO - tqdm - accuracy: 0.8375, batch_loss: 0.4066, loss: 0.4548 ||:   8%|7         | 5/65 [00:58<11:44, 11.74s/it]
2022-04-13 18:21:41,717 - INFO - tqdm - accuracy: 0.8438, batch_loss: 0.3992, loss: 0.4455 ||:   9%|9         | 6/65 [01:09<11:25, 11.62s/it]
2022-04-13 18:21:53,199 - INFO - tqdm - accuracy: 0.8482, batch_loss: 0.3872, loss: 0.4372 ||:  11%|#         | 7/65 [01:21<11:11, 11.58s/it]
2022-04-13 18:22:04,613 - INFO - tqdm - accuracy: 0.8594, batch_loss: 0.2844, loss: 0.4181 ||:  12%|#2        | 8/65 [01:32<10:56, 11.53s/it]
2022-04-13 18:22:16,205 - INFO - tqdm - accuracy: 0.8542, batch_loss: 0.3839, loss: 0.4143 ||:  14%|#3        | 9/65 [01:44<10:46, 11.55s/it]
2022-04-13 18:22:27,824 - INFO - tqdm - accuracy: 0.8625, batch_loss: 0.2886, loss: 0.4017 ||:  15%|#5        | 10/65 [01:55<10:36, 11.57s/it]
2022-04-13 18:22:39,304 - INFO - tqdm - accuracy: 0.8580, batch_loss: 0.3690, loss: 0.3988 ||:  17%|#6        | 11/65 [02:07<10:23, 11.54s/it]
2022-04-13 18:22:50,880 - INFO - tqdm - accuracy: 0.8620, batch_loss: 0.2437, loss: 0.3858 ||:  18%|#8        | 12/65 [02:19<10:12, 11.55s/it]
2022-04-13 18:23:02,440 - INFO - tqdm - accuracy: 0.8678, batch_loss: 0.2227, loss: 0.3733 ||:  20%|##        | 13/65 [02:30<10:00, 11.55s/it]
2022-04-13 18:23:13,980 - INFO - tqdm - accuracy: 0.8594, batch_loss: 0.4267, loss: 0.3771 ||:  22%|##1       | 14/65 [02:42<09:49, 11.55s/it]
2022-04-13 18:23:25,534 - INFO - tqdm - accuracy: 0.8542, batch_loss: 0.4349, loss: 0.3810 ||:  23%|##3       | 15/65 [02:53<09:37, 11.55s/it]
2022-04-13 18:23:37,076 - INFO - tqdm - accuracy: 0.8535, batch_loss: 0.3665, loss: 0.3800 ||:  25%|##4       | 16/65 [03:05<09:25, 11.55s/it]
2022-04-13 18:23:48,596 - INFO - tqdm - accuracy: 0.8493, batch_loss: 0.5077, loss: 0.3876 ||:  26%|##6       | 17/65 [03:16<09:13, 11.54s/it]
2022-04-13 18:24:00,109 - INFO - tqdm - accuracy: 0.8559, batch_loss: 0.1797, loss: 0.3760 ||:  28%|##7       | 18/65 [03:28<09:02, 11.53s/it]
2022-04-13 18:24:11,623 - INFO - tqdm - accuracy: 0.8503, batch_loss: 0.4812, loss: 0.3815 ||:  29%|##9       | 19/65 [03:39<08:50, 11.53s/it]
2022-04-13 18:24:23,125 - INFO - tqdm - accuracy: 0.8422, batch_loss: 0.6431, loss: 0.3946 ||:  31%|###       | 20/65 [03:51<08:38, 11.52s/it]
2022-04-13 18:24:34,603 - INFO - tqdm - accuracy: 0.8438, batch_loss: 0.2889, loss: 0.3896 ||:  32%|###2      | 21/65 [04:02<08:26, 11.51s/it]
2022-04-13 18:24:46,065 - INFO - tqdm - accuracy: 0.8438, batch_loss: 0.3248, loss: 0.3866 ||:  34%|###3      | 22/65 [04:14<08:14, 11.49s/it]
2022-04-13 18:24:57,548 - INFO - tqdm - accuracy: 0.8465, batch_loss: 0.2222, loss: 0.3795 ||:  35%|###5      | 23/65 [04:25<08:02, 11.49s/it]
2022-04-13 18:25:09,015 - INFO - tqdm - accuracy: 0.8490, batch_loss: 0.2700, loss: 0.3749 ||:  37%|###6      | 24/65 [04:37<07:50, 11.48s/it]
2022-04-13 18:25:20,489 - INFO - tqdm - accuracy: 0.8512, batch_loss: 0.2406, loss: 0.3696 ||:  38%|###8      | 25/65 [04:48<07:39, 11.48s/it]
2022-04-13 18:25:32,007 - INFO - tqdm - accuracy: 0.8546, batch_loss: 0.2151, loss: 0.3636 ||:  40%|####      | 26/65 [05:00<07:28, 11.49s/it]
2022-04-13 18:25:43,544 - INFO - tqdm - accuracy: 0.8553, batch_loss: 0.2575, loss: 0.3597 ||:  42%|####1     | 27/65 [05:11<07:17, 11.51s/it]
2022-04-13 18:25:55,132 - INFO - tqdm - accuracy: 0.8560, batch_loss: 0.2966, loss: 0.3574 ||:  43%|####3     | 28/65 [05:23<07:06, 11.53s/it]
2022-04-13 18:26:06,719 - INFO - tqdm - accuracy: 0.8556, batch_loss: 0.2501, loss: 0.3537 ||:  45%|####4     | 29/65 [05:34<06:55, 11.55s/it]
2022-04-13 18:26:18,322 - INFO - tqdm - accuracy: 0.8542, batch_loss: 0.2637, loss: 0.3507 ||:  46%|####6     | 30/65 [05:46<06:44, 11.56s/it]
2022-04-13 18:26:29,889 - INFO - tqdm - accuracy: 0.8558, batch_loss: 0.3926, loss: 0.3521 ||:  48%|####7     | 31/65 [05:58<06:33, 11.56s/it]
2022-04-13 18:26:41,329 - INFO - tqdm - accuracy: 0.8555, batch_loss: 0.3393, loss: 0.3517 ||:  49%|####9     | 32/65 [06:09<06:20, 11.53s/it]
2022-04-13 18:26:52,893 - INFO - tqdm - accuracy: 0.8551, batch_loss: 0.3120, loss: 0.3505 ||:  51%|#####     | 33/65 [06:21<06:09, 11.54s/it]
2022-04-13 18:27:04,438 - INFO - tqdm - accuracy: 0.8548, batch_loss: 0.3563, loss: 0.3506 ||:  52%|#####2    | 34/65 [06:32<05:57, 11.54s/it]
2022-04-13 18:27:15,983 - INFO - tqdm - accuracy: 0.8545, batch_loss: 0.3324, loss: 0.3501 ||:  54%|#####3    | 35/65 [06:44<05:46, 11.54s/it]
2022-04-13 18:27:27,521 - INFO - tqdm - accuracy: 0.8524, batch_loss: 0.4616, loss: 0.3532 ||:  55%|#####5    | 36/65 [06:55<05:34, 11.54s/it]
2022-04-13 18:27:39,061 - INFO - tqdm - accuracy: 0.8539, batch_loss: 0.2265, loss: 0.3498 ||:  57%|#####6    | 37/65 [07:07<05:23, 11.54s/it]
2022-04-13 18:27:50,620 - INFO - tqdm - accuracy: 0.8528, batch_loss: 0.3388, loss: 0.3495 ||:  58%|#####8    | 38/65 [07:18<05:11, 11.55s/it]
2022-04-13 18:28:02,154 - INFO - tqdm - accuracy: 0.8518, batch_loss: 0.4565, loss: 0.3523 ||:  60%|######    | 39/65 [07:30<05:00, 11.54s/it]
2022-04-13 18:28:13,696 - INFO - tqdm - accuracy: 0.8523, batch_loss: 0.3084, loss: 0.3512 ||:  62%|######1   | 40/65 [07:41<04:48, 11.54s/it]
2022-04-13 18:28:25,247 - INFO - tqdm - accuracy: 0.8514, batch_loss: 0.3086, loss: 0.3501 ||:  63%|######3   | 41/65 [07:53<04:37, 11.54s/it]
2022-04-13 18:28:36,772 - INFO - tqdm - accuracy: 0.8519, batch_loss: 0.3328, loss: 0.3497 ||:  65%|######4   | 42/65 [08:04<04:25, 11.54s/it]
2022-04-13 18:28:48,316 - INFO - tqdm - accuracy: 0.8532, batch_loss: 0.3031, loss: 0.3486 ||:  66%|######6   | 43/65 [08:16<04:13, 11.54s/it]
2022-04-13 18:28:59,854 - INFO - tqdm - accuracy: 0.8509, batch_loss: 0.5393, loss: 0.3530 ||:  68%|######7   | 44/65 [08:27<04:02, 11.54s/it]
2022-04-13 18:29:11,389 - INFO - tqdm - accuracy: 0.8486, batch_loss: 0.3836, loss: 0.3536 ||:  69%|######9   | 45/65 [08:39<03:50, 11.54s/it]
2022-04-13 18:29:22,905 - INFO - tqdm - accuracy: 0.8478, batch_loss: 0.3277, loss: 0.3531 ||:  71%|#######   | 46/65 [08:51<03:39, 11.53s/it]
2022-04-13 18:29:34,440 - INFO - tqdm - accuracy: 0.8491, batch_loss: 0.3242, loss: 0.3525 ||:  72%|#######2  | 47/65 [09:02<03:27, 11.53s/it]
2022-04-13 18:29:45,963 - INFO - tqdm - accuracy: 0.8522, batch_loss: 0.1117, loss: 0.3474 ||:  74%|#######3  | 48/65 [09:14<03:16, 11.53s/it]
2022-04-13 18:29:57,488 - INFO - tqdm - accuracy: 0.8520, batch_loss: 0.3548, loss: 0.3476 ||:  75%|#######5  | 49/65 [09:25<03:04, 11.53s/it]
2022-04-13 18:30:09,004 - INFO - tqdm - accuracy: 0.8512, batch_loss: 0.5074, loss: 0.3508 ||:  77%|#######6  | 50/65 [09:37<02:52, 11.52s/it]
2022-04-13 18:30:20,501 - INFO - tqdm - accuracy: 0.8499, batch_loss: 0.4901, loss: 0.3535 ||:  78%|#######8  | 51/65 [09:48<02:41, 11.52s/it]
2022-04-13 18:30:31,774 - INFO - tqdm - accuracy: 0.8486, batch_loss: 0.4776, loss: 0.3559 ||:  80%|########  | 52/65 [09:59<02:28, 11.44s/it]
2022-04-13 18:30:43,276 - INFO - tqdm - accuracy: 0.8479, batch_loss: 0.3810, loss: 0.3564 ||:  82%|########1 | 53/65 [10:11<02:17, 11.46s/it]
2022-04-13 18:30:54,774 - INFO - tqdm - accuracy: 0.8461, batch_loss: 0.4277, loss: 0.3577 ||:  83%|########3 | 54/65 [10:22<02:06, 11.47s/it]
2022-04-13 18:31:06,298 - INFO - tqdm - accuracy: 0.8449, batch_loss: 0.3646, loss: 0.3578 ||:  85%|########4 | 55/65 [10:34<01:54, 11.49s/it]
2022-04-13 18:31:17,807 - INFO - tqdm - accuracy: 0.8460, batch_loss: 0.3840, loss: 0.3583 ||:  86%|########6 | 56/65 [10:45<01:43, 11.49s/it]
2022-04-13 18:31:29,308 - INFO - tqdm - accuracy: 0.8454, batch_loss: 0.4033, loss: 0.3591 ||:  88%|########7 | 57/65 [10:57<01:31, 11.50s/it]
2022-04-13 18:31:40,816 - INFO - tqdm - accuracy: 0.8470, batch_loss: 0.2479, loss: 0.3572 ||:  89%|########9 | 58/65 [11:08<01:20, 11.50s/it]
2022-04-13 18:31:51,967 - INFO - tqdm - accuracy: 0.8474, batch_loss: 0.3298, loss: 0.3567 ||:  91%|######### | 59/65 [11:20<01:08, 11.40s/it]
2022-04-13 18:32:03,491 - INFO - tqdm - accuracy: 0.8478, batch_loss: 0.2875, loss: 0.3555 ||:  92%|#########2| 60/65 [11:31<00:57, 11.43s/it]
2022-04-13 18:32:14,999 - INFO - tqdm - accuracy: 0.8473, batch_loss: 0.2911, loss: 0.3545 ||:  94%|#########3| 61/65 [11:43<00:45, 11.46s/it]
2022-04-13 18:32:26,520 - INFO - tqdm - accuracy: 0.8477, batch_loss: 0.3420, loss: 0.3543 ||:  95%|#########5| 62/65 [11:54<00:34, 11.48s/it]
2022-04-13 18:32:38,024 - INFO - tqdm - accuracy: 0.8471, batch_loss: 0.3456, loss: 0.3541 ||:  97%|#########6| 63/65 [12:06<00:22, 11.48s/it]
2022-04-13 18:32:49,535 - INFO - tqdm - accuracy: 0.8476, batch_loss: 0.2864, loss: 0.3531 ||:  98%|#########8| 64/65 [12:17<00:11, 11.49s/it]
2022-04-13 18:32:54,646 - INFO - tqdm - accuracy: 0.8476, batch_loss: 0.3575, loss: 0.3532 ||: 100%|##########| 65/65 [12:22<00:00,  9.58s/it]
2022-04-13 18:32:54,646 - INFO - tqdm - accuracy: 0.8476, batch_loss: 0.3575, loss: 0.3532 ||: 100%|##########| 65/65 [12:22<00:00, 11.43s/it]
2022-04-13 18:32:57,516 - INFO - allennlp.training.trainer - Validating
2022-04-13 18:32:57,517 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-13 18:33:07,605 - INFO - tqdm - accuracy: 0.6250, batch_loss: 0.0200, loss: 1.1337 ||:  35%|###4      | 40/115 [00:10<00:19,  3.94it/s]
2022-04-13 18:33:17,736 - INFO - tqdm - accuracy: 0.6687, batch_loss: 0.4122, loss: 0.9486 ||:  70%|######9   | 80/115 [00:20<00:08,  3.91it/s]
2022-04-13 18:33:26,605 - INFO - tqdm - accuracy: 0.6594, batch_loss: 0.0551, loss: 0.9231 ||: 100%|##########| 115/115 [00:29<00:00,  3.96it/s]
2022-04-13 18:33:26,605 - INFO - tqdm - accuracy: 0.6594, batch_loss: 0.0551, loss: 0.9231 ||: 100%|##########| 115/115 [00:29<00:00,  3.95it/s]
2022-04-13 18:33:26,605 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-13 18:33:26,606 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.848  |     0.659
2022-04-13 18:33:26,606 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-13 18:33:26,606 - INFO - allennlp.training.tensorboard_writer - loss               |     0.353  |     0.923
2022-04-13 18:33:26,606 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5802.840  |       N/A
2022-04-13 18:33:32,974 - INFO - allennlp.training.trainer - Epoch duration: 0:13:01.101587
2022-04-13 18:33:32,975 - INFO - allennlp.training.trainer - Estimated training time remaining: 2:23:22
2022-04-13 18:33:32,975 - INFO - allennlp.training.trainer - Epoch 4/14
2022-04-13 18:33:32,975 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-13 18:33:32,975 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-13 18:33:32,977 - INFO - allennlp.training.trainer - Training
2022-04-13 18:33:32,977 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-13 18:33:44,373 - INFO - tqdm - accuracy: 0.9062, batch_loss: 0.2436, loss: 0.2436 ||:   2%|1         | 1/65 [00:11<12:09, 11.40s/it]
2022-04-13 18:33:56,105 - INFO - tqdm - accuracy: 0.9219, batch_loss: 0.1425, loss: 0.1930 ||:   3%|3         | 2/65 [00:23<12:10, 11.59s/it]
2022-04-13 18:34:07,966 - INFO - tqdm - accuracy: 0.9271, batch_loss: 0.2749, loss: 0.2203 ||:   5%|4         | 3/65 [00:34<12:06, 11.72s/it]
2022-04-13 18:34:19,490 - INFO - tqdm - accuracy: 0.9219, batch_loss: 0.2415, loss: 0.2256 ||:   6%|6         | 4/65 [00:46<11:50, 11.64s/it]
2022-04-13 18:34:30,963 - INFO - tqdm - accuracy: 0.9250, batch_loss: 0.1944, loss: 0.2194 ||:   8%|7         | 5/65 [00:57<11:34, 11.58s/it]
2022-04-13 18:34:42,414 - INFO - tqdm - accuracy: 0.9375, batch_loss: 0.1205, loss: 0.2029 ||:   9%|9         | 6/65 [01:09<11:20, 11.54s/it]
2022-04-13 18:34:53,921 - INFO - tqdm - accuracy: 0.9330, batch_loss: 0.1761, loss: 0.1991 ||:  11%|#         | 7/65 [01:20<11:08, 11.53s/it]
2022-04-13 18:35:05,435 - INFO - tqdm - accuracy: 0.9375, batch_loss: 0.1451, loss: 0.1923 ||:  12%|#2        | 8/65 [01:32<10:56, 11.52s/it]
2022-04-13 18:35:16,929 - INFO - tqdm - accuracy: 0.9306, batch_loss: 0.1835, loss: 0.1914 ||:  14%|#3        | 9/65 [01:43<10:44, 11.51s/it]
2022-04-13 18:35:28,385 - INFO - tqdm - accuracy: 0.9313, batch_loss: 0.1565, loss: 0.1879 ||:  15%|#5        | 10/65 [01:55<10:32, 11.50s/it]
2022-04-13 18:35:39,871 - INFO - tqdm - accuracy: 0.9290, batch_loss: 0.2008, loss: 0.1890 ||:  17%|#6        | 11/65 [02:06<10:20, 11.49s/it]
2022-04-13 18:35:51,341 - INFO - tqdm - accuracy: 0.9323, batch_loss: 0.0935, loss: 0.1811 ||:  18%|#8        | 12/65 [02:18<10:08, 11.49s/it]
2022-04-13 18:36:02,886 - INFO - tqdm - accuracy: 0.9327, batch_loss: 0.2286, loss: 0.1847 ||:  20%|##        | 13/65 [02:29<09:58, 11.50s/it]
2022-04-13 18:36:14,490 - INFO - tqdm - accuracy: 0.9330, batch_loss: 0.0723, loss: 0.1767 ||:  22%|##1       | 14/65 [02:41<09:48, 11.53s/it]
2022-04-13 18:36:26,062 - INFO - tqdm - accuracy: 0.9333, batch_loss: 0.1896, loss: 0.1776 ||:  23%|##3       | 15/65 [02:53<09:37, 11.55s/it]
2022-04-13 18:36:37,596 - INFO - tqdm - accuracy: 0.9336, batch_loss: 0.1277, loss: 0.1745 ||:  25%|##4       | 16/65 [03:04<09:25, 11.54s/it]
2022-04-13 18:36:49,120 - INFO - tqdm - accuracy: 0.9357, batch_loss: 0.1143, loss: 0.1709 ||:  26%|##6       | 17/65 [03:16<09:13, 11.54s/it]
2022-04-13 18:37:00,635 - INFO - tqdm - accuracy: 0.9375, batch_loss: 0.1505, loss: 0.1698 ||:  28%|##7       | 18/65 [03:27<09:01, 11.53s/it]
2022-04-13 18:37:12,157 - INFO - tqdm - accuracy: 0.9391, batch_loss: 0.0996, loss: 0.1661 ||:  29%|##9       | 19/65 [03:39<08:50, 11.53s/it]
2022-04-13 18:37:23,673 - INFO - tqdm - accuracy: 0.9359, batch_loss: 0.2670, loss: 0.1711 ||:  31%|###       | 20/65 [03:50<08:38, 11.52s/it]
2022-04-13 18:37:35,159 - INFO - tqdm - accuracy: 0.9390, batch_loss: 0.0725, loss: 0.1664 ||:  32%|###2      | 21/65 [04:02<08:26, 11.51s/it]
2022-04-13 18:37:46,652 - INFO - tqdm - accuracy: 0.9375, batch_loss: 0.1611, loss: 0.1662 ||:  34%|###3      | 22/65 [04:13<08:14, 11.51s/it]
2022-04-13 18:37:58,135 - INFO - tqdm - accuracy: 0.9321, batch_loss: 0.3136, loss: 0.1726 ||:  35%|###5      | 23/65 [04:25<08:02, 11.50s/it]
2022-04-13 18:38:09,629 - INFO - tqdm - accuracy: 0.9323, batch_loss: 0.1307, loss: 0.1709 ||:  37%|###6      | 24/65 [04:36<07:51, 11.50s/it]
2022-04-13 18:38:21,112 - INFO - tqdm - accuracy: 0.9325, batch_loss: 0.1849, loss: 0.1714 ||:  38%|###8      | 25/65 [04:48<07:39, 11.49s/it]
2022-04-13 18:38:32,615 - INFO - tqdm - accuracy: 0.9339, batch_loss: 0.0904, loss: 0.1683 ||:  40%|####      | 26/65 [04:59<07:28, 11.50s/it]
2022-04-13 18:38:44,116 - INFO - tqdm - accuracy: 0.9363, batch_loss: 0.0436, loss: 0.1637 ||:  42%|####1     | 27/65 [05:11<07:16, 11.50s/it]
2022-04-13 18:38:55,620 - INFO - tqdm - accuracy: 0.9364, batch_loss: 0.3436, loss: 0.1701 ||:  43%|####3     | 28/65 [05:22<07:05, 11.50s/it]
2022-04-13 18:39:07,108 - INFO - tqdm - accuracy: 0.9386, batch_loss: 0.0586, loss: 0.1663 ||:  45%|####4     | 29/65 [05:34<06:53, 11.50s/it]
2022-04-13 18:39:18,492 - INFO - tqdm - accuracy: 0.9385, batch_loss: 0.1396, loss: 0.1654 ||:  46%|####6     | 30/65 [05:45<06:41, 11.46s/it]
2022-04-13 18:39:29,997 - INFO - tqdm - accuracy: 0.9405, batch_loss: 0.0658, loss: 0.1622 ||:  48%|####7     | 31/65 [05:57<06:30, 11.48s/it]
2022-04-13 18:39:41,523 - INFO - tqdm - accuracy: 0.9414, batch_loss: 0.0851, loss: 0.1597 ||:  49%|####9     | 32/65 [06:08<06:19, 11.49s/it]
2022-04-13 18:39:53,048 - INFO - tqdm - accuracy: 0.9413, batch_loss: 0.1793, loss: 0.1603 ||:  51%|#####     | 33/65 [06:20<06:08, 11.50s/it]
2022-04-13 18:40:04,565 - INFO - tqdm - accuracy: 0.9412, batch_loss: 0.2683, loss: 0.1635 ||:  52%|#####2    | 34/65 [06:31<05:56, 11.51s/it]
2022-04-13 18:40:16,095 - INFO - tqdm - accuracy: 0.9411, batch_loss: 0.2361, loss: 0.1656 ||:  54%|#####3    | 35/65 [06:43<05:45, 11.51s/it]
2022-04-13 18:40:27,630 - INFO - tqdm - accuracy: 0.9410, batch_loss: 0.1334, loss: 0.1647 ||:  55%|#####5    | 36/65 [06:54<05:34, 11.52s/it]
2022-04-13 18:40:38,809 - INFO - tqdm - accuracy: 0.9408, batch_loss: 0.1559, loss: 0.1645 ||:  57%|#####6    | 37/65 [07:05<05:19, 11.42s/it]
2022-04-13 18:40:50,333 - INFO - tqdm - accuracy: 0.9407, batch_loss: 0.0906, loss: 0.1625 ||:  58%|#####8    | 38/65 [07:17<05:09, 11.45s/it]
2022-04-13 18:41:01,898 - INFO - tqdm - accuracy: 0.9415, batch_loss: 0.1540, loss: 0.1623 ||:  60%|######    | 39/65 [07:28<04:58, 11.48s/it]
2022-04-13 18:41:13,814 - INFO - tqdm - accuracy: 0.9398, batch_loss: 0.2599, loss: 0.1647 ||:  62%|######1   | 40/65 [07:40<04:50, 11.61s/it]
2022-04-13 18:41:25,314 - INFO - tqdm - accuracy: 0.9382, batch_loss: 0.1596, loss: 0.1646 ||:  63%|######3   | 41/65 [07:52<04:37, 11.58s/it]
2022-04-13 18:41:36,863 - INFO - tqdm - accuracy: 0.9375, batch_loss: 0.2116, loss: 0.1657 ||:  65%|######4   | 42/65 [08:03<04:26, 11.57s/it]
2022-04-13 18:41:48,418 - INFO - tqdm - accuracy: 0.9389, batch_loss: 0.0800, loss: 0.1637 ||:  66%|######6   | 43/65 [08:15<04:14, 11.57s/it]
2022-04-13 18:41:59,956 - INFO - tqdm - accuracy: 0.9367, batch_loss: 0.3710, loss: 0.1684 ||:  68%|######7   | 44/65 [08:26<04:02, 11.56s/it]
2022-04-13 18:42:11,375 - INFO - tqdm - accuracy: 0.9375, batch_loss: 0.1206, loss: 0.1674 ||:  69%|######9   | 45/65 [08:38<03:50, 11.52s/it]
2022-04-13 18:42:22,927 - INFO - tqdm - accuracy: 0.9354, batch_loss: 0.4048, loss: 0.1725 ||:  71%|#######   | 46/65 [08:49<03:39, 11.53s/it]
2022-04-13 18:42:34,478 - INFO - tqdm - accuracy: 0.9361, batch_loss: 0.1067, loss: 0.1711 ||:  72%|#######2  | 47/65 [09:01<03:27, 11.53s/it]
2022-04-13 18:42:45,995 - INFO - tqdm - accuracy: 0.9342, batch_loss: 0.2358, loss: 0.1725 ||:  74%|#######3  | 48/65 [09:13<03:15, 11.53s/it]
2022-04-13 18:42:57,501 - INFO - tqdm - accuracy: 0.9355, batch_loss: 0.0709, loss: 0.1704 ||:  75%|#######5  | 49/65 [09:24<03:04, 11.52s/it]
2022-04-13 18:43:08,987 - INFO - tqdm - accuracy: 0.9356, batch_loss: 0.1551, loss: 0.1701 ||:  77%|#######6  | 50/65 [09:36<02:52, 11.51s/it]
2022-04-13 18:43:20,455 - INFO - tqdm - accuracy: 0.9362, batch_loss: 0.0964, loss: 0.1687 ||:  78%|#######8  | 51/65 [09:47<02:40, 11.50s/it]
2022-04-13 18:43:31,911 - INFO - tqdm - accuracy: 0.9363, batch_loss: 0.1246, loss: 0.1678 ||:  80%|########  | 52/65 [09:58<02:29, 11.49s/it]
2022-04-13 18:43:43,355 - INFO - tqdm - accuracy: 0.9357, batch_loss: 0.1831, loss: 0.1681 ||:  82%|########1 | 53/65 [10:10<02:17, 11.47s/it]
2022-04-13 18:43:54,802 - INFO - tqdm - accuracy: 0.9334, batch_loss: 0.3829, loss: 0.1721 ||:  83%|########3 | 54/65 [10:21<02:06, 11.47s/it]
2022-04-13 18:44:06,245 - INFO - tqdm - accuracy: 0.9335, batch_loss: 0.1858, loss: 0.1723 ||:  85%|########4 | 55/65 [10:33<01:54, 11.46s/it]
2022-04-13 18:44:17,690 - INFO - tqdm - accuracy: 0.9336, batch_loss: 0.1476, loss: 0.1719 ||:  86%|########6 | 56/65 [10:44<01:43, 11.45s/it]
2022-04-13 18:44:29,110 - INFO - tqdm - accuracy: 0.9325, batch_loss: 0.3269, loss: 0.1746 ||:  88%|########7 | 57/65 [10:56<01:31, 11.44s/it]
2022-04-13 18:44:40,540 - INFO - tqdm - accuracy: 0.9326, batch_loss: 0.1923, loss: 0.1749 ||:  89%|########9 | 58/65 [11:07<01:20, 11.44s/it]
2022-04-13 18:44:51,941 - INFO - tqdm - accuracy: 0.9332, batch_loss: 0.1096, loss: 0.1738 ||:  91%|######### | 59/65 [11:18<01:08, 11.43s/it]
2022-04-13 18:45:03,353 - INFO - tqdm - accuracy: 0.9323, batch_loss: 0.2322, loss: 0.1748 ||:  92%|#########2| 60/65 [11:30<00:57, 11.42s/it]
2022-04-13 18:45:14,742 - INFO - tqdm - accuracy: 0.9323, batch_loss: 0.1590, loss: 0.1745 ||:  94%|#########3| 61/65 [11:41<00:45, 11.41s/it]
2022-04-13 18:45:25,896 - INFO - tqdm - accuracy: 0.9329, batch_loss: 0.1295, loss: 0.1738 ||:  95%|#########5| 62/65 [11:52<00:34, 11.34s/it]
2022-04-13 18:45:37,273 - INFO - tqdm - accuracy: 0.9335, batch_loss: 0.0998, loss: 0.1726 ||:  97%|#########6| 63/65 [12:04<00:22, 11.35s/it]
2022-04-13 18:45:48,647 - INFO - tqdm - accuracy: 0.9326, batch_loss: 0.2561, loss: 0.1739 ||:  98%|#########8| 64/65 [12:15<00:11, 11.36s/it]
2022-04-13 18:45:53,703 - INFO - tqdm - accuracy: 0.9326, batch_loss: 0.2150, loss: 0.1746 ||: 100%|##########| 65/65 [12:20<00:00,  9.47s/it]
2022-04-13 18:45:53,703 - INFO - tqdm - accuracy: 0.9326, batch_loss: 0.2150, loss: 0.1746 ||: 100%|##########| 65/65 [12:20<00:00, 11.40s/it]
2022-04-13 18:45:56,535 - INFO - allennlp.training.trainer - Validating
2022-04-13 18:45:56,536 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-13 18:46:06,736 - INFO - tqdm - accuracy: 0.6098, batch_loss: 0.0097, loss: 1.3520 ||:  36%|###5      | 41/115 [00:10<00:18,  4.01it/s]
2022-04-13 18:46:16,941 - INFO - tqdm - accuracy: 0.6341, batch_loss: 0.5810, loss: 1.3216 ||:  71%|#######1  | 82/115 [00:20<00:08,  4.03it/s]
2022-04-13 18:46:24,961 - INFO - tqdm - accuracy: 0.6201, batch_loss: 0.0026, loss: 1.3423 ||: 100%|##########| 115/115 [00:28<00:00,  4.03it/s]
2022-04-13 18:46:24,961 - INFO - tqdm - accuracy: 0.6201, batch_loss: 0.0026, loss: 1.3423 ||: 100%|##########| 115/115 [00:28<00:00,  4.05it/s]
2022-04-13 18:46:24,961 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-13 18:46:24,962 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.933  |     0.620
2022-04-13 18:46:24,963 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-13 18:46:24,963 - INFO - allennlp.training.tensorboard_writer - loss               |     0.175  |     1.342
2022-04-13 18:46:24,964 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5802.840  |       N/A
2022-04-13 18:46:31,330 - INFO - allennlp.training.trainer - Epoch duration: 0:12:58.355019
2022-04-13 18:46:31,330 - INFO - allennlp.training.trainer - Estimated training time remaining: 2:10:12
2022-04-13 18:46:31,330 - INFO - allennlp.training.trainer - Epoch 5/14
2022-04-13 18:46:31,330 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-13 18:46:31,331 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-13 18:46:31,332 - INFO - allennlp.training.trainer - Training
2022-04-13 18:46:31,332 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-13 18:46:42,304 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.1115, loss: 0.1115 ||:   2%|1         | 1/65 [00:10<11:42, 10.97s/it]
2022-04-13 18:46:53,443 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0799, loss: 0.0957 ||:   3%|3         | 2/65 [00:22<11:37, 11.07s/it]
2022-04-13 18:47:04,596 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.1464, loss: 0.1126 ||:   5%|4         | 3/65 [00:33<11:28, 11.11s/it]
2022-04-13 18:47:15,844 - INFO - tqdm - accuracy: 0.9531, batch_loss: 0.1355, loss: 0.1183 ||:   6%|6         | 4/65 [00:44<11:20, 11.16s/it]
2022-04-13 18:47:27,060 - INFO - tqdm - accuracy: 0.9563, batch_loss: 0.1011, loss: 0.1149 ||:   8%|7         | 5/65 [00:55<11:10, 11.18s/it]
2022-04-13 18:47:38,271 - INFO - tqdm - accuracy: 0.9583, batch_loss: 0.1010, loss: 0.1126 ||:   9%|9         | 6/65 [01:06<11:00, 11.19s/it]
2022-04-13 18:47:49,387 - INFO - tqdm - accuracy: 0.9598, batch_loss: 0.0751, loss: 0.1072 ||:  11%|#         | 7/65 [01:18<10:47, 11.17s/it]
2022-04-13 18:48:00,662 - INFO - tqdm - accuracy: 0.9609, batch_loss: 0.0875, loss: 0.1048 ||:  12%|#2        | 8/65 [01:29<10:38, 11.20s/it]
2022-04-13 18:48:11,954 - INFO - tqdm - accuracy: 0.9653, batch_loss: 0.0298, loss: 0.0964 ||:  14%|#3        | 9/65 [01:40<10:28, 11.23s/it]
2022-04-13 18:48:23,287 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.0394, loss: 0.0907 ||:  15%|#5        | 10/65 [01:51<10:19, 11.26s/it]
2022-04-13 18:48:34,626 - INFO - tqdm - accuracy: 0.9716, batch_loss: 0.0467, loss: 0.0867 ||:  17%|#6        | 11/65 [02:03<10:09, 11.29s/it]
2022-04-13 18:48:45,961 - INFO - tqdm - accuracy: 0.9740, batch_loss: 0.0452, loss: 0.0833 ||:  18%|#8        | 12/65 [02:14<09:58, 11.30s/it]
2022-04-13 18:48:57,334 - INFO - tqdm - accuracy: 0.9712, batch_loss: 0.1311, loss: 0.0869 ||:  20%|##        | 13/65 [02:26<09:48, 11.32s/it]
2022-04-13 18:49:08,710 - INFO - tqdm - accuracy: 0.9643, batch_loss: 0.2798, loss: 0.1007 ||:  22%|##1       | 14/65 [02:37<09:38, 11.34s/it]
2022-04-13 18:49:20,113 - INFO - tqdm - accuracy: 0.9667, batch_loss: 0.0314, loss: 0.0961 ||:  23%|##3       | 15/65 [02:48<09:27, 11.36s/it]
2022-04-13 18:49:31,168 - INFO - tqdm - accuracy: 0.9648, batch_loss: 0.0722, loss: 0.0946 ||:  25%|##4       | 16/65 [02:59<09:12, 11.27s/it]
2022-04-13 18:49:42,559 - INFO - tqdm - accuracy: 0.9669, batch_loss: 0.0423, loss: 0.0915 ||:  26%|##6       | 17/65 [03:11<09:02, 11.30s/it]
2022-04-13 18:49:53,951 - INFO - tqdm - accuracy: 0.9670, batch_loss: 0.1272, loss: 0.0935 ||:  28%|##7       | 18/65 [03:22<08:52, 11.33s/it]
2022-04-13 18:50:05,347 - INFO - tqdm - accuracy: 0.9654, batch_loss: 0.1756, loss: 0.0978 ||:  29%|##9       | 19/65 [03:34<08:42, 11.35s/it]
2022-04-13 18:50:16,714 - INFO - tqdm - accuracy: 0.9671, batch_loss: 0.0465, loss: 0.0953 ||:  31%|###       | 20/65 [03:45<08:30, 11.36s/it]
2022-04-13 18:50:28,103 - INFO - tqdm - accuracy: 0.9642, batch_loss: 0.2338, loss: 0.1019 ||:  32%|###2      | 21/65 [03:56<08:20, 11.37s/it]
2022-04-13 18:50:39,490 - INFO - tqdm - accuracy: 0.9630, batch_loss: 0.1182, loss: 0.1026 ||:  34%|###3      | 22/65 [04:08<08:08, 11.37s/it]
2022-04-13 18:50:50,857 - INFO - tqdm - accuracy: 0.9605, batch_loss: 0.1145, loss: 0.1031 ||:  35%|###5      | 23/65 [04:19<07:57, 11.37s/it]
2022-04-13 18:51:02,278 - INFO - tqdm - accuracy: 0.9622, batch_loss: 0.0811, loss: 0.1022 ||:  37%|###6      | 24/65 [04:30<07:46, 11.39s/it]
2022-04-13 18:51:13,662 - INFO - tqdm - accuracy: 0.9612, batch_loss: 0.1987, loss: 0.1061 ||:  38%|###8      | 25/65 [04:42<07:35, 11.39s/it]
2022-04-13 18:51:25,013 - INFO - tqdm - accuracy: 0.9603, batch_loss: 0.0968, loss: 0.1057 ||:  40%|####      | 26/65 [04:53<07:23, 11.37s/it]
2022-04-13 18:51:36,372 - INFO - tqdm - accuracy: 0.9606, batch_loss: 0.0948, loss: 0.1053 ||:  42%|####1     | 27/65 [05:05<07:12, 11.37s/it]
2022-04-13 18:51:47,735 - INFO - tqdm - accuracy: 0.9609, batch_loss: 0.0700, loss: 0.1040 ||:  43%|####3     | 28/65 [05:16<07:00, 11.37s/it]
2022-04-13 18:51:59,094 - INFO - tqdm - accuracy: 0.9622, batch_loss: 0.0313, loss: 0.1015 ||:  45%|####4     | 29/65 [05:27<06:49, 11.37s/it]
2022-04-13 18:52:10,447 - INFO - tqdm - accuracy: 0.9614, batch_loss: 0.2278, loss: 0.1057 ||:  46%|####6     | 30/65 [05:39<06:37, 11.36s/it]
2022-04-13 18:52:21,806 - INFO - tqdm - accuracy: 0.9627, batch_loss: 0.0241, loss: 0.1031 ||:  48%|####7     | 31/65 [05:50<06:26, 11.36s/it]
2022-04-13 18:52:33,160 - INFO - tqdm - accuracy: 0.9638, batch_loss: 0.0192, loss: 0.1005 ||:  49%|####9     | 32/65 [06:01<06:14, 11.36s/it]
2022-04-13 18:52:44,497 - INFO - tqdm - accuracy: 0.9649, batch_loss: 0.0205, loss: 0.0981 ||:  51%|#####     | 33/65 [06:13<06:03, 11.35s/it]
2022-04-13 18:52:55,861 - INFO - tqdm - accuracy: 0.9641, batch_loss: 0.0717, loss: 0.0973 ||:  52%|#####2    | 34/65 [06:24<05:52, 11.36s/it]
2022-04-13 18:53:07,237 - INFO - tqdm - accuracy: 0.9651, batch_loss: 0.0229, loss: 0.0952 ||:  54%|#####3    | 35/65 [06:35<05:40, 11.36s/it]
2022-04-13 18:53:18,632 - INFO - tqdm - accuracy: 0.9661, batch_loss: 0.0323, loss: 0.0934 ||:  55%|#####5    | 36/65 [06:47<05:29, 11.37s/it]
2022-04-13 18:53:30,041 - INFO - tqdm - accuracy: 0.9670, batch_loss: 0.0288, loss: 0.0917 ||:  57%|#####6    | 37/65 [06:58<05:18, 11.38s/it]
2022-04-13 18:53:41,491 - INFO - tqdm - accuracy: 0.9663, batch_loss: 0.1514, loss: 0.0932 ||:  58%|#####8    | 38/65 [07:10<05:07, 11.40s/it]
2022-04-13 18:53:52,916 - INFO - tqdm - accuracy: 0.9663, batch_loss: 0.0505, loss: 0.0921 ||:  60%|######    | 39/65 [07:21<04:56, 11.41s/it]
2022-04-13 18:54:04,365 - INFO - tqdm - accuracy: 0.9633, batch_loss: 0.3706, loss: 0.0991 ||:  62%|######1   | 40/65 [07:33<04:45, 11.42s/it]
2022-04-13 18:54:15,806 - INFO - tqdm - accuracy: 0.9641, batch_loss: 0.0078, loss: 0.0969 ||:  63%|######3   | 41/65 [07:44<04:34, 11.43s/it]
2022-04-13 18:54:27,232 - INFO - tqdm - accuracy: 0.9635, batch_loss: 0.1065, loss: 0.0971 ||:  65%|######4   | 42/65 [07:55<04:22, 11.43s/it]
2022-04-13 18:54:38,658 - INFO - tqdm - accuracy: 0.9636, batch_loss: 0.1369, loss: 0.0980 ||:  66%|######6   | 43/65 [08:07<04:11, 11.43s/it]
2022-04-13 18:54:50,070 - INFO - tqdm - accuracy: 0.9623, batch_loss: 0.2912, loss: 0.1024 ||:  68%|######7   | 44/65 [08:18<03:59, 11.42s/it]
2022-04-13 18:55:01,210 - INFO - tqdm - accuracy: 0.9625, batch_loss: 0.0444, loss: 0.1011 ||:  69%|######9   | 45/65 [08:29<03:46, 11.34s/it]
2022-04-13 18:55:12,617 - INFO - tqdm - accuracy: 0.9619, batch_loss: 0.1849, loss: 0.1030 ||:  71%|#######   | 46/65 [08:41<03:35, 11.36s/it]
2022-04-13 18:55:24,000 - INFO - tqdm - accuracy: 0.9627, batch_loss: 0.0413, loss: 0.1016 ||:  72%|#######2  | 47/65 [08:52<03:24, 11.37s/it]
2022-04-13 18:55:35,380 - INFO - tqdm - accuracy: 0.9622, batch_loss: 0.1089, loss: 0.1018 ||:  74%|#######3  | 48/65 [09:04<03:13, 11.37s/it]
2022-04-13 18:55:46,756 - INFO - tqdm - accuracy: 0.9630, batch_loss: 0.0229, loss: 0.1002 ||:  75%|#######5  | 49/65 [09:15<03:01, 11.37s/it]
2022-04-13 18:55:58,114 - INFO - tqdm - accuracy: 0.9637, batch_loss: 0.0308, loss: 0.0988 ||:  77%|#######6  | 50/65 [09:26<02:50, 11.37s/it]
2022-04-13 18:56:09,470 - INFO - tqdm - accuracy: 0.9638, batch_loss: 0.0467, loss: 0.0978 ||:  78%|#######8  | 51/65 [09:38<02:39, 11.36s/it]
2022-04-13 18:56:20,823 - INFO - tqdm - accuracy: 0.9639, batch_loss: 0.1033, loss: 0.0979 ||:  80%|########  | 52/65 [09:49<02:27, 11.36s/it]
2022-04-13 18:56:32,170 - INFO - tqdm - accuracy: 0.9634, batch_loss: 0.1132, loss: 0.0982 ||:  82%|########1 | 53/65 [10:00<02:16, 11.36s/it]
2022-04-13 18:56:43,506 - INFO - tqdm - accuracy: 0.9641, batch_loss: 0.0409, loss: 0.0971 ||:  83%|########3 | 54/65 [10:12<02:04, 11.35s/it]
2022-04-13 18:56:54,851 - INFO - tqdm - accuracy: 0.9636, batch_loss: 0.1085, loss: 0.0973 ||:  85%|########4 | 55/65 [10:23<01:53, 11.35s/it]
2022-04-13 18:57:06,195 - INFO - tqdm - accuracy: 0.9631, batch_loss: 0.1047, loss: 0.0975 ||:  86%|########6 | 56/65 [10:34<01:42, 11.35s/it]
2022-04-13 18:57:17,515 - INFO - tqdm - accuracy: 0.9632, batch_loss: 0.0923, loss: 0.0974 ||:  88%|########7 | 57/65 [10:46<01:30, 11.34s/it]
2022-04-13 18:57:28,851 - INFO - tqdm - accuracy: 0.9628, batch_loss: 0.2533, loss: 0.1000 ||:  89%|########9 | 58/65 [10:57<01:19, 11.34s/it]
2022-04-13 18:57:40,057 - INFO - tqdm - accuracy: 0.9624, batch_loss: 0.0878, loss: 0.0998 ||:  91%|######### | 59/65 [11:08<01:07, 11.30s/it]
2022-04-13 18:57:51,383 - INFO - tqdm - accuracy: 0.9620, batch_loss: 0.1142, loss: 0.1001 ||:  92%|#########2| 60/65 [11:20<00:56, 11.31s/it]
2022-04-13 18:58:02,731 - INFO - tqdm - accuracy: 0.9610, batch_loss: 0.2060, loss: 0.1018 ||:  94%|#########3| 61/65 [11:31<00:45, 11.32s/it]
2022-04-13 18:58:14,072 - INFO - tqdm - accuracy: 0.9602, batch_loss: 0.2304, loss: 0.1039 ||:  95%|#########5| 62/65 [11:42<00:33, 11.33s/it]
2022-04-13 18:58:25,411 - INFO - tqdm - accuracy: 0.9603, batch_loss: 0.1085, loss: 0.1040 ||:  97%|#########6| 63/65 [11:54<00:22, 11.33s/it]
2022-04-13 18:58:36,756 - INFO - tqdm - accuracy: 0.9604, batch_loss: 0.0933, loss: 0.1038 ||:  98%|#########8| 64/65 [12:05<00:11, 11.33s/it]
2022-04-13 18:58:41,797 - INFO - tqdm - accuracy: 0.9602, batch_loss: 0.0833, loss: 0.1035 ||: 100%|##########| 65/65 [12:10<00:00,  9.45s/it]
2022-04-13 18:58:41,797 - INFO - tqdm - accuracy: 0.9602, batch_loss: 0.0833, loss: 0.1035 ||: 100%|##########| 65/65 [12:10<00:00, 11.24s/it]
2022-04-13 18:58:44,627 - INFO - allennlp.training.trainer - Validating
2022-04-13 18:58:44,629 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-13 18:58:54,824 - INFO - tqdm - accuracy: 0.6220, batch_loss: 0.0823, loss: 1.6936 ||:  36%|###5      | 41/115 [00:10<00:18,  4.03it/s]
2022-04-13 18:59:05,042 - INFO - tqdm - accuracy: 0.6545, batch_loss: 3.6172, loss: 1.4688 ||:  72%|#######2  | 83/115 [00:20<00:07,  4.09it/s]
2022-04-13 18:59:13,005 - INFO - tqdm - accuracy: 0.6157, batch_loss: 0.0183, loss: 1.5991 ||: 100%|##########| 115/115 [00:28<00:00,  4.02it/s]
2022-04-13 18:59:13,005 - INFO - tqdm - accuracy: 0.6157, batch_loss: 0.0183, loss: 1.5991 ||: 100%|##########| 115/115 [00:28<00:00,  4.05it/s]
2022-04-13 18:59:13,006 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-13 18:59:13,006 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.960  |     0.616
2022-04-13 18:59:13,007 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-13 18:59:13,008 - INFO - allennlp.training.tensorboard_writer - loss               |     0.103  |     1.599
2022-04-13 18:59:13,008 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5802.840  |       N/A
2022-04-13 18:59:18,956 - INFO - allennlp.training.trainer - Epoch duration: 0:12:47.626116
2022-04-13 18:59:18,957 - INFO - allennlp.training.trainer - Estimated training time remaining: 1:56:51
2022-04-13 18:59:18,957 - INFO - allennlp.training.trainer - Epoch 6/14
2022-04-13 18:59:18,957 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-13 18:59:18,957 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-13 18:59:18,958 - INFO - allennlp.training.trainer - Training
2022-04-13 18:59:18,959 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-13 18:59:30,078 - INFO - tqdm - accuracy: 0.9375, batch_loss: 0.1238, loss: 0.1238 ||:   2%|1         | 1/65 [00:11<11:51, 11.12s/it]
2022-04-13 18:59:41,232 - INFO - tqdm - accuracy: 0.9531, batch_loss: 0.0632, loss: 0.0935 ||:   3%|3         | 2/65 [00:22<11:41, 11.14s/it]
2022-04-13 18:59:52,409 - INFO - tqdm - accuracy: 0.9583, batch_loss: 0.0914, loss: 0.0928 ||:   5%|4         | 3/65 [00:33<11:31, 11.16s/it]
2022-04-13 19:00:03,629 - INFO - tqdm - accuracy: 0.9531, batch_loss: 0.2423, loss: 0.1302 ||:   6%|6         | 4/65 [00:44<11:22, 11.18s/it]
2022-04-13 19:00:14,850 - INFO - tqdm - accuracy: 0.9563, batch_loss: 0.1038, loss: 0.1249 ||:   8%|7         | 5/65 [00:55<11:11, 11.20s/it]
2022-04-13 19:00:26,070 - INFO - tqdm - accuracy: 0.9635, batch_loss: 0.0658, loss: 0.1151 ||:   9%|9         | 6/65 [01:07<11:01, 11.20s/it]
2022-04-13 19:00:37,330 - INFO - tqdm - accuracy: 0.9643, batch_loss: 0.1908, loss: 0.1259 ||:  11%|#         | 7/65 [01:18<10:50, 11.22s/it]
2022-04-13 19:00:48,603 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.0316, loss: 0.1141 ||:  12%|#2        | 8/65 [01:29<10:40, 11.24s/it]
2022-04-13 19:00:59,916 - INFO - tqdm - accuracy: 0.9722, batch_loss: 0.0295, loss: 0.1047 ||:  14%|#3        | 9/65 [01:40<10:30, 11.26s/it]
2022-04-13 19:01:11,745 - INFO - tqdm - accuracy: 0.9625, batch_loss: 0.4600, loss: 0.1402 ||:  15%|#5        | 10/65 [01:52<10:29, 11.44s/it]
2022-04-13 19:01:23,045 - INFO - tqdm - accuracy: 0.9631, batch_loss: 0.0968, loss: 0.1363 ||:  17%|#6        | 11/65 [02:04<10:15, 11.39s/it]
2022-04-13 19:01:34,396 - INFO - tqdm - accuracy: 0.9609, batch_loss: 0.0826, loss: 0.1318 ||:  18%|#8        | 12/65 [02:15<10:03, 11.38s/it]
2022-04-13 19:01:45,736 - INFO - tqdm - accuracy: 0.9615, batch_loss: 0.1071, loss: 0.1299 ||:  20%|##        | 13/65 [02:26<09:51, 11.37s/it]
2022-04-13 19:01:57,116 - INFO - tqdm - accuracy: 0.9621, batch_loss: 0.1018, loss: 0.1279 ||:  22%|##1       | 14/65 [02:38<09:39, 11.37s/it]
2022-04-13 19:02:08,486 - INFO - tqdm - accuracy: 0.9625, batch_loss: 0.1371, loss: 0.1285 ||:  23%|##3       | 15/65 [02:49<09:28, 11.37s/it]
2022-04-13 19:02:19,899 - INFO - tqdm - accuracy: 0.9629, batch_loss: 0.0793, loss: 0.1254 ||:  25%|##4       | 16/65 [03:00<09:17, 11.38s/it]
2022-04-13 19:02:31,283 - INFO - tqdm - accuracy: 0.9632, batch_loss: 0.0553, loss: 0.1213 ||:  26%|##6       | 17/65 [03:12<09:06, 11.38s/it]
2022-04-13 19:02:42,658 - INFO - tqdm - accuracy: 0.9635, batch_loss: 0.1725, loss: 0.1242 ||:  28%|##7       | 18/65 [03:23<08:54, 11.38s/it]
2022-04-13 19:02:54,032 - INFO - tqdm - accuracy: 0.9655, batch_loss: 0.0444, loss: 0.1200 ||:  29%|##9       | 19/65 [03:35<08:43, 11.38s/it]
2022-04-13 19:03:05,397 - INFO - tqdm - accuracy: 0.9656, batch_loss: 0.0629, loss: 0.1171 ||:  31%|###       | 20/65 [03:46<08:31, 11.37s/it]
2022-04-13 19:03:16,773 - INFO - tqdm - accuracy: 0.9673, batch_loss: 0.0283, loss: 0.1129 ||:  32%|###2      | 21/65 [03:57<08:20, 11.38s/it]
2022-04-13 19:03:28,148 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.0290, loss: 0.1091 ||:  34%|###3      | 22/65 [04:09<08:09, 11.38s/it]
2022-04-13 19:03:39,517 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.0579, loss: 0.1068 ||:  35%|###5      | 23/65 [04:20<07:57, 11.37s/it]
2022-04-13 19:03:50,898 - INFO - tqdm - accuracy: 0.9674, batch_loss: 0.2296, loss: 0.1120 ||:  37%|###6      | 24/65 [04:31<07:46, 11.38s/it]
2022-04-13 19:04:02,272 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.0314, loss: 0.1087 ||:  38%|###8      | 25/65 [04:43<07:34, 11.37s/it]
2022-04-13 19:04:13,650 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.0729, loss: 0.1074 ||:  40%|####      | 26/65 [04:54<07:23, 11.38s/it]
2022-04-13 19:04:25,009 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.0585, loss: 0.1055 ||:  42%|####1     | 27/65 [05:06<07:12, 11.37s/it]
2022-04-13 19:04:36,385 - INFO - tqdm - accuracy: 0.9699, batch_loss: 0.0347, loss: 0.1030 ||:  43%|####3     | 28/65 [05:17<07:00, 11.37s/it]
2022-04-13 19:04:47,717 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.0684, loss: 0.1018 ||:  45%|####4     | 29/65 [05:28<06:48, 11.36s/it]
2022-04-13 19:04:58,947 - INFO - tqdm - accuracy: 0.9677, batch_loss: 0.1447, loss: 0.1033 ||:  46%|####6     | 30/65 [05:39<06:36, 11.32s/it]
2022-04-13 19:05:10,309 - INFO - tqdm - accuracy: 0.9677, batch_loss: 0.0502, loss: 0.1015 ||:  48%|####7     | 31/65 [05:51<06:25, 11.33s/it]
2022-04-13 19:05:21,676 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.0129, loss: 0.0988 ||:  49%|####9     | 32/65 [06:02<06:14, 11.34s/it]
2022-04-13 19:05:33,044 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.1400, loss: 0.1000 ||:  51%|#####     | 33/65 [06:14<06:03, 11.35s/it]
2022-04-13 19:05:44,396 - INFO - tqdm - accuracy: 0.9697, batch_loss: 0.0219, loss: 0.0977 ||:  52%|#####2    | 34/65 [06:25<05:51, 11.35s/it]
2022-04-13 19:05:55,733 - INFO - tqdm - accuracy: 0.9705, batch_loss: 0.0456, loss: 0.0962 ||:  54%|#####3    | 35/65 [06:36<05:40, 11.35s/it]
2022-04-13 19:06:07,100 - INFO - tqdm - accuracy: 0.9714, batch_loss: 0.0531, loss: 0.0950 ||:  55%|#####5    | 36/65 [06:48<05:29, 11.35s/it]
2022-04-13 19:06:18,470 - INFO - tqdm - accuracy: 0.9713, batch_loss: 0.0336, loss: 0.0934 ||:  57%|#####6    | 37/65 [06:59<05:18, 11.36s/it]
2022-04-13 19:06:29,820 - INFO - tqdm - accuracy: 0.9712, batch_loss: 0.1048, loss: 0.0937 ||:  58%|#####8    | 38/65 [07:10<05:06, 11.36s/it]
2022-04-13 19:06:41,168 - INFO - tqdm - accuracy: 0.9712, batch_loss: 0.0822, loss: 0.0934 ||:  60%|######    | 39/65 [07:22<04:55, 11.35s/it]
2022-04-13 19:06:52,519 - INFO - tqdm - accuracy: 0.9695, batch_loss: 0.1329, loss: 0.0944 ||:  62%|######1   | 40/65 [07:33<04:43, 11.35s/it]
2022-04-13 19:07:03,544 - INFO - tqdm - accuracy: 0.9695, batch_loss: 0.0815, loss: 0.0941 ||:  63%|######3   | 41/65 [07:44<04:30, 11.25s/it]
2022-04-13 19:07:14,889 - INFO - tqdm - accuracy: 0.9695, batch_loss: 0.0525, loss: 0.0931 ||:  65%|######4   | 42/65 [07:55<04:19, 11.28s/it]
2022-04-13 19:07:25,980 - INFO - tqdm - accuracy: 0.9687, batch_loss: 0.1375, loss: 0.0941 ||:  66%|######6   | 43/65 [08:07<04:06, 11.22s/it]
2022-04-13 19:07:37,345 - INFO - tqdm - accuracy: 0.9680, batch_loss: 0.1214, loss: 0.0947 ||:  68%|######7   | 44/65 [08:18<03:56, 11.27s/it]
2022-04-13 19:07:48,709 - INFO - tqdm - accuracy: 0.9666, batch_loss: 0.1860, loss: 0.0967 ||:  69%|######9   | 45/65 [08:29<03:45, 11.30s/it]
2022-04-13 19:08:00,087 - INFO - tqdm - accuracy: 0.9674, batch_loss: 0.0230, loss: 0.0951 ||:  71%|#######   | 46/65 [08:41<03:35, 11.32s/it]
2022-04-13 19:08:11,437 - INFO - tqdm - accuracy: 0.9681, batch_loss: 0.0287, loss: 0.0937 ||:  72%|#######2  | 47/65 [08:52<03:23, 11.33s/it]
2022-04-13 19:08:22,789 - INFO - tqdm - accuracy: 0.9681, batch_loss: 0.1076, loss: 0.0940 ||:  74%|#######3  | 48/65 [09:03<03:12, 11.34s/it]
2022-04-13 19:08:34,140 - INFO - tqdm - accuracy: 0.9687, batch_loss: 0.0122, loss: 0.0924 ||:  75%|#######5  | 49/65 [09:15<03:01, 11.34s/it]
2022-04-13 19:08:45,500 - INFO - tqdm - accuracy: 0.9694, batch_loss: 0.0195, loss: 0.0909 ||:  77%|#######6  | 50/65 [09:26<02:50, 11.35s/it]
2022-04-13 19:08:56,850 - INFO - tqdm - accuracy: 0.9700, batch_loss: 0.0201, loss: 0.0895 ||:  78%|#######8  | 51/65 [09:37<02:38, 11.35s/it]
2022-04-13 19:09:08,107 - INFO - tqdm - accuracy: 0.9693, batch_loss: 0.0885, loss: 0.0895 ||:  80%|########  | 52/65 [09:49<02:27, 11.32s/it]
2022-04-13 19:09:19,483 - INFO - tqdm - accuracy: 0.9693, batch_loss: 0.0701, loss: 0.0891 ||:  82%|########1 | 53/65 [10:00<02:16, 11.34s/it]
2022-04-13 19:09:30,889 - INFO - tqdm - accuracy: 0.9699, batch_loss: 0.0254, loss: 0.0879 ||:  83%|########3 | 54/65 [10:11<02:04, 11.36s/it]
2022-04-13 19:09:42,299 - INFO - tqdm - accuracy: 0.9693, batch_loss: 0.1013, loss: 0.0882 ||:  85%|########4 | 55/65 [10:23<01:53, 11.37s/it]
2022-04-13 19:09:53,615 - INFO - tqdm - accuracy: 0.9693, batch_loss: 0.2308, loss: 0.0907 ||:  86%|########6 | 56/65 [10:34<01:42, 11.36s/it]
2022-04-13 19:10:05,064 - INFO - tqdm - accuracy: 0.9687, batch_loss: 0.0889, loss: 0.0907 ||:  88%|########7 | 57/65 [10:46<01:31, 11.38s/it]
2022-04-13 19:10:16,519 - INFO - tqdm - accuracy: 0.9693, batch_loss: 0.0046, loss: 0.0892 ||:  89%|########9 | 58/65 [10:57<01:19, 11.41s/it]
2022-04-13 19:10:27,968 - INFO - tqdm - accuracy: 0.9693, batch_loss: 0.0547, loss: 0.0886 ||:  91%|######### | 59/65 [11:09<01:08, 11.42s/it]
2022-04-13 19:10:39,406 - INFO - tqdm - accuracy: 0.9698, batch_loss: 0.0191, loss: 0.0875 ||:  92%|#########2| 60/65 [11:20<00:57, 11.42s/it]
2022-04-13 19:10:50,836 - INFO - tqdm - accuracy: 0.9698, batch_loss: 0.0428, loss: 0.0867 ||:  94%|#########3| 61/65 [11:31<00:45, 11.43s/it]
2022-04-13 19:11:02,236 - INFO - tqdm - accuracy: 0.9697, batch_loss: 0.0806, loss: 0.0866 ||:  95%|#########5| 62/65 [11:43<00:34, 11.42s/it]
2022-04-13 19:11:13,643 - INFO - tqdm - accuracy: 0.9692, batch_loss: 0.1303, loss: 0.0873 ||:  97%|#########6| 63/65 [11:54<00:22, 11.41s/it]
2022-04-13 19:11:25,037 - INFO - tqdm - accuracy: 0.9692, batch_loss: 0.0661, loss: 0.0870 ||:  98%|#########8| 64/65 [12:06<00:11, 11.41s/it]
2022-04-13 19:11:30,106 - INFO - tqdm - accuracy: 0.9689, batch_loss: 0.1025, loss: 0.0872 ||: 100%|##########| 65/65 [12:11<00:00,  9.51s/it]
2022-04-13 19:11:30,106 - INFO - tqdm - accuracy: 0.9689, batch_loss: 0.1025, loss: 0.0872 ||: 100%|##########| 65/65 [12:11<00:00, 11.25s/it]
2022-04-13 19:11:32,956 - INFO - allennlp.training.trainer - Validating
2022-04-13 19:11:32,958 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-13 19:11:43,057 - INFO - tqdm - accuracy: 0.6173, batch_loss: 5.7978, loss: 1.5995 ||:  36%|###5      | 41/115 [00:10<00:18,  4.01it/s]
2022-04-13 19:11:53,188 - INFO - tqdm - accuracy: 0.6503, batch_loss: 0.0102, loss: 1.5470 ||:  71%|#######1  | 82/115 [00:20<00:07,  4.34it/s]
2022-04-13 19:12:01,416 - INFO - tqdm - accuracy: 0.6463, batch_loss: 0.1919, loss: 1.5636 ||: 100%|##########| 115/115 [00:28<00:00,  4.01it/s]
2022-04-13 19:12:01,416 - INFO - tqdm - accuracy: 0.6463, batch_loss: 0.1919, loss: 1.5636 ||: 100%|##########| 115/115 [00:28<00:00,  4.04it/s]
2022-04-13 19:12:01,416 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-13 19:12:01,417 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.969  |     0.646
2022-04-13 19:12:01,418 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-13 19:12:01,419 - INFO - allennlp.training.tensorboard_writer - loss               |     0.087  |     1.564
2022-04-13 19:12:01,420 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5802.840  |       N/A
2022-04-13 19:12:07,713 - INFO - allennlp.training.trainer - Epoch duration: 0:12:48.755803
2022-04-13 19:12:07,713 - INFO - allennlp.training.trainer - Estimated training time remaining: 1:43:40
2022-04-13 19:12:07,713 - INFO - allennlp.training.trainer - Epoch 7/14
2022-04-13 19:12:07,713 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-13 19:12:07,714 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-13 19:12:07,715 - INFO - allennlp.training.trainer - Training
2022-04-13 19:12:07,716 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-13 19:12:18,847 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0378, loss: 0.0378 ||:   2%|1         | 1/65 [00:11<11:52, 11.13s/it]
2022-04-13 19:12:30,001 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0192, loss: 0.0285 ||:   3%|3         | 2/65 [00:22<11:42, 11.14s/it]
2022-04-13 19:12:41,177 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.1242, loss: 0.0604 ||:   5%|4         | 3/65 [00:33<11:31, 11.16s/it]
2022-04-13 19:12:52,383 - INFO - tqdm - accuracy: 0.9922, batch_loss: 0.0523, loss: 0.0584 ||:   6%|6         | 4/65 [00:44<11:21, 11.18s/it]
2022-04-13 19:13:03,606 - INFO - tqdm - accuracy: 0.9875, batch_loss: 0.1188, loss: 0.0705 ||:   8%|7         | 5/65 [00:55<11:11, 11.19s/it]
2022-04-13 19:13:14,849 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0410, loss: 0.0656 ||:   9%|9         | 6/65 [01:07<11:01, 11.21s/it]
2022-04-13 19:13:26,115 - INFO - tqdm - accuracy: 0.9821, batch_loss: 0.0775, loss: 0.0673 ||:  11%|#         | 7/65 [01:18<10:51, 11.23s/it]
2022-04-13 19:13:37,382 - INFO - tqdm - accuracy: 0.9766, batch_loss: 0.1274, loss: 0.0748 ||:  12%|#2        | 8/65 [01:29<10:40, 11.24s/it]
2022-04-13 19:13:48,659 - INFO - tqdm - accuracy: 0.9757, batch_loss: 0.0647, loss: 0.0737 ||:  14%|#3        | 9/65 [01:40<10:30, 11.25s/it]
2022-04-13 19:13:59,950 - INFO - tqdm - accuracy: 0.9781, batch_loss: 0.0366, loss: 0.0700 ||:  15%|#5        | 10/65 [01:52<10:19, 11.26s/it]
2022-04-13 19:14:11,252 - INFO - tqdm - accuracy: 0.9801, batch_loss: 0.0409, loss: 0.0673 ||:  17%|#6        | 11/65 [02:03<10:08, 11.28s/it]
2022-04-13 19:14:22,565 - INFO - tqdm - accuracy: 0.9766, batch_loss: 0.0840, loss: 0.0687 ||:  18%|#8        | 12/65 [02:14<09:58, 11.29s/it]
2022-04-13 19:14:33,878 - INFO - tqdm - accuracy: 0.9784, batch_loss: 0.0250, loss: 0.0654 ||:  20%|##        | 13/65 [02:26<09:47, 11.29s/it]
2022-04-13 19:14:45,215 - INFO - tqdm - accuracy: 0.9777, batch_loss: 0.0524, loss: 0.0644 ||:  22%|##1       | 14/65 [02:37<09:36, 11.31s/it]
2022-04-13 19:14:56,441 - INFO - tqdm - accuracy: 0.9771, batch_loss: 0.0383, loss: 0.0627 ||:  23%|##3       | 15/65 [02:48<09:24, 11.28s/it]
2022-04-13 19:15:07,812 - INFO - tqdm - accuracy: 0.9766, batch_loss: 0.0780, loss: 0.0636 ||:  25%|##4       | 16/65 [03:00<09:14, 11.31s/it]
2022-04-13 19:15:19,080 - INFO - tqdm - accuracy: 0.9779, batch_loss: 0.0240, loss: 0.0613 ||:  26%|##6       | 17/65 [03:11<09:02, 11.30s/it]
2022-04-13 19:15:30,481 - INFO - tqdm - accuracy: 0.9792, batch_loss: 0.0209, loss: 0.0591 ||:  28%|##7       | 18/65 [03:22<08:52, 11.33s/it]
2022-04-13 19:15:41,911 - INFO - tqdm - accuracy: 0.9803, batch_loss: 0.0249, loss: 0.0573 ||:  29%|##9       | 19/65 [03:34<08:42, 11.36s/it]
2022-04-13 19:15:53,198 - INFO - tqdm - accuracy: 0.9812, batch_loss: 0.0153, loss: 0.0552 ||:  31%|###       | 20/65 [03:45<08:30, 11.34s/it]
2022-04-13 19:16:04,588 - INFO - tqdm - accuracy: 0.9821, batch_loss: 0.0551, loss: 0.0552 ||:  32%|###2      | 21/65 [03:56<08:19, 11.35s/it]
2022-04-13 19:16:15,972 - INFO - tqdm - accuracy: 0.9815, batch_loss: 0.1229, loss: 0.0582 ||:  34%|###3      | 22/65 [04:08<08:08, 11.36s/it]
2022-04-13 19:16:27,356 - INFO - tqdm - accuracy: 0.9823, batch_loss: 0.0134, loss: 0.0563 ||:  35%|###5      | 23/65 [04:19<07:57, 11.37s/it]
2022-04-13 19:16:38,718 - INFO - tqdm - accuracy: 0.9831, batch_loss: 0.0034, loss: 0.0541 ||:  37%|###6      | 24/65 [04:31<07:46, 11.37s/it]
2022-04-13 19:16:50,049 - INFO - tqdm - accuracy: 0.9838, batch_loss: 0.0136, loss: 0.0525 ||:  38%|###8      | 25/65 [04:42<07:34, 11.36s/it]
2022-04-13 19:17:01,439 - INFO - tqdm - accuracy: 0.9832, batch_loss: 0.0496, loss: 0.0524 ||:  40%|####      | 26/65 [04:53<07:23, 11.37s/it]
2022-04-13 19:17:12,804 - INFO - tqdm - accuracy: 0.9826, batch_loss: 0.0380, loss: 0.0518 ||:  42%|####1     | 27/65 [05:05<07:11, 11.37s/it]
2022-04-13 19:17:24,152 - INFO - tqdm - accuracy: 0.9821, batch_loss: 0.0925, loss: 0.0533 ||:  43%|####3     | 28/65 [05:16<07:00, 11.36s/it]
2022-04-13 19:17:35,507 - INFO - tqdm - accuracy: 0.9828, batch_loss: 0.0328, loss: 0.0526 ||:  45%|####4     | 29/65 [05:27<06:48, 11.36s/it]
2022-04-13 19:17:46,861 - INFO - tqdm - accuracy: 0.9833, batch_loss: 0.0283, loss: 0.0518 ||:  46%|####6     | 30/65 [05:39<06:37, 11.36s/it]
2022-04-13 19:17:58,200 - INFO - tqdm - accuracy: 0.9829, batch_loss: 0.0327, loss: 0.0512 ||:  48%|####7     | 31/65 [05:50<06:25, 11.35s/it]
2022-04-13 19:18:09,563 - INFO - tqdm - accuracy: 0.9834, batch_loss: 0.0243, loss: 0.0503 ||:  49%|####9     | 32/65 [06:01<06:14, 11.36s/it]
2022-04-13 19:18:20,910 - INFO - tqdm - accuracy: 0.9830, batch_loss: 0.0257, loss: 0.0496 ||:  51%|#####     | 33/65 [06:13<06:03, 11.35s/it]
2022-04-13 19:18:32,264 - INFO - tqdm - accuracy: 0.9825, batch_loss: 0.0893, loss: 0.0507 ||:  52%|#####2    | 34/65 [06:24<05:51, 11.35s/it]
2022-04-13 19:18:43,606 - INFO - tqdm - accuracy: 0.9821, batch_loss: 0.0781, loss: 0.0515 ||:  54%|#####3    | 35/65 [06:35<05:40, 11.35s/it]
2022-04-13 19:18:54,963 - INFO - tqdm - accuracy: 0.9826, batch_loss: 0.0141, loss: 0.0505 ||:  55%|#####5    | 36/65 [06:47<05:29, 11.35s/it]
2022-04-13 19:19:06,310 - INFO - tqdm - accuracy: 0.9814, batch_loss: 0.0841, loss: 0.0514 ||:  57%|#####6    | 37/65 [06:58<05:17, 11.35s/it]
2022-04-13 19:19:17,681 - INFO - tqdm - accuracy: 0.9819, batch_loss: 0.0124, loss: 0.0504 ||:  58%|#####8    | 38/65 [07:09<05:06, 11.36s/it]
2022-04-13 19:19:29,023 - INFO - tqdm - accuracy: 0.9824, batch_loss: 0.0087, loss: 0.0493 ||:  60%|######    | 39/65 [07:21<04:55, 11.35s/it]
2022-04-13 19:19:40,380 - INFO - tqdm - accuracy: 0.9820, batch_loss: 0.1520, loss: 0.0519 ||:  62%|######1   | 40/65 [07:32<04:43, 11.35s/it]
2022-04-13 19:19:51,745 - INFO - tqdm - accuracy: 0.9825, batch_loss: 0.0241, loss: 0.0512 ||:  63%|######3   | 41/65 [07:44<04:32, 11.36s/it]
2022-04-13 19:20:03,104 - INFO - tqdm - accuracy: 0.9829, batch_loss: 0.0057, loss: 0.0501 ||:  65%|######4   | 42/65 [07:55<04:21, 11.36s/it]
2022-04-13 19:20:14,460 - INFO - tqdm - accuracy: 0.9833, batch_loss: 0.0131, loss: 0.0492 ||:  66%|######6   | 43/65 [08:06<04:09, 11.36s/it]
2022-04-13 19:20:25,825 - INFO - tqdm - accuracy: 0.9830, batch_loss: 0.0483, loss: 0.0492 ||:  68%|######7   | 44/65 [08:18<03:58, 11.36s/it]
2022-04-13 19:20:37,571 - INFO - tqdm - accuracy: 0.9833, batch_loss: 0.0064, loss: 0.0483 ||:  69%|######9   | 45/65 [08:29<03:49, 11.48s/it]
2022-04-13 19:20:48,883 - INFO - tqdm - accuracy: 0.9830, batch_loss: 0.0291, loss: 0.0479 ||:  71%|#######   | 46/65 [08:41<03:37, 11.43s/it]
2022-04-13 19:21:00,243 - INFO - tqdm - accuracy: 0.9834, batch_loss: 0.0075, loss: 0.0470 ||:  72%|#######2  | 47/65 [08:52<03:25, 11.41s/it]
2022-04-13 19:21:11,590 - INFO - tqdm - accuracy: 0.9837, batch_loss: 0.0333, loss: 0.0467 ||:  74%|#######3  | 48/65 [09:03<03:13, 11.39s/it]
2022-04-13 19:21:22,941 - INFO - tqdm - accuracy: 0.9841, batch_loss: 0.0080, loss: 0.0459 ||:  75%|#######5  | 49/65 [09:15<03:02, 11.38s/it]
2022-04-13 19:21:34,306 - INFO - tqdm - accuracy: 0.9838, batch_loss: 0.1080, loss: 0.0472 ||:  77%|#######6  | 50/65 [09:26<02:50, 11.37s/it]
2022-04-13 19:21:45,673 - INFO - tqdm - accuracy: 0.9835, batch_loss: 0.0534, loss: 0.0473 ||:  78%|#######8  | 51/65 [09:37<02:39, 11.37s/it]
2022-04-13 19:21:57,045 - INFO - tqdm - accuracy: 0.9832, batch_loss: 0.0481, loss: 0.0473 ||:  80%|########  | 52/65 [09:49<02:27, 11.37s/it]
2022-04-13 19:22:08,436 - INFO - tqdm - accuracy: 0.9835, batch_loss: 0.0137, loss: 0.0467 ||:  82%|########1 | 53/65 [10:00<02:16, 11.38s/it]
2022-04-13 19:22:19,808 - INFO - tqdm - accuracy: 0.9832, batch_loss: 0.0442, loss: 0.0466 ||:  83%|########3 | 54/65 [10:12<02:05, 11.38s/it]
2022-04-13 19:22:31,204 - INFO - tqdm - accuracy: 0.9830, batch_loss: 0.0925, loss: 0.0475 ||:  85%|########4 | 55/65 [10:23<01:53, 11.38s/it]
2022-04-13 19:22:42,617 - INFO - tqdm - accuracy: 0.9833, batch_loss: 0.0340, loss: 0.0472 ||:  86%|########6 | 56/65 [10:34<01:42, 11.39s/it]
2022-04-13 19:22:54,022 - INFO - tqdm - accuracy: 0.9836, batch_loss: 0.0113, loss: 0.0466 ||:  88%|########7 | 57/65 [10:46<01:31, 11.40s/it]
2022-04-13 19:23:05,447 - INFO - tqdm - accuracy: 0.9833, batch_loss: 0.1551, loss: 0.0485 ||:  89%|########9 | 58/65 [10:57<01:19, 11.40s/it]
2022-04-13 19:23:16,276 - INFO - tqdm - accuracy: 0.9836, batch_loss: 0.0028, loss: 0.0477 ||:  91%|######### | 59/65 [11:08<01:07, 11.23s/it]
2022-04-13 19:23:27,706 - INFO - tqdm - accuracy: 0.9838, batch_loss: 0.0086, loss: 0.0470 ||:  92%|#########2| 60/65 [11:19<00:56, 11.29s/it]
2022-04-13 19:23:39,155 - INFO - tqdm - accuracy: 0.9841, batch_loss: 0.0225, loss: 0.0466 ||:  94%|#########3| 61/65 [11:31<00:45, 11.34s/it]
2022-04-13 19:23:50,596 - INFO - tqdm - accuracy: 0.9839, batch_loss: 0.0378, loss: 0.0465 ||:  95%|#########5| 62/65 [11:42<00:34, 11.37s/it]
2022-04-13 19:24:02,054 - INFO - tqdm - accuracy: 0.9841, batch_loss: 0.0022, loss: 0.0458 ||:  97%|#########6| 63/65 [11:54<00:22, 11.40s/it]
2022-04-13 19:24:13,477 - INFO - tqdm - accuracy: 0.9839, batch_loss: 0.0667, loss: 0.0461 ||:  98%|#########8| 64/65 [12:05<00:11, 11.40s/it]
2022-04-13 19:24:18,536 - INFO - tqdm - accuracy: 0.9840, batch_loss: 0.0063, loss: 0.0455 ||: 100%|##########| 65/65 [12:10<00:00,  9.50s/it]
2022-04-13 19:24:18,536 - INFO - tqdm - accuracy: 0.9840, batch_loss: 0.0063, loss: 0.0455 ||: 100%|##########| 65/65 [12:10<00:00, 11.24s/it]
2022-04-13 19:24:21,387 - INFO - allennlp.training.trainer - Validating
2022-04-13 19:24:21,389 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-13 19:24:31,532 - INFO - tqdm - accuracy: 0.6707, batch_loss: 3.4624, loss: 1.5234 ||:  36%|###5      | 41/115 [00:10<00:18,  4.04it/s]
2022-04-13 19:24:41,651 - INFO - tqdm - accuracy: 0.6687, batch_loss: 0.0062, loss: 1.7184 ||:  71%|#######1  | 82/115 [00:20<00:08,  3.99it/s]
2022-04-13 19:24:49,897 - INFO - tqdm - accuracy: 0.6463, batch_loss: 3.3556, loss: 1.7450 ||: 100%|##########| 115/115 [00:28<00:00,  4.00it/s]
2022-04-13 19:24:49,897 - INFO - tqdm - accuracy: 0.6463, batch_loss: 3.3556, loss: 1.7450 ||: 100%|##########| 115/115 [00:28<00:00,  4.03it/s]
2022-04-13 19:24:49,897 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-13 19:24:49,898 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.984  |     0.646
2022-04-13 19:24:49,899 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-13 19:24:49,899 - INFO - allennlp.training.tensorboard_writer - loss               |     0.045  |     1.745
2022-04-13 19:24:49,899 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5802.840  |       N/A
2022-04-13 19:24:56,078 - INFO - allennlp.training.trainer - Epoch duration: 0:12:48.365177
2022-04-13 19:24:56,078 - INFO - allennlp.training.trainer - Estimated training time remaining: 1:30:34
2022-04-13 19:24:56,079 - INFO - allennlp.training.trainer - Epoch 8/14
2022-04-13 19:24:56,079 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-13 19:24:56,079 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-13 19:24:56,081 - INFO - allennlp.training.trainer - Training
2022-04-13 19:24:56,081 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-13 19:25:07,199 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.0910, loss: 0.0910 ||:   2%|1         | 1/65 [00:11<11:51, 11.12s/it]
2022-04-13 19:25:18,356 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0067, loss: 0.0488 ||:   3%|3         | 2/65 [00:22<11:41, 11.14s/it]
2022-04-13 19:25:29,544 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.0033, loss: 0.0337 ||:   5%|4         | 3/65 [00:33<11:32, 11.16s/it]
2022-04-13 19:25:40,739 - INFO - tqdm - accuracy: 0.9922, batch_loss: 0.0081, loss: 0.0273 ||:   6%|6         | 4/65 [00:44<11:21, 11.18s/it]
2022-04-13 19:25:51,957 - INFO - tqdm - accuracy: 0.9938, batch_loss: 0.0112, loss: 0.0241 ||:   8%|7         | 5/65 [00:55<11:11, 11.19s/it]
2022-04-13 19:26:03,201 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.0451, loss: 0.0276 ||:   9%|9         | 6/65 [01:07<11:01, 11.21s/it]
2022-04-13 19:26:14,437 - INFO - tqdm - accuracy: 0.9911, batch_loss: 0.0053, loss: 0.0244 ||:  11%|#         | 7/65 [01:18<10:50, 11.22s/it]
2022-04-13 19:26:25,710 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0906, loss: 0.0327 ||:  12%|#2        | 8/65 [01:29<10:40, 11.24s/it]
2022-04-13 19:26:36,972 - INFO - tqdm - accuracy: 0.9861, batch_loss: 0.0066, loss: 0.0298 ||:  14%|#3        | 9/65 [01:40<10:29, 11.24s/it]
2022-04-13 19:26:48,240 - INFO - tqdm - accuracy: 0.9875, batch_loss: 0.0038, loss: 0.0272 ||:  15%|#5        | 10/65 [01:52<10:18, 11.25s/it]
2022-04-13 19:26:59,517 - INFO - tqdm - accuracy: 0.9886, batch_loss: 0.0067, loss: 0.0253 ||:  17%|#6        | 11/65 [02:03<10:07, 11.26s/it]
2022-04-13 19:27:10,819 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.0031, loss: 0.0235 ||:  18%|#8        | 12/65 [02:14<09:57, 11.27s/it]
2022-04-13 19:27:22,106 - INFO - tqdm - accuracy: 0.9880, batch_loss: 0.0343, loss: 0.0243 ||:  20%|##        | 13/65 [02:26<09:46, 11.28s/it]
2022-04-13 19:27:33,395 - INFO - tqdm - accuracy: 0.9888, batch_loss: 0.0124, loss: 0.0234 ||:  22%|##1       | 14/65 [02:37<09:35, 11.28s/it]
2022-04-13 19:27:44,692 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.0115, loss: 0.0227 ||:  23%|##3       | 15/65 [02:48<09:24, 11.29s/it]
2022-04-13 19:27:55,990 - INFO - tqdm - accuracy: 0.9902, batch_loss: 0.0082, loss: 0.0217 ||:  25%|##4       | 16/65 [02:59<09:13, 11.29s/it]
2022-04-13 19:28:07,300 - INFO - tqdm - accuracy: 0.9908, batch_loss: 0.0104, loss: 0.0211 ||:  26%|##6       | 17/65 [03:11<09:02, 11.30s/it]
2022-04-13 19:28:18,621 - INFO - tqdm - accuracy: 0.9913, batch_loss: 0.0198, loss: 0.0210 ||:  28%|##7       | 18/65 [03:22<08:51, 11.30s/it]
2022-04-13 19:28:29,944 - INFO - tqdm - accuracy: 0.9918, batch_loss: 0.0214, loss: 0.0210 ||:  29%|##9       | 19/65 [03:33<08:40, 11.31s/it]
2022-04-13 19:28:41,266 - INFO - tqdm - accuracy: 0.9891, batch_loss: 0.1473, loss: 0.0273 ||:  31%|###       | 20/65 [03:45<08:29, 11.31s/it]
2022-04-13 19:28:52,579 - INFO - tqdm - accuracy: 0.9866, batch_loss: 0.1407, loss: 0.0327 ||:  32%|###2      | 21/65 [03:56<08:17, 11.31s/it]
2022-04-13 19:29:03,904 - INFO - tqdm - accuracy: 0.9872, batch_loss: 0.0048, loss: 0.0315 ||:  34%|###3      | 22/65 [04:07<08:06, 11.32s/it]
2022-04-13 19:29:15,215 - INFO - tqdm - accuracy: 0.9878, batch_loss: 0.0072, loss: 0.0304 ||:  35%|###5      | 23/65 [04:19<07:55, 11.31s/it]
2022-04-13 19:29:26,526 - INFO - tqdm - accuracy: 0.9883, batch_loss: 0.0076, loss: 0.0295 ||:  37%|###6      | 24/65 [04:30<07:43, 11.31s/it]
2022-04-13 19:29:37,849 - INFO - tqdm - accuracy: 0.9888, batch_loss: 0.0063, loss: 0.0285 ||:  38%|###8      | 25/65 [04:41<07:32, 11.32s/it]
2022-04-13 19:29:49,187 - INFO - tqdm - accuracy: 0.9892, batch_loss: 0.0098, loss: 0.0278 ||:  40%|####      | 26/65 [04:53<07:21, 11.32s/it]
2022-04-13 19:30:00,301 - INFO - tqdm - accuracy: 0.9884, batch_loss: 0.0391, loss: 0.0282 ||:  42%|####1     | 27/65 [05:04<07:07, 11.26s/it]
2022-04-13 19:30:11,648 - INFO - tqdm - accuracy: 0.9888, batch_loss: 0.0085, loss: 0.0275 ||:  43%|####3     | 28/65 [05:15<06:57, 11.29s/it]
2022-04-13 19:30:22,969 - INFO - tqdm - accuracy: 0.9892, batch_loss: 0.0040, loss: 0.0267 ||:  45%|####4     | 29/65 [05:26<06:46, 11.30s/it]
2022-04-13 19:30:34,337 - INFO - tqdm - accuracy: 0.9875, batch_loss: 0.1251, loss: 0.0300 ||:  46%|####6     | 30/65 [05:38<06:36, 11.32s/it]
2022-04-13 19:30:45,705 - INFO - tqdm - accuracy: 0.9879, batch_loss: 0.0034, loss: 0.0291 ||:  48%|####7     | 31/65 [05:49<06:25, 11.33s/it]
2022-04-13 19:30:57,073 - INFO - tqdm - accuracy: 0.9883, batch_loss: 0.0070, loss: 0.0285 ||:  49%|####9     | 32/65 [06:00<06:14, 11.34s/it]
2022-04-13 19:31:08,457 - INFO - tqdm - accuracy: 0.9877, batch_loss: 0.1257, loss: 0.0314 ||:  51%|#####     | 33/65 [06:12<06:03, 11.36s/it]
2022-04-13 19:31:19,860 - INFO - tqdm - accuracy: 0.9881, batch_loss: 0.0114, loss: 0.0308 ||:  52%|#####2    | 34/65 [06:23<05:52, 11.37s/it]
2022-04-13 19:31:31,282 - INFO - tqdm - accuracy: 0.9884, batch_loss: 0.0011, loss: 0.0300 ||:  54%|#####3    | 35/65 [06:35<05:41, 11.39s/it]
2022-04-13 19:31:42,712 - INFO - tqdm - accuracy: 0.9887, batch_loss: 0.0098, loss: 0.0294 ||:  55%|#####5    | 36/65 [06:46<05:30, 11.40s/it]
2022-04-13 19:31:54,148 - INFO - tqdm - accuracy: 0.9890, batch_loss: 0.0184, loss: 0.0291 ||:  57%|#####6    | 37/65 [06:58<05:19, 11.41s/it]
2022-04-13 19:32:05,597 - INFO - tqdm - accuracy: 0.9893, batch_loss: 0.0032, loss: 0.0284 ||:  58%|#####8    | 38/65 [07:09<05:08, 11.42s/it]
2022-04-13 19:32:16,782 - INFO - tqdm - accuracy: 0.9872, batch_loss: 0.1525, loss: 0.0316 ||:  60%|######    | 39/65 [07:20<04:55, 11.35s/it]
2022-04-13 19:32:28,230 - INFO - tqdm - accuracy: 0.9875, batch_loss: 0.0162, loss: 0.0312 ||:  62%|######1   | 40/65 [07:32<04:44, 11.38s/it]
2022-04-13 19:32:39,641 - INFO - tqdm - accuracy: 0.9870, batch_loss: 0.0960, loss: 0.0328 ||:  63%|######3   | 41/65 [07:43<04:33, 11.39s/it]
2022-04-13 19:32:51,058 - INFO - tqdm - accuracy: 0.9866, batch_loss: 0.0540, loss: 0.0333 ||:  65%|######4   | 42/65 [07:54<04:22, 11.40s/it]
2022-04-13 19:33:02,463 - INFO - tqdm - accuracy: 0.9869, batch_loss: 0.0029, loss: 0.0326 ||:  66%|######6   | 43/65 [08:06<04:10, 11.40s/it]
2022-04-13 19:33:13,869 - INFO - tqdm - accuracy: 0.9872, batch_loss: 0.0121, loss: 0.0321 ||:  68%|######7   | 44/65 [08:17<03:59, 11.40s/it]
2022-04-13 19:33:25,260 - INFO - tqdm - accuracy: 0.9868, batch_loss: 0.0643, loss: 0.0328 ||:  69%|######9   | 45/65 [08:29<03:47, 11.40s/it]
2022-04-13 19:33:36,651 - INFO - tqdm - accuracy: 0.9871, batch_loss: 0.0042, loss: 0.0322 ||:  71%|#######   | 46/65 [08:40<03:36, 11.40s/it]
2022-04-13 19:33:48,027 - INFO - tqdm - accuracy: 0.9874, batch_loss: 0.0064, loss: 0.0317 ||:  72%|#######2  | 47/65 [08:51<03:25, 11.39s/it]
2022-04-13 19:33:59,416 - INFO - tqdm - accuracy: 0.9876, batch_loss: 0.0032, loss: 0.0311 ||:  74%|#######3  | 48/65 [09:03<03:13, 11.39s/it]
2022-04-13 19:34:10,773 - INFO - tqdm - accuracy: 0.9879, batch_loss: 0.0029, loss: 0.0305 ||:  75%|#######5  | 49/65 [09:14<03:02, 11.38s/it]
2022-04-13 19:34:22,148 - INFO - tqdm - accuracy: 0.9881, batch_loss: 0.0066, loss: 0.0300 ||:  77%|#######6  | 50/65 [09:26<02:50, 11.38s/it]
2022-04-13 19:34:33,515 - INFO - tqdm - accuracy: 0.9871, batch_loss: 0.1270, loss: 0.0319 ||:  78%|#######8  | 51/65 [09:37<02:39, 11.38s/it]
2022-04-13 19:34:44,908 - INFO - tqdm - accuracy: 0.9874, batch_loss: 0.0129, loss: 0.0316 ||:  80%|########  | 52/65 [09:48<02:27, 11.38s/it]
2022-04-13 19:34:56,290 - INFO - tqdm - accuracy: 0.9876, batch_loss: 0.0030, loss: 0.0310 ||:  82%|########1 | 53/65 [10:00<02:16, 11.38s/it]
2022-04-13 19:35:07,672 - INFO - tqdm - accuracy: 0.9873, batch_loss: 0.0344, loss: 0.0311 ||:  83%|########3 | 54/65 [10:11<02:05, 11.38s/it]
2022-04-13 19:35:19,036 - INFO - tqdm - accuracy: 0.9869, batch_loss: 0.0844, loss: 0.0321 ||:  85%|########4 | 55/65 [10:22<01:53, 11.38s/it]
2022-04-13 19:35:30,419 - INFO - tqdm - accuracy: 0.9866, batch_loss: 0.1006, loss: 0.0333 ||:  86%|########6 | 56/65 [10:34<01:42, 11.38s/it]
2022-04-13 19:35:41,810 - INFO - tqdm - accuracy: 0.9868, batch_loss: 0.0121, loss: 0.0329 ||:  88%|########7 | 57/65 [10:45<01:31, 11.38s/it]
2022-04-13 19:35:53,202 - INFO - tqdm - accuracy: 0.9871, batch_loss: 0.0020, loss: 0.0324 ||:  89%|########9 | 58/65 [10:57<01:19, 11.38s/it]
2022-04-13 19:36:04,575 - INFO - tqdm - accuracy: 0.9873, batch_loss: 0.0027, loss: 0.0319 ||:  91%|######### | 59/65 [11:08<01:08, 11.38s/it]
2022-04-13 19:36:15,595 - INFO - tqdm - accuracy: 0.9875, batch_loss: 0.0034, loss: 0.0314 ||:  92%|#########2| 60/65 [11:19<00:56, 11.27s/it]
2022-04-13 19:36:26,963 - INFO - tqdm - accuracy: 0.9872, batch_loss: 0.0693, loss: 0.0320 ||:  94%|#########3| 61/65 [11:30<00:45, 11.30s/it]
2022-04-13 19:36:38,349 - INFO - tqdm - accuracy: 0.9869, batch_loss: 0.0491, loss: 0.0323 ||:  95%|#########5| 62/65 [11:42<00:33, 11.33s/it]
2022-04-13 19:36:49,718 - INFO - tqdm - accuracy: 0.9866, batch_loss: 0.0945, loss: 0.0333 ||:  97%|#########6| 63/65 [11:53<00:22, 11.34s/it]
2022-04-13 19:37:01,107 - INFO - tqdm - accuracy: 0.9868, batch_loss: 0.0128, loss: 0.0330 ||:  98%|#########8| 64/65 [12:05<00:11, 11.35s/it]
2022-04-13 19:37:06,043 - INFO - tqdm - accuracy: 0.9869, batch_loss: 0.0018, loss: 0.0325 ||: 100%|##########| 65/65 [12:09<00:00,  9.43s/it]
2022-04-13 19:37:06,043 - INFO - tqdm - accuracy: 0.9869, batch_loss: 0.0018, loss: 0.0325 ||: 100%|##########| 65/65 [12:09<00:00, 11.23s/it]
2022-04-13 19:37:08,908 - INFO - allennlp.training.trainer - Validating
2022-04-13 19:37:08,910 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-13 19:37:19,144 - INFO - tqdm - accuracy: 0.5976, batch_loss: 4.9796, loss: 2.0740 ||:  36%|###5      | 41/115 [00:10<00:18,  4.00it/s]
2022-04-13 19:37:29,387 - INFO - tqdm - accuracy: 0.6098, batch_loss: 3.8869, loss: 2.1011 ||:  71%|#######1  | 82/115 [00:20<00:08,  3.98it/s]
2022-04-13 19:37:37,427 - INFO - tqdm - accuracy: 0.6201, batch_loss: 0.0139, loss: 1.9528 ||: 100%|##########| 115/115 [00:28<00:00,  4.07it/s]
2022-04-13 19:37:37,427 - INFO - tqdm - accuracy: 0.6201, batch_loss: 0.0139, loss: 1.9528 ||: 100%|##########| 115/115 [00:28<00:00,  4.03it/s]
2022-04-13 19:37:37,428 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-13 19:37:37,429 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.987  |     0.620
2022-04-13 19:37:37,429 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-13 19:37:37,430 - INFO - allennlp.training.tensorboard_writer - loss               |     0.032  |     1.953
2022-04-13 19:37:37,430 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5802.840  |       N/A
2022-04-13 19:37:43,526 - INFO - allennlp.training.trainer - Epoch duration: 0:12:47.446969
2022-04-13 19:37:43,526 - INFO - allennlp.training.trainer - Estimated training time remaining: 1:17:32
2022-04-13 19:37:43,526 - INFO - allennlp.training.trainer - Epoch 9/14
2022-04-13 19:37:43,526 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-13 19:37:43,526 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-13 19:37:43,528 - INFO - allennlp.training.trainer - Training
2022-04-13 19:37:43,528 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-13 19:37:54,690 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.0319, loss: 0.0319 ||:   2%|1         | 1/65 [00:11<11:54, 11.16s/it]
2022-04-13 19:38:05,910 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0021, loss: 0.0170 ||:   3%|3         | 2/65 [00:22<11:45, 11.20s/it]
2022-04-13 19:38:17,204 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.0131, loss: 0.0157 ||:   5%|4         | 3/65 [00:33<11:36, 11.24s/it]
2022-04-13 19:38:28,346 - INFO - tqdm - accuracy: 0.9922, batch_loss: 0.0029, loss: 0.0125 ||:   6%|6         | 4/65 [00:44<11:23, 11.20s/it]
2022-04-13 19:38:39,633 - INFO - tqdm - accuracy: 0.9938, batch_loss: 0.0108, loss: 0.0122 ||:   8%|7         | 5/65 [00:56<11:13, 11.23s/it]
2022-04-13 19:38:50,914 - INFO - tqdm - accuracy: 0.9948, batch_loss: 0.0064, loss: 0.0112 ||:   9%|9         | 6/65 [01:07<11:03, 11.25s/it]
2022-04-13 19:39:02,230 - INFO - tqdm - accuracy: 0.9955, batch_loss: 0.0112, loss: 0.0112 ||:  11%|#         | 7/65 [01:18<10:53, 11.27s/it]
2022-04-13 19:39:13,506 - INFO - tqdm - accuracy: 0.9961, batch_loss: 0.0057, loss: 0.0105 ||:  12%|#2        | 8/65 [01:29<10:42, 11.27s/it]
2022-04-13 19:39:24,802 - INFO - tqdm - accuracy: 0.9965, batch_loss: 0.0015, loss: 0.0095 ||:  14%|#3        | 9/65 [01:41<10:31, 11.28s/it]
2022-04-13 19:39:36,085 - INFO - tqdm - accuracy: 0.9969, batch_loss: 0.0016, loss: 0.0087 ||:  15%|#5        | 10/65 [01:52<10:20, 11.28s/it]
2022-04-13 19:39:47,379 - INFO - tqdm - accuracy: 0.9972, batch_loss: 0.0041, loss: 0.0083 ||:  17%|#6        | 11/65 [02:03<10:09, 11.28s/it]
2022-04-13 19:39:58,696 - INFO - tqdm - accuracy: 0.9974, batch_loss: 0.0049, loss: 0.0080 ||:  18%|#8        | 12/65 [02:15<09:58, 11.29s/it]
2022-04-13 19:40:10,006 - INFO - tqdm - accuracy: 0.9976, batch_loss: 0.0030, loss: 0.0076 ||:  20%|##        | 13/65 [02:26<09:47, 11.30s/it]
2022-04-13 19:40:21,309 - INFO - tqdm - accuracy: 0.9978, batch_loss: 0.0025, loss: 0.0073 ||:  22%|##1       | 14/65 [02:37<09:36, 11.30s/it]
2022-04-13 19:40:33,047 - INFO - tqdm - accuracy: 0.9979, batch_loss: 0.0019, loss: 0.0069 ||:  23%|##3       | 15/65 [02:49<09:31, 11.43s/it]
2022-04-13 19:40:44,319 - INFO - tqdm - accuracy: 0.9980, batch_loss: 0.0021, loss: 0.0066 ||:  25%|##4       | 16/65 [03:00<09:17, 11.38s/it]
2022-04-13 19:40:55,629 - INFO - tqdm - accuracy: 0.9982, batch_loss: 0.0025, loss: 0.0064 ||:  26%|##6       | 17/65 [03:12<09:05, 11.36s/it]
2022-04-13 19:41:06,926 - INFO - tqdm - accuracy: 0.9983, batch_loss: 0.0181, loss: 0.0070 ||:  28%|##7       | 18/65 [03:23<08:53, 11.34s/it]
2022-04-13 19:41:18,243 - INFO - tqdm - accuracy: 0.9984, batch_loss: 0.0151, loss: 0.0074 ||:  29%|##9       | 19/65 [03:34<08:41, 11.33s/it]
2022-04-13 19:41:29,568 - INFO - tqdm - accuracy: 0.9984, batch_loss: 0.0207, loss: 0.0081 ||:  31%|###       | 20/65 [03:46<08:29, 11.33s/it]
2022-04-13 19:41:40,869 - INFO - tqdm - accuracy: 0.9985, batch_loss: 0.0120, loss: 0.0083 ||:  32%|###2      | 21/65 [03:57<08:18, 11.32s/it]
2022-04-13 19:41:52,169 - INFO - tqdm - accuracy: 0.9986, batch_loss: 0.0035, loss: 0.0081 ||:  34%|###3      | 22/65 [04:08<08:06, 11.32s/it]
2022-04-13 19:42:03,502 - INFO - tqdm - accuracy: 0.9986, batch_loss: 0.0020, loss: 0.0078 ||:  35%|###5      | 23/65 [04:19<07:55, 11.32s/it]
2022-04-13 19:42:14,837 - INFO - tqdm - accuracy: 0.9974, batch_loss: 0.0374, loss: 0.0090 ||:  37%|###6      | 24/65 [04:31<07:44, 11.33s/it]
2022-04-13 19:42:26,158 - INFO - tqdm - accuracy: 0.9975, batch_loss: 0.0177, loss: 0.0094 ||:  38%|###8      | 25/65 [04:42<07:32, 11.32s/it]
2022-04-13 19:42:37,483 - INFO - tqdm - accuracy: 0.9976, batch_loss: 0.0022, loss: 0.0091 ||:  40%|####      | 26/65 [04:53<07:21, 11.32s/it]
2022-04-13 19:42:48,821 - INFO - tqdm - accuracy: 0.9977, batch_loss: 0.0020, loss: 0.0089 ||:  42%|####1     | 27/65 [05:05<07:10, 11.33s/it]
2022-04-13 19:43:00,183 - INFO - tqdm - accuracy: 0.9967, batch_loss: 0.1680, loss: 0.0145 ||:  43%|####3     | 28/65 [05:16<06:59, 11.34s/it]
2022-04-13 19:43:11,548 - INFO - tqdm - accuracy: 0.9968, batch_loss: 0.0063, loss: 0.0143 ||:  45%|####4     | 29/65 [05:28<06:48, 11.35s/it]
2022-04-13 19:43:22,769 - INFO - tqdm - accuracy: 0.9969, batch_loss: 0.0046, loss: 0.0139 ||:  46%|####6     | 30/65 [05:39<06:35, 11.31s/it]
2022-04-13 19:43:34,146 - INFO - tqdm - accuracy: 0.9970, batch_loss: 0.0086, loss: 0.0138 ||:  48%|####7     | 31/65 [05:50<06:25, 11.33s/it]
2022-04-13 19:43:45,490 - INFO - tqdm - accuracy: 0.9961, batch_loss: 0.1539, loss: 0.0181 ||:  49%|####9     | 32/65 [06:01<06:14, 11.33s/it]
2022-04-13 19:43:56,848 - INFO - tqdm - accuracy: 0.9953, batch_loss: 0.0604, loss: 0.0194 ||:  51%|#####     | 33/65 [06:13<06:02, 11.34s/it]
2022-04-13 19:44:08,221 - INFO - tqdm - accuracy: 0.9954, batch_loss: 0.0091, loss: 0.0191 ||:  52%|#####2    | 34/65 [06:24<05:51, 11.35s/it]
2022-04-13 19:44:19,231 - INFO - tqdm - accuracy: 0.9955, batch_loss: 0.0018, loss: 0.0186 ||:  54%|#####3    | 35/65 [06:35<05:37, 11.25s/it]
2022-04-13 19:44:30,599 - INFO - tqdm - accuracy: 0.9957, batch_loss: 0.0012, loss: 0.0181 ||:  55%|#####5    | 36/65 [06:47<05:27, 11.28s/it]
2022-04-13 19:44:41,958 - INFO - tqdm - accuracy: 0.9958, batch_loss: 0.0044, loss: 0.0178 ||:  57%|#####6    | 37/65 [06:58<05:16, 11.31s/it]
2022-04-13 19:44:53,332 - INFO - tqdm - accuracy: 0.9959, batch_loss: 0.0158, loss: 0.0177 ||:  58%|#####8    | 38/65 [07:09<05:05, 11.33s/it]
2022-04-13 19:45:04,695 - INFO - tqdm - accuracy: 0.9960, batch_loss: 0.0032, loss: 0.0173 ||:  60%|######    | 39/65 [07:21<04:54, 11.34s/it]
2022-04-13 19:45:16,077 - INFO - tqdm - accuracy: 0.9961, batch_loss: 0.0013, loss: 0.0169 ||:  62%|######1   | 40/65 [07:32<04:43, 11.35s/it]
2022-04-13 19:45:27,483 - INFO - tqdm - accuracy: 0.9962, batch_loss: 0.0113, loss: 0.0168 ||:  63%|######3   | 41/65 [07:43<04:32, 11.37s/it]
2022-04-13 19:45:38,917 - INFO - tqdm - accuracy: 0.9963, batch_loss: 0.0136, loss: 0.0167 ||:  65%|######4   | 42/65 [07:55<04:21, 11.39s/it]
2022-04-13 19:45:50,366 - INFO - tqdm - accuracy: 0.9964, batch_loss: 0.0012, loss: 0.0164 ||:  66%|######6   | 43/65 [08:06<04:10, 11.41s/it]
2022-04-13 19:46:01,811 - INFO - tqdm - accuracy: 0.9957, batch_loss: 0.1379, loss: 0.0191 ||:  68%|######7   | 44/65 [08:18<03:59, 11.42s/it]
2022-04-13 19:46:13,274 - INFO - tqdm - accuracy: 0.9951, batch_loss: 0.0820, loss: 0.0205 ||:  69%|######9   | 45/65 [08:29<03:48, 11.43s/it]
2022-04-13 19:46:24,710 - INFO - tqdm - accuracy: 0.9952, batch_loss: 0.0101, loss: 0.0203 ||:  71%|#######   | 46/65 [08:41<03:37, 11.43s/it]
2022-04-13 19:46:36,152 - INFO - tqdm - accuracy: 0.9953, batch_loss: 0.0083, loss: 0.0200 ||:  72%|#######2  | 47/65 [08:52<03:25, 11.44s/it]
2022-04-13 19:46:47,572 - INFO - tqdm - accuracy: 0.9954, batch_loss: 0.0218, loss: 0.0201 ||:  74%|#######3  | 48/65 [09:04<03:14, 11.43s/it]
2022-04-13 19:46:58,984 - INFO - tqdm - accuracy: 0.9955, batch_loss: 0.0071, loss: 0.0198 ||:  75%|#######5  | 49/65 [09:15<03:02, 11.43s/it]
2022-04-13 19:47:10,120 - INFO - tqdm - accuracy: 0.9956, batch_loss: 0.0058, loss: 0.0195 ||:  77%|#######6  | 50/65 [09:26<02:50, 11.34s/it]
2022-04-13 19:47:21,502 - INFO - tqdm - accuracy: 0.9957, batch_loss: 0.0066, loss: 0.0193 ||:  78%|#######8  | 51/65 [09:37<02:38, 11.35s/it]
2022-04-13 19:47:32,856 - INFO - tqdm - accuracy: 0.9958, batch_loss: 0.0037, loss: 0.0190 ||:  80%|########  | 52/65 [09:49<02:27, 11.35s/it]
2022-04-13 19:47:44,208 - INFO - tqdm - accuracy: 0.9953, batch_loss: 0.0527, loss: 0.0196 ||:  82%|########1 | 53/65 [10:00<02:16, 11.35s/it]
2022-04-13 19:47:55,582 - INFO - tqdm - accuracy: 0.9954, batch_loss: 0.0038, loss: 0.0193 ||:  83%|########3 | 54/65 [10:12<02:04, 11.36s/it]
2022-04-13 19:48:06,966 - INFO - tqdm - accuracy: 0.9955, batch_loss: 0.0040, loss: 0.0190 ||:  85%|########4 | 55/65 [10:23<01:53, 11.37s/it]
2022-04-13 19:48:18,349 - INFO - tqdm - accuracy: 0.9955, batch_loss: 0.0104, loss: 0.0189 ||:  86%|########6 | 56/65 [10:34<01:42, 11.37s/it]
2022-04-13 19:48:29,724 - INFO - tqdm - accuracy: 0.9956, batch_loss: 0.0203, loss: 0.0189 ||:  88%|########7 | 57/65 [10:46<01:30, 11.37s/it]
2022-04-13 19:48:41,101 - INFO - tqdm - accuracy: 0.9951, batch_loss: 0.0739, loss: 0.0199 ||:  89%|########9 | 58/65 [10:57<01:19, 11.37s/it]
2022-04-13 19:48:52,448 - INFO - tqdm - accuracy: 0.9947, batch_loss: 0.0343, loss: 0.0201 ||:  91%|######### | 59/65 [11:08<01:08, 11.37s/it]
2022-04-13 19:49:03,817 - INFO - tqdm - accuracy: 0.9948, batch_loss: 0.0026, loss: 0.0198 ||:  92%|#########2| 60/65 [11:20<00:56, 11.37s/it]
2022-04-13 19:49:15,188 - INFO - tqdm - accuracy: 0.9949, batch_loss: 0.0017, loss: 0.0195 ||:  94%|#########3| 61/65 [11:31<00:45, 11.37s/it]
2022-04-13 19:49:26,573 - INFO - tqdm - accuracy: 0.9950, batch_loss: 0.0140, loss: 0.0194 ||:  95%|#########5| 62/65 [11:43<00:34, 11.37s/it]
2022-04-13 19:49:37,957 - INFO - tqdm - accuracy: 0.9950, batch_loss: 0.0047, loss: 0.0192 ||:  97%|#########6| 63/65 [11:54<00:22, 11.38s/it]
2022-04-13 19:49:49,324 - INFO - tqdm - accuracy: 0.9951, batch_loss: 0.0054, loss: 0.0190 ||:  98%|#########8| 64/65 [12:05<00:11, 11.37s/it]
2022-04-13 19:49:54,360 - INFO - tqdm - accuracy: 0.9951, batch_loss: 0.0012, loss: 0.0187 ||: 100%|##########| 65/65 [12:10<00:00,  9.47s/it]
2022-04-13 19:49:54,361 - INFO - tqdm - accuracy: 0.9951, batch_loss: 0.0012, loss: 0.0187 ||: 100%|##########| 65/65 [12:10<00:00, 11.24s/it]
2022-04-13 19:49:57,227 - INFO - allennlp.training.trainer - Validating
2022-04-13 19:49:57,228 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-13 19:50:07,236 - INFO - tqdm - accuracy: 0.6049, batch_loss: 3.9213, loss: 2.2063 ||:  36%|###5      | 41/115 [00:10<00:18,  4.03it/s]
2022-04-13 19:50:17,461 - INFO - tqdm - accuracy: 0.6258, batch_loss: 0.0006, loss: 2.0856 ||:  71%|#######1  | 82/115 [00:20<00:08,  4.01it/s]
2022-04-13 19:50:25,684 - INFO - tqdm - accuracy: 0.6288, batch_loss: 0.2115, loss: 2.0178 ||: 100%|##########| 115/115 [00:28<00:00,  4.01it/s]
2022-04-13 19:50:25,685 - INFO - tqdm - accuracy: 0.6288, batch_loss: 0.2115, loss: 2.0178 ||: 100%|##########| 115/115 [00:28<00:00,  4.04it/s]
2022-04-13 19:50:25,685 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-13 19:50:25,685 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.995  |     0.629
2022-04-13 19:50:25,686 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-13 19:50:25,687 - INFO - allennlp.training.tensorboard_writer - loss               |     0.019  |     2.018
2022-04-13 19:50:25,687 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5802.840  |       N/A
2022-04-13 19:50:31,646 - INFO - allennlp.training.trainer - Epoch duration: 0:12:48.120444
2022-04-13 19:50:31,647 - INFO - allennlp.training.trainer - Estimated training time remaining: 1:04:33
2022-04-13 19:50:31,647 - INFO - allennlp.training.trainer - Epoch 10/14
2022-04-13 19:50:31,647 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-13 19:50:31,647 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-13 19:50:31,648 - INFO - allennlp.training.trainer - Training
2022-04-13 19:50:31,649 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-13 19:50:42,723 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0011, loss: 0.0011 ||:   2%|1         | 1/65 [00:11<11:48, 11.07s/it]
2022-04-13 19:50:53,614 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0451, loss: 0.0231 ||:   3%|3         | 2/65 [00:21<11:30, 10.97s/it]
2022-04-13 19:51:04,790 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.0035, loss: 0.0166 ||:   5%|4         | 3/65 [00:33<11:25, 11.06s/it]
2022-04-13 19:51:15,991 - INFO - tqdm - accuracy: 0.9922, batch_loss: 0.0040, loss: 0.0134 ||:   6%|6         | 4/65 [00:44<11:18, 11.12s/it]
2022-04-13 19:51:27,094 - INFO - tqdm - accuracy: 0.9938, batch_loss: 0.0026, loss: 0.0113 ||:   8%|7         | 5/65 [00:55<11:06, 11.11s/it]
2022-04-13 19:51:38,340 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.1283, loss: 0.0308 ||:   9%|9         | 6/65 [01:06<10:58, 11.16s/it]
2022-04-13 19:51:49,609 - INFO - tqdm - accuracy: 0.9911, batch_loss: 0.0261, loss: 0.0301 ||:  11%|#         | 7/65 [01:17<10:49, 11.19s/it]
2022-04-13 19:52:00,871 - INFO - tqdm - accuracy: 0.9922, batch_loss: 0.0026, loss: 0.0267 ||:  12%|#2        | 8/65 [01:29<10:39, 11.22s/it]
2022-04-13 19:52:12,148 - INFO - tqdm - accuracy: 0.9931, batch_loss: 0.0047, loss: 0.0242 ||:  14%|#3        | 9/65 [01:40<10:29, 11.23s/it]
2022-04-13 19:52:23,445 - INFO - tqdm - accuracy: 0.9938, batch_loss: 0.0054, loss: 0.0223 ||:  15%|#5        | 10/65 [01:51<10:18, 11.25s/it]
2022-04-13 19:52:34,745 - INFO - tqdm - accuracy: 0.9943, batch_loss: 0.0072, loss: 0.0210 ||:  17%|#6        | 11/65 [02:03<10:08, 11.27s/it]
2022-04-13 19:52:46,063 - INFO - tqdm - accuracy: 0.9948, batch_loss: 0.0113, loss: 0.0202 ||:  18%|#8        | 12/65 [02:14<09:58, 11.28s/it]
2022-04-13 19:52:57,379 - INFO - tqdm - accuracy: 0.9952, batch_loss: 0.0019, loss: 0.0188 ||:  20%|##        | 13/65 [02:25<09:47, 11.29s/it]
2022-04-13 19:53:08,703 - INFO - tqdm - accuracy: 0.9955, batch_loss: 0.0138, loss: 0.0184 ||:  22%|##1       | 14/65 [02:37<09:36, 11.30s/it]
2022-04-13 19:53:20,022 - INFO - tqdm - accuracy: 0.9958, batch_loss: 0.0019, loss: 0.0173 ||:  23%|##3       | 15/65 [02:48<09:25, 11.31s/it]
2022-04-13 19:53:31,333 - INFO - tqdm - accuracy: 0.9961, batch_loss: 0.0030, loss: 0.0164 ||:  25%|##4       | 16/65 [02:59<09:14, 11.31s/it]
2022-04-13 19:53:42,647 - INFO - tqdm - accuracy: 0.9963, batch_loss: 0.0013, loss: 0.0155 ||:  26%|##6       | 17/65 [03:10<09:02, 11.31s/it]
2022-04-13 19:53:53,990 - INFO - tqdm - accuracy: 0.9965, batch_loss: 0.0032, loss: 0.0148 ||:  28%|##7       | 18/65 [03:22<08:52, 11.32s/it]
2022-04-13 19:54:05,334 - INFO - tqdm - accuracy: 0.9967, batch_loss: 0.0041, loss: 0.0143 ||:  29%|##9       | 19/65 [03:33<08:41, 11.33s/it]
2022-04-13 19:54:16,652 - INFO - tqdm - accuracy: 0.9969, batch_loss: 0.0172, loss: 0.0144 ||:  31%|###       | 20/65 [03:45<08:29, 11.32s/it]
2022-04-13 19:54:27,972 - INFO - tqdm - accuracy: 0.9955, batch_loss: 0.0368, loss: 0.0155 ||:  32%|###2      | 21/65 [03:56<08:18, 11.32s/it]
2022-04-13 19:54:39,321 - INFO - tqdm - accuracy: 0.9943, batch_loss: 0.0301, loss: 0.0162 ||:  34%|###3      | 22/65 [04:07<08:07, 11.33s/it]
2022-04-13 19:54:50,349 - INFO - tqdm - accuracy: 0.9946, batch_loss: 0.0082, loss: 0.0158 ||:  35%|###5      | 23/65 [04:18<07:52, 11.24s/it]
2022-04-13 19:55:01,704 - INFO - tqdm - accuracy: 0.9948, batch_loss: 0.0025, loss: 0.0153 ||:  37%|###6      | 24/65 [04:30<07:42, 11.27s/it]
2022-04-13 19:55:13,123 - INFO - tqdm - accuracy: 0.9937, batch_loss: 0.0601, loss: 0.0170 ||:  38%|###8      | 25/65 [04:41<07:32, 11.32s/it]
2022-04-13 19:55:24,428 - INFO - tqdm - accuracy: 0.9928, batch_loss: 0.0365, loss: 0.0178 ||:  40%|####      | 26/65 [04:52<07:21, 11.31s/it]
2022-04-13 19:55:35,863 - INFO - tqdm - accuracy: 0.9930, batch_loss: 0.0046, loss: 0.0173 ||:  42%|####1     | 27/65 [05:04<07:11, 11.35s/it]
2022-04-13 19:55:47,294 - INFO - tqdm - accuracy: 0.9933, batch_loss: 0.0109, loss: 0.0171 ||:  43%|####3     | 28/65 [05:15<07:00, 11.37s/it]
2022-04-13 19:55:58,700 - INFO - tqdm - accuracy: 0.9935, batch_loss: 0.0140, loss: 0.0170 ||:  45%|####4     | 29/65 [05:27<06:49, 11.38s/it]
2022-04-13 19:56:10,120 - INFO - tqdm - accuracy: 0.9937, batch_loss: 0.0009, loss: 0.0164 ||:  46%|####6     | 30/65 [05:38<06:38, 11.39s/it]
2022-04-13 19:56:21,534 - INFO - tqdm - accuracy: 0.9939, batch_loss: 0.0153, loss: 0.0164 ||:  48%|####7     | 31/65 [05:49<06:27, 11.40s/it]
2022-04-13 19:56:32,949 - INFO - tqdm - accuracy: 0.9941, batch_loss: 0.0023, loss: 0.0160 ||:  49%|####9     | 32/65 [06:01<06:16, 11.40s/it]
2022-04-13 19:56:44,344 - INFO - tqdm - accuracy: 0.9943, batch_loss: 0.0029, loss: 0.0156 ||:  51%|#####     | 33/65 [06:12<06:04, 11.40s/it]
2022-04-13 19:56:55,737 - INFO - tqdm - accuracy: 0.9945, batch_loss: 0.0018, loss: 0.0152 ||:  52%|#####2    | 34/65 [06:24<05:53, 11.40s/it]
2022-04-13 19:57:07,133 - INFO - tqdm - accuracy: 0.9937, batch_loss: 0.0410, loss: 0.0159 ||:  54%|#####3    | 35/65 [06:35<05:41, 11.40s/it]
2022-04-13 19:57:18,540 - INFO - tqdm - accuracy: 0.9939, batch_loss: 0.0146, loss: 0.0159 ||:  55%|#####5    | 36/65 [06:46<05:30, 11.40s/it]
2022-04-13 19:57:29,930 - INFO - tqdm - accuracy: 0.9941, batch_loss: 0.0009, loss: 0.0155 ||:  57%|#####6    | 37/65 [06:58<05:19, 11.40s/it]
2022-04-13 19:57:41,315 - INFO - tqdm - accuracy: 0.9942, batch_loss: 0.0049, loss: 0.0152 ||:  58%|#####8    | 38/65 [07:09<05:07, 11.39s/it]
2022-04-13 19:57:52,703 - INFO - tqdm - accuracy: 0.9944, batch_loss: 0.0016, loss: 0.0148 ||:  60%|######    | 39/65 [07:21<04:56, 11.39s/it]
2022-04-13 19:58:04,075 - INFO - tqdm - accuracy: 0.9945, batch_loss: 0.0071, loss: 0.0146 ||:  62%|######1   | 40/65 [07:32<04:44, 11.39s/it]
2022-04-13 19:58:15,469 - INFO - tqdm - accuracy: 0.9947, batch_loss: 0.0008, loss: 0.0143 ||:  63%|######3   | 41/65 [07:43<04:33, 11.39s/it]
2022-04-13 19:58:26,870 - INFO - tqdm - accuracy: 0.9940, batch_loss: 0.0487, loss: 0.0151 ||:  65%|######4   | 42/65 [07:55<04:22, 11.39s/it]
2022-04-13 19:58:38,262 - INFO - tqdm - accuracy: 0.9942, batch_loss: 0.0030, loss: 0.0148 ||:  66%|######6   | 43/65 [08:06<04:10, 11.39s/it]
2022-04-13 19:58:49,657 - INFO - tqdm - accuracy: 0.9943, batch_loss: 0.0039, loss: 0.0146 ||:  68%|######7   | 44/65 [08:18<03:59, 11.39s/it]
2022-04-13 19:59:01,012 - INFO - tqdm - accuracy: 0.9937, batch_loss: 0.1116, loss: 0.0167 ||:  69%|######9   | 45/65 [08:29<03:47, 11.38s/it]
2022-04-13 19:59:12,401 - INFO - tqdm - accuracy: 0.9939, batch_loss: 0.0022, loss: 0.0164 ||:  71%|#######   | 46/65 [08:40<03:36, 11.38s/it]
2022-04-13 19:59:23,781 - INFO - tqdm - accuracy: 0.9940, batch_loss: 0.0309, loss: 0.0167 ||:  72%|#######2  | 47/65 [08:52<03:24, 11.38s/it]
2022-04-13 19:59:35,186 - INFO - tqdm - accuracy: 0.9941, batch_loss: 0.0017, loss: 0.0164 ||:  74%|#######3  | 48/65 [09:03<03:13, 11.39s/it]
2022-04-13 19:59:46,553 - INFO - tqdm - accuracy: 0.9943, batch_loss: 0.0013, loss: 0.0161 ||:  75%|#######5  | 49/65 [09:14<03:02, 11.38s/it]
2022-04-13 19:59:58,340 - INFO - tqdm - accuracy: 0.9944, batch_loss: 0.0023, loss: 0.0158 ||:  77%|#######6  | 50/65 [09:26<02:52, 11.50s/it]
2022-04-13 20:00:09,671 - INFO - tqdm - accuracy: 0.9945, batch_loss: 0.0008, loss: 0.0155 ||:  78%|#######8  | 51/65 [09:38<02:40, 11.45s/it]
2022-04-13 20:00:21,033 - INFO - tqdm - accuracy: 0.9946, batch_loss: 0.0049, loss: 0.0153 ||:  80%|########  | 52/65 [09:49<02:28, 11.42s/it]
2022-04-13 20:00:32,410 - INFO - tqdm - accuracy: 0.9947, batch_loss: 0.0023, loss: 0.0151 ||:  82%|########1 | 53/65 [10:00<02:16, 11.41s/it]
2022-04-13 20:00:43,789 - INFO - tqdm - accuracy: 0.9948, batch_loss: 0.0045, loss: 0.0149 ||:  83%|########3 | 54/65 [10:12<02:05, 11.40s/it]
2022-04-13 20:00:55,157 - INFO - tqdm - accuracy: 0.9949, batch_loss: 0.0022, loss: 0.0147 ||:  85%|########4 | 55/65 [10:23<01:53, 11.39s/it]
2022-04-13 20:01:06,539 - INFO - tqdm - accuracy: 0.9944, batch_loss: 0.1201, loss: 0.0166 ||:  86%|########6 | 56/65 [10:34<01:42, 11.39s/it]
2022-04-13 20:01:17,923 - INFO - tqdm - accuracy: 0.9940, batch_loss: 0.0271, loss: 0.0167 ||:  88%|########7 | 57/65 [10:46<01:31, 11.39s/it]
2022-04-13 20:01:29,290 - INFO - tqdm - accuracy: 0.9941, batch_loss: 0.0092, loss: 0.0166 ||:  89%|########9 | 58/65 [10:57<01:19, 11.38s/it]
2022-04-13 20:01:40,666 - INFO - tqdm - accuracy: 0.9942, batch_loss: 0.0068, loss: 0.0164 ||:  91%|######### | 59/65 [11:09<01:08, 11.38s/it]
2022-04-13 20:01:52,032 - INFO - tqdm - accuracy: 0.9943, batch_loss: 0.0153, loss: 0.0164 ||:  92%|#########2| 60/65 [11:20<00:56, 11.38s/it]
2022-04-13 20:02:03,405 - INFO - tqdm - accuracy: 0.9944, batch_loss: 0.0015, loss: 0.0162 ||:  94%|#########3| 61/65 [11:31<00:45, 11.37s/it]
2022-04-13 20:02:14,805 - INFO - tqdm - accuracy: 0.9945, batch_loss: 0.0027, loss: 0.0160 ||:  95%|#########5| 62/65 [11:43<00:34, 11.38s/it]
2022-04-13 20:02:26,222 - INFO - tqdm - accuracy: 0.9945, batch_loss: 0.0045, loss: 0.0158 ||:  97%|#########6| 63/65 [11:54<00:22, 11.39s/it]
2022-04-13 20:02:37,642 - INFO - tqdm - accuracy: 0.9941, batch_loss: 0.0597, loss: 0.0165 ||:  98%|#########8| 64/65 [12:05<00:11, 11.40s/it]
2022-04-13 20:02:42,698 - INFO - tqdm - accuracy: 0.9942, batch_loss: 0.0013, loss: 0.0162 ||: 100%|##########| 65/65 [12:11<00:00,  9.50s/it]
2022-04-13 20:02:42,699 - INFO - tqdm - accuracy: 0.9942, batch_loss: 0.0013, loss: 0.0162 ||: 100%|##########| 65/65 [12:11<00:00, 11.25s/it]
2022-04-13 20:02:45,577 - INFO - allennlp.training.trainer - Validating
2022-04-13 20:02:45,579 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-13 20:02:55,625 - INFO - tqdm - accuracy: 0.6625, batch_loss: 0.0001, loss: 1.8253 ||:  35%|###4      | 40/115 [00:10<00:18,  4.01it/s]
2022-04-13 20:03:05,685 - INFO - tqdm - accuracy: 0.6273, batch_loss: 3.1276, loss: 2.1107 ||:  70%|#######   | 81/115 [00:20<00:08,  4.00it/s]
2022-04-13 20:03:14,206 - INFO - tqdm - accuracy: 0.6419, batch_loss: 4.4309, loss: 2.0319 ||: 100%|##########| 115/115 [00:28<00:00,  4.00it/s]
2022-04-13 20:03:14,207 - INFO - tqdm - accuracy: 0.6419, batch_loss: 4.4309, loss: 2.0319 ||: 100%|##########| 115/115 [00:28<00:00,  4.02it/s]
2022-04-13 20:03:14,207 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-13 20:03:14,207 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.994  |     0.642
2022-04-13 20:03:14,208 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-13 20:03:14,209 - INFO - allennlp.training.tensorboard_writer - loss               |     0.016  |     2.032
2022-04-13 20:03:14,210 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5802.840  |       N/A
2022-04-13 20:03:20,296 - INFO - allennlp.training.trainer - Epoch duration: 0:12:48.649336
2022-04-13 20:03:20,296 - INFO - allennlp.training.trainer - Estimated training time remaining: 0:51:36
2022-04-13 20:03:20,297 - INFO - allennlp.training.trainer - Epoch 11/14
2022-04-13 20:03:20,297 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-13 20:03:20,297 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-13 20:03:20,298 - INFO - allennlp.training.trainer - Training
2022-04-13 20:03:20,298 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-13 20:03:31,457 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0006, loss: 0.0006 ||:   2%|1         | 1/65 [00:11<11:54, 11.16s/it]
2022-04-13 20:03:42,681 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0384, loss: 0.0195 ||:   3%|3         | 2/65 [00:22<11:45, 11.20s/it]
2022-04-13 20:03:53,915 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.0084, loss: 0.0158 ||:   5%|4         | 3/65 [00:33<11:35, 11.21s/it]
2022-04-13 20:04:05,162 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0969, loss: 0.0361 ||:   6%|6         | 4/65 [00:44<11:24, 11.23s/it]
2022-04-13 20:04:16,420 - INFO - tqdm - accuracy: 0.9875, batch_loss: 0.0018, loss: 0.0292 ||:   8%|7         | 5/65 [00:56<11:14, 11.24s/it]
2022-04-13 20:04:27,702 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.0009, loss: 0.0245 ||:   9%|9         | 6/65 [01:07<11:03, 11.25s/it]
2022-04-13 20:04:38,981 - INFO - tqdm - accuracy: 0.9911, batch_loss: 0.0022, loss: 0.0213 ||:  11%|#         | 7/65 [01:18<10:53, 11.26s/it]
2022-04-13 20:04:50,272 - INFO - tqdm - accuracy: 0.9922, batch_loss: 0.0062, loss: 0.0194 ||:  12%|#2        | 8/65 [01:29<10:42, 11.27s/it]
2022-04-13 20:05:01,580 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.0231, loss: 0.0198 ||:  14%|#3        | 9/65 [01:41<10:31, 11.28s/it]
2022-04-13 20:05:12,886 - INFO - tqdm - accuracy: 0.9906, batch_loss: 0.0099, loss: 0.0188 ||:  15%|#5        | 10/65 [01:52<10:20, 11.29s/it]
2022-04-13 20:05:24,203 - INFO - tqdm - accuracy: 0.9858, batch_loss: 0.1452, loss: 0.0303 ||:  17%|#6        | 11/65 [02:03<10:10, 11.30s/it]
2022-04-13 20:05:35,538 - INFO - tqdm - accuracy: 0.9870, batch_loss: 0.0009, loss: 0.0279 ||:  18%|#8        | 12/65 [02:15<09:59, 11.31s/it]
2022-04-13 20:05:46,876 - INFO - tqdm - accuracy: 0.9880, batch_loss: 0.0200, loss: 0.0273 ||:  20%|##        | 13/65 [02:26<09:48, 11.32s/it]
2022-04-13 20:05:58,228 - INFO - tqdm - accuracy: 0.9888, batch_loss: 0.0034, loss: 0.0256 ||:  22%|##1       | 14/65 [02:37<09:37, 11.33s/it]
2022-04-13 20:06:09,583 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.0041, loss: 0.0241 ||:  23%|##3       | 15/65 [02:49<09:26, 11.34s/it]
2022-04-13 20:06:20,807 - INFO - tqdm - accuracy: 0.9902, batch_loss: 0.0008, loss: 0.0227 ||:  25%|##4       | 16/65 [03:00<09:13, 11.30s/it]
2022-04-13 20:06:32,173 - INFO - tqdm - accuracy: 0.9908, batch_loss: 0.0023, loss: 0.0215 ||:  26%|##6       | 17/65 [03:11<09:03, 11.32s/it]
2022-04-13 20:06:43,523 - INFO - tqdm - accuracy: 0.9913, batch_loss: 0.0024, loss: 0.0204 ||:  28%|##7       | 18/65 [03:23<08:52, 11.33s/it]
2022-04-13 20:06:54,870 - INFO - tqdm - accuracy: 0.9918, batch_loss: 0.0048, loss: 0.0196 ||:  29%|##9       | 19/65 [03:34<08:41, 11.34s/it]
2022-04-13 20:07:06,228 - INFO - tqdm - accuracy: 0.9922, batch_loss: 0.0057, loss: 0.0189 ||:  31%|###       | 20/65 [03:45<08:30, 11.34s/it]
2022-04-13 20:07:17,580 - INFO - tqdm - accuracy: 0.9926, batch_loss: 0.0030, loss: 0.0181 ||:  32%|###2      | 21/65 [03:57<08:19, 11.34s/it]
2022-04-13 20:07:28,942 - INFO - tqdm - accuracy: 0.9929, batch_loss: 0.0035, loss: 0.0175 ||:  34%|###3      | 22/65 [04:08<08:08, 11.35s/it]
2022-04-13 20:07:40,283 - INFO - tqdm - accuracy: 0.9932, batch_loss: 0.0008, loss: 0.0167 ||:  35%|###5      | 23/65 [04:19<07:56, 11.35s/it]
2022-04-13 20:07:51,649 - INFO - tqdm - accuracy: 0.9922, batch_loss: 0.0567, loss: 0.0184 ||:  37%|###6      | 24/65 [04:31<07:45, 11.35s/it]
2022-04-13 20:08:03,019 - INFO - tqdm - accuracy: 0.9925, batch_loss: 0.0056, loss: 0.0179 ||:  38%|###8      | 25/65 [04:42<07:34, 11.36s/it]
2022-04-13 20:08:14,393 - INFO - tqdm - accuracy: 0.9928, batch_loss: 0.0026, loss: 0.0173 ||:  40%|####      | 26/65 [04:54<07:23, 11.36s/it]
2022-04-13 20:08:25,777 - INFO - tqdm - accuracy: 0.9931, batch_loss: 0.0010, loss: 0.0167 ||:  42%|####1     | 27/65 [05:05<07:12, 11.37s/it]
2022-04-13 20:08:37,185 - INFO - tqdm - accuracy: 0.9933, batch_loss: 0.0023, loss: 0.0162 ||:  43%|####3     | 28/65 [05:16<07:01, 11.38s/it]
2022-04-13 20:08:48,598 - INFO - tqdm - accuracy: 0.9935, batch_loss: 0.0018, loss: 0.0157 ||:  45%|####4     | 29/65 [05:28<06:50, 11.39s/it]
2022-04-13 20:09:00,044 - INFO - tqdm - accuracy: 0.9938, batch_loss: 0.0040, loss: 0.0153 ||:  46%|####6     | 30/65 [05:39<06:39, 11.41s/it]
2022-04-13 20:09:11,492 - INFO - tqdm - accuracy: 0.9940, batch_loss: 0.0021, loss: 0.0149 ||:  48%|####7     | 31/65 [05:51<06:28, 11.42s/it]
2022-04-13 20:09:22,932 - INFO - tqdm - accuracy: 0.9941, batch_loss: 0.0007, loss: 0.0144 ||:  49%|####9     | 32/65 [06:02<06:17, 11.43s/it]
2022-04-13 20:09:34,392 - INFO - tqdm - accuracy: 0.9943, batch_loss: 0.0105, loss: 0.0143 ||:  51%|#####     | 33/65 [06:14<06:05, 11.44s/it]
2022-04-13 20:09:45,832 - INFO - tqdm - accuracy: 0.9926, batch_loss: 0.0933, loss: 0.0166 ||:  52%|#####2    | 34/65 [06:25<05:54, 11.44s/it]
2022-04-13 20:09:57,249 - INFO - tqdm - accuracy: 0.9929, batch_loss: 0.0045, loss: 0.0163 ||:  54%|#####3    | 35/65 [06:36<05:42, 11.43s/it]
2022-04-13 20:10:08,685 - INFO - tqdm - accuracy: 0.9931, batch_loss: 0.0018, loss: 0.0159 ||:  55%|#####5    | 36/65 [06:48<05:31, 11.43s/it]
2022-04-13 20:10:20,132 - INFO - tqdm - accuracy: 0.9924, batch_loss: 0.0686, loss: 0.0173 ||:  57%|#####6    | 37/65 [06:59<05:20, 11.44s/it]
2022-04-13 20:10:31,564 - INFO - tqdm - accuracy: 0.9926, batch_loss: 0.0028, loss: 0.0169 ||:  58%|#####8    | 38/65 [07:11<05:08, 11.44s/it]
2022-04-13 20:10:42,986 - INFO - tqdm - accuracy: 0.9928, batch_loss: 0.0177, loss: 0.0169 ||:  60%|######    | 39/65 [07:22<04:57, 11.43s/it]
2022-04-13 20:10:54,413 - INFO - tqdm - accuracy: 0.9930, batch_loss: 0.0095, loss: 0.0168 ||:  62%|######1   | 40/65 [07:34<04:45, 11.43s/it]
2022-04-13 20:11:05,840 - INFO - tqdm - accuracy: 0.9931, batch_loss: 0.0018, loss: 0.0164 ||:  63%|######3   | 41/65 [07:45<04:34, 11.43s/it]
2022-04-13 20:11:17,244 - INFO - tqdm - accuracy: 0.9933, batch_loss: 0.0024, loss: 0.0161 ||:  65%|######4   | 42/65 [07:56<04:22, 11.42s/it]
2022-04-13 20:11:28,512 - INFO - tqdm - accuracy: 0.9935, batch_loss: 0.0065, loss: 0.0158 ||:  66%|######6   | 43/65 [08:08<04:10, 11.38s/it]
2022-04-13 20:11:39,925 - INFO - tqdm - accuracy: 0.9936, batch_loss: 0.0040, loss: 0.0156 ||:  68%|######7   | 44/65 [08:19<03:59, 11.39s/it]
2022-04-13 20:11:50,989 - INFO - tqdm - accuracy: 0.9931, batch_loss: 0.0917, loss: 0.0173 ||:  69%|######9   | 45/65 [08:30<03:45, 11.29s/it]
2022-04-13 20:12:02,414 - INFO - tqdm - accuracy: 0.9932, batch_loss: 0.0131, loss: 0.0172 ||:  71%|#######   | 46/65 [08:42<03:35, 11.33s/it]
2022-04-13 20:12:13,819 - INFO - tqdm - accuracy: 0.9927, batch_loss: 0.1242, loss: 0.0194 ||:  72%|#######2  | 47/65 [08:53<03:24, 11.35s/it]
2022-04-13 20:12:25,235 - INFO - tqdm - accuracy: 0.9928, batch_loss: 0.0013, loss: 0.0191 ||:  74%|#######3  | 48/65 [09:04<03:13, 11.37s/it]
2022-04-13 20:12:36,629 - INFO - tqdm - accuracy: 0.9930, batch_loss: 0.0025, loss: 0.0187 ||:  75%|#######5  | 49/65 [09:16<03:02, 11.38s/it]
2022-04-13 20:12:48,030 - INFO - tqdm - accuracy: 0.9931, batch_loss: 0.0010, loss: 0.0184 ||:  77%|#######6  | 50/65 [09:27<02:50, 11.39s/it]
2022-04-13 20:12:59,408 - INFO - tqdm - accuracy: 0.9926, batch_loss: 0.0433, loss: 0.0189 ||:  78%|#######8  | 51/65 [09:39<02:39, 11.38s/it]
2022-04-13 20:13:10,800 - INFO - tqdm - accuracy: 0.9928, batch_loss: 0.0012, loss: 0.0185 ||:  80%|########  | 52/65 [09:50<02:28, 11.39s/it]
2022-04-13 20:13:22,200 - INFO - tqdm - accuracy: 0.9929, batch_loss: 0.0056, loss: 0.0183 ||:  82%|########1 | 53/65 [10:01<02:16, 11.39s/it]
2022-04-13 20:13:33,597 - INFO - tqdm - accuracy: 0.9931, batch_loss: 0.0061, loss: 0.0181 ||:  83%|########3 | 54/65 [10:13<02:05, 11.39s/it]
2022-04-13 20:13:45,000 - INFO - tqdm - accuracy: 0.9932, batch_loss: 0.0023, loss: 0.0178 ||:  85%|########4 | 55/65 [10:24<01:53, 11.40s/it]
2022-04-13 20:13:56,381 - INFO - tqdm - accuracy: 0.9933, batch_loss: 0.0016, loss: 0.0175 ||:  86%|########6 | 56/65 [10:36<01:42, 11.39s/it]
2022-04-13 20:14:07,501 - INFO - tqdm - accuracy: 0.9934, batch_loss: 0.0016, loss: 0.0172 ||:  88%|########7 | 57/65 [10:47<01:30, 11.31s/it]
2022-04-13 20:14:18,899 - INFO - tqdm - accuracy: 0.9935, batch_loss: 0.0075, loss: 0.0170 ||:  89%|########9 | 58/65 [10:58<01:19, 11.34s/it]
2022-04-13 20:14:30,296 - INFO - tqdm - accuracy: 0.9936, batch_loss: 0.0007, loss: 0.0168 ||:  91%|######### | 59/65 [11:09<01:08, 11.35s/it]
2022-04-13 20:14:41,691 - INFO - tqdm - accuracy: 0.9937, batch_loss: 0.0046, loss: 0.0166 ||:  92%|#########2| 60/65 [11:21<00:56, 11.37s/it]
2022-04-13 20:14:53,105 - INFO - tqdm - accuracy: 0.9938, batch_loss: 0.0037, loss: 0.0163 ||:  94%|#########3| 61/65 [11:32<00:45, 11.38s/it]
2022-04-13 20:15:04,503 - INFO - tqdm - accuracy: 0.9939, batch_loss: 0.0214, loss: 0.0164 ||:  95%|#########5| 62/65 [11:44<00:34, 11.39s/it]
2022-04-13 20:15:15,947 - INFO - tqdm - accuracy: 0.9940, batch_loss: 0.0010, loss: 0.0162 ||:  97%|#########6| 63/65 [11:55<00:22, 11.40s/it]
2022-04-13 20:15:27,413 - INFO - tqdm - accuracy: 0.9941, batch_loss: 0.0010, loss: 0.0159 ||:  98%|#########8| 64/65 [12:07<00:11, 11.42s/it]
2022-04-13 20:15:32,493 - INFO - tqdm - accuracy: 0.9942, batch_loss: 0.0573, loss: 0.0166 ||: 100%|##########| 65/65 [12:12<00:00,  9.52s/it]
2022-04-13 20:15:32,493 - INFO - tqdm - accuracy: 0.9942, batch_loss: 0.0573, loss: 0.0166 ||: 100%|##########| 65/65 [12:12<00:00, 11.26s/it]
2022-04-13 20:15:35,361 - INFO - allennlp.training.trainer - Validating
2022-04-13 20:15:35,362 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-13 20:15:45,394 - INFO - tqdm - accuracy: 0.5875, batch_loss: 0.0791, loss: 2.5043 ||:  35%|###4      | 40/115 [00:10<00:18,  4.01it/s]
2022-04-13 20:15:55,559 - INFO - tqdm - accuracy: 0.6025, batch_loss: 5.9246, loss: 2.3440 ||:  70%|#######   | 81/115 [00:20<00:07,  4.45it/s]
2022-04-13 20:16:04,009 - INFO - tqdm - accuracy: 0.6114, batch_loss: 2.6332, loss: 2.1925 ||: 100%|##########| 115/115 [00:28<00:00,  3.98it/s]
2022-04-13 20:16:04,009 - INFO - tqdm - accuracy: 0.6114, batch_loss: 2.6332, loss: 2.1925 ||: 100%|##########| 115/115 [00:28<00:00,  4.01it/s]
2022-04-13 20:16:04,009 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-13 20:16:04,010 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.994  |     0.611
2022-04-13 20:16:04,011 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-13 20:16:04,011 - INFO - allennlp.training.tensorboard_writer - loss               |     0.017  |     2.192
2022-04-13 20:16:04,011 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5802.840  |       N/A
2022-04-13 20:16:09,985 - INFO - allennlp.training.trainer - Epoch duration: 0:12:49.688011
2022-04-13 20:16:09,985 - INFO - allennlp.training.trainer - Estimated training time remaining: 0:38:41
2022-04-13 20:16:09,985 - INFO - allennlp.training.trainer - Epoch 12/14
2022-04-13 20:16:09,985 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-13 20:16:09,985 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-13 20:16:09,987 - INFO - allennlp.training.trainer - Training
2022-04-13 20:16:09,987 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-13 20:16:21,128 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.0663, loss: 0.0663 ||:   2%|1         | 1/65 [00:11<11:53, 11.14s/it]
2022-04-13 20:16:32,337 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0015, loss: 0.0339 ||:   3%|3         | 2/65 [00:22<11:44, 11.18s/it]
2022-04-13 20:16:43,458 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.0016, loss: 0.0231 ||:   5%|4         | 3/65 [00:33<11:31, 11.15s/it]
2022-04-13 20:16:54,726 - INFO - tqdm - accuracy: 0.9922, batch_loss: 0.0032, loss: 0.0181 ||:   6%|6         | 4/65 [00:44<11:23, 11.20s/it]
2022-04-13 20:17:06,003 - INFO - tqdm - accuracy: 0.9938, batch_loss: 0.0015, loss: 0.0148 ||:   8%|7         | 5/65 [00:56<11:13, 11.23s/it]
2022-04-13 20:17:17,290 - INFO - tqdm - accuracy: 0.9948, batch_loss: 0.0075, loss: 0.0136 ||:   9%|9         | 6/65 [01:07<11:03, 11.25s/it]
2022-04-13 20:17:28,581 - INFO - tqdm - accuracy: 0.9955, batch_loss: 0.0025, loss: 0.0120 ||:  11%|#         | 7/65 [01:18<10:53, 11.26s/it]
2022-04-13 20:17:39,892 - INFO - tqdm - accuracy: 0.9961, batch_loss: 0.0004, loss: 0.0105 ||:  12%|#2        | 8/65 [01:29<10:42, 11.28s/it]
2022-04-13 20:17:51,216 - INFO - tqdm - accuracy: 0.9965, batch_loss: 0.0010, loss: 0.0095 ||:  14%|#3        | 9/65 [01:41<10:32, 11.29s/it]
2022-04-13 20:18:02,535 - INFO - tqdm - accuracy: 0.9969, batch_loss: 0.0009, loss: 0.0086 ||:  15%|#5        | 10/65 [01:52<10:21, 11.30s/it]
2022-04-13 20:18:13,856 - INFO - tqdm - accuracy: 0.9972, batch_loss: 0.0011, loss: 0.0079 ||:  17%|#6        | 11/65 [02:03<10:10, 11.31s/it]
2022-04-13 20:18:25,191 - INFO - tqdm - accuracy: 0.9974, batch_loss: 0.0010, loss: 0.0074 ||:  18%|#8        | 12/65 [02:15<09:59, 11.32s/it]
2022-04-13 20:18:36,195 - INFO - tqdm - accuracy: 0.9976, batch_loss: 0.0014, loss: 0.0069 ||:  20%|##        | 13/65 [02:26<09:43, 11.22s/it]
2022-04-13 20:18:47,544 - INFO - tqdm - accuracy: 0.9978, batch_loss: 0.0028, loss: 0.0066 ||:  22%|##1       | 14/65 [02:37<09:34, 11.26s/it]
2022-04-13 20:18:58,879 - INFO - tqdm - accuracy: 0.9979, batch_loss: 0.0047, loss: 0.0065 ||:  23%|##3       | 15/65 [02:48<09:24, 11.28s/it]
2022-04-13 20:19:10,232 - INFO - tqdm - accuracy: 0.9961, batch_loss: 0.1027, loss: 0.0125 ||:  25%|##4       | 16/65 [03:00<09:13, 11.30s/it]
2022-04-13 20:19:21,586 - INFO - tqdm - accuracy: 0.9963, batch_loss: 0.0009, loss: 0.0118 ||:  26%|##6       | 17/65 [03:11<09:03, 11.32s/it]
2022-04-13 20:19:32,950 - INFO - tqdm - accuracy: 0.9965, batch_loss: 0.0046, loss: 0.0114 ||:  28%|##7       | 18/65 [03:22<08:52, 11.33s/it]
2022-04-13 20:19:44,299 - INFO - tqdm - accuracy: 0.9967, batch_loss: 0.0019, loss: 0.0109 ||:  29%|##9       | 19/65 [03:34<08:41, 11.34s/it]
2022-04-13 20:19:56,125 - INFO - tqdm - accuracy: 0.9969, batch_loss: 0.0109, loss: 0.0109 ||:  31%|###       | 20/65 [03:46<08:36, 11.48s/it]
2022-04-13 20:20:07,451 - INFO - tqdm - accuracy: 0.9970, batch_loss: 0.0081, loss: 0.0108 ||:  32%|###2      | 21/65 [03:57<08:23, 11.44s/it]
2022-04-13 20:20:18,847 - INFO - tqdm - accuracy: 0.9972, batch_loss: 0.0130, loss: 0.0109 ||:  34%|###3      | 22/65 [04:08<08:11, 11.42s/it]
2022-04-13 20:20:30,251 - INFO - tqdm - accuracy: 0.9973, batch_loss: 0.0013, loss: 0.0105 ||:  35%|###5      | 23/65 [04:20<07:59, 11.42s/it]
2022-04-13 20:20:41,675 - INFO - tqdm - accuracy: 0.9974, batch_loss: 0.0017, loss: 0.0101 ||:  37%|###6      | 24/65 [04:31<07:48, 11.42s/it]
2022-04-13 20:20:52,860 - INFO - tqdm - accuracy: 0.9962, batch_loss: 0.1880, loss: 0.0172 ||:  38%|###8      | 25/65 [04:42<07:33, 11.35s/it]
2022-04-13 20:21:04,322 - INFO - tqdm - accuracy: 0.9964, batch_loss: 0.0012, loss: 0.0166 ||:  40%|####      | 26/65 [04:54<07:23, 11.38s/it]
2022-04-13 20:21:15,741 - INFO - tqdm - accuracy: 0.9965, batch_loss: 0.0192, loss: 0.0167 ||:  42%|####1     | 27/65 [05:05<07:12, 11.39s/it]
2022-04-13 20:21:27,146 - INFO - tqdm - accuracy: 0.9966, batch_loss: 0.0022, loss: 0.0162 ||:  43%|####3     | 28/65 [05:17<07:01, 11.40s/it]
2022-04-13 20:21:38,559 - INFO - tqdm - accuracy: 0.9968, batch_loss: 0.0213, loss: 0.0164 ||:  45%|####4     | 29/65 [05:28<06:50, 11.40s/it]
2022-04-13 20:21:49,945 - INFO - tqdm - accuracy: 0.9958, batch_loss: 0.1004, loss: 0.0192 ||:  46%|####6     | 30/65 [05:39<06:38, 11.40s/it]
2022-04-13 20:22:01,336 - INFO - tqdm - accuracy: 0.9960, batch_loss: 0.0016, loss: 0.0186 ||:  48%|####7     | 31/65 [05:51<06:27, 11.40s/it]
2022-04-13 20:22:12,604 - INFO - tqdm - accuracy: 0.9961, batch_loss: 0.0053, loss: 0.0182 ||:  49%|####9     | 32/65 [06:02<06:14, 11.36s/it]
2022-04-13 20:22:23,984 - INFO - tqdm - accuracy: 0.9962, batch_loss: 0.0083, loss: 0.0179 ||:  51%|#####     | 33/65 [06:13<06:03, 11.36s/it]
2022-04-13 20:22:35,380 - INFO - tqdm - accuracy: 0.9963, batch_loss: 0.0009, loss: 0.0174 ||:  52%|#####2    | 34/65 [06:25<05:52, 11.37s/it]
2022-04-13 20:22:46,746 - INFO - tqdm - accuracy: 0.9964, batch_loss: 0.0063, loss: 0.0171 ||:  54%|#####3    | 35/65 [06:36<05:41, 11.37s/it]
2022-04-13 20:22:58,114 - INFO - tqdm - accuracy: 0.9965, batch_loss: 0.0126, loss: 0.0169 ||:  55%|#####5    | 36/65 [06:48<05:29, 11.37s/it]
2022-04-13 20:23:09,479 - INFO - tqdm - accuracy: 0.9966, batch_loss: 0.0005, loss: 0.0165 ||:  57%|#####6    | 37/65 [06:59<05:18, 11.37s/it]
2022-04-13 20:23:20,851 - INFO - tqdm - accuracy: 0.9967, batch_loss: 0.0203, loss: 0.0166 ||:  58%|#####8    | 38/65 [07:10<05:06, 11.37s/it]
2022-04-13 20:23:32,196 - INFO - tqdm - accuracy: 0.9968, batch_loss: 0.0125, loss: 0.0165 ||:  60%|######    | 39/65 [07:22<04:55, 11.36s/it]
2022-04-13 20:23:43,559 - INFO - tqdm - accuracy: 0.9969, batch_loss: 0.0181, loss: 0.0165 ||:  62%|######1   | 40/65 [07:33<04:44, 11.36s/it]
2022-04-13 20:23:54,930 - INFO - tqdm - accuracy: 0.9969, batch_loss: 0.0010, loss: 0.0162 ||:  63%|######3   | 41/65 [07:44<04:32, 11.37s/it]
2022-04-13 20:24:06,280 - INFO - tqdm - accuracy: 0.9970, batch_loss: 0.0007, loss: 0.0158 ||:  65%|######4   | 42/65 [07:56<04:21, 11.36s/it]
2022-04-13 20:24:17,640 - INFO - tqdm - accuracy: 0.9971, batch_loss: 0.0008, loss: 0.0154 ||:  66%|######6   | 43/65 [08:07<04:09, 11.36s/it]
2022-04-13 20:24:29,042 - INFO - tqdm - accuracy: 0.9972, batch_loss: 0.0012, loss: 0.0151 ||:  68%|######7   | 44/65 [08:19<03:58, 11.37s/it]
2022-04-13 20:24:40,412 - INFO - tqdm - accuracy: 0.9972, batch_loss: 0.0009, loss: 0.0148 ||:  69%|######9   | 45/65 [08:30<03:47, 11.37s/it]
2022-04-13 20:24:51,791 - INFO - tqdm - accuracy: 0.9973, batch_loss: 0.0009, loss: 0.0145 ||:  71%|#######   | 46/65 [08:41<03:36, 11.37s/it]
2022-04-13 20:25:03,166 - INFO - tqdm - accuracy: 0.9973, batch_loss: 0.0030, loss: 0.0143 ||:  72%|#######2  | 47/65 [08:53<03:24, 11.37s/it]
2022-04-13 20:25:14,549 - INFO - tqdm - accuracy: 0.9974, batch_loss: 0.0007, loss: 0.0140 ||:  74%|#######3  | 48/65 [09:04<03:13, 11.38s/it]
2022-04-13 20:25:25,921 - INFO - tqdm - accuracy: 0.9974, batch_loss: 0.0007, loss: 0.0137 ||:  75%|#######5  | 49/65 [09:15<03:02, 11.38s/it]
2022-04-13 20:25:37,293 - INFO - tqdm - accuracy: 0.9975, batch_loss: 0.0012, loss: 0.0135 ||:  77%|#######6  | 50/65 [09:27<02:50, 11.37s/it]
2022-04-13 20:25:48,659 - INFO - tqdm - accuracy: 0.9975, batch_loss: 0.0012, loss: 0.0132 ||:  78%|#######8  | 51/65 [09:38<02:39, 11.37s/it]
2022-04-13 20:26:00,036 - INFO - tqdm - accuracy: 0.9976, batch_loss: 0.0013, loss: 0.0130 ||:  80%|########  | 52/65 [09:50<02:27, 11.37s/it]
2022-04-13 20:26:11,398 - INFO - tqdm - accuracy: 0.9976, batch_loss: 0.0006, loss: 0.0127 ||:  82%|########1 | 53/65 [10:01<02:16, 11.37s/it]
2022-04-13 20:26:22,771 - INFO - tqdm - accuracy: 0.9977, batch_loss: 0.0011, loss: 0.0125 ||:  83%|########3 | 54/65 [10:12<02:05, 11.37s/it]
2022-04-13 20:26:34,138 - INFO - tqdm - accuracy: 0.9977, batch_loss: 0.0010, loss: 0.0123 ||:  85%|########4 | 55/65 [10:24<01:53, 11.37s/it]
2022-04-13 20:26:45,524 - INFO - tqdm - accuracy: 0.9978, batch_loss: 0.0007, loss: 0.0121 ||:  86%|########6 | 56/65 [10:35<01:42, 11.37s/it]
2022-04-13 20:26:56,890 - INFO - tqdm - accuracy: 0.9978, batch_loss: 0.0054, loss: 0.0120 ||:  88%|########7 | 57/65 [10:46<01:30, 11.37s/it]
2022-04-13 20:27:08,252 - INFO - tqdm - accuracy: 0.9978, batch_loss: 0.0021, loss: 0.0118 ||:  89%|########9 | 58/65 [10:58<01:19, 11.37s/it]
2022-04-13 20:27:19,610 - INFO - tqdm - accuracy: 0.9979, batch_loss: 0.0055, loss: 0.0117 ||:  91%|######### | 59/65 [11:09<01:08, 11.37s/it]
2022-04-13 20:27:30,956 - INFO - tqdm - accuracy: 0.9979, batch_loss: 0.0025, loss: 0.0116 ||:  92%|#########2| 60/65 [11:20<00:56, 11.36s/it]
2022-04-13 20:27:42,333 - INFO - tqdm - accuracy: 0.9979, batch_loss: 0.0034, loss: 0.0114 ||:  94%|#########3| 61/65 [11:32<00:45, 11.37s/it]
2022-04-13 20:27:53,708 - INFO - tqdm - accuracy: 0.9980, batch_loss: 0.0029, loss: 0.0113 ||:  95%|#########5| 62/65 [11:43<00:34, 11.37s/it]
2022-04-13 20:28:05,074 - INFO - tqdm - accuracy: 0.9980, batch_loss: 0.0218, loss: 0.0115 ||:  97%|#########6| 63/65 [11:55<00:22, 11.37s/it]
2022-04-13 20:28:16,325 - INFO - tqdm - accuracy: 0.9980, batch_loss: 0.0030, loss: 0.0113 ||:  98%|#########8| 64/65 [12:06<00:11, 11.33s/it]
2022-04-13 20:28:21,367 - INFO - tqdm - accuracy: 0.9981, batch_loss: 0.0004, loss: 0.0112 ||: 100%|##########| 65/65 [12:11<00:00,  9.45s/it]
2022-04-13 20:28:21,367 - INFO - tqdm - accuracy: 0.9981, batch_loss: 0.0004, loss: 0.0112 ||: 100%|##########| 65/65 [12:11<00:00, 11.25s/it]
2022-04-13 20:28:24,215 - INFO - allennlp.training.trainer - Validating
2022-04-13 20:28:24,216 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-13 20:28:34,458 - INFO - tqdm - accuracy: 0.7108, batch_loss: 0.0194, loss: 1.6494 ||:  37%|###6      | 42/115 [00:10<00:18,  4.04it/s]
2022-04-13 20:28:44,654 - INFO - tqdm - accuracy: 0.6182, batch_loss: 0.8860, loss: 2.0166 ||:  72%|#######2  | 83/115 [00:20<00:07,  4.02it/s]
2022-04-13 20:28:52,609 - INFO - tqdm - accuracy: 0.6026, batch_loss: 0.3545, loss: 2.2152 ||: 100%|##########| 115/115 [00:28<00:00,  4.01it/s]
2022-04-13 20:28:52,610 - INFO - tqdm - accuracy: 0.6026, batch_loss: 0.3545, loss: 2.2152 ||: 100%|##########| 115/115 [00:28<00:00,  4.05it/s]
2022-04-13 20:28:52,610 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-13 20:28:52,611 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.998  |     0.603
2022-04-13 20:28:52,612 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-13 20:28:52,613 - INFO - allennlp.training.tensorboard_writer - loss               |     0.011  |     2.215
2022-04-13 20:28:52,613 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5802.840  |       N/A
2022-04-13 20:28:58,774 - INFO - allennlp.training.trainer - Epoch duration: 0:12:48.789036
2022-04-13 20:28:58,774 - INFO - allennlp.training.trainer - Estimated training time remaining: 0:25:46
2022-04-13 20:28:58,774 - INFO - allennlp.training.trainer - Epoch 13/14
2022-04-13 20:28:58,774 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-13 20:28:58,775 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-13 20:28:58,776 - INFO - allennlp.training.trainer - Training
2022-04-13 20:28:58,777 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-13 20:29:09,848 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0008, loss: 0.0008 ||:   2%|1         | 1/65 [00:11<11:48, 11.07s/it]
2022-04-13 20:29:20,998 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0011, loss: 0.0010 ||:   3%|3         | 2/65 [00:22<11:40, 11.12s/it]
2022-04-13 20:29:32,170 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0006, loss: 0.0008 ||:   5%|4         | 3/65 [00:33<11:30, 11.14s/it]
2022-04-13 20:29:43,373 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0017, loss: 0.0011 ||:   6%|6         | 4/65 [00:44<11:21, 11.17s/it]
2022-04-13 20:29:54,616 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0030, loss: 0.0014 ||:   8%|7         | 5/65 [00:55<11:11, 11.19s/it]
2022-04-13 20:30:05,861 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0007, loss: 0.0013 ||:   9%|9         | 6/65 [01:07<11:01, 11.21s/it]
2022-04-13 20:30:17,141 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0011, loss: 0.0013 ||:  11%|#         | 7/65 [01:18<10:51, 11.23s/it]
2022-04-13 20:30:28,401 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0013, loss: 0.0013 ||:  12%|#2        | 8/65 [01:29<10:40, 11.24s/it]
2022-04-13 20:30:39,678 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0009, loss: 0.0012 ||:  14%|#3        | 9/65 [01:40<10:30, 11.25s/it]
2022-04-13 20:30:50,961 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0024, loss: 0.0014 ||:  15%|#5        | 10/65 [01:52<10:19, 11.26s/it]
2022-04-13 20:31:02,274 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0023, loss: 0.0015 ||:  17%|#6        | 11/65 [02:03<10:08, 11.28s/it]
2022-04-13 20:31:13,588 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0010, loss: 0.0014 ||:  18%|#8        | 12/65 [02:14<09:58, 11.29s/it]
2022-04-13 20:31:24,925 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0019, loss: 0.0015 ||:  20%|##        | 13/65 [02:26<09:47, 11.30s/it]
2022-04-13 20:31:36,276 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0125, loss: 0.0022 ||:  22%|##1       | 14/65 [02:37<09:37, 11.32s/it]
2022-04-13 20:31:47,628 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0006, loss: 0.0021 ||:  23%|##3       | 15/65 [02:48<09:26, 11.33s/it]
2022-04-13 20:31:59,029 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0017, loss: 0.0021 ||:  25%|##4       | 16/65 [03:00<09:16, 11.35s/it]
2022-04-13 20:32:10,442 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0007, loss: 0.0020 ||:  26%|##6       | 17/65 [03:11<09:05, 11.37s/it]
2022-04-13 20:32:21,510 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0008, loss: 0.0020 ||:  28%|##7       | 18/65 [03:22<08:50, 11.28s/it]
2022-04-13 20:32:32,920 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0018, loss: 0.0020 ||:  29%|##9       | 19/65 [03:34<08:40, 11.32s/it]
2022-04-13 20:32:44,322 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0013, loss: 0.0019 ||:  31%|###       | 20/65 [03:45<08:30, 11.34s/it]
2022-04-13 20:32:55,730 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0014, loss: 0.0019 ||:  32%|###2      | 21/65 [03:56<08:19, 11.36s/it]
2022-04-13 20:33:07,128 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0021, loss: 0.0019 ||:  34%|###3      | 22/65 [04:08<08:09, 11.37s/it]
2022-04-13 20:33:18,506 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0026, loss: 0.0019 ||:  35%|###5      | 23/65 [04:19<07:57, 11.37s/it]
2022-04-13 20:33:29,856 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0010, loss: 0.0019 ||:  37%|###6      | 24/65 [04:31<07:46, 11.37s/it]
2022-04-13 20:33:41,217 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0008, loss: 0.0019 ||:  38%|###8      | 25/65 [04:42<07:34, 11.37s/it]
2022-04-13 20:33:52,583 - INFO - tqdm - accuracy: 0.9988, batch_loss: 0.0486, loss: 0.0037 ||:  40%|####      | 26/65 [04:53<07:23, 11.37s/it]
2022-04-13 20:34:03,945 - INFO - tqdm - accuracy: 0.9977, batch_loss: 0.0798, loss: 0.0065 ||:  42%|####1     | 27/65 [05:05<07:11, 11.36s/it]
2022-04-13 20:34:15,321 - INFO - tqdm - accuracy: 0.9966, batch_loss: 0.0574, loss: 0.0083 ||:  43%|####3     | 28/65 [05:16<07:00, 11.37s/it]
2022-04-13 20:34:26,671 - INFO - tqdm - accuracy: 0.9968, batch_loss: 0.0046, loss: 0.0082 ||:  45%|####4     | 29/65 [05:27<06:49, 11.36s/it]
2022-04-13 20:34:38,027 - INFO - tqdm - accuracy: 0.9969, batch_loss: 0.0006, loss: 0.0079 ||:  46%|####6     | 30/65 [05:39<06:37, 11.36s/it]
2022-04-13 20:34:49,379 - INFO - tqdm - accuracy: 0.9970, batch_loss: 0.0105, loss: 0.0080 ||:  48%|####7     | 31/65 [05:50<06:26, 11.36s/it]
2022-04-13 20:35:00,724 - INFO - tqdm - accuracy: 0.9971, batch_loss: 0.0017, loss: 0.0078 ||:  49%|####9     | 32/65 [06:01<06:14, 11.35s/it]
2022-04-13 20:35:12,067 - INFO - tqdm - accuracy: 0.9962, batch_loss: 0.0801, loss: 0.0100 ||:  51%|#####     | 33/65 [06:13<06:03, 11.35s/it]
2022-04-13 20:35:23,273 - INFO - tqdm - accuracy: 0.9963, batch_loss: 0.0273, loss: 0.0105 ||:  52%|#####2    | 34/65 [06:24<05:50, 11.31s/it]
2022-04-13 20:35:34,629 - INFO - tqdm - accuracy: 0.9955, batch_loss: 0.0294, loss: 0.0110 ||:  54%|#####3    | 35/65 [06:35<05:39, 11.32s/it]
2022-04-13 20:35:45,976 - INFO - tqdm - accuracy: 0.9948, batch_loss: 0.0770, loss: 0.0129 ||:  55%|#####5    | 36/65 [06:47<05:28, 11.33s/it]
2022-04-13 20:35:57,316 - INFO - tqdm - accuracy: 0.9949, batch_loss: 0.0007, loss: 0.0125 ||:  57%|#####6    | 37/65 [06:58<05:17, 11.33s/it]
2022-04-13 20:36:08,662 - INFO - tqdm - accuracy: 0.9951, batch_loss: 0.0095, loss: 0.0125 ||:  58%|#####8    | 38/65 [07:09<05:06, 11.34s/it]
2022-04-13 20:36:20,019 - INFO - tqdm - accuracy: 0.9952, batch_loss: 0.0133, loss: 0.0125 ||:  60%|######    | 39/65 [07:21<04:54, 11.34s/it]
2022-04-13 20:36:31,382 - INFO - tqdm - accuracy: 0.9953, batch_loss: 0.0042, loss: 0.0123 ||:  62%|######1   | 40/65 [07:32<04:43, 11.35s/it]
2022-04-13 20:36:42,730 - INFO - tqdm - accuracy: 0.9954, batch_loss: 0.0063, loss: 0.0121 ||:  63%|######3   | 41/65 [07:43<04:32, 11.35s/it]
2022-04-13 20:36:54,090 - INFO - tqdm - accuracy: 0.9948, batch_loss: 0.0294, loss: 0.0125 ||:  65%|######4   | 42/65 [07:55<04:21, 11.35s/it]
2022-04-13 20:37:05,435 - INFO - tqdm - accuracy: 0.9949, batch_loss: 0.0022, loss: 0.0123 ||:  66%|######6   | 43/65 [08:06<04:09, 11.35s/it]
2022-04-13 20:37:16,787 - INFO - tqdm - accuracy: 0.9950, batch_loss: 0.0020, loss: 0.0121 ||:  68%|######7   | 44/65 [08:18<03:58, 11.35s/it]
2022-04-13 20:37:28,157 - INFO - tqdm - accuracy: 0.9944, batch_loss: 0.1393, loss: 0.0149 ||:  69%|######9   | 45/65 [08:29<03:47, 11.36s/it]
2022-04-13 20:37:39,300 - INFO - tqdm - accuracy: 0.9946, batch_loss: 0.0018, loss: 0.0146 ||:  71%|#######   | 46/65 [08:40<03:34, 11.29s/it]
2022-04-13 20:37:50,702 - INFO - tqdm - accuracy: 0.9947, batch_loss: 0.0010, loss: 0.0143 ||:  72%|#######2  | 47/65 [08:51<03:23, 11.33s/it]
2022-04-13 20:38:02,128 - INFO - tqdm - accuracy: 0.9948, batch_loss: 0.0013, loss: 0.0140 ||:  74%|#######3  | 48/65 [09:03<03:13, 11.36s/it]
2022-04-13 20:38:13,537 - INFO - tqdm - accuracy: 0.9949, batch_loss: 0.0020, loss: 0.0138 ||:  75%|#######5  | 49/65 [09:14<03:01, 11.37s/it]
2022-04-13 20:38:24,846 - INFO - tqdm - accuracy: 0.9950, batch_loss: 0.0051, loss: 0.0136 ||:  77%|#######6  | 50/65 [09:26<02:50, 11.35s/it]
2022-04-13 20:38:36,283 - INFO - tqdm - accuracy: 0.9951, batch_loss: 0.0022, loss: 0.0134 ||:  78%|#######8  | 51/65 [09:37<02:39, 11.38s/it]
2022-04-13 20:38:47,730 - INFO - tqdm - accuracy: 0.9946, batch_loss: 0.2567, loss: 0.0181 ||:  80%|########  | 52/65 [09:48<02:28, 11.40s/it]
2022-04-13 20:38:59,158 - INFO - tqdm - accuracy: 0.9947, batch_loss: 0.0012, loss: 0.0178 ||:  82%|########1 | 53/65 [10:00<02:16, 11.41s/it]
2022-04-13 20:39:10,578 - INFO - tqdm - accuracy: 0.9948, batch_loss: 0.0007, loss: 0.0174 ||:  83%|########3 | 54/65 [10:11<02:05, 11.41s/it]
2022-04-13 20:39:22,463 - INFO - tqdm - accuracy: 0.9943, batch_loss: 0.0396, loss: 0.0179 ||:  85%|########4 | 55/65 [10:23<01:55, 11.55s/it]
2022-04-13 20:39:33,839 - INFO - tqdm - accuracy: 0.9944, batch_loss: 0.0012, loss: 0.0176 ||:  86%|########6 | 56/65 [10:35<01:43, 11.50s/it]
2022-04-13 20:39:45,237 - INFO - tqdm - accuracy: 0.9945, batch_loss: 0.0006, loss: 0.0173 ||:  88%|########7 | 57/65 [10:46<01:31, 11.47s/it]
2022-04-13 20:39:56,651 - INFO - tqdm - accuracy: 0.9946, batch_loss: 0.0004, loss: 0.0170 ||:  89%|########9 | 58/65 [10:57<01:20, 11.45s/it]
2022-04-13 20:40:08,074 - INFO - tqdm - accuracy: 0.9947, batch_loss: 0.0008, loss: 0.0167 ||:  91%|######### | 59/65 [11:09<01:08, 11.44s/it]
2022-04-13 20:40:19,470 - INFO - tqdm - accuracy: 0.9948, batch_loss: 0.0008, loss: 0.0164 ||:  92%|#########2| 60/65 [11:20<00:57, 11.43s/it]
2022-04-13 20:40:30,874 - INFO - tqdm - accuracy: 0.9949, batch_loss: 0.0005, loss: 0.0162 ||:  94%|#########3| 61/65 [11:32<00:45, 11.42s/it]
2022-04-13 20:40:42,251 - INFO - tqdm - accuracy: 0.9950, batch_loss: 0.0034, loss: 0.0160 ||:  95%|#########5| 62/65 [11:43<00:34, 11.41s/it]
2022-04-13 20:40:53,657 - INFO - tqdm - accuracy: 0.9950, batch_loss: 0.0007, loss: 0.0157 ||:  97%|#########6| 63/65 [11:54<00:22, 11.41s/it]
2022-04-13 20:41:05,041 - INFO - tqdm - accuracy: 0.9951, batch_loss: 0.0020, loss: 0.0155 ||:  98%|#########8| 64/65 [12:06<00:11, 11.40s/it]
2022-04-13 20:41:10,102 - INFO - tqdm - accuracy: 0.9951, batch_loss: 0.0136, loss: 0.0155 ||: 100%|##########| 65/65 [12:11<00:00,  9.50s/it]
2022-04-13 20:41:10,102 - INFO - tqdm - accuracy: 0.9951, batch_loss: 0.0136, loss: 0.0155 ||: 100%|##########| 65/65 [12:11<00:00, 11.25s/it]
2022-04-13 20:41:12,966 - INFO - allennlp.training.trainer - Validating
2022-04-13 20:41:12,967 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-13 20:41:23,102 - INFO - tqdm - accuracy: 0.7407, batch_loss: 0.0002, loss: 1.5346 ||:  36%|###5      | 41/115 [00:10<00:18,  4.03it/s]
2022-04-13 20:41:33,306 - INFO - tqdm - accuracy: 0.6196, batch_loss: 4.3250, loss: 2.2135 ||:  71%|#######1  | 82/115 [00:20<00:08,  4.01it/s]
2022-04-13 20:41:41,448 - INFO - tqdm - accuracy: 0.6114, batch_loss: 0.0224, loss: 2.2193 ||: 100%|##########| 115/115 [00:28<00:00,  4.00it/s]
2022-04-13 20:41:41,449 - INFO - tqdm - accuracy: 0.6114, batch_loss: 0.0224, loss: 2.2193 ||: 100%|##########| 115/115 [00:28<00:00,  4.04it/s]
2022-04-13 20:41:41,449 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-13 20:41:41,449 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.995  |     0.611
2022-04-13 20:41:41,449 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-13 20:41:41,450 - INFO - allennlp.training.tensorboard_writer - loss               |     0.015  |     2.219
2022-04-13 20:41:41,450 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5802.840  |       N/A
2022-04-13 20:41:47,508 - INFO - allennlp.training.trainer - Epoch duration: 0:12:48.734022
2022-04-13 20:41:47,509 - INFO - allennlp.training.trainer - Estimated training time remaining: 0:12:53
2022-04-13 20:41:47,509 - INFO - allennlp.training.trainer - Epoch 14/14
2022-04-13 20:41:47,509 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-13 20:41:47,509 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-13 20:41:47,510 - INFO - allennlp.training.trainer - Training
2022-04-13 20:41:47,511 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-13 20:41:58,620 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0004, loss: 0.0004 ||:   2%|1         | 1/65 [00:11<11:50, 11.11s/it]
2022-04-13 20:42:09,768 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0024, loss: 0.0014 ||:   3%|3         | 2/65 [00:22<11:41, 11.13s/it]
2022-04-13 20:42:20,943 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0010, loss: 0.0013 ||:   5%|4         | 3/65 [00:33<11:31, 11.15s/it]
2022-04-13 20:42:32,156 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0039, loss: 0.0019 ||:   6%|6         | 4/65 [00:44<11:21, 11.18s/it]
2022-04-13 20:42:43,396 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0111, loss: 0.0038 ||:   8%|7         | 5/65 [00:55<11:11, 11.20s/it]
2022-04-13 20:42:54,626 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0046, loss: 0.0039 ||:   9%|9         | 6/65 [01:07<11:01, 11.21s/it]
2022-04-13 20:43:05,904 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0054, loss: 0.0041 ||:  11%|#         | 7/65 [01:18<10:51, 11.23s/it]
2022-04-13 20:43:17,170 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0045, loss: 0.0042 ||:  12%|#2        | 8/65 [01:29<10:40, 11.24s/it]
2022-04-13 20:43:28,451 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0029, loss: 0.0040 ||:  14%|#3        | 9/65 [01:40<10:30, 11.25s/it]
2022-04-13 20:43:39,729 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0016, loss: 0.0038 ||:  15%|#5        | 10/65 [01:52<10:19, 11.26s/it]
2022-04-13 20:43:51,021 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0028, loss: 0.0037 ||:  17%|#6        | 11/65 [02:03<10:08, 11.27s/it]
2022-04-13 20:44:02,322 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0008, loss: 0.0035 ||:  18%|#8        | 12/65 [02:14<09:57, 11.28s/it]
2022-04-13 20:44:13,620 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0040, loss: 0.0035 ||:  20%|##        | 13/65 [02:26<09:46, 11.29s/it]
2022-04-13 20:44:24,917 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0059, loss: 0.0037 ||:  22%|##1       | 14/65 [02:37<09:35, 11.29s/it]
2022-04-13 20:44:36,237 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0114, loss: 0.0042 ||:  23%|##3       | 15/65 [02:48<09:24, 11.30s/it]
2022-04-13 20:44:47,543 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0051, loss: 0.0043 ||:  25%|##4       | 16/65 [03:00<09:13, 11.30s/it]
2022-04-13 20:44:58,879 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0028, loss: 0.0042 ||:  26%|##6       | 17/65 [03:11<09:02, 11.31s/it]
2022-04-13 20:45:10,188 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0021, loss: 0.0041 ||:  28%|##7       | 18/65 [03:22<08:51, 11.31s/it]
2022-04-13 20:45:21,508 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0004, loss: 0.0039 ||:  29%|##9       | 19/65 [03:33<08:40, 11.31s/it]
2022-04-13 20:45:32,835 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0008, loss: 0.0037 ||:  31%|###       | 20/65 [03:45<08:29, 11.32s/it]
2022-04-13 20:45:44,172 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0007, loss: 0.0036 ||:  32%|###2      | 21/65 [03:56<08:18, 11.32s/it]
2022-04-13 20:45:55,365 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0014, loss: 0.0035 ||:  34%|###3      | 22/65 [04:07<08:05, 11.28s/it]
2022-04-13 20:46:06,706 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0011, loss: 0.0034 ||:  35%|###5      | 23/65 [04:19<07:54, 11.30s/it]
2022-04-13 20:46:18,014 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0068, loss: 0.0035 ||:  37%|###6      | 24/65 [04:30<07:43, 11.30s/it]
2022-04-13 20:46:29,350 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0011, loss: 0.0034 ||:  38%|###8      | 25/65 [04:41<07:32, 11.31s/it]
2022-04-13 20:46:40,685 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0015, loss: 0.0033 ||:  40%|####      | 26/65 [04:53<07:21, 11.32s/it]
2022-04-13 20:46:52,021 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0054, loss: 0.0034 ||:  42%|####1     | 27/65 [05:04<07:10, 11.32s/it]
2022-04-13 20:47:03,381 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0008, loss: 0.0033 ||:  43%|####3     | 28/65 [05:15<06:59, 11.34s/it]
2022-04-13 20:47:14,711 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0011, loss: 0.0032 ||:  45%|####4     | 29/65 [05:27<06:48, 11.33s/it]
2022-04-13 20:47:26,054 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0010, loss: 0.0032 ||:  46%|####6     | 30/65 [05:38<06:36, 11.34s/it]
2022-04-13 20:47:37,429 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0014, loss: 0.0031 ||:  48%|####7     | 31/65 [05:49<06:25, 11.35s/it]
2022-04-13 20:47:48,835 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0008, loss: 0.0030 ||:  49%|####9     | 32/65 [06:01<06:15, 11.37s/it]
2022-04-13 20:48:00,236 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0011, loss: 0.0030 ||:  51%|#####     | 33/65 [06:12<06:04, 11.38s/it]
2022-04-13 20:48:11,653 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0004, loss: 0.0029 ||:  52%|#####2    | 34/65 [06:24<05:53, 11.39s/it]
2022-04-13 20:48:23,069 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0014, loss: 0.0029 ||:  54%|#####3    | 35/65 [06:35<05:41, 11.40s/it]
2022-04-13 20:48:34,515 - INFO - tqdm - accuracy: 0.9991, batch_loss: 0.0340, loss: 0.0037 ||:  55%|#####5    | 36/65 [06:47<05:30, 11.41s/it]
2022-04-13 20:48:45,960 - INFO - tqdm - accuracy: 0.9992, batch_loss: 0.0008, loss: 0.0036 ||:  57%|#####6    | 37/65 [06:58<05:19, 11.42s/it]
2022-04-13 20:48:57,403 - INFO - tqdm - accuracy: 0.9992, batch_loss: 0.0008, loss: 0.0036 ||:  58%|#####8    | 38/65 [07:09<05:08, 11.43s/it]
2022-04-13 20:49:08,841 - INFO - tqdm - accuracy: 0.9992, batch_loss: 0.0034, loss: 0.0036 ||:  60%|######    | 39/65 [07:21<04:57, 11.43s/it]
2022-04-13 20:49:20,283 - INFO - tqdm - accuracy: 0.9992, batch_loss: 0.0059, loss: 0.0036 ||:  62%|######1   | 40/65 [07:32<04:45, 11.43s/it]
2022-04-13 20:49:31,703 - INFO - tqdm - accuracy: 0.9992, batch_loss: 0.0010, loss: 0.0036 ||:  63%|######3   | 41/65 [07:44<04:34, 11.43s/it]
2022-04-13 20:49:43,108 - INFO - tqdm - accuracy: 0.9993, batch_loss: 0.0006, loss: 0.0035 ||:  65%|######4   | 42/65 [07:55<04:22, 11.42s/it]
2022-04-13 20:49:54,503 - INFO - tqdm - accuracy: 0.9993, batch_loss: 0.0007, loss: 0.0034 ||:  66%|######6   | 43/65 [08:06<04:11, 11.41s/it]
2022-04-13 20:50:05,906 - INFO - tqdm - accuracy: 0.9993, batch_loss: 0.0012, loss: 0.0034 ||:  68%|######7   | 44/65 [08:18<03:59, 11.41s/it]
2022-04-13 20:50:17,277 - INFO - tqdm - accuracy: 0.9993, batch_loss: 0.0012, loss: 0.0033 ||:  69%|######9   | 45/65 [08:29<03:47, 11.40s/it]
2022-04-13 20:50:28,684 - INFO - tqdm - accuracy: 0.9993, batch_loss: 0.0005, loss: 0.0033 ||:  71%|#######   | 46/65 [08:41<03:36, 11.40s/it]
2022-04-13 20:50:40,070 - INFO - tqdm - accuracy: 0.9993, batch_loss: 0.0005, loss: 0.0032 ||:  72%|#######2  | 47/65 [08:52<03:25, 11.40s/it]
2022-04-13 20:50:51,465 - INFO - tqdm - accuracy: 0.9987, batch_loss: 0.2600, loss: 0.0086 ||:  74%|#######3  | 48/65 [09:03<03:13, 11.40s/it]
2022-04-13 20:51:02,857 - INFO - tqdm - accuracy: 0.9987, batch_loss: 0.0061, loss: 0.0085 ||:  75%|#######5  | 49/65 [09:15<03:02, 11.39s/it]
2022-04-13 20:51:14,239 - INFO - tqdm - accuracy: 0.9988, batch_loss: 0.0015, loss: 0.0084 ||:  77%|#######6  | 50/65 [09:26<02:50, 11.39s/it]
2022-04-13 20:51:25,634 - INFO - tqdm - accuracy: 0.9988, batch_loss: 0.0006, loss: 0.0082 ||:  78%|#######8  | 51/65 [09:38<02:39, 11.39s/it]
2022-04-13 20:51:36,553 - INFO - tqdm - accuracy: 0.9988, batch_loss: 0.0009, loss: 0.0081 ||:  80%|########  | 52/65 [09:49<02:26, 11.25s/it]
2022-04-13 20:51:47,922 - INFO - tqdm - accuracy: 0.9988, batch_loss: 0.0031, loss: 0.0080 ||:  82%|########1 | 53/65 [10:00<02:15, 11.29s/it]
2022-04-13 20:51:59,293 - INFO - tqdm - accuracy: 0.9988, batch_loss: 0.0005, loss: 0.0078 ||:  83%|########3 | 54/65 [10:11<02:04, 11.31s/it]
2022-04-13 20:52:10,688 - INFO - tqdm - accuracy: 0.9989, batch_loss: 0.0007, loss: 0.0077 ||:  85%|########4 | 55/65 [10:23<01:53, 11.34s/it]
2022-04-13 20:52:21,846 - INFO - tqdm - accuracy: 0.9983, batch_loss: 0.1835, loss: 0.0109 ||:  86%|########6 | 56/65 [10:34<01:41, 11.28s/it]
2022-04-13 20:52:33,193 - INFO - tqdm - accuracy: 0.9984, batch_loss: 0.0083, loss: 0.0108 ||:  88%|########7 | 57/65 [10:45<01:30, 11.30s/it]
2022-04-13 20:52:44,579 - INFO - tqdm - accuracy: 0.9978, batch_loss: 0.0281, loss: 0.0111 ||:  89%|########9 | 58/65 [10:57<01:19, 11.33s/it]
2022-04-13 20:52:55,963 - INFO - tqdm - accuracy: 0.9979, batch_loss: 0.0011, loss: 0.0109 ||:  91%|######### | 59/65 [11:08<01:08, 11.34s/it]
2022-04-13 20:53:07,367 - INFO - tqdm - accuracy: 0.9979, batch_loss: 0.0028, loss: 0.0108 ||:  92%|#########2| 60/65 [11:19<00:56, 11.36s/it]
2022-04-13 20:53:18,764 - INFO - tqdm - accuracy: 0.9979, batch_loss: 0.0015, loss: 0.0106 ||:  94%|#########3| 61/65 [11:31<00:45, 11.37s/it]
2022-04-13 20:53:30,155 - INFO - tqdm - accuracy: 0.9980, batch_loss: 0.0011, loss: 0.0105 ||:  95%|#########5| 62/65 [11:42<00:34, 11.38s/it]
2022-04-13 20:53:41,543 - INFO - tqdm - accuracy: 0.9980, batch_loss: 0.0012, loss: 0.0103 ||:  97%|#########6| 63/65 [11:54<00:22, 11.38s/it]
2022-04-13 20:53:52,925 - INFO - tqdm - accuracy: 0.9980, batch_loss: 0.0005, loss: 0.0102 ||:  98%|#########8| 64/65 [12:05<00:11, 11.38s/it]
2022-04-13 20:53:57,988 - INFO - tqdm - accuracy: 0.9981, batch_loss: 0.0004, loss: 0.0100 ||: 100%|##########| 65/65 [12:10<00:00,  9.49s/it]
2022-04-13 20:53:57,988 - INFO - tqdm - accuracy: 0.9981, batch_loss: 0.0004, loss: 0.0100 ||: 100%|##########| 65/65 [12:10<00:00, 11.24s/it]
2022-04-13 20:54:00,846 - INFO - allennlp.training.trainer - Validating
2022-04-13 20:54:00,848 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-13 20:54:11,044 - INFO - tqdm - accuracy: 0.6220, batch_loss: 7.0247, loss: 2.1983 ||:  36%|###5      | 41/115 [00:10<00:18,  4.02it/s]
2022-04-13 20:54:21,157 - INFO - tqdm - accuracy: 0.6463, batch_loss: 0.0125, loss: 2.1125 ||:  71%|#######1  | 82/115 [00:20<00:07,  4.47it/s]
2022-04-13 20:54:29,272 - INFO - tqdm - accuracy: 0.6201, batch_loss: 1.0315, loss: 2.2639 ||: 100%|##########| 115/115 [00:28<00:00,  4.01it/s]
2022-04-13 20:54:29,272 - INFO - tqdm - accuracy: 0.6201, batch_loss: 1.0315, loss: 2.2639 ||: 100%|##########| 115/115 [00:28<00:00,  4.05it/s]
2022-04-13 20:54:29,272 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-13 20:54:29,273 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.998  |     0.620
2022-04-13 20:54:29,273 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-13 20:54:29,273 - INFO - allennlp.training.tensorboard_writer - loss               |     0.010  |     2.264
2022-04-13 20:54:29,273 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5802.840  |       N/A
2022-04-13 20:54:35,232 - INFO - allennlp.training.trainer - Epoch duration: 0:12:47.723228
2022-04-13 20:54:35,232 - INFO - allennlp.training.checkpointer - loading best weights
2022-04-13 20:54:36,210 - INFO - allennlp.common.util - Metrics: {
  "best_epoch": 1,
  "peak_worker_0_memory_MB": 5802.83984375,
  "peak_gpu_0_memory_MB": 9580.5712890625,
  "training_duration": "3:13:04.413651",
  "training_start_epoch": 0,
  "training_epochs": 14,
  "epoch": 14,
  "training_accuracy": 0.9980591945657448,
  "training_loss": 0.010041219864727351,
  "training_worker_0_memory_MB": 5802.83984375,
  "training_gpu_0_memory_MB": 9580.5712890625,
  "validation_accuracy": 0.6200873362445415,
  "validation_loss": 2.263881813335731,
  "best_validation_accuracy": 0.6637554585152838,
  "best_validation_loss": 0.6646289987408597
}
2022-04-13 20:54:36,210 - INFO - allennlp.models.archival - archiving weights and vocabulary to ./output_ir-ora-d_hyper0/model.tar.gz
