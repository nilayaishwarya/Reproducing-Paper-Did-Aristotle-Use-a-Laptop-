2022-04-09 17:34:40,483 - INFO - allennlp.common.params - random_seed = 42
2022-04-09 17:34:40,483 - INFO - allennlp.common.params - numpy_seed = 42
2022-04-09 17:34:40,483 - INFO - allennlp.common.params - pytorch_seed = 42
2022-04-09 17:34:41,151 - INFO - allennlp.common.checks - Pytorch version: 1.7.1
2022-04-09 17:34:41,151 - INFO - allennlp.common.params - type = default
2022-04-09 17:34:41,152 - INFO - allennlp.common.params - dataset_reader.type = strategy_qa_reader
2022-04-09 17:34:41,152 - INFO - allennlp.common.params - dataset_reader.lazy = False
2022-04-09 17:34:41,152 - INFO - allennlp.common.params - dataset_reader.cache_directory = None
2022-04-09 17:34:41,152 - INFO - allennlp.common.params - dataset_reader.max_instances = None
2022-04-09 17:34:41,152 - INFO - allennlp.common.params - dataset_reader.manual_distributed_sharding = False
2022-04-09 17:34:41,152 - INFO - allennlp.common.params - dataset_reader.manual_multi_process_sharding = False
2022-04-09 17:34:41,152 - INFO - allennlp.common.params - dataset_reader.tokenizer_wrapper.type = default
2022-04-09 17:34:41,153 - INFO - allennlp.common.params - dataset_reader.tokenizer_wrapper.pretrained_model = roberta-large
2022-04-09 17:34:41,153 - INFO - allennlp.common.params - dataset_reader.tokenizer_wrapper.call_kwargs.return_offsets_mapping = True
2022-04-09 17:34:41,153 - INFO - allennlp.common.params - dataset_reader.tokenizer_wrapper.call_kwargs.return_special_tokens_mask = True
2022-04-09 17:34:41,153 - INFO - allennlp.common.params - dataset_reader.tokenizer_wrapper.call_kwargs.truncation = only_first
2022-04-09 17:34:42,333 - INFO - allennlp.common.params - dataset_reader.save_tokenizer = True
2022-04-09 17:34:42,333 - INFO - allennlp.common.params - dataset_reader.is_training = True
2022-04-09 17:34:42,333 - INFO - allennlp.common.params - dataset_reader.pickle.action = None
2022-04-09 17:34:42,333 - INFO - allennlp.common.params - dataset_reader.pickle.file_name = None
2022-04-09 17:34:42,333 - INFO - allennlp.common.params - dataset_reader.pickle.path = ../pickle
2022-04-09 17:34:42,333 - INFO - allennlp.common.params - dataset_reader.pickle.save_even_when_max_instances = False
2022-04-09 17:34:42,334 - INFO - allennlp.common.params - dataset_reader.paragraphs_source = IR-ORA-D
2022-04-09 17:34:42,334 - INFO - allennlp.common.params - dataset_reader.answer_last_decomposition_step = False
2022-04-09 17:34:42,334 - INFO - allennlp.common.params - dataset_reader.skip_if_context_missing = True
2022-04-09 17:34:42,334 - INFO - allennlp.common.params - dataset_reader.paragraphs_limit = 10
2022-04-09 17:34:42,334 - INFO - allennlp.common.params - dataset_reader.generated_decompositions_paths = None
2022-04-09 17:34:42,334 - INFO - allennlp.common.params - dataset_reader.save_elasticsearch_cache = True
2022-04-09 17:34:42,781 - INFO - filelock - Lock 140429903969616 acquired on cache.lock
2022-04-09 17:34:43,458 - INFO - filelock - Lock 140429903969616 released on cache.lock
2022-04-09 17:34:43,458 - INFO - allennlp.common.params - train_data_path = data/strategyqa/train.json
2022-04-09 17:34:43,459 - INFO - allennlp.common.params - vocabulary = <allennlp.common.lazy.Lazy object at 0x7fb86d4ee090>
2022-04-09 17:34:43,459 - INFO - allennlp.common.params - datasets_for_vocab_creation = None
2022-04-09 17:34:43,459 - INFO - allennlp.common.params - validation_dataset_reader.type = strategy_qa_reader
2022-04-09 17:34:43,459 - INFO - allennlp.common.params - validation_dataset_reader.lazy = False
2022-04-09 17:34:43,459 - INFO - allennlp.common.params - validation_dataset_reader.cache_directory = None
2022-04-09 17:34:43,459 - INFO - allennlp.common.params - validation_dataset_reader.max_instances = None
2022-04-09 17:34:43,459 - INFO - allennlp.common.params - validation_dataset_reader.manual_distributed_sharding = False
2022-04-09 17:34:43,459 - INFO - allennlp.common.params - validation_dataset_reader.manual_multi_process_sharding = False
2022-04-09 17:34:43,460 - INFO - allennlp.common.params - validation_dataset_reader.tokenizer_wrapper.type = default
2022-04-09 17:34:43,460 - INFO - allennlp.common.params - validation_dataset_reader.tokenizer_wrapper.pretrained_model = roberta-large
2022-04-09 17:34:43,460 - INFO - allennlp.common.params - validation_dataset_reader.tokenizer_wrapper.call_kwargs.return_offsets_mapping = True
2022-04-09 17:34:43,460 - INFO - allennlp.common.params - validation_dataset_reader.tokenizer_wrapper.call_kwargs.return_special_tokens_mask = True
2022-04-09 17:34:43,460 - INFO - allennlp.common.params - validation_dataset_reader.tokenizer_wrapper.call_kwargs.truncation = only_first
2022-04-09 17:34:44,641 - INFO - allennlp.common.params - validation_dataset_reader.save_tokenizer = False
2022-04-09 17:34:44,642 - INFO - allennlp.common.params - validation_dataset_reader.is_training = False
2022-04-09 17:34:44,642 - INFO - allennlp.common.params - validation_dataset_reader.pickle.action = None
2022-04-09 17:34:44,642 - INFO - allennlp.common.params - validation_dataset_reader.pickle.file_name = None
2022-04-09 17:34:44,642 - INFO - allennlp.common.params - validation_dataset_reader.pickle.path = ../pickle
2022-04-09 17:34:44,642 - INFO - allennlp.common.params - validation_dataset_reader.pickle.save_even_when_max_instances = False
2022-04-09 17:34:44,642 - INFO - allennlp.common.params - validation_dataset_reader.paragraphs_source = IR-ORA-D
2022-04-09 17:34:44,642 - INFO - allennlp.common.params - validation_dataset_reader.answer_last_decomposition_step = False
2022-04-09 17:34:44,642 - INFO - allennlp.common.params - validation_dataset_reader.skip_if_context_missing = True
2022-04-09 17:34:44,642 - INFO - allennlp.common.params - validation_dataset_reader.paragraphs_limit = 10
2022-04-09 17:34:44,642 - INFO - allennlp.common.params - validation_dataset_reader.generated_decompositions_paths = None
2022-04-09 17:34:44,642 - INFO - allennlp.common.params - validation_dataset_reader.save_elasticsearch_cache = True
2022-04-09 17:34:44,642 - INFO - filelock - Lock 140429905036624 acquired on cache.lock
2022-04-09 17:34:45,483 - INFO - filelock - Lock 140429905036624 released on cache.lock
2022-04-09 17:34:45,483 - INFO - allennlp.common.params - validation_data_path = data/strategyqa/dev.json
2022-04-09 17:34:45,485 - INFO - allennlp.common.params - validation_data_loader = None
2022-04-09 17:34:45,485 - INFO - allennlp.common.params - test_data_path = None
2022-04-09 17:34:45,485 - INFO - allennlp.common.params - evaluate_on_test = True
2022-04-09 17:34:45,485 - INFO - allennlp.common.params - batch_weight_key = 
2022-04-09 17:34:45,485 - INFO - allennlp.training.util - Reading training data from data/strategyqa/train.json
2022-04-09 17:34:45,485 - INFO - tqdm - reading instances: 0it [00:00, ?it/s]
2022-04-09 17:34:45,485 - INFO - src.data.dataset_readers.base.base_dataset_reader - Reading the dataset from scratch
2022-04-09 17:34:45,486 - INFO - src.data.dataset_readers.strategy_qa_reader - Reading the dataset:
2022-04-09 17:34:45,486 - INFO - src.data.dataset_readers.strategy_qa_reader - Reading file at data/strategyqa/train.json
2022-04-09 17:34:52,884 - INFO - filelock - Lock 140429248391248 acquired on cache.lock
2022-04-09 17:34:53,704 - INFO - filelock - Lock 140429248391248 released on cache.lock
2022-04-09 17:34:53,743 - INFO - allennlp.training.util - Reading validation data from data/strategyqa/dev.json
2022-04-09 17:34:53,743 - INFO - tqdm - reading instances: 0it [00:00, ?it/s]
2022-04-09 17:34:53,744 - INFO - src.data.dataset_readers.base.base_dataset_reader - Reading the dataset from scratch
2022-04-09 17:34:53,744 - INFO - src.data.dataset_readers.strategy_qa_reader - Reading the dataset:
2022-04-09 17:34:53,744 - INFO - src.data.dataset_readers.strategy_qa_reader - Reading file at data/strategyqa/dev.json
2022-04-09 17:34:54,225 - INFO - src.data.dataset_readers.utils.elasticsearch_utils - Empty results from Elasticsearch for: actions commercial pilots take concerning engine noise arriving departing Orange County, California?
2022-04-09 17:34:54,546 - INFO - filelock - Lock 140429255805072 acquired on cache.lock
2022-04-09 17:34:55,311 - INFO - filelock - Lock 140429255805072 released on cache.lock
2022-04-09 17:34:55,346 - INFO - allennlp.common.params - type = from_instances
2022-04-09 17:34:55,347 - INFO - allennlp.common.params - min_count = None
2022-04-09 17:34:55,347 - INFO - allennlp.common.params - max_vocab_size = None
2022-04-09 17:34:55,348 - INFO - allennlp.common.params - non_padded_namespaces = ('*tags', '*labels')
2022-04-09 17:34:55,348 - INFO - allennlp.common.params - pretrained_files = None
2022-04-09 17:34:55,348 - INFO - allennlp.common.params - only_include_pretrained_words = False
2022-04-09 17:34:55,348 - INFO - allennlp.common.params - tokens_to_add = None
2022-04-09 17:34:55,348 - INFO - allennlp.common.params - min_pretrained_embeddings = None
2022-04-09 17:34:55,348 - INFO - allennlp.common.params - padding_token = @@PADDING@@
2022-04-09 17:34:55,348 - INFO - allennlp.common.params - oov_token = @@UNKNOWN@@
2022-04-09 17:34:55,348 - INFO - allennlp.data.vocabulary - Fitting token dictionary from dataset.
2022-04-09 17:34:55,349 - INFO - tqdm - building vocab: 0it [00:00, ?it/s]
2022-04-09 17:34:55,355 - INFO - allennlp.common.params - model.type = hf_classifier
2022-04-09 17:34:55,355 - INFO - allennlp.common.params - model.regularizer = None
2022-04-09 17:34:55,355 - INFO - allennlp.common.params - model.pretrained_model = roberta-large
2022-04-09 17:34:55,356 - INFO - allennlp.common.params - model.tokenizer_wrapper.type = default
2022-04-09 17:34:55,356 - INFO - allennlp.common.params - model.tokenizer_wrapper.pretrained_model = roberta-large
2022-04-09 17:34:55,356 - INFO - allennlp.common.params - model.tokenizer_wrapper.call_kwargs.return_offsets_mapping = True
2022-04-09 17:34:55,356 - INFO - allennlp.common.params - model.tokenizer_wrapper.call_kwargs.return_special_tokens_mask = True
2022-04-09 17:34:55,356 - INFO - allennlp.common.params - model.tokenizer_wrapper.call_kwargs.truncation = only_first
2022-04-09 17:34:56,547 - INFO - allennlp.common.params - model.num_labels = 2
2022-04-09 17:34:56,547 - INFO - allennlp.common.params - model.label_namespace = labels
2022-04-09 17:34:56,547 - INFO - allennlp.common.params - model.transformer_weights_path = None
2022-04-09 17:34:56,548 - INFO - allennlp.common.params - model.initializer.regexes.0.1.type = pretrained
2022-04-09 17:34:56,548 - INFO - allennlp.common.params - model.initializer.regexes.0.1.weights_file_path = output_roberta-star/model.tar.gz
2022-04-09 17:34:56,549 - INFO - allennlp.models.archival - extracting archive file output_roberta-star/model.tar.gz to temp dir /tmp/tmp26rfx4g4
2022-04-09 17:35:08,509 - INFO - allennlp.models.archival - removing temporary unarchived model dir at /tmp/tmp26rfx4g4
2022-04-09 17:35:08,680 - INFO - allennlp.common.params - model.initializer.prevent_regexes = None
2022-04-09 17:35:23,355 - INFO - allennlp.nn.initializers - Initializing parameters
2022-04-09 17:35:23,355 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.embeddings.word_embeddings.weight using .* initializer
2022-04-09 17:35:23,365 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.embeddings.position_embeddings.weight using .* initializer
2022-04-09 17:35:23,365 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.embeddings.token_type_embeddings.weight using .* initializer
2022-04-09 17:35:23,365 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.embeddings.LayerNorm.weight using .* initializer
2022-04-09 17:35:23,365 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.embeddings.LayerNorm.bias using .* initializer
2022-04-09 17:35:23,366 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.self.query.weight using .* initializer
2022-04-09 17:35:23,366 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.self.query.bias using .* initializer
2022-04-09 17:35:23,366 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.self.key.weight using .* initializer
2022-04-09 17:35:23,366 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.self.key.bias using .* initializer
2022-04-09 17:35:23,367 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.self.value.weight using .* initializer
2022-04-09 17:35:23,367 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.self.value.bias using .* initializer
2022-04-09 17:35:23,367 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.output.dense.weight using .* initializer
2022-04-09 17:35:23,368 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.output.dense.bias using .* initializer
2022-04-09 17:35:23,368 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.output.LayerNorm.weight using .* initializer
2022-04-09 17:35:23,368 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.output.LayerNorm.bias using .* initializer
2022-04-09 17:35:23,368 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.intermediate.dense.weight using .* initializer
2022-04-09 17:35:23,369 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.intermediate.dense.bias using .* initializer
2022-04-09 17:35:23,369 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.output.dense.weight using .* initializer
2022-04-09 17:35:23,370 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.output.dense.bias using .* initializer
2022-04-09 17:35:23,370 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.output.LayerNorm.weight using .* initializer
2022-04-09 17:35:23,370 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.output.LayerNorm.bias using .* initializer
2022-04-09 17:35:23,370 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.self.query.weight using .* initializer
2022-04-09 17:35:23,371 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.self.query.bias using .* initializer
2022-04-09 17:35:23,371 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.self.key.weight using .* initializer
2022-04-09 17:35:23,371 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.self.key.bias using .* initializer
2022-04-09 17:35:23,372 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.self.value.weight using .* initializer
2022-04-09 17:35:23,372 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.self.value.bias using .* initializer
2022-04-09 17:35:23,372 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.output.dense.weight using .* initializer
2022-04-09 17:35:23,372 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.output.dense.bias using .* initializer
2022-04-09 17:35:23,373 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.output.LayerNorm.weight using .* initializer
2022-04-09 17:35:23,373 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.output.LayerNorm.bias using .* initializer
2022-04-09 17:35:23,373 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.intermediate.dense.weight using .* initializer
2022-04-09 17:35:23,374 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.intermediate.dense.bias using .* initializer
2022-04-09 17:35:23,374 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.output.dense.weight using .* initializer
2022-04-09 17:35:23,375 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.output.dense.bias using .* initializer
2022-04-09 17:35:23,375 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.output.LayerNorm.weight using .* initializer
2022-04-09 17:35:23,375 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.output.LayerNorm.bias using .* initializer
2022-04-09 17:35:23,375 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.self.query.weight using .* initializer
2022-04-09 17:35:23,376 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.self.query.bias using .* initializer
2022-04-09 17:35:23,376 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.self.key.weight using .* initializer
2022-04-09 17:35:23,376 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.self.key.bias using .* initializer
2022-04-09 17:35:23,376 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.self.value.weight using .* initializer
2022-04-09 17:35:23,377 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.self.value.bias using .* initializer
2022-04-09 17:35:23,377 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.output.dense.weight using .* initializer
2022-04-09 17:35:23,377 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.output.dense.bias using .* initializer
2022-04-09 17:35:23,377 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.output.LayerNorm.weight using .* initializer
2022-04-09 17:35:23,378 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.output.LayerNorm.bias using .* initializer
2022-04-09 17:35:23,378 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.intermediate.dense.weight using .* initializer
2022-04-09 17:35:23,379 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.intermediate.dense.bias using .* initializer
2022-04-09 17:35:23,379 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.output.dense.weight using .* initializer
2022-04-09 17:35:23,380 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.output.dense.bias using .* initializer
2022-04-09 17:35:23,380 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.output.LayerNorm.weight using .* initializer
2022-04-09 17:35:23,380 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.output.LayerNorm.bias using .* initializer
2022-04-09 17:35:23,380 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.self.query.weight using .* initializer
2022-04-09 17:35:23,381 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.self.query.bias using .* initializer
2022-04-09 17:35:23,381 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.self.key.weight using .* initializer
2022-04-09 17:35:23,381 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.self.key.bias using .* initializer
2022-04-09 17:35:23,381 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.self.value.weight using .* initializer
2022-04-09 17:35:23,382 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.self.value.bias using .* initializer
2022-04-09 17:35:23,382 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.output.dense.weight using .* initializer
2022-04-09 17:35:23,382 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.output.dense.bias using .* initializer
2022-04-09 17:35:23,382 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.output.LayerNorm.weight using .* initializer
2022-04-09 17:35:23,383 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.output.LayerNorm.bias using .* initializer
2022-04-09 17:35:23,383 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.intermediate.dense.weight using .* initializer
2022-04-09 17:35:23,384 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.intermediate.dense.bias using .* initializer
2022-04-09 17:35:23,384 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.output.dense.weight using .* initializer
2022-04-09 17:35:23,385 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.output.dense.bias using .* initializer
2022-04-09 17:35:23,385 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.output.LayerNorm.weight using .* initializer
2022-04-09 17:35:23,385 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.output.LayerNorm.bias using .* initializer
2022-04-09 17:35:23,385 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.self.query.weight using .* initializer
2022-04-09 17:35:23,386 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.self.query.bias using .* initializer
2022-04-09 17:35:23,386 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.self.key.weight using .* initializer
2022-04-09 17:35:23,386 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.self.key.bias using .* initializer
2022-04-09 17:35:23,386 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.self.value.weight using .* initializer
2022-04-09 17:35:23,387 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.self.value.bias using .* initializer
2022-04-09 17:35:23,387 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.output.dense.weight using .* initializer
2022-04-09 17:35:23,387 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.output.dense.bias using .* initializer
2022-04-09 17:35:23,387 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.output.LayerNorm.weight using .* initializer
2022-04-09 17:35:23,387 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.output.LayerNorm.bias using .* initializer
2022-04-09 17:35:23,388 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.intermediate.dense.weight using .* initializer
2022-04-09 17:35:23,388 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.intermediate.dense.bias using .* initializer
2022-04-09 17:35:23,389 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.output.dense.weight using .* initializer
2022-04-09 17:35:23,390 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.output.dense.bias using .* initializer
2022-04-09 17:35:23,390 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.output.LayerNorm.weight using .* initializer
2022-04-09 17:35:23,390 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.output.LayerNorm.bias using .* initializer
2022-04-09 17:35:23,390 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.self.query.weight using .* initializer
2022-04-09 17:35:23,390 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.self.query.bias using .* initializer
2022-04-09 17:35:23,390 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.self.key.weight using .* initializer
2022-04-09 17:35:23,391 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.self.key.bias using .* initializer
2022-04-09 17:35:23,391 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.self.value.weight using .* initializer
2022-04-09 17:35:23,391 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.self.value.bias using .* initializer
2022-04-09 17:35:23,392 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.output.dense.weight using .* initializer
2022-04-09 17:35:23,392 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.output.dense.bias using .* initializer
2022-04-09 17:35:23,392 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.output.LayerNorm.weight using .* initializer
2022-04-09 17:35:23,392 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.output.LayerNorm.bias using .* initializer
2022-04-09 17:35:23,392 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.intermediate.dense.weight using .* initializer
2022-04-09 17:35:23,393 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.intermediate.dense.bias using .* initializer
2022-04-09 17:35:23,393 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.output.dense.weight using .* initializer
2022-04-09 17:35:23,394 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.output.dense.bias using .* initializer
2022-04-09 17:35:23,394 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.output.LayerNorm.weight using .* initializer
2022-04-09 17:35:23,395 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.output.LayerNorm.bias using .* initializer
2022-04-09 17:35:23,395 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.self.query.weight using .* initializer
2022-04-09 17:35:23,395 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.self.query.bias using .* initializer
2022-04-09 17:35:23,395 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.self.key.weight using .* initializer
2022-04-09 17:35:23,396 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.self.key.bias using .* initializer
2022-04-09 17:35:23,396 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.self.value.weight using .* initializer
2022-04-09 17:35:23,396 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.self.value.bias using .* initializer
2022-04-09 17:35:23,396 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.output.dense.weight using .* initializer
2022-04-09 17:35:23,397 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.output.dense.bias using .* initializer
2022-04-09 17:35:23,397 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.output.LayerNorm.weight using .* initializer
2022-04-09 17:35:23,397 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.output.LayerNorm.bias using .* initializer
2022-04-09 17:35:23,397 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.intermediate.dense.weight using .* initializer
2022-04-09 17:35:23,398 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.intermediate.dense.bias using .* initializer
2022-04-09 17:35:23,398 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.output.dense.weight using .* initializer
2022-04-09 17:35:23,399 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.output.dense.bias using .* initializer
2022-04-09 17:35:23,399 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.output.LayerNorm.weight using .* initializer
2022-04-09 17:35:23,399 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.output.LayerNorm.bias using .* initializer
2022-04-09 17:35:23,399 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.self.query.weight using .* initializer
2022-04-09 17:35:23,400 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.self.query.bias using .* initializer
2022-04-09 17:35:23,400 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.self.key.weight using .* initializer
2022-04-09 17:35:23,400 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.self.key.bias using .* initializer
2022-04-09 17:35:23,401 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.self.value.weight using .* initializer
2022-04-09 17:35:23,401 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.self.value.bias using .* initializer
2022-04-09 17:35:23,401 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.output.dense.weight using .* initializer
2022-04-09 17:35:23,402 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.output.dense.bias using .* initializer
2022-04-09 17:35:23,402 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.output.LayerNorm.weight using .* initializer
2022-04-09 17:35:23,402 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.output.LayerNorm.bias using .* initializer
2022-04-09 17:35:23,402 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.intermediate.dense.weight using .* initializer
2022-04-09 17:35:23,403 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.intermediate.dense.bias using .* initializer
2022-04-09 17:35:23,403 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.output.dense.weight using .* initializer
2022-04-09 17:35:23,404 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.output.dense.bias using .* initializer
2022-04-09 17:35:23,404 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.output.LayerNorm.weight using .* initializer
2022-04-09 17:35:23,405 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.output.LayerNorm.bias using .* initializer
2022-04-09 17:35:23,405 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.self.query.weight using .* initializer
2022-04-09 17:35:23,405 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.self.query.bias using .* initializer
2022-04-09 17:35:23,405 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.self.key.weight using .* initializer
2022-04-09 17:35:23,406 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.self.key.bias using .* initializer
2022-04-09 17:35:23,406 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.self.value.weight using .* initializer
2022-04-09 17:35:23,406 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.self.value.bias using .* initializer
2022-04-09 17:35:23,406 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.output.dense.weight using .* initializer
2022-04-09 17:35:23,407 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.output.dense.bias using .* initializer
2022-04-09 17:35:23,407 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.output.LayerNorm.weight using .* initializer
2022-04-09 17:35:23,407 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.output.LayerNorm.bias using .* initializer
2022-04-09 17:35:23,407 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.intermediate.dense.weight using .* initializer
2022-04-09 17:35:23,408 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.intermediate.dense.bias using .* initializer
2022-04-09 17:35:23,409 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.output.dense.weight using .* initializer
2022-04-09 17:35:23,410 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.output.dense.bias using .* initializer
2022-04-09 17:35:23,410 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.output.LayerNorm.weight using .* initializer
2022-04-09 17:35:23,410 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.output.LayerNorm.bias using .* initializer
2022-04-09 17:35:23,410 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.self.query.weight using .* initializer
2022-04-09 17:35:23,410 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.self.query.bias using .* initializer
2022-04-09 17:35:23,411 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.self.key.weight using .* initializer
2022-04-09 17:35:23,411 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.self.key.bias using .* initializer
2022-04-09 17:35:23,411 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.self.value.weight using .* initializer
2022-04-09 17:35:23,411 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.self.value.bias using .* initializer
2022-04-09 17:35:23,412 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.output.dense.weight using .* initializer
2022-04-09 17:35:23,412 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.output.dense.bias using .* initializer
2022-04-09 17:35:23,412 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.output.LayerNorm.weight using .* initializer
2022-04-09 17:35:23,412 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.output.LayerNorm.bias using .* initializer
2022-04-09 17:35:23,412 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.intermediate.dense.weight using .* initializer
2022-04-09 17:35:23,413 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.intermediate.dense.bias using .* initializer
2022-04-09 17:35:23,414 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.output.dense.weight using .* initializer
2022-04-09 17:35:23,414 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.output.dense.bias using .* initializer
2022-04-09 17:35:23,415 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.output.LayerNorm.weight using .* initializer
2022-04-09 17:35:23,415 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.output.LayerNorm.bias using .* initializer
2022-04-09 17:35:23,415 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.self.query.weight using .* initializer
2022-04-09 17:35:23,415 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.self.query.bias using .* initializer
2022-04-09 17:35:23,415 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.self.key.weight using .* initializer
2022-04-09 17:35:23,416 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.self.key.bias using .* initializer
2022-04-09 17:35:23,416 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.self.value.weight using .* initializer
2022-04-09 17:35:23,416 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.self.value.bias using .* initializer
2022-04-09 17:35:23,416 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.output.dense.weight using .* initializer
2022-04-09 17:35:23,417 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.output.dense.bias using .* initializer
2022-04-09 17:35:23,417 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.output.LayerNorm.weight using .* initializer
2022-04-09 17:35:23,417 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.output.LayerNorm.bias using .* initializer
2022-04-09 17:35:23,417 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.intermediate.dense.weight using .* initializer
2022-04-09 17:35:23,418 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.intermediate.dense.bias using .* initializer
2022-04-09 17:35:23,418 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.output.dense.weight using .* initializer
2022-04-09 17:35:23,419 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.output.dense.bias using .* initializer
2022-04-09 17:35:23,419 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.output.LayerNorm.weight using .* initializer
2022-04-09 17:35:23,420 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.output.LayerNorm.bias using .* initializer
2022-04-09 17:35:23,420 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.self.query.weight using .* initializer
2022-04-09 17:35:23,420 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.self.query.bias using .* initializer
2022-04-09 17:35:23,420 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.self.key.weight using .* initializer
2022-04-09 17:35:23,421 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.self.key.bias using .* initializer
2022-04-09 17:35:23,421 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.self.value.weight using .* initializer
2022-04-09 17:35:23,421 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.self.value.bias using .* initializer
2022-04-09 17:35:23,422 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.output.dense.weight using .* initializer
2022-04-09 17:35:23,422 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.output.dense.bias using .* initializer
2022-04-09 17:35:23,422 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.output.LayerNorm.weight using .* initializer
2022-04-09 17:35:23,422 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.output.LayerNorm.bias using .* initializer
2022-04-09 17:35:23,422 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.intermediate.dense.weight using .* initializer
2022-04-09 17:35:23,423 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.intermediate.dense.bias using .* initializer
2022-04-09 17:35:23,424 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.output.dense.weight using .* initializer
2022-04-09 17:35:23,425 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.output.dense.bias using .* initializer
2022-04-09 17:35:23,425 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.output.LayerNorm.weight using .* initializer
2022-04-09 17:35:23,425 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.output.LayerNorm.bias using .* initializer
2022-04-09 17:35:23,425 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.self.query.weight using .* initializer
2022-04-09 17:35:23,425 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.self.query.bias using .* initializer
2022-04-09 17:35:23,426 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.self.key.weight using .* initializer
2022-04-09 17:35:23,426 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.self.key.bias using .* initializer
2022-04-09 17:35:23,426 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.self.value.weight using .* initializer
2022-04-09 17:35:23,427 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.self.value.bias using .* initializer
2022-04-09 17:35:23,427 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.output.dense.weight using .* initializer
2022-04-09 17:35:23,427 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.output.dense.bias using .* initializer
2022-04-09 17:35:23,427 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.output.LayerNorm.weight using .* initializer
2022-04-09 17:35:23,428 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.output.LayerNorm.bias using .* initializer
2022-04-09 17:35:23,428 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.intermediate.dense.weight using .* initializer
2022-04-09 17:35:23,429 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.intermediate.dense.bias using .* initializer
2022-04-09 17:35:23,429 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.output.dense.weight using .* initializer
2022-04-09 17:35:23,430 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.output.dense.bias using .* initializer
2022-04-09 17:35:23,430 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.output.LayerNorm.weight using .* initializer
2022-04-09 17:35:23,430 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.output.LayerNorm.bias using .* initializer
2022-04-09 17:35:23,430 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.self.query.weight using .* initializer
2022-04-09 17:35:23,431 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.self.query.bias using .* initializer
2022-04-09 17:35:23,431 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.self.key.weight using .* initializer
2022-04-09 17:35:23,431 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.self.key.bias using .* initializer
2022-04-09 17:35:23,432 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.self.value.weight using .* initializer
2022-04-09 17:35:23,432 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.self.value.bias using .* initializer
2022-04-09 17:35:23,432 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.output.dense.weight using .* initializer
2022-04-09 17:35:23,433 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.output.dense.bias using .* initializer
2022-04-09 17:35:23,433 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.output.LayerNorm.weight using .* initializer
2022-04-09 17:35:23,433 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.output.LayerNorm.bias using .* initializer
2022-04-09 17:35:23,433 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.intermediate.dense.weight using .* initializer
2022-04-09 17:35:23,434 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.intermediate.dense.bias using .* initializer
2022-04-09 17:35:23,434 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.output.dense.weight using .* initializer
2022-04-09 17:35:23,435 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.output.dense.bias using .* initializer
2022-04-09 17:35:23,435 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.output.LayerNorm.weight using .* initializer
2022-04-09 17:35:23,436 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.output.LayerNorm.bias using .* initializer
2022-04-09 17:35:23,436 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.self.query.weight using .* initializer
2022-04-09 17:35:23,436 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.self.query.bias using .* initializer
2022-04-09 17:35:23,436 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.self.key.weight using .* initializer
2022-04-09 17:35:23,437 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.self.key.bias using .* initializer
2022-04-09 17:35:23,437 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.self.value.weight using .* initializer
2022-04-09 17:35:23,437 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.self.value.bias using .* initializer
2022-04-09 17:35:23,438 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.output.dense.weight using .* initializer
2022-04-09 17:35:23,438 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.output.dense.bias using .* initializer
2022-04-09 17:35:23,438 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.output.LayerNorm.weight using .* initializer
2022-04-09 17:35:23,438 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.output.LayerNorm.bias using .* initializer
2022-04-09 17:35:23,439 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.intermediate.dense.weight using .* initializer
2022-04-09 17:35:23,440 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.intermediate.dense.bias using .* initializer
2022-04-09 17:35:23,440 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.output.dense.weight using .* initializer
2022-04-09 17:35:23,441 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.output.dense.bias using .* initializer
2022-04-09 17:35:23,441 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.output.LayerNorm.weight using .* initializer
2022-04-09 17:35:23,441 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.output.LayerNorm.bias using .* initializer
2022-04-09 17:35:23,441 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.self.query.weight using .* initializer
2022-04-09 17:35:23,442 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.self.query.bias using .* initializer
2022-04-09 17:35:23,442 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.self.key.weight using .* initializer
2022-04-09 17:35:23,442 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.self.key.bias using .* initializer
2022-04-09 17:35:23,442 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.self.value.weight using .* initializer
2022-04-09 17:35:23,443 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.self.value.bias using .* initializer
2022-04-09 17:35:23,443 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.output.dense.weight using .* initializer
2022-04-09 17:35:23,443 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.output.dense.bias using .* initializer
2022-04-09 17:35:23,444 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.output.LayerNorm.weight using .* initializer
2022-04-09 17:35:23,444 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.output.LayerNorm.bias using .* initializer
2022-04-09 17:35:23,444 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.intermediate.dense.weight using .* initializer
2022-04-09 17:35:23,445 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.intermediate.dense.bias using .* initializer
2022-04-09 17:35:23,445 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.output.dense.weight using .* initializer
2022-04-09 17:35:23,446 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.output.dense.bias using .* initializer
2022-04-09 17:35:23,446 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.output.LayerNorm.weight using .* initializer
2022-04-09 17:35:23,446 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.output.LayerNorm.bias using .* initializer
2022-04-09 17:35:23,446 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.self.query.weight using .* initializer
2022-04-09 17:35:23,447 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.self.query.bias using .* initializer
2022-04-09 17:35:23,447 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.self.key.weight using .* initializer
2022-04-09 17:35:23,448 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.self.key.bias using .* initializer
2022-04-09 17:35:23,448 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.self.value.weight using .* initializer
2022-04-09 17:35:23,448 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.self.value.bias using .* initializer
2022-04-09 17:35:23,448 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.output.dense.weight using .* initializer
2022-04-09 17:35:23,449 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.output.dense.bias using .* initializer
2022-04-09 17:35:23,449 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.output.LayerNorm.weight using .* initializer
2022-04-09 17:35:23,449 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.output.LayerNorm.bias using .* initializer
2022-04-09 17:35:23,449 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.intermediate.dense.weight using .* initializer
2022-04-09 17:35:23,450 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.intermediate.dense.bias using .* initializer
2022-04-09 17:35:23,450 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.output.dense.weight using .* initializer
2022-04-09 17:35:23,451 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.output.dense.bias using .* initializer
2022-04-09 17:35:23,451 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.output.LayerNorm.weight using .* initializer
2022-04-09 17:35:23,452 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.output.LayerNorm.bias using .* initializer
2022-04-09 17:35:23,452 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.self.query.weight using .* initializer
2022-04-09 17:35:23,452 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.self.query.bias using .* initializer
2022-04-09 17:35:23,452 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.self.key.weight using .* initializer
2022-04-09 17:35:23,453 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.self.key.bias using .* initializer
2022-04-09 17:35:23,453 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.self.value.weight using .* initializer
2022-04-09 17:35:23,453 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.self.value.bias using .* initializer
2022-04-09 17:35:23,454 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.output.dense.weight using .* initializer
2022-04-09 17:35:23,454 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.output.dense.bias using .* initializer
2022-04-09 17:35:23,454 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.output.LayerNorm.weight using .* initializer
2022-04-09 17:35:23,454 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.output.LayerNorm.bias using .* initializer
2022-04-09 17:35:23,455 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.intermediate.dense.weight using .* initializer
2022-04-09 17:35:23,456 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.intermediate.dense.bias using .* initializer
2022-04-09 17:35:23,456 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.output.dense.weight using .* initializer
2022-04-09 17:35:23,457 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.output.dense.bias using .* initializer
2022-04-09 17:35:23,457 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.output.LayerNorm.weight using .* initializer
2022-04-09 17:35:23,457 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.output.LayerNorm.bias using .* initializer
2022-04-09 17:35:23,457 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.self.query.weight using .* initializer
2022-04-09 17:35:23,458 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.self.query.bias using .* initializer
2022-04-09 17:35:23,458 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.self.key.weight using .* initializer
2022-04-09 17:35:23,458 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.self.key.bias using .* initializer
2022-04-09 17:35:23,458 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.self.value.weight using .* initializer
2022-04-09 17:35:23,459 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.self.value.bias using .* initializer
2022-04-09 17:35:23,459 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.output.dense.weight using .* initializer
2022-04-09 17:35:23,459 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.output.dense.bias using .* initializer
2022-04-09 17:35:23,460 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.output.LayerNorm.weight using .* initializer
2022-04-09 17:35:23,460 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.output.LayerNorm.bias using .* initializer
2022-04-09 17:35:23,460 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.intermediate.dense.weight using .* initializer
2022-04-09 17:35:23,461 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.intermediate.dense.bias using .* initializer
2022-04-09 17:35:23,461 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.output.dense.weight using .* initializer
2022-04-09 17:35:23,462 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.output.dense.bias using .* initializer
2022-04-09 17:35:23,462 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.output.LayerNorm.weight using .* initializer
2022-04-09 17:35:23,462 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.output.LayerNorm.bias using .* initializer
2022-04-09 17:35:23,462 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.self.query.weight using .* initializer
2022-04-09 17:35:23,463 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.self.query.bias using .* initializer
2022-04-09 17:35:23,463 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.self.key.weight using .* initializer
2022-04-09 17:35:23,463 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.self.key.bias using .* initializer
2022-04-09 17:35:23,464 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.self.value.weight using .* initializer
2022-04-09 17:35:23,464 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.self.value.bias using .* initializer
2022-04-09 17:35:23,464 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.output.dense.weight using .* initializer
2022-04-09 17:35:23,465 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.output.dense.bias using .* initializer
2022-04-09 17:35:23,465 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.output.LayerNorm.weight using .* initializer
2022-04-09 17:35:23,465 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.output.LayerNorm.bias using .* initializer
2022-04-09 17:35:23,465 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.intermediate.dense.weight using .* initializer
2022-04-09 17:35:23,466 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.intermediate.dense.bias using .* initializer
2022-04-09 17:35:23,466 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.output.dense.weight using .* initializer
2022-04-09 17:35:23,467 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.output.dense.bias using .* initializer
2022-04-09 17:35:23,467 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.output.LayerNorm.weight using .* initializer
2022-04-09 17:35:23,468 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.output.LayerNorm.bias using .* initializer
2022-04-09 17:35:23,468 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.self.query.weight using .* initializer
2022-04-09 17:35:23,468 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.self.query.bias using .* initializer
2022-04-09 17:35:23,468 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.self.key.weight using .* initializer
2022-04-09 17:35:23,469 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.self.key.bias using .* initializer
2022-04-09 17:35:23,469 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.self.value.weight using .* initializer
2022-04-09 17:35:23,469 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.self.value.bias using .* initializer
2022-04-09 17:35:23,469 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.output.dense.weight using .* initializer
2022-04-09 17:35:23,470 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.output.dense.bias using .* initializer
2022-04-09 17:35:23,470 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.output.LayerNorm.weight using .* initializer
2022-04-09 17:35:23,470 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.output.LayerNorm.bias using .* initializer
2022-04-09 17:35:23,470 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.intermediate.dense.weight using .* initializer
2022-04-09 17:35:23,471 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.intermediate.dense.bias using .* initializer
2022-04-09 17:35:23,472 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.output.dense.weight using .* initializer
2022-04-09 17:35:23,472 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.output.dense.bias using .* initializer
2022-04-09 17:35:23,473 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.output.LayerNorm.weight using .* initializer
2022-04-09 17:35:23,473 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.output.LayerNorm.bias using .* initializer
2022-04-09 17:35:23,473 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.self.query.weight using .* initializer
2022-04-09 17:35:23,473 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.self.query.bias using .* initializer
2022-04-09 17:35:23,474 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.self.key.weight using .* initializer
2022-04-09 17:35:23,474 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.self.key.bias using .* initializer
2022-04-09 17:35:23,474 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.self.value.weight using .* initializer
2022-04-09 17:35:23,475 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.self.value.bias using .* initializer
2022-04-09 17:35:23,475 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.output.dense.weight using .* initializer
2022-04-09 17:35:23,475 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.output.dense.bias using .* initializer
2022-04-09 17:35:23,475 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.output.LayerNorm.weight using .* initializer
2022-04-09 17:35:23,475 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.output.LayerNorm.bias using .* initializer
2022-04-09 17:35:23,476 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.intermediate.dense.weight using .* initializer
2022-04-09 17:35:23,477 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.intermediate.dense.bias using .* initializer
2022-04-09 17:35:23,477 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.output.dense.weight using .* initializer
2022-04-09 17:35:23,478 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.output.dense.bias using .* initializer
2022-04-09 17:35:23,478 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.output.LayerNorm.weight using .* initializer
2022-04-09 17:35:23,478 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.output.LayerNorm.bias using .* initializer
2022-04-09 17:35:23,478 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.self.query.weight using .* initializer
2022-04-09 17:35:23,479 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.self.query.bias using .* initializer
2022-04-09 17:35:23,479 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.self.key.weight using .* initializer
2022-04-09 17:35:23,479 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.self.key.bias using .* initializer
2022-04-09 17:35:23,479 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.self.value.weight using .* initializer
2022-04-09 17:35:23,480 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.self.value.bias using .* initializer
2022-04-09 17:35:23,480 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.output.dense.weight using .* initializer
2022-04-09 17:35:23,480 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.output.dense.bias using .* initializer
2022-04-09 17:35:23,481 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.output.LayerNorm.weight using .* initializer
2022-04-09 17:35:23,481 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.output.LayerNorm.bias using .* initializer
2022-04-09 17:35:23,481 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.intermediate.dense.weight using .* initializer
2022-04-09 17:35:23,482 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.intermediate.dense.bias using .* initializer
2022-04-09 17:35:23,482 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.output.dense.weight using .* initializer
2022-04-09 17:35:23,483 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.output.dense.bias using .* initializer
2022-04-09 17:35:23,483 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.output.LayerNorm.weight using .* initializer
2022-04-09 17:35:23,483 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.output.LayerNorm.bias using .* initializer
2022-04-09 17:35:23,484 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.self.query.weight using .* initializer
2022-04-09 17:35:23,484 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.self.query.bias using .* initializer
2022-04-09 17:35:23,484 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.self.key.weight using .* initializer
2022-04-09 17:35:23,485 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.self.key.bias using .* initializer
2022-04-09 17:35:23,485 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.self.value.weight using .* initializer
2022-04-09 17:35:23,485 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.self.value.bias using .* initializer
2022-04-09 17:35:23,485 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.output.dense.weight using .* initializer
2022-04-09 17:35:23,486 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.output.dense.bias using .* initializer
2022-04-09 17:35:23,486 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.output.LayerNorm.weight using .* initializer
2022-04-09 17:35:23,486 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.output.LayerNorm.bias using .* initializer
2022-04-09 17:35:23,486 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.intermediate.dense.weight using .* initializer
2022-04-09 17:35:23,487 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.intermediate.dense.bias using .* initializer
2022-04-09 17:35:23,487 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.output.dense.weight using .* initializer
2022-04-09 17:35:23,488 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.output.dense.bias using .* initializer
2022-04-09 17:35:23,489 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.output.LayerNorm.weight using .* initializer
2022-04-09 17:35:23,489 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.output.LayerNorm.bias using .* initializer
2022-04-09 17:35:23,489 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.pooler.dense.weight using .* initializer
2022-04-09 17:35:23,489 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.pooler.dense.bias using .* initializer
2022-04-09 17:35:23,490 - INFO - allennlp.nn.initializers - Initializing _classifier.classifier.dense.weight using .* initializer
2022-04-09 17:35:23,490 - INFO - allennlp.nn.initializers - Initializing _classifier.classifier.dense.bias using .* initializer
2022-04-09 17:35:23,490 - INFO - allennlp.nn.initializers - Initializing _classifier.classifier.out_proj.weight using .* initializer
2022-04-09 17:35:23,490 - INFO - allennlp.nn.initializers - Initializing _classifier.classifier.out_proj.bias using .* initializer
2022-04-09 17:35:23,491 - INFO - allennlp.nn.initializers - Done initializing parameters; the following parameters are using their default initialization from their code
2022-04-09 17:35:24,055 - INFO - filelock - Lock 140429248391120 acquired on ./output_ir-ora-d_hyper1/vocabulary/.lock
2022-04-09 17:35:24,056 - INFO - filelock - Lock 140429248391120 released on ./output_ir-ora-d_hyper1/vocabulary/.lock
2022-04-09 17:35:24,056 - INFO - allennlp.common.params - data_loader.type = pytorch_dataloader
2022-04-09 17:35:24,057 - INFO - allennlp.common.params - data_loader.batch_size = 1
2022-04-09 17:35:24,057 - INFO - allennlp.common.params - data_loader.shuffle = False
2022-04-09 17:35:24,057 - INFO - allennlp.common.params - data_loader.sampler = None
2022-04-09 17:35:24,057 - INFO - allennlp.common.params - data_loader.num_workers = 0
2022-04-09 17:35:24,057 - INFO - allennlp.common.params - data_loader.pin_memory = False
2022-04-09 17:35:24,057 - INFO - allennlp.common.params - data_loader.drop_last = False
2022-04-09 17:35:24,058 - INFO - allennlp.common.params - data_loader.timeout = 0
2022-04-09 17:35:24,058 - INFO - allennlp.common.params - data_loader.worker_init_fn = None
2022-04-09 17:35:24,058 - INFO - allennlp.common.params - data_loader.multiprocessing_context = None
2022-04-09 17:35:24,058 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None
2022-04-09 17:35:24,058 - INFO - allennlp.common.params - data_loader.batch_sampler.type = bucket
2022-04-09 17:35:24,059 - INFO - allennlp.common.params - data_loader.batch_sampler.batch_size = 2
2022-04-09 17:35:24,059 - INFO - allennlp.common.params - data_loader.batch_sampler.sorting_keys = None
2022-04-09 17:35:24,059 - INFO - allennlp.common.params - data_loader.batch_sampler.padding_noise = 0.1
2022-04-09 17:35:24,059 - INFO - allennlp.common.params - data_loader.batch_sampler.drop_last = False
2022-04-09 17:35:24,060 - INFO - allennlp.common.params - data_loader.type = pytorch_dataloader
2022-04-09 17:35:24,060 - INFO - allennlp.common.params - data_loader.batch_size = 1
2022-04-09 17:35:24,060 - INFO - allennlp.common.params - data_loader.shuffle = False
2022-04-09 17:35:24,061 - INFO - allennlp.common.params - data_loader.sampler = None
2022-04-09 17:35:24,061 - INFO - allennlp.common.params - data_loader.num_workers = 0
2022-04-09 17:35:24,061 - INFO - allennlp.common.params - data_loader.pin_memory = False
2022-04-09 17:35:24,061 - INFO - allennlp.common.params - data_loader.drop_last = False
2022-04-09 17:35:24,061 - INFO - allennlp.common.params - data_loader.timeout = 0
2022-04-09 17:35:24,062 - INFO - allennlp.common.params - data_loader.worker_init_fn = None
2022-04-09 17:35:24,062 - INFO - allennlp.common.params - data_loader.multiprocessing_context = None
2022-04-09 17:35:24,062 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None
2022-04-09 17:35:24,062 - INFO - allennlp.common.params - data_loader.batch_sampler.type = bucket
2022-04-09 17:35:24,062 - INFO - allennlp.common.params - data_loader.batch_sampler.batch_size = 2
2022-04-09 17:35:24,063 - INFO - allennlp.common.params - data_loader.batch_sampler.sorting_keys = None
2022-04-09 17:35:24,063 - INFO - allennlp.common.params - data_loader.batch_sampler.padding_noise = 0.1
2022-04-09 17:35:24,063 - INFO - allennlp.common.params - data_loader.batch_sampler.drop_last = False
2022-04-09 17:35:24,063 - INFO - allennlp.common.params - trainer.type = gradient_descent
2022-04-09 17:35:24,064 - INFO - allennlp.common.params - trainer.patience = None
2022-04-09 17:35:24,064 - INFO - allennlp.common.params - trainer.validation_metric = +accuracy
2022-04-09 17:35:24,064 - INFO - allennlp.common.params - trainer.num_epochs = 10
2022-04-09 17:35:24,064 - INFO - allennlp.common.params - trainer.cuda_device = 0
2022-04-09 17:35:24,065 - INFO - allennlp.common.params - trainer.grad_norm = None
2022-04-09 17:35:24,065 - INFO - allennlp.common.params - trainer.grad_clipping = None
2022-04-09 17:35:24,065 - INFO - allennlp.common.params - trainer.distributed = False
2022-04-09 17:35:24,065 - INFO - allennlp.common.params - trainer.world_size = 1
2022-04-09 17:35:24,065 - INFO - allennlp.common.params - trainer.num_gradient_accumulation_steps = 16
2022-04-09 17:35:24,065 - INFO - allennlp.common.params - trainer.use_amp = False
2022-04-09 17:35:24,065 - INFO - allennlp.common.params - trainer.no_grad = None
2022-04-09 17:35:24,066 - INFO - allennlp.common.params - trainer.momentum_scheduler = None
2022-04-09 17:35:24,066 - INFO - allennlp.common.params - trainer.tensorboard_writer = <allennlp.common.lazy.Lazy object at 0x7fb86309c390>
2022-04-09 17:35:24,066 - INFO - allennlp.common.params - trainer.moving_average = None
2022-04-09 17:35:24,066 - INFO - allennlp.common.params - trainer.checkpointer = <allennlp.common.lazy.Lazy object at 0x7fb86309c410>
2022-04-09 17:35:24,066 - INFO - allennlp.common.params - trainer.batch_callbacks = None
2022-04-09 17:35:24,067 - INFO - allennlp.common.params - trainer.epoch_callbacks = None
2022-04-09 17:35:24,067 - INFO - allennlp.common.params - trainer.end_callbacks = None
2022-04-09 17:35:24,067 - INFO - allennlp.common.params - trainer.trainer_callbacks = None
2022-04-09 17:35:26,825 - INFO - allennlp.common.params - trainer.optimizer.type = huggingface_adamw
2022-04-09 17:35:26,825 - INFO - allennlp.common.params - trainer.optimizer.parameter_groups = None
2022-04-09 17:35:26,825 - INFO - allennlp.common.params - trainer.optimizer.lr = 1e-05
2022-04-09 17:35:26,826 - INFO - allennlp.common.params - trainer.optimizer.betas = [0.9, 0.98]
2022-04-09 17:35:26,826 - INFO - allennlp.common.params - trainer.optimizer.eps = 1e-06
2022-04-09 17:35:26,826 - INFO - allennlp.common.params - trainer.optimizer.weight_decay = 0.05
2022-04-09 17:35:26,826 - INFO - allennlp.common.params - trainer.optimizer.correct_bias = True
2022-04-09 17:35:26,826 - INFO - allennlp.training.optimizers - Number of trainable parameters: 356411394
2022-04-09 17:35:26,828 - INFO - allennlp.common.util - The following parameters are Frozen (without gradient):
2022-04-09 17:35:26,831 - INFO - allennlp.common.util - The following parameters are Tunable (with gradient):
2022-04-09 17:35:26,831 - INFO - allennlp.common.util - _classifier.roberta.embeddings.word_embeddings.weight
2022-04-09 17:35:26,831 - INFO - allennlp.common.util - _classifier.roberta.embeddings.position_embeddings.weight
2022-04-09 17:35:26,831 - INFO - allennlp.common.util - _classifier.roberta.embeddings.token_type_embeddings.weight
2022-04-09 17:35:26,832 - INFO - allennlp.common.util - _classifier.roberta.embeddings.LayerNorm.weight
2022-04-09 17:35:26,832 - INFO - allennlp.common.util - _classifier.roberta.embeddings.LayerNorm.bias
2022-04-09 17:35:26,832 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.self.query.weight
2022-04-09 17:35:26,832 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.self.query.bias
2022-04-09 17:35:26,832 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.self.key.weight
2022-04-09 17:35:26,832 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.self.key.bias
2022-04-09 17:35:26,832 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.self.value.weight
2022-04-09 17:35:26,832 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.self.value.bias
2022-04-09 17:35:26,832 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.output.dense.weight
2022-04-09 17:35:26,833 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.output.dense.bias
2022-04-09 17:35:26,833 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.output.LayerNorm.weight
2022-04-09 17:35:26,833 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.output.LayerNorm.bias
2022-04-09 17:35:26,833 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.intermediate.dense.weight
2022-04-09 17:35:26,833 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.intermediate.dense.bias
2022-04-09 17:35:26,833 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.output.dense.weight
2022-04-09 17:35:26,833 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.output.dense.bias
2022-04-09 17:35:26,833 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.output.LayerNorm.weight
2022-04-09 17:35:26,834 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.output.LayerNorm.bias
2022-04-09 17:35:26,834 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.self.query.weight
2022-04-09 17:35:26,834 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.self.query.bias
2022-04-09 17:35:26,834 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.self.key.weight
2022-04-09 17:35:26,834 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.self.key.bias
2022-04-09 17:35:26,834 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.self.value.weight
2022-04-09 17:35:26,834 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.self.value.bias
2022-04-09 17:35:26,834 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.output.dense.weight
2022-04-09 17:35:26,835 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.output.dense.bias
2022-04-09 17:35:26,835 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.output.LayerNorm.weight
2022-04-09 17:35:26,835 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.output.LayerNorm.bias
2022-04-09 17:35:26,835 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.intermediate.dense.weight
2022-04-09 17:35:26,835 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.intermediate.dense.bias
2022-04-09 17:35:26,835 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.output.dense.weight
2022-04-09 17:35:26,835 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.output.dense.bias
2022-04-09 17:35:26,835 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.output.LayerNorm.weight
2022-04-09 17:35:26,835 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.output.LayerNorm.bias
2022-04-09 17:35:26,836 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.self.query.weight
2022-04-09 17:35:26,836 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.self.query.bias
2022-04-09 17:35:26,836 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.self.key.weight
2022-04-09 17:35:26,836 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.self.key.bias
2022-04-09 17:35:26,836 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.self.value.weight
2022-04-09 17:35:26,836 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.self.value.bias
2022-04-09 17:35:26,836 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.output.dense.weight
2022-04-09 17:35:26,836 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.output.dense.bias
2022-04-09 17:35:26,837 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.output.LayerNorm.weight
2022-04-09 17:35:26,837 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.output.LayerNorm.bias
2022-04-09 17:35:26,837 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.intermediate.dense.weight
2022-04-09 17:35:26,837 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.intermediate.dense.bias
2022-04-09 17:35:26,837 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.output.dense.weight
2022-04-09 17:35:26,837 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.output.dense.bias
2022-04-09 17:35:26,837 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.output.LayerNorm.weight
2022-04-09 17:35:26,837 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.output.LayerNorm.bias
2022-04-09 17:35:26,837 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.self.query.weight
2022-04-09 17:35:26,838 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.self.query.bias
2022-04-09 17:35:26,838 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.self.key.weight
2022-04-09 17:35:26,838 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.self.key.bias
2022-04-09 17:35:26,838 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.self.value.weight
2022-04-09 17:35:26,838 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.self.value.bias
2022-04-09 17:35:26,838 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.output.dense.weight
2022-04-09 17:35:26,838 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.output.dense.bias
2022-04-09 17:35:26,838 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.output.LayerNorm.weight
2022-04-09 17:35:26,839 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.output.LayerNorm.bias
2022-04-09 17:35:26,839 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.intermediate.dense.weight
2022-04-09 17:35:26,839 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.intermediate.dense.bias
2022-04-09 17:35:26,839 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.output.dense.weight
2022-04-09 17:35:26,839 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.output.dense.bias
2022-04-09 17:35:26,839 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.output.LayerNorm.weight
2022-04-09 17:35:26,839 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.output.LayerNorm.bias
2022-04-09 17:35:26,839 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.self.query.weight
2022-04-09 17:35:26,840 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.self.query.bias
2022-04-09 17:35:26,840 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.self.key.weight
2022-04-09 17:35:26,840 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.self.key.bias
2022-04-09 17:35:26,840 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.self.value.weight
2022-04-09 17:35:26,840 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.self.value.bias
2022-04-09 17:35:26,840 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.output.dense.weight
2022-04-09 17:35:26,840 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.output.dense.bias
2022-04-09 17:35:26,840 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.output.LayerNorm.weight
2022-04-09 17:35:26,841 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.output.LayerNorm.bias
2022-04-09 17:35:26,841 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.intermediate.dense.weight
2022-04-09 17:35:26,841 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.intermediate.dense.bias
2022-04-09 17:35:26,841 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.output.dense.weight
2022-04-09 17:35:26,841 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.output.dense.bias
2022-04-09 17:35:26,841 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.output.LayerNorm.weight
2022-04-09 17:35:26,841 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.output.LayerNorm.bias
2022-04-09 17:35:26,841 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.self.query.weight
2022-04-09 17:35:26,841 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.self.query.bias
2022-04-09 17:35:26,842 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.self.key.weight
2022-04-09 17:35:26,842 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.self.key.bias
2022-04-09 17:35:26,842 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.self.value.weight
2022-04-09 17:35:26,842 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.self.value.bias
2022-04-09 17:35:26,842 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.output.dense.weight
2022-04-09 17:35:26,842 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.output.dense.bias
2022-04-09 17:35:26,842 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.output.LayerNorm.weight
2022-04-09 17:35:26,842 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.output.LayerNorm.bias
2022-04-09 17:35:26,843 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.intermediate.dense.weight
2022-04-09 17:35:26,843 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.intermediate.dense.bias
2022-04-09 17:35:26,843 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.output.dense.weight
2022-04-09 17:35:26,843 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.output.dense.bias
2022-04-09 17:35:26,843 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.output.LayerNorm.weight
2022-04-09 17:35:26,843 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.output.LayerNorm.bias
2022-04-09 17:35:26,843 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.self.query.weight
2022-04-09 17:35:26,843 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.self.query.bias
2022-04-09 17:35:26,844 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.self.key.weight
2022-04-09 17:35:26,844 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.self.key.bias
2022-04-09 17:35:26,844 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.self.value.weight
2022-04-09 17:35:26,844 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.self.value.bias
2022-04-09 17:35:26,844 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.output.dense.weight
2022-04-09 17:35:26,844 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.output.dense.bias
2022-04-09 17:35:26,844 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.output.LayerNorm.weight
2022-04-09 17:35:26,844 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.output.LayerNorm.bias
2022-04-09 17:35:26,845 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.intermediate.dense.weight
2022-04-09 17:35:26,845 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.intermediate.dense.bias
2022-04-09 17:35:26,845 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.output.dense.weight
2022-04-09 17:35:26,845 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.output.dense.bias
2022-04-09 17:35:26,845 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.output.LayerNorm.weight
2022-04-09 17:35:26,845 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.output.LayerNorm.bias
2022-04-09 17:35:26,845 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.self.query.weight
2022-04-09 17:35:26,845 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.self.query.bias
2022-04-09 17:35:26,845 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.self.key.weight
2022-04-09 17:35:26,846 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.self.key.bias
2022-04-09 17:35:26,846 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.self.value.weight
2022-04-09 17:35:26,846 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.self.value.bias
2022-04-09 17:35:26,846 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.output.dense.weight
2022-04-09 17:35:26,846 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.output.dense.bias
2022-04-09 17:35:26,846 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.output.LayerNorm.weight
2022-04-09 17:35:26,846 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.output.LayerNorm.bias
2022-04-09 17:35:26,846 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.intermediate.dense.weight
2022-04-09 17:35:26,847 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.intermediate.dense.bias
2022-04-09 17:35:26,847 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.output.dense.weight
2022-04-09 17:35:26,847 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.output.dense.bias
2022-04-09 17:35:26,847 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.output.LayerNorm.weight
2022-04-09 17:35:26,847 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.output.LayerNorm.bias
2022-04-09 17:35:26,847 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.self.query.weight
2022-04-09 17:35:26,847 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.self.query.bias
2022-04-09 17:35:26,847 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.self.key.weight
2022-04-09 17:35:26,848 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.self.key.bias
2022-04-09 17:35:26,848 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.self.value.weight
2022-04-09 17:35:26,848 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.self.value.bias
2022-04-09 17:35:26,848 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.output.dense.weight
2022-04-09 17:35:26,848 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.output.dense.bias
2022-04-09 17:35:26,848 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.output.LayerNorm.weight
2022-04-09 17:35:26,848 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.output.LayerNorm.bias
2022-04-09 17:35:26,848 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.intermediate.dense.weight
2022-04-09 17:35:26,848 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.intermediate.dense.bias
2022-04-09 17:35:26,849 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.output.dense.weight
2022-04-09 17:35:26,849 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.output.dense.bias
2022-04-09 17:35:26,849 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.output.LayerNorm.weight
2022-04-09 17:35:26,849 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.output.LayerNorm.bias
2022-04-09 17:35:26,849 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.self.query.weight
2022-04-09 17:35:26,849 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.self.query.bias
2022-04-09 17:35:26,849 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.self.key.weight
2022-04-09 17:35:26,849 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.self.key.bias
2022-04-09 17:35:26,850 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.self.value.weight
2022-04-09 17:35:26,850 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.self.value.bias
2022-04-09 17:35:26,850 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.output.dense.weight
2022-04-09 17:35:26,850 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.output.dense.bias
2022-04-09 17:35:26,850 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.output.LayerNorm.weight
2022-04-09 17:35:26,850 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.output.LayerNorm.bias
2022-04-09 17:35:26,850 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.intermediate.dense.weight
2022-04-09 17:35:26,850 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.intermediate.dense.bias
2022-04-09 17:35:26,851 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.output.dense.weight
2022-04-09 17:35:26,851 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.output.dense.bias
2022-04-09 17:35:26,851 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.output.LayerNorm.weight
2022-04-09 17:35:26,851 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.output.LayerNorm.bias
2022-04-09 17:35:26,851 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.self.query.weight
2022-04-09 17:35:26,851 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.self.query.bias
2022-04-09 17:35:26,851 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.self.key.weight
2022-04-09 17:35:26,851 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.self.key.bias
2022-04-09 17:35:26,851 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.self.value.weight
2022-04-09 17:35:26,851 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.self.value.bias
2022-04-09 17:35:26,851 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.output.dense.weight
2022-04-09 17:35:26,851 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.output.dense.bias
2022-04-09 17:35:26,851 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.output.LayerNorm.weight
2022-04-09 17:35:26,851 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.output.LayerNorm.bias
2022-04-09 17:35:26,852 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.intermediate.dense.weight
2022-04-09 17:35:26,852 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.intermediate.dense.bias
2022-04-09 17:35:26,852 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.output.dense.weight
2022-04-09 17:35:26,852 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.output.dense.bias
2022-04-09 17:35:26,852 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.output.LayerNorm.weight
2022-04-09 17:35:26,852 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.output.LayerNorm.bias
2022-04-09 17:35:26,852 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.self.query.weight
2022-04-09 17:35:26,852 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.self.query.bias
2022-04-09 17:35:26,852 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.self.key.weight
2022-04-09 17:35:26,852 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.self.key.bias
2022-04-09 17:35:26,852 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.self.value.weight
2022-04-09 17:35:26,852 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.self.value.bias
2022-04-09 17:35:26,852 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.output.dense.weight
2022-04-09 17:35:26,852 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.output.dense.bias
2022-04-09 17:35:26,852 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.output.LayerNorm.weight
2022-04-09 17:35:26,852 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.output.LayerNorm.bias
2022-04-09 17:35:26,852 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.intermediate.dense.weight
2022-04-09 17:35:26,852 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.intermediate.dense.bias
2022-04-09 17:35:26,853 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.output.dense.weight
2022-04-09 17:35:26,853 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.output.dense.bias
2022-04-09 17:35:26,853 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.output.LayerNorm.weight
2022-04-09 17:35:26,853 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.output.LayerNorm.bias
2022-04-09 17:35:26,853 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.self.query.weight
2022-04-09 17:35:26,853 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.self.query.bias
2022-04-09 17:35:26,853 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.self.key.weight
2022-04-09 17:35:26,853 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.self.key.bias
2022-04-09 17:35:26,853 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.self.value.weight
2022-04-09 17:35:26,853 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.self.value.bias
2022-04-09 17:35:26,853 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.output.dense.weight
2022-04-09 17:35:26,853 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.output.dense.bias
2022-04-09 17:35:26,853 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.output.LayerNorm.weight
2022-04-09 17:35:26,853 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.output.LayerNorm.bias
2022-04-09 17:35:26,853 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.intermediate.dense.weight
2022-04-09 17:35:26,853 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.intermediate.dense.bias
2022-04-09 17:35:26,853 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.output.dense.weight
2022-04-09 17:35:26,853 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.output.dense.bias
2022-04-09 17:35:26,854 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.output.LayerNorm.weight
2022-04-09 17:35:26,854 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.output.LayerNorm.bias
2022-04-09 17:35:26,854 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.self.query.weight
2022-04-09 17:35:26,854 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.self.query.bias
2022-04-09 17:35:26,854 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.self.key.weight
2022-04-09 17:35:26,854 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.self.key.bias
2022-04-09 17:35:26,854 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.self.value.weight
2022-04-09 17:35:26,854 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.self.value.bias
2022-04-09 17:35:26,854 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.output.dense.weight
2022-04-09 17:35:26,854 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.output.dense.bias
2022-04-09 17:35:26,854 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.output.LayerNorm.weight
2022-04-09 17:35:26,854 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.output.LayerNorm.bias
2022-04-09 17:35:26,854 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.intermediate.dense.weight
2022-04-09 17:35:26,854 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.intermediate.dense.bias
2022-04-09 17:35:26,854 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.output.dense.weight
2022-04-09 17:35:26,854 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.output.dense.bias
2022-04-09 17:35:26,854 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.output.LayerNorm.weight
2022-04-09 17:35:26,855 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.output.LayerNorm.bias
2022-04-09 17:35:26,855 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.self.query.weight
2022-04-09 17:35:26,855 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.self.query.bias
2022-04-09 17:35:26,855 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.self.key.weight
2022-04-09 17:35:26,855 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.self.key.bias
2022-04-09 17:35:26,855 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.self.value.weight
2022-04-09 17:35:26,855 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.self.value.bias
2022-04-09 17:35:26,855 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.output.dense.weight
2022-04-09 17:35:26,855 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.output.dense.bias
2022-04-09 17:35:26,855 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.output.LayerNorm.weight
2022-04-09 17:35:26,855 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.output.LayerNorm.bias
2022-04-09 17:35:26,855 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.intermediate.dense.weight
2022-04-09 17:35:26,855 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.intermediate.dense.bias
2022-04-09 17:35:26,855 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.output.dense.weight
2022-04-09 17:35:26,855 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.output.dense.bias
2022-04-09 17:35:26,855 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.output.LayerNorm.weight
2022-04-09 17:35:26,855 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.output.LayerNorm.bias
2022-04-09 17:35:26,856 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.self.query.weight
2022-04-09 17:35:26,856 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.self.query.bias
2022-04-09 17:35:26,856 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.self.key.weight
2022-04-09 17:35:26,856 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.self.key.bias
2022-04-09 17:35:26,856 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.self.value.weight
2022-04-09 17:35:26,856 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.self.value.bias
2022-04-09 17:35:26,856 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.output.dense.weight
2022-04-09 17:35:26,856 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.output.dense.bias
2022-04-09 17:35:26,856 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.output.LayerNorm.weight
2022-04-09 17:35:26,856 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.output.LayerNorm.bias
2022-04-09 17:35:26,856 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.intermediate.dense.weight
2022-04-09 17:35:26,856 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.intermediate.dense.bias
2022-04-09 17:35:26,856 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.output.dense.weight
2022-04-09 17:35:26,856 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.output.dense.bias
2022-04-09 17:35:26,856 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.output.LayerNorm.weight
2022-04-09 17:35:26,856 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.output.LayerNorm.bias
2022-04-09 17:35:26,856 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.self.query.weight
2022-04-09 17:35:26,856 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.self.query.bias
2022-04-09 17:35:26,857 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.self.key.weight
2022-04-09 17:35:26,857 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.self.key.bias
2022-04-09 17:35:26,857 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.self.value.weight
2022-04-09 17:35:26,857 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.self.value.bias
2022-04-09 17:35:26,857 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.output.dense.weight
2022-04-09 17:35:26,857 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.output.dense.bias
2022-04-09 17:35:26,857 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.output.LayerNorm.weight
2022-04-09 17:35:26,857 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.output.LayerNorm.bias
2022-04-09 17:35:26,857 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.intermediate.dense.weight
2022-04-09 17:35:26,857 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.intermediate.dense.bias
2022-04-09 17:35:26,857 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.output.dense.weight
2022-04-09 17:35:26,857 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.output.dense.bias
2022-04-09 17:35:26,857 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.output.LayerNorm.weight
2022-04-09 17:35:26,857 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.output.LayerNorm.bias
2022-04-09 17:35:26,857 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.self.query.weight
2022-04-09 17:35:26,857 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.self.query.bias
2022-04-09 17:35:26,857 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.self.key.weight
2022-04-09 17:35:26,857 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.self.key.bias
2022-04-09 17:35:26,857 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.self.value.weight
2022-04-09 17:35:26,858 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.self.value.bias
2022-04-09 17:35:26,858 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.output.dense.weight
2022-04-09 17:35:26,858 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.output.dense.bias
2022-04-09 17:35:26,858 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.output.LayerNorm.weight
2022-04-09 17:35:26,858 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.output.LayerNorm.bias
2022-04-09 17:35:26,858 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.intermediate.dense.weight
2022-04-09 17:35:26,858 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.intermediate.dense.bias
2022-04-09 17:35:26,858 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.output.dense.weight
2022-04-09 17:35:26,858 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.output.dense.bias
2022-04-09 17:35:26,858 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.output.LayerNorm.weight
2022-04-09 17:35:26,858 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.output.LayerNorm.bias
2022-04-09 17:35:26,858 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.self.query.weight
2022-04-09 17:35:26,858 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.self.query.bias
2022-04-09 17:35:26,858 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.self.key.weight
2022-04-09 17:35:26,858 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.self.key.bias
2022-04-09 17:35:26,858 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.self.value.weight
2022-04-09 17:35:26,858 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.self.value.bias
2022-04-09 17:35:26,858 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.output.dense.weight
2022-04-09 17:35:26,859 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.output.dense.bias
2022-04-09 17:35:26,859 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.output.LayerNorm.weight
2022-04-09 17:35:26,859 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.output.LayerNorm.bias
2022-04-09 17:35:26,859 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.intermediate.dense.weight
2022-04-09 17:35:26,859 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.intermediate.dense.bias
2022-04-09 17:35:26,859 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.output.dense.weight
2022-04-09 17:35:26,859 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.output.dense.bias
2022-04-09 17:35:26,859 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.output.LayerNorm.weight
2022-04-09 17:35:26,859 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.output.LayerNorm.bias
2022-04-09 17:35:26,859 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.self.query.weight
2022-04-09 17:35:26,859 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.self.query.bias
2022-04-09 17:35:26,859 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.self.key.weight
2022-04-09 17:35:26,859 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.self.key.bias
2022-04-09 17:35:26,859 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.self.value.weight
2022-04-09 17:35:26,859 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.self.value.bias
2022-04-09 17:35:26,859 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.output.dense.weight
2022-04-09 17:35:26,859 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.output.dense.bias
2022-04-09 17:35:26,859 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.output.LayerNorm.weight
2022-04-09 17:35:26,860 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.output.LayerNorm.bias
2022-04-09 17:35:26,860 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.intermediate.dense.weight
2022-04-09 17:35:26,860 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.intermediate.dense.bias
2022-04-09 17:35:26,860 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.output.dense.weight
2022-04-09 17:35:26,860 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.output.dense.bias
2022-04-09 17:35:26,860 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.output.LayerNorm.weight
2022-04-09 17:35:26,860 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.output.LayerNorm.bias
2022-04-09 17:35:26,860 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.self.query.weight
2022-04-09 17:35:26,860 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.self.query.bias
2022-04-09 17:35:26,860 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.self.key.weight
2022-04-09 17:35:26,860 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.self.key.bias
2022-04-09 17:35:26,860 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.self.value.weight
2022-04-09 17:35:26,860 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.self.value.bias
2022-04-09 17:35:26,860 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.output.dense.weight
2022-04-09 17:35:26,860 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.output.dense.bias
2022-04-09 17:35:26,860 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.output.LayerNorm.weight
2022-04-09 17:35:26,860 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.output.LayerNorm.bias
2022-04-09 17:35:26,861 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.intermediate.dense.weight
2022-04-09 17:35:26,861 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.intermediate.dense.bias
2022-04-09 17:35:26,861 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.output.dense.weight
2022-04-09 17:35:26,861 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.output.dense.bias
2022-04-09 17:35:26,861 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.output.LayerNorm.weight
2022-04-09 17:35:26,861 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.output.LayerNorm.bias
2022-04-09 17:35:26,861 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.self.query.weight
2022-04-09 17:35:26,861 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.self.query.bias
2022-04-09 17:35:26,861 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.self.key.weight
2022-04-09 17:35:26,861 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.self.key.bias
2022-04-09 17:35:26,861 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.self.value.weight
2022-04-09 17:35:26,861 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.self.value.bias
2022-04-09 17:35:26,861 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.output.dense.weight
2022-04-09 17:35:26,861 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.output.dense.bias
2022-04-09 17:35:26,861 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.output.LayerNorm.weight
2022-04-09 17:35:26,861 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.output.LayerNorm.bias
2022-04-09 17:35:26,861 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.intermediate.dense.weight
2022-04-09 17:35:26,861 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.intermediate.dense.bias
2022-04-09 17:35:26,861 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.output.dense.weight
2022-04-09 17:35:26,862 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.output.dense.bias
2022-04-09 17:35:26,862 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.output.LayerNorm.weight
2022-04-09 17:35:26,862 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.output.LayerNorm.bias
2022-04-09 17:35:26,862 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.self.query.weight
2022-04-09 17:35:26,862 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.self.query.bias
2022-04-09 17:35:26,862 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.self.key.weight
2022-04-09 17:35:26,862 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.self.key.bias
2022-04-09 17:35:26,862 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.self.value.weight
2022-04-09 17:35:26,862 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.self.value.bias
2022-04-09 17:35:26,862 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.output.dense.weight
2022-04-09 17:35:26,862 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.output.dense.bias
2022-04-09 17:35:26,862 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.output.LayerNorm.weight
2022-04-09 17:35:26,862 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.output.LayerNorm.bias
2022-04-09 17:35:26,862 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.intermediate.dense.weight
2022-04-09 17:35:26,862 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.intermediate.dense.bias
2022-04-09 17:35:26,862 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.output.dense.weight
2022-04-09 17:35:26,862 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.output.dense.bias
2022-04-09 17:35:26,863 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.output.LayerNorm.weight
2022-04-09 17:35:26,863 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.output.LayerNorm.bias
2022-04-09 17:35:26,863 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.self.query.weight
2022-04-09 17:35:26,863 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.self.query.bias
2022-04-09 17:35:26,863 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.self.key.weight
2022-04-09 17:35:26,863 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.self.key.bias
2022-04-09 17:35:26,863 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.self.value.weight
2022-04-09 17:35:26,863 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.self.value.bias
2022-04-09 17:35:26,863 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.output.dense.weight
2022-04-09 17:35:26,863 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.output.dense.bias
2022-04-09 17:35:26,863 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.output.LayerNorm.weight
2022-04-09 17:35:26,863 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.output.LayerNorm.bias
2022-04-09 17:35:26,863 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.intermediate.dense.weight
2022-04-09 17:35:26,863 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.intermediate.dense.bias
2022-04-09 17:35:26,863 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.output.dense.weight
2022-04-09 17:35:26,863 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.output.dense.bias
2022-04-09 17:35:26,863 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.output.LayerNorm.weight
2022-04-09 17:35:26,864 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.output.LayerNorm.bias
2022-04-09 17:35:26,864 - INFO - allennlp.common.util - _classifier.roberta.pooler.dense.weight
2022-04-09 17:35:26,864 - INFO - allennlp.common.util - _classifier.roberta.pooler.dense.bias
2022-04-09 17:35:26,864 - INFO - allennlp.common.util - _classifier.classifier.dense.weight
2022-04-09 17:35:26,864 - INFO - allennlp.common.util - _classifier.classifier.dense.bias
2022-04-09 17:35:26,864 - INFO - allennlp.common.util - _classifier.classifier.out_proj.weight
2022-04-09 17:35:26,864 - INFO - allennlp.common.util - _classifier.classifier.out_proj.bias
2022-04-09 17:35:26,864 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.type = slanted_triangular
2022-04-09 17:35:26,864 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.cut_frac = 0.06
2022-04-09 17:35:26,864 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.ratio = 32
2022-04-09 17:35:26,865 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.last_epoch = -1
2022-04-09 17:35:26,865 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.gradual_unfreezing = False
2022-04-09 17:35:26,865 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.discriminative_fine_tuning = False
2022-04-09 17:35:26,865 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.decay_factor = 0.38
2022-04-09 17:35:26,865 - INFO - allennlp.common.params - type = default
2022-04-09 17:35:26,865 - INFO - allennlp.common.params - keep_serialized_model_every_num_seconds = None
2022-04-09 17:35:26,865 - INFO - allennlp.common.params - num_serialized_models_to_keep = 2
2022-04-09 17:35:26,865 - INFO - allennlp.common.params - model_save_interval = None
2022-04-09 17:35:26,865 - INFO - allennlp.common.params - summary_interval = 100
2022-04-09 17:35:26,865 - INFO - allennlp.common.params - histogram_interval = None
2022-04-09 17:35:26,865 - INFO - allennlp.common.params - batch_size_interval = None
2022-04-09 17:35:26,865 - INFO - allennlp.common.params - should_log_parameter_statistics = True
2022-04-09 17:35:26,866 - INFO - allennlp.common.params - should_log_learning_rate = False
2022-04-09 17:35:26,866 - INFO - allennlp.common.params - get_batch_num_total = None
2022-04-09 17:35:26,868 - WARNING - allennlp.training.trainer - You provided a validation dataset but patience was set to None, meaning that early stopping is disabled
2022-04-09 17:35:26,964 - INFO - allennlp.training.trainer - Beginning training.
2022-04-09 17:35:26,965 - INFO - allennlp.training.trainer - Epoch 0/9
2022-04-09 17:35:26,965 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-09 17:35:26,965 - INFO - allennlp.training.trainer - GPU 0 memory usage: 1.3G
2022-04-09 17:35:26,966 - INFO - allennlp.training.trainer - Training
2022-04-09 17:35:26,967 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-09 17:35:26,967 - INFO - allennlp.data.samplers.bucket_batch_sampler - No sorting keys given; trying to guess a good one
2022-04-09 17:35:26,967 - INFO - allennlp.data.samplers.bucket_batch_sampler - Using ['tokens'] as the sorting keys
2022-04-09 17:35:46,678 - INFO - tqdm - accuracy: 0.5156, batch_loss: 1.5443, loss: 1.5467 ||:   3%|3         | 2/65 [00:19<10:20,  9.85s/it]
2022-04-09 17:36:06,711 - INFO - tqdm - accuracy: 0.5469, batch_loss: 1.3315, loss: 1.6098 ||:   6%|6         | 4/65 [00:39<10:08,  9.97s/it]
2022-04-09 17:36:16,771 - INFO - tqdm - accuracy: 0.5312, batch_loss: 1.1892, loss: 1.5257 ||:   8%|7         | 5/65 [00:49<10:00, 10.00s/it]
2022-04-09 17:36:26,939 - INFO - tqdm - accuracy: 0.5104, batch_loss: 1.6636, loss: 1.5487 ||:   9%|9         | 6/65 [00:59<09:53, 10.06s/it]
2022-04-09 17:36:37,238 - INFO - tqdm - accuracy: 0.5223, batch_loss: 1.4123, loss: 1.5292 ||:  11%|#         | 7/65 [01:10<09:47, 10.14s/it]
2022-04-09 17:36:47,638 - INFO - tqdm - accuracy: 0.5156, batch_loss: 1.4263, loss: 1.5163 ||:  12%|#2        | 8/65 [01:20<09:42, 10.22s/it]
2022-04-09 17:36:58,157 - INFO - tqdm - accuracy: 0.5104, batch_loss: 1.5718, loss: 1.5225 ||:  14%|#3        | 9/65 [01:31<09:37, 10.31s/it]
2022-04-09 17:37:08,451 - INFO - tqdm - accuracy: 0.4938, batch_loss: 1.3915, loss: 1.5094 ||:  15%|#5        | 10/65 [01:41<09:26, 10.31s/it]
2022-04-09 17:37:18,948 - INFO - tqdm - accuracy: 0.4915, batch_loss: 1.3244, loss: 1.4926 ||:  17%|#6        | 11/65 [01:51<09:19, 10.37s/it]
2022-04-09 17:37:29,532 - INFO - tqdm - accuracy: 0.5104, batch_loss: 0.8429, loss: 1.4384 ||:  18%|#8        | 12/65 [02:02<09:12, 10.43s/it]
2022-04-09 17:37:40,199 - INFO - tqdm - accuracy: 0.5168, batch_loss: 0.7626, loss: 1.3865 ||:  20%|##        | 13/65 [02:13<09:06, 10.50s/it]
2022-04-09 17:37:50,850 - INFO - tqdm - accuracy: 0.5112, batch_loss: 1.0932, loss: 1.3655 ||:  22%|##1       | 14/65 [02:23<08:57, 10.55s/it]
2022-04-09 17:38:01,692 - INFO - tqdm - accuracy: 0.5167, batch_loss: 0.8414, loss: 1.3306 ||:  23%|##3       | 15/65 [02:34<08:51, 10.64s/it]
2022-04-09 17:38:12,632 - INFO - tqdm - accuracy: 0.5000, batch_loss: 0.9007, loss: 1.3037 ||:  25%|##4       | 16/65 [02:45<08:45, 10.73s/it]
2022-04-09 17:38:23,640 - INFO - tqdm - accuracy: 0.5092, batch_loss: 0.5947, loss: 1.2620 ||:  26%|##6       | 17/65 [02:56<08:38, 10.81s/it]
2022-04-09 17:38:34,745 - INFO - tqdm - accuracy: 0.5104, batch_loss: 0.7556, loss: 1.2339 ||:  28%|##7       | 18/65 [03:07<08:32, 10.90s/it]
2022-04-09 17:38:45,897 - INFO - tqdm - accuracy: 0.5082, batch_loss: 0.7580, loss: 1.2088 ||:  29%|##9       | 19/65 [03:18<08:24, 10.98s/it]
2022-04-09 17:38:56,757 - INFO - tqdm - accuracy: 0.5188, batch_loss: 0.6248, loss: 1.1796 ||:  31%|###       | 20/65 [03:29<08:12, 10.94s/it]
2022-04-09 17:39:07,876 - INFO - tqdm - accuracy: 0.5164, batch_loss: 0.8137, loss: 1.1622 ||:  32%|###2      | 21/65 [03:40<08:03, 10.99s/it]
2022-04-09 17:39:19,122 - INFO - tqdm - accuracy: 0.5156, batch_loss: 0.7169, loss: 1.1420 ||:  34%|###3      | 22/65 [03:52<07:56, 11.07s/it]
2022-04-09 17:39:30,731 - INFO - tqdm - accuracy: 0.5109, batch_loss: 0.7420, loss: 1.1246 ||:  35%|###5      | 23/65 [04:03<07:51, 11.23s/it]
2022-04-09 17:39:42,413 - INFO - tqdm - accuracy: 0.5143, batch_loss: 0.7017, loss: 1.1069 ||:  37%|###6      | 24/65 [04:15<07:46, 11.37s/it]
2022-04-09 17:39:53,859 - INFO - tqdm - accuracy: 0.5112, batch_loss: 0.7340, loss: 1.0920 ||:  38%|###8      | 25/65 [04:26<07:35, 11.39s/it]
2022-04-09 17:40:05,136 - INFO - tqdm - accuracy: 0.5036, batch_loss: 0.7161, loss: 1.0776 ||:  40%|####      | 26/65 [04:38<07:22, 11.36s/it]
2022-04-09 17:40:16,375 - INFO - tqdm - accuracy: 0.5035, batch_loss: 0.6902, loss: 1.0632 ||:  42%|####1     | 27/65 [04:49<07:10, 11.32s/it]
2022-04-09 17:40:27,732 - INFO - tqdm - accuracy: 0.5000, batch_loss: 0.7176, loss: 1.0509 ||:  43%|####3     | 28/65 [05:00<06:59, 11.33s/it]
2022-04-09 17:40:39,215 - INFO - tqdm - accuracy: 0.4968, batch_loss: 0.7604, loss: 1.0409 ||:  45%|####4     | 29/65 [05:12<06:49, 11.38s/it]
2022-04-09 17:40:50,708 - INFO - tqdm - accuracy: 0.4938, batch_loss: 0.7182, loss: 1.0301 ||:  46%|####6     | 30/65 [05:23<06:39, 11.41s/it]
2022-04-09 17:41:02,123 - INFO - tqdm - accuracy: 0.4929, batch_loss: 0.7041, loss: 1.0196 ||:  48%|####7     | 31/65 [05:35<06:28, 11.41s/it]
2022-04-09 17:41:13,490 - INFO - tqdm - accuracy: 0.4902, batch_loss: 0.7317, loss: 1.0106 ||:  49%|####9     | 32/65 [05:46<06:16, 11.40s/it]
2022-04-09 17:41:24,862 - INFO - tqdm - accuracy: 0.4896, batch_loss: 0.7291, loss: 1.0021 ||:  51%|#####     | 33/65 [05:57<06:04, 11.39s/it]
2022-04-09 17:41:36,277 - INFO - tqdm - accuracy: 0.4908, batch_loss: 0.6975, loss: 0.9931 ||:  52%|#####2    | 34/65 [06:09<05:53, 11.40s/it]
2022-04-09 17:41:47,718 - INFO - tqdm - accuracy: 0.4902, batch_loss: 0.7128, loss: 0.9851 ||:  54%|#####3    | 35/65 [06:20<05:42, 11.41s/it]
2022-04-09 17:41:59,185 - INFO - tqdm - accuracy: 0.4905, batch_loss: 0.6825, loss: 0.9767 ||:  55%|#####5    | 36/65 [06:32<05:31, 11.43s/it]
2022-04-09 17:42:10,605 - INFO - tqdm - accuracy: 0.4873, batch_loss: 0.7344, loss: 0.9701 ||:  57%|#####6    | 37/65 [06:43<05:19, 11.43s/it]
2022-04-09 17:42:21,992 - INFO - tqdm - accuracy: 0.4893, batch_loss: 0.7012, loss: 0.9631 ||:  58%|#####8    | 38/65 [06:55<05:08, 11.41s/it]
2022-04-09 17:42:33,369 - INFO - tqdm - accuracy: 0.4912, batch_loss: 0.6967, loss: 0.9562 ||:  60%|######    | 39/65 [07:06<04:56, 11.40s/it]
2022-04-09 17:42:44,748 - INFO - tqdm - accuracy: 0.4906, batch_loss: 0.6926, loss: 0.9496 ||:  62%|######1   | 40/65 [07:17<04:44, 11.40s/it]
2022-04-09 17:42:56,149 - INFO - tqdm - accuracy: 0.4878, batch_loss: 0.7172, loss: 0.9440 ||:  63%|######3   | 41/65 [07:29<04:33, 11.40s/it]
2022-04-09 17:43:07,593 - INFO - tqdm - accuracy: 0.4918, batch_loss: 0.6682, loss: 0.9374 ||:  65%|######4   | 42/65 [07:40<04:22, 11.41s/it]
2022-04-09 17:43:19,040 - INFO - tqdm - accuracy: 0.4891, batch_loss: 0.7102, loss: 0.9321 ||:  66%|######6   | 43/65 [07:52<04:11, 11.42s/it]
2022-04-09 17:43:30,494 - INFO - tqdm - accuracy: 0.4922, batch_loss: 0.6537, loss: 0.9258 ||:  68%|######7   | 44/65 [08:03<04:00, 11.43s/it]
2022-04-09 17:43:41,972 - INFO - tqdm - accuracy: 0.4924, batch_loss: 0.7023, loss: 0.9208 ||:  69%|######9   | 45/65 [08:15<03:48, 11.45s/it]
2022-04-09 17:43:53,409 - INFO - tqdm - accuracy: 0.4932, batch_loss: 0.6824, loss: 0.9156 ||:  71%|#######   | 46/65 [08:26<03:37, 11.44s/it]
2022-04-09 17:44:04,788 - INFO - tqdm - accuracy: 0.4953, batch_loss: 0.6809, loss: 0.9107 ||:  72%|#######2  | 47/65 [08:37<03:25, 11.42s/it]
2022-04-09 17:44:16,146 - INFO - tqdm - accuracy: 0.4941, batch_loss: 0.7285, loss: 0.9069 ||:  74%|#######3  | 48/65 [08:49<03:13, 11.40s/it]
2022-04-09 17:44:27,554 - INFO - tqdm - accuracy: 0.4955, batch_loss: 0.6832, loss: 0.9023 ||:  75%|#######5  | 49/65 [09:00<03:02, 11.41s/it]
2022-04-09 17:44:38,963 - INFO - tqdm - accuracy: 0.4963, batch_loss: 0.6933, loss: 0.8981 ||:  77%|#######6  | 50/65 [09:11<02:51, 11.41s/it]
2022-04-09 17:44:50,402 - INFO - tqdm - accuracy: 0.4957, batch_loss: 0.7241, loss: 0.8947 ||:  78%|#######8  | 51/65 [09:23<02:39, 11.42s/it]
2022-04-09 17:45:01,850 - INFO - tqdm - accuracy: 0.5006, batch_loss: 0.6345, loss: 0.8897 ||:  80%|########  | 52/65 [09:34<02:28, 11.43s/it]
2022-04-09 17:45:13,293 - INFO - tqdm - accuracy: 0.5012, batch_loss: 0.6906, loss: 0.8859 ||:  82%|########1 | 53/65 [09:46<02:17, 11.43s/it]
2022-04-09 17:45:24,412 - INFO - tqdm - accuracy: 0.5067, batch_loss: 0.6087, loss: 0.8808 ||:  83%|########3 | 54/65 [09:57<02:04, 11.34s/it]
2022-04-09 17:45:35,854 - INFO - tqdm - accuracy: 0.5048, batch_loss: 0.7367, loss: 0.8782 ||:  85%|########4 | 55/65 [10:08<01:53, 11.37s/it]
2022-04-09 17:45:47,305 - INFO - tqdm - accuracy: 0.5053, batch_loss: 0.6898, loss: 0.8748 ||:  86%|########6 | 56/65 [10:20<01:42, 11.39s/it]
2022-04-09 17:45:58,740 - INFO - tqdm - accuracy: 0.5019, batch_loss: 0.8089, loss: 0.8737 ||:  88%|########7 | 57/65 [10:31<01:31, 11.41s/it]
2022-04-09 17:46:10,177 - INFO - tqdm - accuracy: 0.5019, batch_loss: 0.6807, loss: 0.8703 ||:  89%|########9 | 58/65 [10:43<01:19, 11.42s/it]
2022-04-09 17:46:21,581 - INFO - tqdm - accuracy: 0.5024, batch_loss: 0.7119, loss: 0.8677 ||:  91%|######### | 59/65 [10:54<01:08, 11.41s/it]
2022-04-09 17:46:32,986 - INFO - tqdm - accuracy: 0.5034, batch_loss: 0.6771, loss: 0.8645 ||:  92%|#########2| 60/65 [11:06<00:57, 11.41s/it]
2022-04-09 17:46:44,406 - INFO - tqdm - accuracy: 0.5028, batch_loss: 0.6905, loss: 0.8616 ||:  94%|#########3| 61/65 [11:17<00:45, 11.41s/it]
2022-04-09 17:46:55,830 - INFO - tqdm - accuracy: 0.5038, batch_loss: 0.6851, loss: 0.8588 ||:  95%|#########5| 62/65 [11:28<00:34, 11.42s/it]
2022-04-09 17:47:07,236 - INFO - tqdm - accuracy: 0.5022, batch_loss: 0.7130, loss: 0.8565 ||:  97%|#########6| 63/65 [11:40<00:22, 11.41s/it]
2022-04-09 17:47:18,638 - INFO - tqdm - accuracy: 0.5042, batch_loss: 0.6921, loss: 0.8539 ||:  98%|#########8| 64/65 [11:51<00:11, 11.41s/it]
2022-04-09 17:47:23,690 - INFO - tqdm - accuracy: 0.5032, batch_loss: 0.7171, loss: 0.8518 ||: 100%|##########| 65/65 [11:56<00:00,  9.50s/it]
2022-04-09 17:47:23,691 - INFO - tqdm - accuracy: 0.5032, batch_loss: 0.7171, loss: 0.8518 ||: 100%|##########| 65/65 [11:56<00:00, 11.03s/it]
2022-04-09 17:47:26,532 - INFO - allennlp.training.trainer - Validating
2022-04-09 17:47:26,534 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-09 17:47:26,534 - INFO - allennlp.data.samplers.bucket_batch_sampler - No sorting keys given; trying to guess a good one
2022-04-09 17:47:26,534 - INFO - allennlp.data.samplers.bucket_batch_sampler - Using ['tokens'] as the sorting keys
2022-04-09 17:47:36,552 - INFO - tqdm - accuracy: 0.5000, batch_loss: 0.6773, loss: 0.6952 ||:  35%|###4      | 40/115 [00:10<00:18,  4.00it/s]
2022-04-09 17:47:46,589 - INFO - tqdm - accuracy: 0.4688, batch_loss: 0.6708, loss: 0.7002 ||:  70%|######9   | 80/115 [00:20<00:08,  3.96it/s]
2022-04-09 17:47:55,440 - INFO - tqdm - accuracy: 0.4760, batch_loss: 0.7783, loss: 0.6997 ||: 100%|##########| 115/115 [00:28<00:00,  3.89it/s]
2022-04-09 17:47:55,440 - INFO - tqdm - accuracy: 0.4760, batch_loss: 0.7783, loss: 0.6997 ||: 100%|##########| 115/115 [00:28<00:00,  3.98it/s]
2022-04-09 17:47:55,441 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-09 17:47:55,442 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.503  |     0.476
2022-04-09 17:47:55,443 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  1359.606  |       N/A
2022-04-09 17:47:55,443 - INFO - allennlp.training.tensorboard_writer - loss               |     0.852  |     0.700
2022-04-09 17:47:55,444 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5806.574  |       N/A
2022-04-09 17:48:00,971 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to './output_ir-ora-d_hyper1/best.th'.
2022-04-09 17:48:02,370 - INFO - allennlp.training.trainer - Epoch duration: 0:12:35.405603
2022-04-09 17:48:02,371 - INFO - allennlp.training.trainer - Estimated training time remaining: 1:53:18
2022-04-09 17:48:02,371 - INFO - allennlp.training.trainer - Epoch 1/9
2022-04-09 17:48:02,371 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-09 17:48:02,371 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-09 17:48:02,373 - INFO - allennlp.training.trainer - Training
2022-04-09 17:48:02,373 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-09 17:48:13,603 - INFO - tqdm - accuracy: 0.3438, batch_loss: 0.7164, loss: 0.7164 ||:   2%|1         | 1/65 [00:11<11:58, 11.23s/it]
2022-04-09 17:48:25,070 - INFO - tqdm - accuracy: 0.4219, batch_loss: 0.6917, loss: 0.7041 ||:   3%|3         | 2/65 [00:22<11:56, 11.37s/it]
2022-04-09 17:48:36,747 - INFO - tqdm - accuracy: 0.4062, batch_loss: 0.7237, loss: 0.7106 ||:   5%|4         | 3/65 [00:34<11:53, 11.51s/it]
2022-04-09 17:48:48,212 - INFO - tqdm - accuracy: 0.4531, batch_loss: 0.6840, loss: 0.7040 ||:   6%|6         | 4/65 [00:45<11:41, 11.49s/it]
2022-04-09 17:48:59,608 - INFO - tqdm - accuracy: 0.4688, batch_loss: 0.6872, loss: 0.7006 ||:   8%|7         | 5/65 [00:57<11:27, 11.46s/it]
2022-04-09 17:49:10,977 - INFO - tqdm - accuracy: 0.4896, batch_loss: 0.6818, loss: 0.6975 ||:   9%|9         | 6/65 [01:08<11:14, 11.43s/it]
2022-04-09 17:49:22,324 - INFO - tqdm - accuracy: 0.4866, batch_loss: 0.7022, loss: 0.6982 ||:  11%|#         | 7/65 [01:19<11:01, 11.40s/it]
2022-04-09 17:49:33,798 - INFO - tqdm - accuracy: 0.4922, batch_loss: 0.6965, loss: 0.6980 ||:  12%|#2        | 8/65 [01:31<10:51, 11.42s/it]
2022-04-09 17:49:45,280 - INFO - tqdm - accuracy: 0.5000, batch_loss: 0.6803, loss: 0.6960 ||:  14%|#3        | 9/65 [01:42<10:40, 11.44s/it]
2022-04-09 17:49:56,712 - INFO - tqdm - accuracy: 0.4969, batch_loss: 0.6726, loss: 0.6937 ||:  15%|#5        | 10/65 [01:54<10:29, 11.44s/it]
2022-04-09 17:50:08,104 - INFO - tqdm - accuracy: 0.4972, batch_loss: 0.7309, loss: 0.6970 ||:  17%|#6        | 11/65 [02:05<10:16, 11.42s/it]
2022-04-09 17:50:19,491 - INFO - tqdm - accuracy: 0.5052, batch_loss: 0.6871, loss: 0.6962 ||:  18%|#8        | 12/65 [02:17<10:04, 11.41s/it]
2022-04-09 17:50:30,922 - INFO - tqdm - accuracy: 0.5144, batch_loss: 0.6702, loss: 0.6942 ||:  20%|##        | 13/65 [02:28<09:53, 11.42s/it]
2022-04-09 17:50:42,246 - INFO - tqdm - accuracy: 0.5112, batch_loss: 0.7202, loss: 0.6961 ||:  22%|##1       | 14/65 [02:39<09:40, 11.39s/it]
2022-04-09 17:50:53,711 - INFO - tqdm - accuracy: 0.5083, batch_loss: 0.6973, loss: 0.6962 ||:  23%|##3       | 15/65 [02:51<09:30, 11.41s/it]
2022-04-09 17:51:05,151 - INFO - tqdm - accuracy: 0.5137, batch_loss: 0.6759, loss: 0.6949 ||:  25%|##4       | 16/65 [03:02<09:19, 11.42s/it]
2022-04-09 17:51:16,596 - INFO - tqdm - accuracy: 0.5147, batch_loss: 0.7066, loss: 0.6956 ||:  26%|##6       | 17/65 [03:14<09:08, 11.43s/it]
2022-04-09 17:51:28,034 - INFO - tqdm - accuracy: 0.5191, batch_loss: 0.6366, loss: 0.6923 ||:  28%|##7       | 18/65 [03:25<08:57, 11.43s/it]
2022-04-09 17:51:39,449 - INFO - tqdm - accuracy: 0.5181, batch_loss: 0.6980, loss: 0.6926 ||:  29%|##9       | 19/65 [03:37<08:45, 11.43s/it]
2022-04-09 17:51:50,862 - INFO - tqdm - accuracy: 0.5234, batch_loss: 0.6880, loss: 0.6924 ||:  31%|###       | 20/65 [03:48<08:33, 11.42s/it]
2022-04-09 17:52:02,286 - INFO - tqdm - accuracy: 0.5238, batch_loss: 0.6730, loss: 0.6914 ||:  32%|###2      | 21/65 [03:59<08:22, 11.42s/it]
2022-04-09 17:52:13,547 - INFO - tqdm - accuracy: 0.5256, batch_loss: 0.6978, loss: 0.6917 ||:  34%|###3      | 22/65 [04:11<08:09, 11.37s/it]
2022-04-09 17:52:24,921 - INFO - tqdm - accuracy: 0.5312, batch_loss: 0.6608, loss: 0.6904 ||:  35%|###5      | 23/65 [04:22<07:57, 11.37s/it]
2022-04-09 17:52:36,278 - INFO - tqdm - accuracy: 0.5273, batch_loss: 0.7316, loss: 0.6921 ||:  37%|###6      | 24/65 [04:33<07:46, 11.37s/it]
2022-04-09 17:52:47,650 - INFO - tqdm - accuracy: 0.5312, batch_loss: 0.6549, loss: 0.6906 ||:  38%|###8      | 25/65 [04:45<07:34, 11.37s/it]
2022-04-09 17:52:59,017 - INFO - tqdm - accuracy: 0.5312, batch_loss: 0.6811, loss: 0.6903 ||:  40%|####      | 26/65 [04:56<07:23, 11.37s/it]
2022-04-09 17:53:10,401 - INFO - tqdm - accuracy: 0.5336, batch_loss: 0.6796, loss: 0.6899 ||:  42%|####1     | 27/65 [05:08<07:12, 11.37s/it]
2022-04-09 17:53:21,767 - INFO - tqdm - accuracy: 0.5346, batch_loss: 0.6718, loss: 0.6892 ||:  43%|####3     | 28/65 [05:19<07:00, 11.37s/it]
2022-04-09 17:53:33,121 - INFO - tqdm - accuracy: 0.5366, batch_loss: 0.6961, loss: 0.6894 ||:  45%|####4     | 29/65 [05:30<06:49, 11.37s/it]
2022-04-09 17:53:44,520 - INFO - tqdm - accuracy: 0.5323, batch_loss: 0.7159, loss: 0.6903 ||:  46%|####6     | 30/65 [05:42<06:38, 11.38s/it]
2022-04-09 17:53:55,905 - INFO - tqdm - accuracy: 0.5323, batch_loss: 0.6864, loss: 0.6902 ||:  48%|####7     | 31/65 [05:53<06:26, 11.38s/it]
2022-04-09 17:54:07,308 - INFO - tqdm - accuracy: 0.5303, batch_loss: 0.6682, loss: 0.6895 ||:  49%|####9     | 32/65 [06:04<06:15, 11.39s/it]
2022-04-09 17:54:18,687 - INFO - tqdm - accuracy: 0.5294, batch_loss: 0.6965, loss: 0.6897 ||:  51%|#####     | 33/65 [06:16<06:04, 11.38s/it]
2022-04-09 17:54:30,066 - INFO - tqdm - accuracy: 0.5294, batch_loss: 0.7035, loss: 0.6901 ||:  52%|#####2    | 34/65 [06:27<05:52, 11.38s/it]
2022-04-09 17:54:41,710 - INFO - tqdm - accuracy: 0.5321, batch_loss: 0.6767, loss: 0.6898 ||:  54%|#####3    | 35/65 [06:39<05:43, 11.46s/it]
2022-04-09 17:54:53,058 - INFO - tqdm - accuracy: 0.5356, batch_loss: 0.6474, loss: 0.6886 ||:  55%|#####5    | 36/65 [06:50<05:31, 11.43s/it]
2022-04-09 17:55:04,419 - INFO - tqdm - accuracy: 0.5355, batch_loss: 0.6644, loss: 0.6879 ||:  57%|#####6    | 37/65 [07:02<05:19, 11.41s/it]
2022-04-09 17:55:15,807 - INFO - tqdm - accuracy: 0.5387, batch_loss: 0.6689, loss: 0.6874 ||:  58%|#####8    | 38/65 [07:13<05:07, 11.40s/it]
2022-04-09 17:55:27,214 - INFO - tqdm - accuracy: 0.5377, batch_loss: 0.6947, loss: 0.6876 ||:  60%|######    | 39/65 [07:24<04:56, 11.40s/it]
2022-04-09 17:55:38,637 - INFO - tqdm - accuracy: 0.5391, batch_loss: 0.6670, loss: 0.6871 ||:  62%|######1   | 40/65 [07:36<04:45, 11.41s/it]
2022-04-09 17:55:50,092 - INFO - tqdm - accuracy: 0.5396, batch_loss: 0.6746, loss: 0.6868 ||:  63%|######3   | 41/65 [07:47<04:34, 11.42s/it]
2022-04-09 17:56:01,530 - INFO - tqdm - accuracy: 0.5387, batch_loss: 0.6604, loss: 0.6862 ||:  65%|######4   | 42/65 [07:59<04:22, 11.43s/it]
2022-04-09 17:56:12,971 - INFO - tqdm - accuracy: 0.5400, batch_loss: 0.6802, loss: 0.6860 ||:  66%|######6   | 43/65 [08:10<04:11, 11.43s/it]
2022-04-09 17:56:24,420 - INFO - tqdm - accuracy: 0.5398, batch_loss: 0.6769, loss: 0.6858 ||:  68%|######7   | 44/65 [08:22<04:00, 11.44s/it]
2022-04-09 17:56:35,854 - INFO - tqdm - accuracy: 0.5396, batch_loss: 0.6630, loss: 0.6853 ||:  69%|######9   | 45/65 [08:33<03:48, 11.44s/it]
2022-04-09 17:56:47,282 - INFO - tqdm - accuracy: 0.5374, batch_loss: 0.7259, loss: 0.6862 ||:  71%|#######   | 46/65 [08:44<03:37, 11.43s/it]
2022-04-09 17:56:58,714 - INFO - tqdm - accuracy: 0.5372, batch_loss: 0.7166, loss: 0.6868 ||:  72%|#######2  | 47/65 [08:56<03:25, 11.43s/it]
2022-04-09 17:57:10,150 - INFO - tqdm - accuracy: 0.5378, batch_loss: 0.7085, loss: 0.6873 ||:  74%|#######3  | 48/65 [09:07<03:14, 11.43s/it]
2022-04-09 17:57:21,596 - INFO - tqdm - accuracy: 0.5357, batch_loss: 0.7323, loss: 0.6882 ||:  75%|#######5  | 49/65 [09:19<03:03, 11.44s/it]
2022-04-09 17:57:33,003 - INFO - tqdm - accuracy: 0.5387, batch_loss: 0.6773, loss: 0.6880 ||:  77%|#######6  | 50/65 [09:30<02:51, 11.43s/it]
2022-04-09 17:57:44,445 - INFO - tqdm - accuracy: 0.5368, batch_loss: 0.7092, loss: 0.6884 ||:  78%|#######8  | 51/65 [09:42<02:40, 11.43s/it]
2022-04-09 17:57:55,867 - INFO - tqdm - accuracy: 0.5379, batch_loss: 0.6724, loss: 0.6881 ||:  80%|########  | 52/65 [09:53<02:28, 11.43s/it]
2022-04-09 17:58:07,289 - INFO - tqdm - accuracy: 0.5389, batch_loss: 0.6546, loss: 0.6875 ||:  82%|########1 | 53/65 [10:04<02:17, 11.43s/it]
2022-04-09 17:58:18,705 - INFO - tqdm - accuracy: 0.5388, batch_loss: 0.7035, loss: 0.6878 ||:  83%|########3 | 54/65 [10:16<02:05, 11.42s/it]
2022-04-09 17:58:30,131 - INFO - tqdm - accuracy: 0.5420, batch_loss: 0.6266, loss: 0.6866 ||:  85%|########4 | 55/65 [10:27<01:54, 11.42s/it]
2022-04-09 17:58:41,554 - INFO - tqdm - accuracy: 0.5407, batch_loss: 0.7196, loss: 0.6872 ||:  86%|########6 | 56/65 [10:39<01:42, 11.42s/it]
2022-04-09 17:58:52,733 - INFO - tqdm - accuracy: 0.5444, batch_loss: 0.6164, loss: 0.6860 ||:  88%|########7 | 57/65 [10:50<01:30, 11.35s/it]
2022-04-09 17:59:04,152 - INFO - tqdm - accuracy: 0.5415, batch_loss: 0.7282, loss: 0.6867 ||:  89%|########9 | 58/65 [11:01<01:19, 11.37s/it]
2022-04-09 17:59:15,548 - INFO - tqdm - accuracy: 0.5408, batch_loss: 0.6597, loss: 0.6863 ||:  91%|######### | 59/65 [11:13<01:08, 11.38s/it]
2022-04-09 17:59:26,947 - INFO - tqdm - accuracy: 0.5422, batch_loss: 0.6879, loss: 0.6863 ||:  92%|#########2| 60/65 [11:24<00:56, 11.38s/it]
2022-04-09 17:59:38,365 - INFO - tqdm - accuracy: 0.5441, batch_loss: 0.5681, loss: 0.6844 ||:  94%|#########3| 61/65 [11:35<00:45, 11.39s/it]
2022-04-09 17:59:49,768 - INFO - tqdm - accuracy: 0.5464, batch_loss: 0.5645, loss: 0.6824 ||:  95%|#########5| 62/65 [11:47<00:34, 11.40s/it]
2022-04-09 18:00:01,194 - INFO - tqdm - accuracy: 0.5476, batch_loss: 0.6209, loss: 0.6814 ||:  97%|#########6| 63/65 [11:58<00:22, 11.41s/it]
2022-04-09 18:00:12,243 - INFO - tqdm - accuracy: 0.5471, batch_loss: 0.7265, loss: 0.6821 ||:  98%|#########8| 64/65 [12:09<00:11, 11.30s/it]
2022-04-09 18:00:17,312 - INFO - tqdm - accuracy: 0.5478, batch_loss: 0.7012, loss: 0.6824 ||: 100%|##########| 65/65 [12:14<00:00,  9.43s/it]
2022-04-09 18:00:17,312 - INFO - tqdm - accuracy: 0.5478, batch_loss: 0.7012, loss: 0.6824 ||: 100%|##########| 65/65 [12:14<00:00, 11.31s/it]
2022-04-09 18:00:20,136 - INFO - allennlp.training.trainer - Validating
2022-04-09 18:00:20,138 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-09 18:00:30,338 - INFO - tqdm - accuracy: 0.5610, batch_loss: 0.7563, loss: 0.6768 ||:  36%|###5      | 41/115 [00:10<00:18,  3.98it/s]
2022-04-09 18:00:40,436 - INFO - tqdm - accuracy: 0.5617, batch_loss: 0.5600, loss: 0.7072 ||:  70%|#######   | 81/115 [00:20<00:08,  3.95it/s]
2022-04-09 18:00:48,939 - INFO - tqdm - accuracy: 0.5459, batch_loss: 0.4369, loss: 0.7192 ||: 100%|##########| 115/115 [00:28<00:00,  3.94it/s]
2022-04-09 18:00:48,940 - INFO - tqdm - accuracy: 0.5459, batch_loss: 0.4369, loss: 0.7192 ||: 100%|##########| 115/115 [00:28<00:00,  3.99it/s]
2022-04-09 18:00:48,940 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-09 18:00:48,940 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.548  |     0.546
2022-04-09 18:00:48,941 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-09 18:00:48,941 - INFO - allennlp.training.tensorboard_writer - loss               |     0.682  |     0.719
2022-04-09 18:00:48,941 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5806.574  |       N/A
2022-04-09 18:00:54,598 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to './output_ir-ora-d_hyper1/best.th'.
2022-04-09 18:01:09,977 - INFO - allennlp.training.trainer - Epoch duration: 0:13:07.606057
2022-04-09 18:01:09,977 - INFO - allennlp.training.trainer - Estimated training time remaining: 1:42:52
2022-04-09 18:01:09,977 - INFO - allennlp.training.trainer - Epoch 2/9
2022-04-09 18:01:09,977 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-09 18:01:09,978 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-09 18:01:09,979 - INFO - allennlp.training.trainer - Training
2022-04-09 18:01:09,979 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-09 18:01:21,057 - INFO - tqdm - accuracy: 0.5312, batch_loss: 0.7023, loss: 0.7023 ||:   2%|1         | 1/65 [00:11<11:48, 11.08s/it]
2022-04-09 18:01:32,470 - INFO - tqdm - accuracy: 0.6406, batch_loss: 0.5208, loss: 0.6115 ||:   3%|3         | 2/65 [00:22<11:50, 11.27s/it]
2022-04-09 18:01:43,955 - INFO - tqdm - accuracy: 0.6146, batch_loss: 0.6357, loss: 0.6196 ||:   5%|4         | 3/65 [00:33<11:44, 11.37s/it]
2022-04-09 18:01:55,388 - INFO - tqdm - accuracy: 0.5938, batch_loss: 0.6005, loss: 0.6148 ||:   6%|6         | 4/65 [00:45<11:35, 11.40s/it]
2022-04-09 18:02:06,774 - INFO - tqdm - accuracy: 0.6062, batch_loss: 0.5884, loss: 0.6095 ||:   8%|7         | 5/65 [00:56<11:23, 11.39s/it]
2022-04-09 18:02:18,011 - INFO - tqdm - accuracy: 0.6302, batch_loss: 0.5816, loss: 0.6048 ||:   9%|9         | 6/65 [01:08<11:09, 11.34s/it]
2022-04-09 18:02:29,424 - INFO - tqdm - accuracy: 0.6339, batch_loss: 0.6172, loss: 0.6066 ||:  11%|#         | 7/65 [01:19<10:59, 11.36s/it]
2022-04-09 18:02:40,845 - INFO - tqdm - accuracy: 0.6250, batch_loss: 0.6631, loss: 0.6137 ||:  12%|#2        | 8/65 [01:30<10:48, 11.38s/it]
2022-04-09 18:02:52,303 - INFO - tqdm - accuracy: 0.6458, batch_loss: 0.5419, loss: 0.6057 ||:  14%|#3        | 9/65 [01:42<10:38, 11.41s/it]
2022-04-09 18:03:03,750 - INFO - tqdm - accuracy: 0.6406, batch_loss: 0.6527, loss: 0.6104 ||:  15%|#5        | 10/65 [01:53<10:27, 11.42s/it]
2022-04-09 18:03:15,192 - INFO - tqdm - accuracy: 0.6392, batch_loss: 0.6536, loss: 0.6143 ||:  17%|#6        | 11/65 [02:05<10:16, 11.43s/it]
2022-04-09 18:03:26,654 - INFO - tqdm - accuracy: 0.6510, batch_loss: 0.5979, loss: 0.6130 ||:  18%|#8        | 12/65 [02:16<10:06, 11.44s/it]
2022-04-09 18:03:37,754 - INFO - tqdm - accuracy: 0.6289, batch_loss: 0.7806, loss: 0.6259 ||:  20%|##        | 13/65 [02:27<09:49, 11.33s/it]
2022-04-09 18:03:49,161 - INFO - tqdm - accuracy: 0.6331, batch_loss: 0.6334, loss: 0.6264 ||:  22%|##1       | 14/65 [02:39<09:39, 11.36s/it]
2022-04-09 18:04:00,570 - INFO - tqdm - accuracy: 0.6305, batch_loss: 0.6513, loss: 0.6281 ||:  23%|##3       | 15/65 [02:50<09:28, 11.37s/it]
2022-04-09 18:04:11,948 - INFO - tqdm - accuracy: 0.6360, batch_loss: 0.5247, loss: 0.6216 ||:  25%|##4       | 16/65 [03:01<09:17, 11.37s/it]
2022-04-09 18:04:23,316 - INFO - tqdm - accuracy: 0.6335, batch_loss: 0.7004, loss: 0.6262 ||:  26%|##6       | 17/65 [03:13<09:05, 11.37s/it]
2022-04-09 18:04:34,688 - INFO - tqdm - accuracy: 0.6226, batch_loss: 0.7963, loss: 0.6357 ||:  28%|##7       | 18/65 [03:24<08:54, 11.37s/it]
2022-04-09 18:04:46,083 - INFO - tqdm - accuracy: 0.6310, batch_loss: 0.6115, loss: 0.6344 ||:  29%|##9       | 19/65 [03:36<08:43, 11.38s/it]
2022-04-09 18:04:57,491 - INFO - tqdm - accuracy: 0.6369, batch_loss: 0.6190, loss: 0.6336 ||:  31%|###       | 20/65 [03:47<08:32, 11.39s/it]
2022-04-09 18:05:08,945 - INFO - tqdm - accuracy: 0.6319, batch_loss: 0.6484, loss: 0.6343 ||:  32%|###2      | 21/65 [03:58<08:21, 11.41s/it]
2022-04-09 18:05:20,393 - INFO - tqdm - accuracy: 0.6344, batch_loss: 0.5659, loss: 0.6312 ||:  34%|###3      | 22/65 [04:10<08:11, 11.42s/it]
2022-04-09 18:05:31,856 - INFO - tqdm - accuracy: 0.6340, batch_loss: 0.5616, loss: 0.6282 ||:  35%|###5      | 23/65 [04:21<08:00, 11.43s/it]
2022-04-09 18:05:43,203 - INFO - tqdm - accuracy: 0.6375, batch_loss: 0.6239, loss: 0.6280 ||:  37%|###6      | 24/65 [04:33<07:47, 11.41s/it]
2022-04-09 18:05:54,671 - INFO - tqdm - accuracy: 0.6408, batch_loss: 0.5924, loss: 0.6266 ||:  38%|###8      | 25/65 [04:44<07:37, 11.43s/it]
2022-04-09 18:06:06,128 - INFO - tqdm - accuracy: 0.6426, batch_loss: 0.5724, loss: 0.6245 ||:  40%|####      | 26/65 [04:56<07:25, 11.43s/it]
2022-04-09 18:06:17,579 - INFO - tqdm - accuracy: 0.6489, batch_loss: 0.5473, loss: 0.6216 ||:  42%|####1     | 27/65 [05:07<07:14, 11.44s/it]
2022-04-09 18:06:29,016 - INFO - tqdm - accuracy: 0.6547, batch_loss: 0.5260, loss: 0.6182 ||:  43%|####3     | 28/65 [05:19<07:03, 11.44s/it]
2022-04-09 18:06:40,462 - INFO - tqdm - accuracy: 0.6548, batch_loss: 0.6326, loss: 0.6187 ||:  45%|####4     | 29/65 [05:30<06:51, 11.44s/it]
2022-04-09 18:06:51,884 - INFO - tqdm - accuracy: 0.6559, batch_loss: 0.5338, loss: 0.6159 ||:  46%|####6     | 30/65 [05:41<06:40, 11.44s/it]
2022-04-09 18:07:03,293 - INFO - tqdm - accuracy: 0.6539, batch_loss: 0.5813, loss: 0.6148 ||:  48%|####7     | 31/65 [05:53<06:28, 11.43s/it]
2022-04-09 18:07:14,704 - INFO - tqdm - accuracy: 0.6588, batch_loss: 0.5411, loss: 0.6125 ||:  49%|####9     | 32/65 [06:04<06:16, 11.42s/it]
2022-04-09 18:07:26,127 - INFO - tqdm - accuracy: 0.6588, batch_loss: 0.6909, loss: 0.6149 ||:  51%|#####     | 33/65 [06:16<06:05, 11.42s/it]
2022-04-09 18:07:37,519 - INFO - tqdm - accuracy: 0.6559, batch_loss: 0.6488, loss: 0.6159 ||:  52%|#####2    | 34/65 [06:27<05:53, 11.41s/it]
2022-04-09 18:07:48,925 - INFO - tqdm - accuracy: 0.6586, batch_loss: 0.4922, loss: 0.6123 ||:  54%|#####3    | 35/65 [06:38<05:42, 11.41s/it]
2022-04-09 18:08:00,313 - INFO - tqdm - accuracy: 0.6612, batch_loss: 0.5645, loss: 0.6110 ||:  55%|#####5    | 36/65 [06:50<05:30, 11.40s/it]
2022-04-09 18:08:11,687 - INFO - tqdm - accuracy: 0.6610, batch_loss: 0.6536, loss: 0.6121 ||:  57%|#####6    | 37/65 [07:01<05:19, 11.40s/it]
2022-04-09 18:08:23,060 - INFO - tqdm - accuracy: 0.6658, batch_loss: 0.3961, loss: 0.6065 ||:  58%|#####8    | 38/65 [07:13<05:07, 11.39s/it]
2022-04-09 18:08:34,458 - INFO - tqdm - accuracy: 0.6672, batch_loss: 0.5287, loss: 0.6045 ||:  60%|######    | 39/65 [07:24<04:56, 11.39s/it]
2022-04-09 18:08:45,841 - INFO - tqdm - accuracy: 0.6669, batch_loss: 0.6578, loss: 0.6058 ||:  62%|######1   | 40/65 [07:35<04:44, 11.39s/it]
2022-04-09 18:08:57,242 - INFO - tqdm - accuracy: 0.6674, batch_loss: 0.5518, loss: 0.6045 ||:  63%|######3   | 41/65 [07:47<04:33, 11.39s/it]
2022-04-09 18:09:08,527 - INFO - tqdm - accuracy: 0.6694, batch_loss: 0.5241, loss: 0.6026 ||:  65%|######4   | 42/65 [07:58<04:21, 11.36s/it]
2022-04-09 18:09:19,916 - INFO - tqdm - accuracy: 0.6669, batch_loss: 0.6460, loss: 0.6036 ||:  66%|######6   | 43/65 [08:09<04:10, 11.37s/it]
2022-04-09 18:09:31,324 - INFO - tqdm - accuracy: 0.6674, batch_loss: 0.5682, loss: 0.6028 ||:  68%|######7   | 44/65 [08:21<03:58, 11.38s/it]
2022-04-09 18:09:42,718 - INFO - tqdm - accuracy: 0.6685, batch_loss: 0.6209, loss: 0.6032 ||:  69%|######9   | 45/65 [08:32<03:47, 11.38s/it]
2022-04-09 18:09:54,111 - INFO - tqdm - accuracy: 0.6703, batch_loss: 0.5101, loss: 0.6012 ||:  71%|#######   | 46/65 [08:44<03:36, 11.39s/it]
2022-04-09 18:10:05,490 - INFO - tqdm - accuracy: 0.6700, batch_loss: 0.6747, loss: 0.6027 ||:  72%|#######2  | 47/65 [08:55<03:24, 11.38s/it]
2022-04-09 18:10:16,888 - INFO - tqdm - accuracy: 0.6697, batch_loss: 0.7094, loss: 0.6049 ||:  74%|#######3  | 48/65 [09:06<03:13, 11.39s/it]
2022-04-09 18:10:28,275 - INFO - tqdm - accuracy: 0.6688, batch_loss: 0.5890, loss: 0.6046 ||:  75%|#######5  | 49/65 [09:18<03:02, 11.39s/it]
2022-04-09 18:10:39,666 - INFO - tqdm - accuracy: 0.6679, batch_loss: 0.6199, loss: 0.6049 ||:  77%|#######6  | 50/65 [09:29<02:50, 11.39s/it]
2022-04-09 18:10:51,074 - INFO - tqdm - accuracy: 0.6665, batch_loss: 0.6885, loss: 0.6066 ||:  78%|#######8  | 51/65 [09:41<02:39, 11.39s/it]
2022-04-09 18:11:02,476 - INFO - tqdm - accuracy: 0.6651, batch_loss: 0.5567, loss: 0.6056 ||:  80%|########  | 52/65 [09:52<02:28, 11.40s/it]
2022-04-09 18:11:13,879 - INFO - tqdm - accuracy: 0.6673, batch_loss: 0.5172, loss: 0.6039 ||:  82%|########1 | 53/65 [10:03<02:16, 11.40s/it]
2022-04-09 18:11:25,265 - INFO - tqdm - accuracy: 0.6676, batch_loss: 0.5784, loss: 0.6035 ||:  83%|########3 | 54/65 [10:15<02:05, 11.39s/it]
2022-04-09 18:11:36,668 - INFO - tqdm - accuracy: 0.6680, batch_loss: 0.6558, loss: 0.6044 ||:  85%|########4 | 55/65 [10:26<01:53, 11.40s/it]
2022-04-09 18:11:48,063 - INFO - tqdm - accuracy: 0.6672, batch_loss: 0.5944, loss: 0.6042 ||:  86%|########6 | 56/65 [10:38<01:42, 11.40s/it]
2022-04-09 18:11:59,468 - INFO - tqdm - accuracy: 0.6654, batch_loss: 0.7360, loss: 0.6065 ||:  88%|########7 | 57/65 [10:49<01:31, 11.40s/it]
2022-04-09 18:12:10,669 - INFO - tqdm - accuracy: 0.6668, batch_loss: 0.5390, loss: 0.6054 ||:  89%|########9 | 58/65 [11:00<01:19, 11.34s/it]
2022-04-09 18:12:22,097 - INFO - tqdm - accuracy: 0.6688, batch_loss: 0.4806, loss: 0.6033 ||:  91%|######### | 59/65 [11:12<01:08, 11.37s/it]
2022-04-09 18:12:33,546 - INFO - tqdm - accuracy: 0.6670, batch_loss: 0.6815, loss: 0.6046 ||:  92%|#########2| 60/65 [11:23<00:56, 11.39s/it]
2022-04-09 18:12:44,996 - INFO - tqdm - accuracy: 0.6674, batch_loss: 0.5165, loss: 0.6031 ||:  94%|#########3| 61/65 [11:35<00:45, 11.41s/it]
2022-04-09 18:12:56,454 - INFO - tqdm - accuracy: 0.6652, batch_loss: 0.7274, loss: 0.6051 ||:  95%|#########5| 62/65 [11:46<00:34, 11.42s/it]
2022-04-09 18:13:07,929 - INFO - tqdm - accuracy: 0.6640, batch_loss: 0.5778, loss: 0.6047 ||:  97%|#########6| 63/65 [11:57<00:22, 11.44s/it]
2022-04-09 18:13:19,395 - INFO - tqdm - accuracy: 0.6619, batch_loss: 0.6893, loss: 0.6060 ||:  98%|#########8| 64/65 [12:09<00:11, 11.45s/it]
2022-04-09 18:13:24,481 - INFO - tqdm - accuracy: 0.6623, batch_loss: 0.6921, loss: 0.6073 ||: 100%|##########| 65/65 [12:14<00:00,  9.54s/it]
2022-04-09 18:13:24,481 - INFO - tqdm - accuracy: 0.6623, batch_loss: 0.6921, loss: 0.6073 ||: 100%|##########| 65/65 [12:14<00:00, 11.30s/it]
2022-04-09 18:13:27,338 - INFO - allennlp.training.trainer - Validating
2022-04-09 18:13:27,340 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-09 18:13:37,422 - INFO - tqdm - accuracy: 0.5625, batch_loss: 0.7695, loss: 0.7152 ||:  35%|###4      | 40/115 [00:10<00:18,  3.97it/s]
2022-04-09 18:13:47,600 - INFO - tqdm - accuracy: 0.5901, batch_loss: 0.3369, loss: 0.6576 ||:  70%|#######   | 81/115 [00:20<00:08,  4.00it/s]
2022-04-09 18:13:56,206 - INFO - tqdm - accuracy: 0.6201, batch_loss: 1.2336, loss: 0.6559 ||: 100%|##########| 115/115 [00:28<00:00,  3.98it/s]
2022-04-09 18:13:56,206 - INFO - tqdm - accuracy: 0.6201, batch_loss: 1.2336, loss: 0.6559 ||: 100%|##########| 115/115 [00:28<00:00,  3.98it/s]
2022-04-09 18:13:56,207 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-09 18:13:56,207 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.662  |     0.620
2022-04-09 18:13:56,208 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-09 18:13:56,208 - INFO - allennlp.training.tensorboard_writer - loss               |     0.607  |     0.656
2022-04-09 18:13:56,209 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5806.574  |       N/A
2022-04-09 18:14:02,013 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to './output_ir-ora-d_hyper1/best.th'.
2022-04-09 18:14:15,779 - INFO - allennlp.training.trainer - Epoch duration: 0:13:05.801495
2022-04-09 18:14:15,779 - INFO - allennlp.training.trainer - Estimated training time remaining: 1:30:33
2022-04-09 18:14:15,779 - INFO - allennlp.training.trainer - Epoch 3/9
2022-04-09 18:14:15,779 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-09 18:14:15,779 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-09 18:14:15,781 - INFO - allennlp.training.trainer - Training
2022-04-09 18:14:15,781 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-09 18:14:26,892 - INFO - tqdm - accuracy: 0.7500, batch_loss: 0.5501, loss: 0.5501 ||:   2%|1         | 1/65 [00:11<11:51, 11.11s/it]
2022-04-09 18:14:38,305 - INFO - tqdm - accuracy: 0.6562, batch_loss: 0.6450, loss: 0.5976 ||:   3%|3         | 2/65 [00:22<11:51, 11.29s/it]
2022-04-09 18:14:49,863 - INFO - tqdm - accuracy: 0.6979, batch_loss: 0.5426, loss: 0.5792 ||:   5%|4         | 3/65 [00:34<11:47, 11.41s/it]
2022-04-09 18:15:01,249 - INFO - tqdm - accuracy: 0.7188, batch_loss: 0.4814, loss: 0.5548 ||:   6%|6         | 4/65 [00:45<11:35, 11.40s/it]
2022-04-09 18:15:12,996 - INFO - tqdm - accuracy: 0.7312, batch_loss: 0.4956, loss: 0.5430 ||:   8%|7         | 5/65 [00:57<11:31, 11.53s/it]
2022-04-09 18:15:24,336 - INFO - tqdm - accuracy: 0.7552, batch_loss: 0.4791, loss: 0.5323 ||:   9%|9         | 6/65 [01:08<11:16, 11.46s/it]
2022-04-09 18:15:35,781 - INFO - tqdm - accuracy: 0.7634, batch_loss: 0.4677, loss: 0.5231 ||:  11%|#         | 7/65 [01:19<11:04, 11.46s/it]
2022-04-09 18:15:47,134 - INFO - tqdm - accuracy: 0.7891, batch_loss: 0.4046, loss: 0.5083 ||:  12%|#2        | 8/65 [01:31<10:51, 11.42s/it]
2022-04-09 18:15:58,594 - INFO - tqdm - accuracy: 0.7917, batch_loss: 0.4599, loss: 0.5029 ||:  14%|#3        | 9/65 [01:42<10:40, 11.44s/it]
2022-04-09 18:16:10,011 - INFO - tqdm - accuracy: 0.8000, batch_loss: 0.4122, loss: 0.4938 ||:  15%|#5        | 10/65 [01:54<10:28, 11.43s/it]
2022-04-09 18:16:21,283 - INFO - tqdm - accuracy: 0.8011, batch_loss: 0.4210, loss: 0.4872 ||:  17%|#6        | 11/65 [02:05<10:14, 11.38s/it]
2022-04-09 18:16:32,667 - INFO - tqdm - accuracy: 0.8047, batch_loss: 0.3370, loss: 0.4747 ||:  18%|#8        | 12/65 [02:16<10:03, 11.38s/it]
2022-04-09 18:16:44,042 - INFO - tqdm - accuracy: 0.8101, batch_loss: 0.3495, loss: 0.4651 ||:  20%|##        | 13/65 [02:28<09:51, 11.38s/it]
2022-04-09 18:16:55,474 - INFO - tqdm - accuracy: 0.8036, batch_loss: 0.5332, loss: 0.4699 ||:  22%|##1       | 14/65 [02:39<09:41, 11.40s/it]
2022-04-09 18:17:06,941 - INFO - tqdm - accuracy: 0.8021, batch_loss: 0.4267, loss: 0.4670 ||:  23%|##3       | 15/65 [02:51<09:30, 11.42s/it]
2022-04-09 18:17:18,407 - INFO - tqdm - accuracy: 0.7988, batch_loss: 0.4321, loss: 0.4649 ||:  25%|##4       | 16/65 [03:02<09:20, 11.43s/it]
2022-04-09 18:17:29,850 - INFO - tqdm - accuracy: 0.7923, batch_loss: 0.5380, loss: 0.4692 ||:  26%|##6       | 17/65 [03:14<09:08, 11.44s/it]
2022-04-09 18:17:41,274 - INFO - tqdm - accuracy: 0.8038, batch_loss: 0.2151, loss: 0.4550 ||:  28%|##7       | 18/65 [03:25<08:57, 11.43s/it]
2022-04-09 18:17:52,694 - INFO - tqdm - accuracy: 0.7993, batch_loss: 0.5726, loss: 0.4612 ||:  29%|##9       | 19/65 [03:36<08:45, 11.43s/it]
2022-04-09 18:18:04,080 - INFO - tqdm - accuracy: 0.7922, batch_loss: 0.6334, loss: 0.4698 ||:  31%|###       | 20/65 [03:48<08:33, 11.42s/it]
2022-04-09 18:18:15,453 - INFO - tqdm - accuracy: 0.7932, batch_loss: 0.3552, loss: 0.4644 ||:  32%|###2      | 21/65 [03:59<08:21, 11.40s/it]
2022-04-09 18:18:26,833 - INFO - tqdm - accuracy: 0.7955, batch_loss: 0.4464, loss: 0.4636 ||:  34%|###3      | 22/65 [04:11<08:10, 11.40s/it]
2022-04-09 18:18:38,218 - INFO - tqdm - accuracy: 0.7962, batch_loss: 0.3253, loss: 0.4576 ||:  35%|###5      | 23/65 [04:22<07:58, 11.39s/it]
2022-04-09 18:18:49,583 - INFO - tqdm - accuracy: 0.7982, batch_loss: 0.3687, loss: 0.4539 ||:  37%|###6      | 24/65 [04:33<07:46, 11.38s/it]
2022-04-09 18:19:00,952 - INFO - tqdm - accuracy: 0.8013, batch_loss: 0.3123, loss: 0.4482 ||:  38%|###8      | 25/65 [04:45<07:35, 11.38s/it]
2022-04-09 18:19:12,348 - INFO - tqdm - accuracy: 0.8041, batch_loss: 0.3192, loss: 0.4432 ||:  40%|####      | 26/65 [04:56<07:23, 11.38s/it]
2022-04-09 18:19:23,770 - INFO - tqdm - accuracy: 0.8032, batch_loss: 0.4441, loss: 0.4433 ||:  42%|####1     | 27/65 [05:07<07:13, 11.40s/it]
2022-04-09 18:19:35,194 - INFO - tqdm - accuracy: 0.8036, batch_loss: 0.3788, loss: 0.4410 ||:  43%|####3     | 28/65 [05:19<07:01, 11.40s/it]
2022-04-09 18:19:46,621 - INFO - tqdm - accuracy: 0.8028, batch_loss: 0.3623, loss: 0.4382 ||:  45%|####4     | 29/65 [05:30<06:50, 11.41s/it]
2022-04-09 18:19:58,062 - INFO - tqdm - accuracy: 0.8021, batch_loss: 0.3568, loss: 0.4355 ||:  46%|####6     | 30/65 [05:42<06:39, 11.42s/it]
2022-04-09 18:20:09,531 - INFO - tqdm - accuracy: 0.8034, batch_loss: 0.3936, loss: 0.4342 ||:  48%|####7     | 31/65 [05:53<06:28, 11.43s/it]
2022-04-09 18:20:20,861 - INFO - tqdm - accuracy: 0.8037, batch_loss: 0.4185, loss: 0.4337 ||:  49%|####9     | 32/65 [06:05<06:16, 11.40s/it]
2022-04-09 18:20:32,325 - INFO - tqdm - accuracy: 0.8040, batch_loss: 0.3885, loss: 0.4323 ||:  51%|#####     | 33/65 [06:16<06:05, 11.42s/it]
2022-04-09 18:20:43,783 - INFO - tqdm - accuracy: 0.8051, batch_loss: 0.4349, loss: 0.4324 ||:  52%|#####2    | 34/65 [06:28<05:54, 11.43s/it]
2022-04-09 18:20:55,220 - INFO - tqdm - accuracy: 0.8054, batch_loss: 0.4302, loss: 0.4323 ||:  54%|#####3    | 35/65 [06:39<05:43, 11.43s/it]
2022-04-09 18:21:06,637 - INFO - tqdm - accuracy: 0.8056, batch_loss: 0.4480, loss: 0.4328 ||:  55%|#####5    | 36/65 [06:50<05:31, 11.43s/it]
2022-04-09 18:21:18,061 - INFO - tqdm - accuracy: 0.8057, batch_loss: 0.3641, loss: 0.4309 ||:  57%|#####6    | 37/65 [07:02<05:19, 11.43s/it]
2022-04-09 18:21:29,473 - INFO - tqdm - accuracy: 0.8043, batch_loss: 0.4079, loss: 0.4303 ||:  58%|#####8    | 38/65 [07:13<05:08, 11.42s/it]
2022-04-09 18:21:40,866 - INFO - tqdm - accuracy: 0.8013, batch_loss: 0.5734, loss: 0.4340 ||:  60%|######    | 39/65 [07:25<04:56, 11.41s/it]
2022-04-09 18:21:52,247 - INFO - tqdm - accuracy: 0.7984, batch_loss: 0.4985, loss: 0.4356 ||:  62%|######1   | 40/65 [07:36<04:45, 11.40s/it]
2022-04-09 18:22:03,644 - INFO - tqdm - accuracy: 0.7995, batch_loss: 0.4371, loss: 0.4356 ||:  63%|######3   | 41/65 [07:47<04:33, 11.40s/it]
2022-04-09 18:22:15,039 - INFO - tqdm - accuracy: 0.8006, batch_loss: 0.3644, loss: 0.4339 ||:  65%|######4   | 42/65 [07:59<04:22, 11.40s/it]
2022-04-09 18:22:26,430 - INFO - tqdm - accuracy: 0.8038, batch_loss: 0.2814, loss: 0.4304 ||:  66%|######6   | 43/65 [08:10<04:10, 11.40s/it]
2022-04-09 18:22:37,805 - INFO - tqdm - accuracy: 0.8033, batch_loss: 0.5305, loss: 0.4327 ||:  68%|######7   | 44/65 [08:22<03:59, 11.39s/it]
2022-04-09 18:22:49,206 - INFO - tqdm - accuracy: 0.7986, batch_loss: 0.5631, loss: 0.4356 ||:  69%|######9   | 45/65 [08:33<03:47, 11.39s/it]
2022-04-09 18:23:00,590 - INFO - tqdm - accuracy: 0.7982, batch_loss: 0.4249, loss: 0.4353 ||:  71%|#######   | 46/65 [08:44<03:36, 11.39s/it]
2022-04-09 18:23:11,991 - INFO - tqdm - accuracy: 0.7999, batch_loss: 0.3916, loss: 0.4344 ||:  72%|#######2  | 47/65 [08:56<03:25, 11.39s/it]
2022-04-09 18:23:23,388 - INFO - tqdm - accuracy: 0.8014, batch_loss: 0.3081, loss: 0.4318 ||:  74%|#######3  | 48/65 [09:07<03:13, 11.39s/it]
2022-04-09 18:23:34,775 - INFO - tqdm - accuracy: 0.8017, batch_loss: 0.4363, loss: 0.4319 ||:  75%|#######5  | 49/65 [09:18<03:02, 11.39s/it]
2022-04-09 18:23:46,161 - INFO - tqdm - accuracy: 0.8013, batch_loss: 0.5449, loss: 0.4341 ||:  77%|#######6  | 50/65 [09:30<02:50, 11.39s/it]
2022-04-09 18:23:57,557 - INFO - tqdm - accuracy: 0.7996, batch_loss: 0.6220, loss: 0.4378 ||:  78%|#######8  | 51/65 [09:41<02:39, 11.39s/it]
2022-04-09 18:24:08,703 - INFO - tqdm - accuracy: 0.7981, batch_loss: 0.5556, loss: 0.4401 ||:  80%|########  | 52/65 [09:52<02:27, 11.32s/it]
2022-04-09 18:24:20,111 - INFO - tqdm - accuracy: 0.7978, batch_loss: 0.4307, loss: 0.4399 ||:  82%|########1 | 53/65 [10:04<02:16, 11.35s/it]
2022-04-09 18:24:31,516 - INFO - tqdm - accuracy: 0.7963, batch_loss: 0.5604, loss: 0.4421 ||:  83%|########3 | 54/65 [10:15<02:04, 11.36s/it]
2022-04-09 18:24:42,936 - INFO - tqdm - accuracy: 0.7955, batch_loss: 0.4696, loss: 0.4426 ||:  85%|########4 | 55/65 [10:27<01:53, 11.38s/it]
2022-04-09 18:24:54,338 - INFO - tqdm - accuracy: 0.7941, batch_loss: 0.5052, loss: 0.4437 ||:  86%|########6 | 56/65 [10:38<01:42, 11.39s/it]
2022-04-09 18:25:05,743 - INFO - tqdm - accuracy: 0.7939, batch_loss: 0.4562, loss: 0.4440 ||:  88%|########7 | 57/65 [10:49<01:31, 11.39s/it]
2022-04-09 18:25:17,149 - INFO - tqdm - accuracy: 0.7958, batch_loss: 0.3023, loss: 0.4415 ||:  89%|########9 | 58/65 [11:01<01:19, 11.40s/it]
2022-04-09 18:25:28,204 - INFO - tqdm - accuracy: 0.7960, batch_loss: 0.4253, loss: 0.4412 ||:  91%|######### | 59/65 [11:12<01:07, 11.29s/it]
2022-04-09 18:25:39,600 - INFO - tqdm - accuracy: 0.7968, batch_loss: 0.3593, loss: 0.4399 ||:  92%|#########2| 60/65 [11:23<00:56, 11.32s/it]
2022-04-09 18:25:50,999 - INFO - tqdm - accuracy: 0.7975, batch_loss: 0.3996, loss: 0.4392 ||:  94%|#########3| 61/65 [11:35<00:45, 11.35s/it]
2022-04-09 18:26:02,408 - INFO - tqdm - accuracy: 0.7983, batch_loss: 0.3739, loss: 0.4382 ||:  95%|#########5| 62/65 [11:46<00:34, 11.37s/it]
2022-04-09 18:26:13,774 - INFO - tqdm - accuracy: 0.7985, batch_loss: 0.3958, loss: 0.4375 ||:  97%|#########6| 63/65 [11:57<00:22, 11.37s/it]
2022-04-09 18:26:25,176 - INFO - tqdm - accuracy: 0.7982, batch_loss: 0.3872, loss: 0.4367 ||:  98%|#########8| 64/65 [12:09<00:11, 11.38s/it]
2022-04-09 18:26:30,233 - INFO - tqdm - accuracy: 0.7991, batch_loss: 0.3495, loss: 0.4354 ||: 100%|##########| 65/65 [12:14<00:00,  9.48s/it]
2022-04-09 18:26:30,233 - INFO - tqdm - accuracy: 0.7991, batch_loss: 0.3495, loss: 0.4354 ||: 100%|##########| 65/65 [12:14<00:00, 11.30s/it]
2022-04-09 18:26:33,060 - INFO - allennlp.training.trainer - Validating
2022-04-09 18:26:33,062 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-09 18:26:43,305 - INFO - tqdm - accuracy: 0.6341, batch_loss: 1.2826, loss: 0.9768 ||:  36%|###5      | 41/115 [00:10<00:18,  3.98it/s]
2022-04-09 18:26:53,328 - INFO - tqdm - accuracy: 0.6790, batch_loss: 0.4989, loss: 0.8131 ||:  70%|#######   | 81/115 [00:20<00:08,  3.92it/s]
2022-04-09 18:27:01,891 - INFO - tqdm - accuracy: 0.6638, batch_loss: 0.0504, loss: 0.8003 ||: 100%|##########| 115/115 [00:28<00:00,  4.01it/s]
2022-04-09 18:27:01,892 - INFO - tqdm - accuracy: 0.6638, batch_loss: 0.0504, loss: 0.8003 ||: 100%|##########| 115/115 [00:28<00:00,  3.99it/s]
2022-04-09 18:27:01,892 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-09 18:27:01,893 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.799  |     0.664
2022-04-09 18:27:01,895 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-09 18:27:01,896 - INFO - allennlp.training.tensorboard_writer - loss               |     0.435  |     0.800
2022-04-09 18:27:01,897 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5806.574  |       N/A
2022-04-09 18:27:07,482 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to './output_ir-ora-d_hyper1/best.th'.
2022-04-09 18:27:24,152 - INFO - allennlp.training.trainer - Epoch duration: 0:13:08.373372
2022-04-09 18:27:24,153 - INFO - allennlp.training.trainer - Estimated training time remaining: 1:17:55
2022-04-09 18:27:24,153 - INFO - allennlp.training.trainer - Epoch 4/9
2022-04-09 18:27:24,153 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-09 18:27:24,153 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-09 18:27:24,155 - INFO - allennlp.training.trainer - Training
2022-04-09 18:27:24,155 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-09 18:27:35,212 - INFO - tqdm - accuracy: 0.9062, batch_loss: 0.3265, loss: 0.3265 ||:   2%|1         | 1/65 [00:11<11:47, 11.06s/it]
2022-04-09 18:27:46,566 - INFO - tqdm - accuracy: 0.9219, batch_loss: 0.2258, loss: 0.2762 ||:   3%|3         | 2/65 [00:22<11:47, 11.23s/it]
2022-04-09 18:27:57,955 - INFO - tqdm - accuracy: 0.8750, batch_loss: 0.4364, loss: 0.3296 ||:   5%|4         | 3/65 [00:33<11:40, 11.30s/it]
2022-04-09 18:28:09,341 - INFO - tqdm - accuracy: 0.8672, batch_loss: 0.3444, loss: 0.3333 ||:   6%|6         | 4/65 [00:45<11:31, 11.34s/it]
2022-04-09 18:28:20,681 - INFO - tqdm - accuracy: 0.8625, batch_loss: 0.2752, loss: 0.3217 ||:   8%|7         | 5/65 [00:56<11:20, 11.34s/it]
2022-04-09 18:28:32,062 - INFO - tqdm - accuracy: 0.8750, batch_loss: 0.2710, loss: 0.3132 ||:   9%|9         | 6/65 [01:07<11:09, 11.35s/it]
2022-04-09 18:28:43,505 - INFO - tqdm - accuracy: 0.8795, batch_loss: 0.2607, loss: 0.3057 ||:  11%|#         | 7/65 [01:19<11:00, 11.38s/it]
2022-04-09 18:28:54,975 - INFO - tqdm - accuracy: 0.8867, batch_loss: 0.2076, loss: 0.2935 ||:  12%|#2        | 8/65 [01:30<10:50, 11.41s/it]
2022-04-09 18:29:06,387 - INFO - tqdm - accuracy: 0.8819, batch_loss: 0.2536, loss: 0.2890 ||:  14%|#3        | 9/65 [01:42<10:38, 11.41s/it]
2022-04-09 18:29:17,767 - INFO - tqdm - accuracy: 0.8875, batch_loss: 0.2098, loss: 0.2811 ||:  15%|#5        | 10/65 [01:53<10:27, 11.40s/it]
2022-04-09 18:29:29,157 - INFO - tqdm - accuracy: 0.8892, batch_loss: 0.2999, loss: 0.2828 ||:  17%|#6        | 11/65 [02:05<10:15, 11.40s/it]
2022-04-09 18:29:40,543 - INFO - tqdm - accuracy: 0.8906, batch_loss: 0.2443, loss: 0.2796 ||:  18%|#8        | 12/65 [02:16<10:03, 11.39s/it]
2022-04-09 18:29:51,998 - INFO - tqdm - accuracy: 0.8894, batch_loss: 0.2892, loss: 0.2803 ||:  20%|##        | 13/65 [02:27<09:53, 11.41s/it]
2022-04-09 18:30:03,455 - INFO - tqdm - accuracy: 0.8973, batch_loss: 0.1032, loss: 0.2677 ||:  22%|##1       | 14/65 [02:39<09:42, 11.43s/it]
2022-04-09 18:30:14,864 - INFO - tqdm - accuracy: 0.8979, batch_loss: 0.2647, loss: 0.2675 ||:  23%|##3       | 15/65 [02:50<09:31, 11.42s/it]
2022-04-09 18:30:26,248 - INFO - tqdm - accuracy: 0.8984, batch_loss: 0.1618, loss: 0.2609 ||:  25%|##4       | 16/65 [03:02<09:19, 11.41s/it]
2022-04-09 18:30:37,648 - INFO - tqdm - accuracy: 0.9007, batch_loss: 0.1732, loss: 0.2557 ||:  26%|##6       | 17/65 [03:13<09:07, 11.41s/it]
2022-04-09 18:30:49,104 - INFO - tqdm - accuracy: 0.9028, batch_loss: 0.2189, loss: 0.2537 ||:  28%|##7       | 18/65 [03:24<08:56, 11.42s/it]
2022-04-09 18:31:00,564 - INFO - tqdm - accuracy: 0.9046, batch_loss: 0.1824, loss: 0.2499 ||:  29%|##9       | 19/65 [03:36<08:45, 11.43s/it]
2022-04-09 18:31:12,031 - INFO - tqdm - accuracy: 0.8984, batch_loss: 0.4972, loss: 0.2623 ||:  31%|###       | 20/65 [03:47<08:34, 11.44s/it]
2022-04-09 18:31:23,485 - INFO - tqdm - accuracy: 0.8988, batch_loss: 0.2562, loss: 0.2620 ||:  32%|###2      | 21/65 [03:59<08:23, 11.45s/it]
2022-04-09 18:31:34,945 - INFO - tqdm - accuracy: 0.8991, batch_loss: 0.2123, loss: 0.2597 ||:  34%|###3      | 22/65 [04:10<08:12, 11.45s/it]
2022-04-09 18:31:46,401 - INFO - tqdm - accuracy: 0.8954, batch_loss: 0.4220, loss: 0.2668 ||:  35%|###5      | 23/65 [04:22<08:00, 11.45s/it]
2022-04-09 18:31:57,848 - INFO - tqdm - accuracy: 0.8958, batch_loss: 0.1489, loss: 0.2619 ||:  37%|###6      | 24/65 [04:33<07:49, 11.45s/it]
2022-04-09 18:32:09,287 - INFO - tqdm - accuracy: 0.8912, batch_loss: 0.5318, loss: 0.2727 ||:  38%|###8      | 25/65 [04:45<07:37, 11.45s/it]
2022-04-09 18:32:20,721 - INFO - tqdm - accuracy: 0.8954, batch_loss: 0.1189, loss: 0.2668 ||:  40%|####      | 26/65 [04:56<07:26, 11.44s/it]
2022-04-09 18:32:32,160 - INFO - tqdm - accuracy: 0.8981, batch_loss: 0.1100, loss: 0.2610 ||:  42%|####1     | 27/65 [05:08<07:14, 11.44s/it]
2022-04-09 18:32:43,606 - INFO - tqdm - accuracy: 0.8996, batch_loss: 0.3642, loss: 0.2646 ||:  43%|####3     | 28/65 [05:19<07:03, 11.44s/it]
2022-04-09 18:32:55,059 - INFO - tqdm - accuracy: 0.9019, batch_loss: 0.1361, loss: 0.2602 ||:  45%|####4     | 29/65 [05:30<06:52, 11.45s/it]
2022-04-09 18:33:06,353 - INFO - tqdm - accuracy: 0.9042, batch_loss: 0.1723, loss: 0.2573 ||:  46%|####6     | 30/65 [05:42<06:39, 11.40s/it]
2022-04-09 18:33:17,779 - INFO - tqdm - accuracy: 0.9052, batch_loss: 0.1387, loss: 0.2535 ||:  48%|####7     | 31/65 [05:53<06:27, 11.41s/it]
2022-04-09 18:33:29,191 - INFO - tqdm - accuracy: 0.9062, batch_loss: 0.1548, loss: 0.2504 ||:  49%|####9     | 32/65 [06:05<06:16, 11.41s/it]
2022-04-09 18:33:40,644 - INFO - tqdm - accuracy: 0.9072, batch_loss: 0.1846, loss: 0.2484 ||:  51%|#####     | 33/65 [06:16<06:05, 11.42s/it]
2022-04-09 18:33:52,091 - INFO - tqdm - accuracy: 0.9072, batch_loss: 0.3031, loss: 0.2500 ||:  52%|#####2    | 34/65 [06:27<05:54, 11.43s/it]
2022-04-09 18:34:03,503 - INFO - tqdm - accuracy: 0.9098, batch_loss: 0.0922, loss: 0.2455 ||:  54%|#####3    | 35/65 [06:39<05:42, 11.42s/it]
2022-04-09 18:34:14,914 - INFO - tqdm - accuracy: 0.9106, batch_loss: 0.1759, loss: 0.2435 ||:  55%|#####5    | 36/65 [06:50<05:31, 11.42s/it]
2022-04-09 18:34:25,976 - INFO - tqdm - accuracy: 0.9079, batch_loss: 0.2956, loss: 0.2450 ||:  57%|#####6    | 37/65 [07:01<05:16, 11.31s/it]
2022-04-09 18:34:37,406 - INFO - tqdm - accuracy: 0.9095, batch_loss: 0.0701, loss: 0.2403 ||:  58%|#####8    | 38/65 [07:13<05:06, 11.35s/it]
2022-04-09 18:34:48,796 - INFO - tqdm - accuracy: 0.9102, batch_loss: 0.2473, loss: 0.2405 ||:  60%|######    | 39/65 [07:24<04:55, 11.36s/it]
2022-04-09 18:35:00,663 - INFO - tqdm - accuracy: 0.9093, batch_loss: 0.2648, loss: 0.2411 ||:  62%|######1   | 40/65 [07:36<04:47, 11.51s/it]
2022-04-09 18:35:12,009 - INFO - tqdm - accuracy: 0.9100, batch_loss: 0.1279, loss: 0.2384 ||:  63%|######3   | 41/65 [07:47<04:35, 11.46s/it]
2022-04-09 18:35:23,442 - INFO - tqdm - accuracy: 0.9099, batch_loss: 0.2561, loss: 0.2388 ||:  65%|######4   | 42/65 [07:59<04:23, 11.45s/it]
2022-04-09 18:35:34,874 - INFO - tqdm - accuracy: 0.9105, batch_loss: 0.2307, loss: 0.2386 ||:  66%|######6   | 43/65 [08:10<04:11, 11.45s/it]
2022-04-09 18:35:46,305 - INFO - tqdm - accuracy: 0.9090, batch_loss: 0.3453, loss: 0.2410 ||:  68%|######7   | 44/65 [08:22<04:00, 11.44s/it]
2022-04-09 18:35:57,638 - INFO - tqdm - accuracy: 0.9097, batch_loss: 0.1750, loss: 0.2396 ||:  69%|######9   | 45/65 [08:33<03:48, 11.41s/it]
2022-04-09 18:36:09,100 - INFO - tqdm - accuracy: 0.9075, batch_loss: 0.4699, loss: 0.2446 ||:  71%|#######   | 46/65 [08:44<03:37, 11.43s/it]
2022-04-09 18:36:20,570 - INFO - tqdm - accuracy: 0.9055, batch_loss: 0.3926, loss: 0.2477 ||:  72%|#######2  | 47/65 [08:56<03:25, 11.44s/it]
2022-04-09 18:36:32,025 - INFO - tqdm - accuracy: 0.9055, batch_loss: 0.2141, loss: 0.2470 ||:  74%|#######3  | 48/65 [09:07<03:14, 11.44s/it]
2022-04-09 18:36:43,470 - INFO - tqdm - accuracy: 0.9068, batch_loss: 0.1267, loss: 0.2446 ||:  75%|#######5  | 49/65 [09:19<03:03, 11.44s/it]
2022-04-09 18:36:54,911 - INFO - tqdm - accuracy: 0.9056, batch_loss: 0.3003, loss: 0.2457 ||:  77%|#######6  | 50/65 [09:30<02:51, 11.44s/it]
2022-04-09 18:37:06,352 - INFO - tqdm - accuracy: 0.9068, batch_loss: 0.1311, loss: 0.2434 ||:  78%|#######8  | 51/65 [09:42<02:40, 11.44s/it]
2022-04-09 18:37:17,783 - INFO - tqdm - accuracy: 0.9056, batch_loss: 0.2379, loss: 0.2433 ||:  80%|########  | 52/65 [09:53<02:28, 11.44s/it]
2022-04-09 18:37:29,222 - INFO - tqdm - accuracy: 0.9056, batch_loss: 0.2540, loss: 0.2435 ||:  82%|########1 | 53/65 [10:05<02:17, 11.44s/it]
2022-04-09 18:37:40,648 - INFO - tqdm - accuracy: 0.9056, batch_loss: 0.2585, loss: 0.2438 ||:  83%|########3 | 54/65 [10:16<02:05, 11.44s/it]
2022-04-09 18:37:52,041 - INFO - tqdm - accuracy: 0.9056, batch_loss: 0.2604, loss: 0.2441 ||:  85%|########4 | 55/65 [10:27<01:54, 11.42s/it]
2022-04-09 18:38:03,481 - INFO - tqdm - accuracy: 0.9051, batch_loss: 0.1759, loss: 0.2429 ||:  86%|########6 | 56/65 [10:39<01:42, 11.43s/it]
2022-04-09 18:38:14,928 - INFO - tqdm - accuracy: 0.9035, batch_loss: 0.3301, loss: 0.2444 ||:  88%|########7 | 57/65 [10:50<01:31, 11.43s/it]
2022-04-09 18:38:26,340 - INFO - tqdm - accuracy: 0.9040, batch_loss: 0.2564, loss: 0.2446 ||:  89%|########9 | 58/65 [11:02<01:19, 11.43s/it]
2022-04-09 18:38:37,770 - INFO - tqdm - accuracy: 0.9046, batch_loss: 0.1976, loss: 0.2438 ||:  91%|######### | 59/65 [11:13<01:08, 11.43s/it]
2022-04-09 18:38:49,182 - INFO - tqdm - accuracy: 0.9036, batch_loss: 0.3092, loss: 0.2449 ||:  92%|#########2| 60/65 [11:25<00:57, 11.42s/it]
2022-04-09 18:39:00,599 - INFO - tqdm - accuracy: 0.9047, batch_loss: 0.1463, loss: 0.2433 ||:  94%|#########3| 61/65 [11:36<00:45, 11.42s/it]
2022-04-09 18:39:11,757 - INFO - tqdm - accuracy: 0.9057, batch_loss: 0.1567, loss: 0.2419 ||:  95%|#########5| 62/65 [11:47<00:34, 11.34s/it]
2022-04-09 18:39:23,153 - INFO - tqdm - accuracy: 0.9067, batch_loss: 0.1376, loss: 0.2402 ||:  97%|#########6| 63/65 [11:58<00:22, 11.36s/it]
2022-04-09 18:39:34,560 - INFO - tqdm - accuracy: 0.9057, batch_loss: 0.3656, loss: 0.2422 ||:  98%|#########8| 64/65 [12:10<00:11, 11.37s/it]
2022-04-09 18:39:39,609 - INFO - tqdm - accuracy: 0.9059, batch_loss: 0.2525, loss: 0.2424 ||: 100%|##########| 65/65 [12:15<00:00,  9.48s/it]
2022-04-09 18:39:39,609 - INFO - tqdm - accuracy: 0.9059, batch_loss: 0.2525, loss: 0.2424 ||: 100%|##########| 65/65 [12:15<00:00, 11.31s/it]
2022-04-09 18:39:42,447 - INFO - allennlp.training.trainer - Validating
2022-04-09 18:39:42,449 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-09 18:39:52,471 - INFO - tqdm - accuracy: 0.6125, batch_loss: 0.0042, loss: 1.1279 ||:  35%|###4      | 40/115 [00:10<00:18,  3.98it/s]
2022-04-09 18:40:02,580 - INFO - tqdm - accuracy: 0.6250, batch_loss: 0.0099, loss: 1.1179 ||:  70%|######9   | 80/115 [00:20<00:08,  3.94it/s]
2022-04-09 18:40:11,280 - INFO - tqdm - accuracy: 0.6376, batch_loss: 0.0132, loss: 1.1088 ||: 100%|##########| 115/115 [00:28<00:00,  3.94it/s]
2022-04-09 18:40:11,280 - INFO - tqdm - accuracy: 0.6376, batch_loss: 0.0132, loss: 1.1088 ||: 100%|##########| 115/115 [00:28<00:00,  3.99it/s]
2022-04-09 18:40:11,280 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-09 18:40:11,280 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.906  |     0.638
2022-04-09 18:40:11,281 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-09 18:40:11,281 - INFO - allennlp.training.tensorboard_writer - loss               |     0.242  |     1.109
2022-04-09 18:40:11,281 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5806.574  |       N/A
2022-04-09 18:40:17,354 - INFO - allennlp.training.trainer - Epoch duration: 0:12:53.201005
2022-04-09 18:40:17,354 - INFO - allennlp.training.trainer - Estimated training time remaining: 1:04:50
2022-04-09 18:40:17,354 - INFO - allennlp.training.trainer - Epoch 5/9
2022-04-09 18:40:17,354 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-09 18:40:17,355 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-09 18:40:17,356 - INFO - allennlp.training.trainer - Training
2022-04-09 18:40:17,357 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-09 18:40:28,484 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0836, loss: 0.0836 ||:   2%|1         | 1/65 [00:11<11:52, 11.13s/it]
2022-04-09 18:40:40,008 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.1184, loss: 0.1010 ||:   3%|3         | 2/65 [00:22<11:55, 11.36s/it]
2022-04-09 18:40:51,679 - INFO - tqdm - accuracy: 0.9479, batch_loss: 0.2873, loss: 0.1631 ||:   5%|4         | 3/65 [00:34<11:53, 11.50s/it]
2022-04-09 18:41:03,134 - INFO - tqdm - accuracy: 0.9453, batch_loss: 0.1324, loss: 0.1554 ||:   6%|6         | 4/65 [00:45<11:40, 11.48s/it]
2022-04-09 18:41:14,485 - INFO - tqdm - accuracy: 0.9500, batch_loss: 0.1568, loss: 0.1557 ||:   8%|7         | 5/65 [00:57<11:26, 11.44s/it]
2022-04-09 18:41:25,869 - INFO - tqdm - accuracy: 0.9583, batch_loss: 0.1486, loss: 0.1545 ||:   9%|9         | 6/65 [01:08<11:13, 11.42s/it]
2022-04-09 18:41:37,165 - INFO - tqdm - accuracy: 0.9643, batch_loss: 0.0786, loss: 0.1437 ||:  11%|#         | 7/65 [01:19<10:59, 11.38s/it]
2022-04-09 18:41:48,650 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.0640, loss: 0.1337 ||:  12%|#2        | 8/65 [01:31<10:50, 11.41s/it]
2022-04-09 18:42:00,123 - INFO - tqdm - accuracy: 0.9722, batch_loss: 0.0589, loss: 0.1254 ||:  14%|#3        | 9/65 [01:42<10:40, 11.43s/it]
2022-04-09 18:42:11,546 - INFO - tqdm - accuracy: 0.9719, batch_loss: 0.0977, loss: 0.1226 ||:  15%|#5        | 10/65 [01:54<10:28, 11.43s/it]
2022-04-09 18:42:22,946 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.1326, loss: 0.1235 ||:  17%|#6        | 11/65 [02:05<10:16, 11.42s/it]
2022-04-09 18:42:34,303 - INFO - tqdm - accuracy: 0.9714, batch_loss: 0.0952, loss: 0.1212 ||:  18%|#8        | 12/65 [02:16<10:04, 11.40s/it]
2022-04-09 18:42:45,709 - INFO - tqdm - accuracy: 0.9663, batch_loss: 0.1744, loss: 0.1253 ||:  20%|##        | 13/65 [02:28<09:52, 11.40s/it]
2022-04-09 18:42:57,143 - INFO - tqdm - accuracy: 0.9576, batch_loss: 0.3827, loss: 0.1437 ||:  22%|##1       | 14/65 [02:39<09:42, 11.41s/it]
2022-04-09 18:43:08,595 - INFO - tqdm - accuracy: 0.9604, batch_loss: 0.0481, loss: 0.1373 ||:  23%|##3       | 15/65 [02:51<09:31, 11.42s/it]
2022-04-09 18:43:19,692 - INFO - tqdm - accuracy: 0.9589, batch_loss: 0.1103, loss: 0.1356 ||:  25%|##4       | 16/65 [03:02<09:14, 11.33s/it]
2022-04-09 18:43:31,140 - INFO - tqdm - accuracy: 0.9613, batch_loss: 0.0666, loss: 0.1315 ||:  26%|##6       | 17/65 [03:13<09:05, 11.36s/it]
2022-04-09 18:43:42,615 - INFO - tqdm - accuracy: 0.9583, batch_loss: 0.2075, loss: 0.1358 ||:  28%|##7       | 18/65 [03:25<08:55, 11.40s/it]
2022-04-09 18:43:54,072 - INFO - tqdm - accuracy: 0.9588, batch_loss: 0.0855, loss: 0.1331 ||:  29%|##9       | 19/65 [03:36<08:45, 11.41s/it]
2022-04-09 18:44:05,549 - INFO - tqdm - accuracy: 0.9609, batch_loss: 0.0280, loss: 0.1279 ||:  31%|###       | 20/65 [03:48<08:34, 11.43s/it]
2022-04-09 18:44:17,023 - INFO - tqdm - accuracy: 0.9598, batch_loss: 0.2418, loss: 0.1333 ||:  32%|###2      | 21/65 [03:59<08:23, 11.45s/it]
2022-04-09 18:44:28,482 - INFO - tqdm - accuracy: 0.9573, batch_loss: 0.2790, loss: 0.1399 ||:  34%|###3      | 22/65 [04:11<08:12, 11.45s/it]
2022-04-09 18:44:39,922 - INFO - tqdm - accuracy: 0.9524, batch_loss: 0.3025, loss: 0.1470 ||:  35%|###5      | 23/65 [04:22<08:00, 11.45s/it]
2022-04-09 18:44:51,368 - INFO - tqdm - accuracy: 0.9518, batch_loss: 0.1030, loss: 0.1451 ||:  37%|###6      | 24/65 [04:34<07:49, 11.45s/it]
2022-04-09 18:45:02,808 - INFO - tqdm - accuracy: 0.9512, batch_loss: 0.1168, loss: 0.1440 ||:  38%|###8      | 25/65 [04:45<07:37, 11.44s/it]
2022-04-09 18:45:14,258 - INFO - tqdm - accuracy: 0.9507, batch_loss: 0.0846, loss: 0.1417 ||:  40%|####      | 26/65 [04:56<07:26, 11.45s/it]
2022-04-09 18:45:25,691 - INFO - tqdm - accuracy: 0.9525, batch_loss: 0.0974, loss: 0.1401 ||:  42%|####1     | 27/65 [05:08<07:14, 11.44s/it]
2022-04-09 18:45:37,124 - INFO - tqdm - accuracy: 0.9520, batch_loss: 0.1287, loss: 0.1397 ||:  43%|####3     | 28/65 [05:19<07:03, 11.44s/it]
2022-04-09 18:45:48,561 - INFO - tqdm - accuracy: 0.9536, batch_loss: 0.0443, loss: 0.1364 ||:  45%|####4     | 29/65 [05:31<06:51, 11.44s/it]
2022-04-09 18:46:00,003 - INFO - tqdm - accuracy: 0.9520, batch_loss: 0.2160, loss: 0.1390 ||:  46%|####6     | 30/65 [05:42<06:40, 11.44s/it]
2022-04-09 18:46:11,404 - INFO - tqdm - accuracy: 0.9526, batch_loss: 0.0983, loss: 0.1377 ||:  48%|####7     | 31/65 [05:54<06:28, 11.43s/it]
2022-04-09 18:46:22,790 - INFO - tqdm - accuracy: 0.9531, batch_loss: 0.0626, loss: 0.1354 ||:  49%|####9     | 32/65 [06:05<06:16, 11.42s/it]
2022-04-09 18:46:34,203 - INFO - tqdm - accuracy: 0.9517, batch_loss: 0.1118, loss: 0.1347 ||:  51%|#####     | 33/65 [06:16<06:05, 11.41s/it]
2022-04-09 18:46:45,606 - INFO - tqdm - accuracy: 0.9522, batch_loss: 0.0882, loss: 0.1333 ||:  52%|#####2    | 34/65 [06:28<05:53, 11.41s/it]
2022-04-09 18:46:57,012 - INFO - tqdm - accuracy: 0.9526, batch_loss: 0.0532, loss: 0.1310 ||:  54%|#####3    | 35/65 [06:39<05:42, 11.41s/it]
2022-04-09 18:47:08,419 - INFO - tqdm - accuracy: 0.9531, batch_loss: 0.0646, loss: 0.1292 ||:  55%|#####5    | 36/65 [06:51<05:30, 11.41s/it]
2022-04-09 18:47:19,808 - INFO - tqdm - accuracy: 0.9544, batch_loss: 0.0911, loss: 0.1281 ||:  57%|#####6    | 37/65 [07:02<05:19, 11.40s/it]
2022-04-09 18:47:31,186 - INFO - tqdm - accuracy: 0.9547, batch_loss: 0.0672, loss: 0.1265 ||:  58%|#####8    | 38/65 [07:13<05:07, 11.40s/it]
2022-04-09 18:47:42,579 - INFO - tqdm - accuracy: 0.9543, batch_loss: 0.1147, loss: 0.1262 ||:  60%|######    | 39/65 [07:25<04:56, 11.39s/it]
2022-04-09 18:47:53,975 - INFO - tqdm - accuracy: 0.9523, batch_loss: 0.2960, loss: 0.1305 ||:  62%|######1   | 40/65 [07:36<04:44, 11.40s/it]
2022-04-09 18:48:05,367 - INFO - tqdm - accuracy: 0.9535, batch_loss: 0.0180, loss: 0.1277 ||:  63%|######3   | 41/65 [07:48<04:33, 11.39s/it]
2022-04-09 18:48:16,773 - INFO - tqdm - accuracy: 0.9538, batch_loss: 0.1106, loss: 0.1273 ||:  65%|######4   | 42/65 [07:59<04:22, 11.40s/it]
2022-04-09 18:48:28,152 - INFO - tqdm - accuracy: 0.9535, batch_loss: 0.1679, loss: 0.1283 ||:  66%|######6   | 43/65 [08:10<04:10, 11.39s/it]
2022-04-09 18:48:39,535 - INFO - tqdm - accuracy: 0.9524, batch_loss: 0.2838, loss: 0.1318 ||:  68%|######7   | 44/65 [08:22<03:59, 11.39s/it]
2022-04-09 18:48:50,670 - INFO - tqdm - accuracy: 0.9521, batch_loss: 0.1651, loss: 0.1325 ||:  69%|######9   | 45/65 [08:33<03:46, 11.31s/it]
2022-04-09 18:49:02,062 - INFO - tqdm - accuracy: 0.9517, batch_loss: 0.1250, loss: 0.1324 ||:  71%|#######   | 46/65 [08:44<03:35, 11.34s/it]
2022-04-09 18:49:13,464 - INFO - tqdm - accuracy: 0.9514, batch_loss: 0.1459, loss: 0.1327 ||:  72%|#######2  | 47/65 [08:56<03:24, 11.36s/it]
2022-04-09 18:49:24,830 - INFO - tqdm - accuracy: 0.9511, batch_loss: 0.1443, loss: 0.1329 ||:  74%|#######3  | 48/65 [09:07<03:13, 11.36s/it]
2022-04-09 18:49:36,242 - INFO - tqdm - accuracy: 0.9515, batch_loss: 0.1114, loss: 0.1325 ||:  75%|#######5  | 49/65 [09:18<03:01, 11.37s/it]
2022-04-09 18:49:47,636 - INFO - tqdm - accuracy: 0.9518, batch_loss: 0.0596, loss: 0.1310 ||:  77%|#######6  | 50/65 [09:30<02:50, 11.38s/it]
2022-04-09 18:49:59,028 - INFO - tqdm - accuracy: 0.9528, batch_loss: 0.0655, loss: 0.1297 ||:  78%|#######8  | 51/65 [09:41<02:39, 11.38s/it]
2022-04-09 18:50:10,397 - INFO - tqdm - accuracy: 0.9525, batch_loss: 0.1961, loss: 0.1310 ||:  80%|########  | 52/65 [09:53<02:27, 11.38s/it]
2022-04-09 18:50:21,778 - INFO - tqdm - accuracy: 0.9534, batch_loss: 0.0442, loss: 0.1294 ||:  82%|########1 | 53/65 [10:04<02:16, 11.38s/it]
2022-04-09 18:50:33,158 - INFO - tqdm - accuracy: 0.9537, batch_loss: 0.1058, loss: 0.1289 ||:  83%|########3 | 54/65 [10:15<02:05, 11.38s/it]
2022-04-09 18:50:44,549 - INFO - tqdm - accuracy: 0.9528, batch_loss: 0.2056, loss: 0.1303 ||:  85%|########4 | 55/65 [10:27<01:53, 11.38s/it]
2022-04-09 18:50:55,939 - INFO - tqdm - accuracy: 0.9525, batch_loss: 0.2412, loss: 0.1323 ||:  86%|########6 | 56/65 [10:38<01:42, 11.39s/it]
2022-04-09 18:51:07,353 - INFO - tqdm - accuracy: 0.9528, batch_loss: 0.0987, loss: 0.1317 ||:  88%|########7 | 57/65 [10:49<01:31, 11.39s/it]
2022-04-09 18:51:18,771 - INFO - tqdm - accuracy: 0.9526, batch_loss: 0.2271, loss: 0.1334 ||:  89%|########9 | 58/65 [11:01<01:19, 11.40s/it]
2022-04-09 18:51:30,101 - INFO - tqdm - accuracy: 0.9523, batch_loss: 0.1240, loss: 0.1332 ||:  91%|######### | 59/65 [11:12<01:08, 11.38s/it]
2022-04-09 18:51:41,547 - INFO - tqdm - accuracy: 0.9510, batch_loss: 0.2530, loss: 0.1352 ||:  92%|#########2| 60/65 [11:24<00:56, 11.40s/it]
2022-04-09 18:51:52,992 - INFO - tqdm - accuracy: 0.9513, batch_loss: 0.1047, loss: 0.1347 ||:  94%|#########3| 61/65 [11:35<00:45, 11.41s/it]
2022-04-09 18:52:04,443 - INFO - tqdm - accuracy: 0.9516, batch_loss: 0.0633, loss: 0.1335 ||:  95%|#########5| 62/65 [11:47<00:34, 11.42s/it]
2022-04-09 18:52:15,896 - INFO - tqdm - accuracy: 0.9514, batch_loss: 0.1950, loss: 0.1345 ||:  97%|#########6| 63/65 [11:58<00:22, 11.43s/it]
2022-04-09 18:52:27,350 - INFO - tqdm - accuracy: 0.9511, batch_loss: 0.1119, loss: 0.1342 ||:  98%|#########8| 64/65 [12:09<00:11, 11.44s/it]
2022-04-09 18:52:32,435 - INFO - tqdm - accuracy: 0.9510, batch_loss: 0.2100, loss: 0.1353 ||: 100%|##########| 65/65 [12:15<00:00,  9.53s/it]
2022-04-09 18:52:32,435 - INFO - tqdm - accuracy: 0.9510, batch_loss: 0.2100, loss: 0.1353 ||: 100%|##########| 65/65 [12:15<00:00, 11.31s/it]
2022-04-09 18:52:35,269 - INFO - allennlp.training.trainer - Validating
2022-04-09 18:52:35,270 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-09 18:52:45,322 - INFO - tqdm - accuracy: 0.5625, batch_loss: 6.4344, loss: 1.9709 ||:  35%|###4      | 40/115 [00:10<00:18,  3.97it/s]
2022-04-09 18:52:55,487 - INFO - tqdm - accuracy: 0.6522, batch_loss: 0.6805, loss: 1.4921 ||:  70%|#######   | 81/115 [00:20<00:08,  4.09it/s]
2022-04-09 18:53:04,099 - INFO - tqdm - accuracy: 0.6332, batch_loss: 0.0024, loss: 1.5428 ||: 100%|##########| 115/115 [00:28<00:00,  3.95it/s]
2022-04-09 18:53:04,100 - INFO - tqdm - accuracy: 0.6332, batch_loss: 0.0024, loss: 1.5428 ||: 100%|##########| 115/115 [00:28<00:00,  3.99it/s]
2022-04-09 18:53:04,100 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-09 18:53:04,101 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.951  |     0.633
2022-04-09 18:53:04,102 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-09 18:53:04,102 - INFO - allennlp.training.tensorboard_writer - loss               |     0.135  |     1.543
2022-04-09 18:53:04,102 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5806.574  |       N/A
2022-04-09 18:53:10,065 - INFO - allennlp.training.trainer - Epoch duration: 0:12:52.710795
2022-04-09 18:53:10,065 - INFO - allennlp.training.trainer - Estimated training time remaining: 0:51:48
2022-04-09 18:53:10,065 - INFO - allennlp.training.trainer - Epoch 6/9
2022-04-09 18:53:10,065 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-09 18:53:10,066 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-09 18:53:10,067 - INFO - allennlp.training.trainer - Training
2022-04-09 18:53:10,068 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-09 18:53:21,365 - INFO - tqdm - accuracy: 0.8750, batch_loss: 0.2201, loss: 0.2201 ||:   2%|1         | 1/65 [00:11<12:02, 11.30s/it]
2022-04-09 18:53:32,909 - INFO - tqdm - accuracy: 0.9375, batch_loss: 0.0273, loss: 0.1237 ||:   3%|3         | 2/65 [00:22<12:00, 11.44s/it]
2022-04-09 18:53:44,574 - INFO - tqdm - accuracy: 0.9375, batch_loss: 0.0782, loss: 0.1085 ||:   5%|4         | 3/65 [00:34<11:55, 11.54s/it]
2022-04-09 18:53:55,969 - INFO - tqdm - accuracy: 0.9453, batch_loss: 0.0865, loss: 0.1030 ||:   6%|6         | 4/65 [00:45<11:40, 11.49s/it]
2022-04-09 18:54:07,330 - INFO - tqdm - accuracy: 0.9437, batch_loss: 0.1651, loss: 0.1154 ||:   8%|7         | 5/65 [00:57<11:26, 11.44s/it]
2022-04-09 18:54:18,706 - INFO - tqdm - accuracy: 0.9479, batch_loss: 0.0706, loss: 0.1080 ||:   9%|9         | 6/65 [01:08<11:13, 11.42s/it]
2022-04-09 18:54:30,099 - INFO - tqdm - accuracy: 0.9509, batch_loss: 0.1086, loss: 0.1080 ||:  11%|#         | 7/65 [01:20<11:01, 11.41s/it]
2022-04-09 18:54:41,520 - INFO - tqdm - accuracy: 0.9531, batch_loss: 0.0397, loss: 0.0995 ||:  12%|#2        | 8/65 [01:31<10:50, 11.41s/it]
2022-04-09 18:54:52,929 - INFO - tqdm - accuracy: 0.9514, batch_loss: 0.1119, loss: 0.1009 ||:  14%|#3        | 9/65 [01:42<10:39, 11.41s/it]
2022-04-09 18:55:04,758 - INFO - tqdm - accuracy: 0.9437, batch_loss: 0.3084, loss: 0.1216 ||:  15%|#5        | 10/65 [01:54<10:34, 11.54s/it]
2022-04-09 18:55:16,154 - INFO - tqdm - accuracy: 0.9460, batch_loss: 0.0656, loss: 0.1165 ||:  17%|#6        | 11/65 [02:06<10:20, 11.50s/it]
2022-04-09 18:55:27,598 - INFO - tqdm - accuracy: 0.9453, batch_loss: 0.1302, loss: 0.1177 ||:  18%|#8        | 12/65 [02:17<10:08, 11.48s/it]
2022-04-09 18:55:39,072 - INFO - tqdm - accuracy: 0.9495, batch_loss: 0.0397, loss: 0.1117 ||:  20%|##        | 13/65 [02:29<09:56, 11.48s/it]
2022-04-09 18:55:50,505 - INFO - tqdm - accuracy: 0.9487, batch_loss: 0.1455, loss: 0.1141 ||:  22%|##1       | 14/65 [02:40<09:44, 11.46s/it]
2022-04-09 18:56:01,959 - INFO - tqdm - accuracy: 0.9479, batch_loss: 0.0938, loss: 0.1127 ||:  23%|##3       | 15/65 [02:51<09:33, 11.46s/it]
2022-04-09 18:56:13,381 - INFO - tqdm - accuracy: 0.9492, batch_loss: 0.0425, loss: 0.1083 ||:  25%|##4       | 16/65 [03:03<09:21, 11.45s/it]
2022-04-09 18:56:24,796 - INFO - tqdm - accuracy: 0.9504, batch_loss: 0.0690, loss: 0.1060 ||:  26%|##6       | 17/65 [03:14<09:09, 11.44s/it]
2022-04-09 18:56:36,180 - INFO - tqdm - accuracy: 0.9514, batch_loss: 0.0748, loss: 0.1043 ||:  28%|##7       | 18/65 [03:26<08:56, 11.42s/it]
2022-04-09 18:56:47,591 - INFO - tqdm - accuracy: 0.9523, batch_loss: 0.1030, loss: 0.1042 ||:  29%|##9       | 19/65 [03:37<08:45, 11.42s/it]
2022-04-09 18:56:58,970 - INFO - tqdm - accuracy: 0.9531, batch_loss: 0.0678, loss: 0.1024 ||:  31%|###       | 20/65 [03:48<08:33, 11.41s/it]
2022-04-09 18:57:10,339 - INFO - tqdm - accuracy: 0.9539, batch_loss: 0.1077, loss: 0.1027 ||:  32%|###2      | 21/65 [04:00<08:21, 11.40s/it]
2022-04-09 18:57:21,691 - INFO - tqdm - accuracy: 0.9560, batch_loss: 0.0389, loss: 0.0998 ||:  34%|###3      | 22/65 [04:11<08:09, 11.38s/it]
2022-04-09 18:57:33,049 - INFO - tqdm - accuracy: 0.9565, batch_loss: 0.1435, loss: 0.1017 ||:  35%|###5      | 23/65 [04:22<07:57, 11.38s/it]
2022-04-09 18:57:44,413 - INFO - tqdm - accuracy: 0.9531, batch_loss: 0.2236, loss: 0.1067 ||:  37%|###6      | 24/65 [04:34<07:46, 11.37s/it]
2022-04-09 18:57:55,781 - INFO - tqdm - accuracy: 0.9525, batch_loss: 0.1253, loss: 0.1075 ||:  38%|###8      | 25/65 [04:45<07:34, 11.37s/it]
2022-04-09 18:58:07,204 - INFO - tqdm - accuracy: 0.9531, batch_loss: 0.0884, loss: 0.1068 ||:  40%|####      | 26/65 [04:57<07:24, 11.39s/it]
2022-04-09 18:58:18,635 - INFO - tqdm - accuracy: 0.9537, batch_loss: 0.0429, loss: 0.1044 ||:  42%|####1     | 27/65 [05:08<07:13, 11.40s/it]
2022-04-09 18:58:30,076 - INFO - tqdm - accuracy: 0.9554, batch_loss: 0.0599, loss: 0.1028 ||:  43%|####3     | 28/65 [05:20<07:02, 11.41s/it]
2022-04-09 18:58:41,507 - INFO - tqdm - accuracy: 0.9569, batch_loss: 0.0113, loss: 0.0996 ||:  45%|####4     | 29/65 [05:31<06:51, 11.42s/it]
2022-04-09 18:58:52,810 - INFO - tqdm - accuracy: 0.9552, batch_loss: 0.1295, loss: 0.1006 ||:  46%|####6     | 30/65 [05:42<06:38, 11.38s/it]
2022-04-09 18:59:04,223 - INFO - tqdm - accuracy: 0.9567, batch_loss: 0.0398, loss: 0.0987 ||:  48%|####7     | 31/65 [05:54<06:27, 11.39s/it]
2022-04-09 18:59:15,612 - INFO - tqdm - accuracy: 0.9580, batch_loss: 0.0152, loss: 0.0961 ||:  49%|####9     | 32/65 [06:05<06:15, 11.39s/it]
2022-04-09 18:59:27,025 - INFO - tqdm - accuracy: 0.9593, batch_loss: 0.0425, loss: 0.0944 ||:  51%|#####     | 33/65 [06:16<06:04, 11.40s/it]
2022-04-09 18:59:38,393 - INFO - tqdm - accuracy: 0.9605, batch_loss: 0.0272, loss: 0.0925 ||:  52%|#####2    | 34/65 [06:28<05:53, 11.39s/it]
2022-04-09 18:59:49,777 - INFO - tqdm - accuracy: 0.9616, batch_loss: 0.0339, loss: 0.0908 ||:  54%|#####3    | 35/65 [06:39<05:41, 11.39s/it]
2022-04-09 19:00:01,153 - INFO - tqdm - accuracy: 0.9627, batch_loss: 0.0351, loss: 0.0893 ||:  55%|#####5    | 36/65 [06:51<05:30, 11.38s/it]
2022-04-09 19:00:12,525 - INFO - tqdm - accuracy: 0.9628, batch_loss: 0.0367, loss: 0.0878 ||:  57%|#####6    | 37/65 [07:02<05:18, 11.38s/it]
2022-04-09 19:00:23,912 - INFO - tqdm - accuracy: 0.9630, batch_loss: 0.1201, loss: 0.0887 ||:  58%|#####8    | 38/65 [07:13<05:07, 11.38s/it]
2022-04-09 19:00:35,292 - INFO - tqdm - accuracy: 0.9631, batch_loss: 0.0772, loss: 0.0884 ||:  60%|######    | 39/65 [07:25<04:55, 11.38s/it]
2022-04-09 19:00:46,688 - INFO - tqdm - accuracy: 0.9617, batch_loss: 0.2201, loss: 0.0917 ||:  62%|######1   | 40/65 [07:36<04:44, 11.39s/it]
2022-04-09 19:00:57,750 - INFO - tqdm - accuracy: 0.9619, batch_loss: 0.1029, loss: 0.0920 ||:  63%|######3   | 41/65 [07:47<04:30, 11.29s/it]
2022-04-09 19:01:09,164 - INFO - tqdm - accuracy: 0.9605, batch_loss: 0.1596, loss: 0.0936 ||:  65%|######4   | 42/65 [07:59<04:20, 11.33s/it]
2022-04-09 19:01:20,343 - INFO - tqdm - accuracy: 0.9607, batch_loss: 0.1121, loss: 0.0940 ||:  66%|######6   | 43/65 [08:10<04:08, 11.28s/it]
2022-04-09 19:01:31,768 - INFO - tqdm - accuracy: 0.9609, batch_loss: 0.0694, loss: 0.0934 ||:  68%|######7   | 44/65 [08:21<03:57, 11.32s/it]
2022-04-09 19:01:43,211 - INFO - tqdm - accuracy: 0.9604, batch_loss: 0.1721, loss: 0.0952 ||:  69%|######9   | 45/65 [08:33<03:47, 11.36s/it]
2022-04-09 19:01:54,660 - INFO - tqdm - accuracy: 0.9613, batch_loss: 0.0264, loss: 0.0937 ||:  71%|#######   | 46/65 [08:44<03:36, 11.39s/it]
2022-04-09 19:02:06,108 - INFO - tqdm - accuracy: 0.9621, batch_loss: 0.0461, loss: 0.0927 ||:  72%|#######2  | 47/65 [08:56<03:25, 11.41s/it]
2022-04-09 19:02:17,567 - INFO - tqdm - accuracy: 0.9616, batch_loss: 0.2164, loss: 0.0953 ||:  74%|#######3  | 48/65 [09:07<03:14, 11.42s/it]
2022-04-09 19:02:29,052 - INFO - tqdm - accuracy: 0.9623, batch_loss: 0.0121, loss: 0.0936 ||:  75%|#######5  | 49/65 [09:18<03:03, 11.44s/it]
2022-04-09 19:02:40,508 - INFO - tqdm - accuracy: 0.9631, batch_loss: 0.0273, loss: 0.0922 ||:  77%|#######6  | 50/65 [09:30<02:51, 11.45s/it]
2022-04-09 19:02:51,970 - INFO - tqdm - accuracy: 0.9638, batch_loss: 0.0151, loss: 0.0907 ||:  78%|#######8  | 51/65 [09:41<02:40, 11.45s/it]
2022-04-09 19:03:03,280 - INFO - tqdm - accuracy: 0.9645, batch_loss: 0.0202, loss: 0.0894 ||:  80%|########  | 52/65 [09:53<02:28, 11.41s/it]
2022-04-09 19:03:14,697 - INFO - tqdm - accuracy: 0.9646, batch_loss: 0.0945, loss: 0.0895 ||:  82%|########1 | 53/65 [10:04<02:16, 11.41s/it]
2022-04-09 19:03:26,097 - INFO - tqdm - accuracy: 0.9653, batch_loss: 0.0314, loss: 0.0884 ||:  83%|########3 | 54/65 [10:16<02:05, 11.41s/it]
2022-04-09 19:03:37,505 - INFO - tqdm - accuracy: 0.9659, batch_loss: 0.0128, loss: 0.0870 ||:  85%|########4 | 55/65 [10:27<01:54, 11.41s/it]
2022-04-09 19:03:48,776 - INFO - tqdm - accuracy: 0.9643, batch_loss: 0.2736, loss: 0.0903 ||:  86%|########6 | 56/65 [10:38<01:42, 11.37s/it]
2022-04-09 19:04:00,149 - INFO - tqdm - accuracy: 0.9638, batch_loss: 0.0796, loss: 0.0902 ||:  88%|########7 | 57/65 [10:50<01:30, 11.37s/it]
2022-04-09 19:04:11,512 - INFO - tqdm - accuracy: 0.9639, batch_loss: 0.0484, loss: 0.0894 ||:  89%|########9 | 58/65 [11:01<01:19, 11.37s/it]
2022-04-09 19:04:22,909 - INFO - tqdm - accuracy: 0.9645, batch_loss: 0.0115, loss: 0.0881 ||:  91%|######### | 59/65 [11:12<01:08, 11.38s/it]
2022-04-09 19:04:34,320 - INFO - tqdm - accuracy: 0.9651, batch_loss: 0.0159, loss: 0.0869 ||:  92%|#########2| 60/65 [11:24<00:56, 11.39s/it]
2022-04-09 19:04:45,726 - INFO - tqdm - accuracy: 0.9651, batch_loss: 0.0579, loss: 0.0864 ||:  94%|#########3| 61/65 [11:35<00:45, 11.39s/it]
2022-04-09 19:04:57,145 - INFO - tqdm - accuracy: 0.9647, batch_loss: 0.1165, loss: 0.0869 ||:  95%|#########5| 62/65 [11:47<00:34, 11.40s/it]
2022-04-09 19:05:08,593 - INFO - tqdm - accuracy: 0.9643, batch_loss: 0.1516, loss: 0.0879 ||:  97%|#########6| 63/65 [11:58<00:22, 11.41s/it]
2022-04-09 19:05:20,044 - INFO - tqdm - accuracy: 0.9648, batch_loss: 0.0351, loss: 0.0871 ||:  98%|#########8| 64/65 [12:09<00:11, 11.43s/it]
2022-04-09 19:05:25,140 - INFO - tqdm - accuracy: 0.9646, batch_loss: 0.1356, loss: 0.0879 ||: 100%|##########| 65/65 [12:15<00:00,  9.53s/it]
2022-04-09 19:05:25,141 - INFO - tqdm - accuracy: 0.9646, batch_loss: 0.1356, loss: 0.0879 ||: 100%|##########| 65/65 [12:15<00:00, 11.31s/it]
2022-04-09 19:05:27,988 - INFO - allennlp.training.trainer - Validating
2022-04-09 19:05:27,990 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-09 19:05:38,163 - INFO - tqdm - accuracy: 0.5802, batch_loss: 5.7796, loss: 1.7666 ||:  36%|###5      | 41/115 [00:10<00:18,  3.97it/s]
2022-04-09 19:05:48,210 - INFO - tqdm - accuracy: 0.6087, batch_loss: 0.1494, loss: 1.6146 ||:  70%|#######   | 81/115 [00:20<00:07,  4.37it/s]
2022-04-09 19:05:56,840 - INFO - tqdm - accuracy: 0.6157, batch_loss: 1.1700, loss: 1.5688 ||: 100%|##########| 115/115 [00:28<00:00,  3.94it/s]
2022-04-09 19:05:56,840 - INFO - tqdm - accuracy: 0.6157, batch_loss: 1.1700, loss: 1.5688 ||: 100%|##########| 115/115 [00:28<00:00,  3.99it/s]
2022-04-09 19:05:56,840 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-09 19:05:56,841 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.965  |     0.616
2022-04-09 19:05:56,842 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-09 19:05:56,842 - INFO - allennlp.training.tensorboard_writer - loss               |     0.088  |     1.569
2022-04-09 19:05:56,843 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5806.574  |       N/A
2022-04-09 19:06:03,123 - INFO - allennlp.training.trainer - Epoch duration: 0:12:53.057691
2022-04-09 19:06:03,123 - INFO - allennlp.training.trainer - Estimated training time remaining: 0:38:49
2022-04-09 19:06:03,123 - INFO - allennlp.training.trainer - Epoch 7/9
2022-04-09 19:06:03,124 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-09 19:06:03,124 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-09 19:06:03,125 - INFO - allennlp.training.trainer - Training
2022-04-09 19:06:03,125 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-09 19:06:14,370 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0428, loss: 0.0428 ||:   2%|1         | 1/65 [00:11<11:59, 11.24s/it]
2022-04-09 19:06:25,874 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0805, loss: 0.0616 ||:   3%|3         | 2/65 [00:22<11:58, 11.40s/it]
2022-04-09 19:06:37,542 - INFO - tqdm - accuracy: 0.9792, batch_loss: 0.1188, loss: 0.0807 ||:   5%|4         | 3/65 [00:34<11:54, 11.52s/it]
2022-04-09 19:06:48,986 - INFO - tqdm - accuracy: 0.9766, batch_loss: 0.0804, loss: 0.0806 ||:   6%|6         | 4/65 [00:45<11:40, 11.49s/it]
2022-04-09 19:07:00,362 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.1339, loss: 0.0913 ||:   8%|7         | 5/65 [00:57<11:26, 11.45s/it]
2022-04-09 19:07:11,744 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.0296, loss: 0.0810 ||:   9%|9         | 6/65 [01:08<11:14, 11.43s/it]
2022-04-09 19:07:23,152 - INFO - tqdm - accuracy: 0.9732, batch_loss: 0.0227, loss: 0.0727 ||:  11%|#         | 7/65 [01:20<11:02, 11.42s/it]
2022-04-09 19:07:34,598 - INFO - tqdm - accuracy: 0.9766, batch_loss: 0.0183, loss: 0.0659 ||:  12%|#2        | 8/65 [01:31<10:51, 11.43s/it]
2022-04-09 19:07:46,079 - INFO - tqdm - accuracy: 0.9757, batch_loss: 0.0485, loss: 0.0639 ||:  14%|#3        | 9/65 [01:42<10:40, 11.44s/it]
2022-04-09 19:07:57,544 - INFO - tqdm - accuracy: 0.9781, batch_loss: 0.0571, loss: 0.0633 ||:  15%|#5        | 10/65 [01:54<10:29, 11.45s/it]
2022-04-09 19:08:08,999 - INFO - tqdm - accuracy: 0.9773, batch_loss: 0.0443, loss: 0.0615 ||:  17%|#6        | 11/65 [02:05<10:18, 11.45s/it]
2022-04-09 19:08:20,439 - INFO - tqdm - accuracy: 0.9766, batch_loss: 0.0577, loss: 0.0612 ||:  18%|#8        | 12/65 [02:17<10:06, 11.45s/it]
2022-04-09 19:08:31,882 - INFO - tqdm - accuracy: 0.9760, batch_loss: 0.0619, loss: 0.0613 ||:  20%|##        | 13/65 [02:28<09:55, 11.45s/it]
2022-04-09 19:08:43,322 - INFO - tqdm - accuracy: 0.9754, batch_loss: 0.0301, loss: 0.0590 ||:  22%|##1       | 14/65 [02:40<09:43, 11.44s/it]
2022-04-09 19:08:54,608 - INFO - tqdm - accuracy: 0.9750, batch_loss: 0.0732, loss: 0.0600 ||:  23%|##3       | 15/65 [02:51<09:29, 11.40s/it]
2022-04-09 19:09:06,024 - INFO - tqdm - accuracy: 0.9766, batch_loss: 0.0254, loss: 0.0578 ||:  25%|##4       | 16/65 [03:02<09:18, 11.40s/it]
2022-04-09 19:09:17,297 - INFO - tqdm - accuracy: 0.9779, batch_loss: 0.0249, loss: 0.0559 ||:  26%|##6       | 17/65 [03:14<09:05, 11.36s/it]
2022-04-09 19:09:28,700 - INFO - tqdm - accuracy: 0.9792, batch_loss: 0.0134, loss: 0.0535 ||:  28%|##7       | 18/65 [03:25<08:54, 11.38s/it]
2022-04-09 19:09:40,077 - INFO - tqdm - accuracy: 0.9803, batch_loss: 0.0243, loss: 0.0520 ||:  29%|##9       | 19/65 [03:36<08:43, 11.38s/it]
2022-04-09 19:09:51,329 - INFO - tqdm - accuracy: 0.9797, batch_loss: 0.0605, loss: 0.0524 ||:  31%|###       | 20/65 [03:48<08:30, 11.34s/it]
2022-04-09 19:10:02,714 - INFO - tqdm - accuracy: 0.9792, batch_loss: 0.0515, loss: 0.0524 ||:  32%|###2      | 21/65 [03:59<08:19, 11.35s/it]
2022-04-09 19:10:14,091 - INFO - tqdm - accuracy: 0.9787, batch_loss: 0.0768, loss: 0.0535 ||:  34%|###3      | 22/65 [04:10<08:08, 11.36s/it]
2022-04-09 19:10:25,481 - INFO - tqdm - accuracy: 0.9796, batch_loss: 0.0478, loss: 0.0532 ||:  35%|###5      | 23/65 [04:22<07:57, 11.37s/it]
2022-04-09 19:10:36,883 - INFO - tqdm - accuracy: 0.9805, batch_loss: 0.0143, loss: 0.0516 ||:  37%|###6      | 24/65 [04:33<07:46, 11.38s/it]
2022-04-09 19:10:48,329 - INFO - tqdm - accuracy: 0.9812, batch_loss: 0.0150, loss: 0.0501 ||:  38%|###8      | 25/65 [04:45<07:35, 11.40s/it]
2022-04-09 19:10:59,767 - INFO - tqdm - accuracy: 0.9820, batch_loss: 0.0248, loss: 0.0492 ||:  40%|####      | 26/65 [04:56<07:25, 11.41s/it]
2022-04-09 19:11:11,203 - INFO - tqdm - accuracy: 0.9826, batch_loss: 0.0250, loss: 0.0483 ||:  42%|####1     | 27/65 [05:08<07:13, 11.42s/it]
2022-04-09 19:11:22,634 - INFO - tqdm - accuracy: 0.9833, batch_loss: 0.0044, loss: 0.0467 ||:  43%|####3     | 28/65 [05:19<07:02, 11.42s/it]
2022-04-09 19:11:34,091 - INFO - tqdm - accuracy: 0.9828, batch_loss: 0.1358, loss: 0.0498 ||:  45%|####4     | 29/65 [05:30<06:51, 11.43s/it]
2022-04-09 19:11:45,550 - INFO - tqdm - accuracy: 0.9823, batch_loss: 0.1441, loss: 0.0529 ||:  46%|####6     | 30/65 [05:42<06:40, 11.44s/it]
2022-04-09 19:11:57,001 - INFO - tqdm - accuracy: 0.9819, batch_loss: 0.0983, loss: 0.0544 ||:  48%|####7     | 31/65 [05:53<06:29, 11.44s/it]
2022-04-09 19:12:08,458 - INFO - tqdm - accuracy: 0.9814, batch_loss: 0.0624, loss: 0.0546 ||:  49%|####9     | 32/65 [06:05<06:17, 11.45s/it]
2022-04-09 19:12:19,934 - INFO - tqdm - accuracy: 0.9820, batch_loss: 0.0118, loss: 0.0533 ||:  51%|#####     | 33/65 [06:16<06:06, 11.46s/it]
2022-04-09 19:12:31,384 - INFO - tqdm - accuracy: 0.9807, batch_loss: 0.1370, loss: 0.0558 ||:  52%|#####2    | 34/65 [06:28<05:55, 11.45s/it]
2022-04-09 19:12:42,796 - INFO - tqdm - accuracy: 0.9804, batch_loss: 0.1211, loss: 0.0577 ||:  54%|#####3    | 35/65 [06:39<05:43, 11.44s/it]
2022-04-09 19:12:54,198 - INFO - tqdm - accuracy: 0.9792, batch_loss: 0.0745, loss: 0.0581 ||:  55%|#####5    | 36/65 [06:51<05:31, 11.43s/it]
2022-04-09 19:13:05,615 - INFO - tqdm - accuracy: 0.9789, batch_loss: 0.1561, loss: 0.0608 ||:  57%|#####6    | 37/65 [07:02<05:19, 11.43s/it]
2022-04-09 19:13:17,019 - INFO - tqdm - accuracy: 0.9794, batch_loss: 0.0067, loss: 0.0594 ||:  58%|#####8    | 38/65 [07:13<05:08, 11.42s/it]
2022-04-09 19:13:28,402 - INFO - tqdm - accuracy: 0.9800, batch_loss: 0.0254, loss: 0.0585 ||:  60%|######    | 39/65 [07:25<04:56, 11.41s/it]
2022-04-09 19:13:39,787 - INFO - tqdm - accuracy: 0.9797, batch_loss: 0.0318, loss: 0.0578 ||:  62%|######1   | 40/65 [07:36<04:45, 11.40s/it]
2022-04-09 19:13:51,175 - INFO - tqdm - accuracy: 0.9794, batch_loss: 0.0557, loss: 0.0578 ||:  63%|######3   | 41/65 [07:48<04:33, 11.40s/it]
2022-04-09 19:14:02,574 - INFO - tqdm - accuracy: 0.9799, batch_loss: 0.0135, loss: 0.0567 ||:  65%|######4   | 42/65 [07:59<04:22, 11.40s/it]
2022-04-09 19:14:13,998 - INFO - tqdm - accuracy: 0.9797, batch_loss: 0.0467, loss: 0.0565 ||:  66%|######6   | 43/65 [08:10<04:10, 11.41s/it]
2022-04-09 19:14:25,446 - INFO - tqdm - accuracy: 0.9801, batch_loss: 0.0218, loss: 0.0557 ||:  68%|######7   | 44/65 [08:22<03:59, 11.42s/it]
2022-04-09 19:14:37,277 - INFO - tqdm - accuracy: 0.9806, batch_loss: 0.0073, loss: 0.0546 ||:  69%|######9   | 45/65 [08:34<03:50, 11.54s/it]
2022-04-09 19:14:48,693 - INFO - tqdm - accuracy: 0.9810, batch_loss: 0.0191, loss: 0.0538 ||:  71%|#######   | 46/65 [08:45<03:38, 11.50s/it]
2022-04-09 19:15:00,145 - INFO - tqdm - accuracy: 0.9814, batch_loss: 0.0394, loss: 0.0535 ||:  72%|#######2  | 47/65 [08:57<03:26, 11.49s/it]
2022-04-09 19:15:11,588 - INFO - tqdm - accuracy: 0.9811, batch_loss: 0.1069, loss: 0.0546 ||:  74%|#######3  | 48/65 [09:08<03:15, 11.47s/it]
2022-04-09 19:15:23,068 - INFO - tqdm - accuracy: 0.9815, batch_loss: 0.0194, loss: 0.0539 ||:  75%|#######5  | 49/65 [09:19<03:03, 11.48s/it]
2022-04-09 19:15:34,520 - INFO - tqdm - accuracy: 0.9819, batch_loss: 0.0285, loss: 0.0534 ||:  77%|#######6  | 50/65 [09:31<02:52, 11.47s/it]
2022-04-09 19:15:45,979 - INFO - tqdm - accuracy: 0.9822, batch_loss: 0.0140, loss: 0.0526 ||:  78%|#######8  | 51/65 [09:42<02:40, 11.47s/it]
2022-04-09 19:15:57,435 - INFO - tqdm - accuracy: 0.9820, batch_loss: 0.0725, loss: 0.0530 ||:  80%|########  | 52/65 [09:54<02:29, 11.46s/it]
2022-04-09 19:16:08,887 - INFO - tqdm - accuracy: 0.9817, batch_loss: 0.1107, loss: 0.0541 ||:  82%|########1 | 53/65 [10:05<02:17, 11.46s/it]
2022-04-09 19:16:20,322 - INFO - tqdm - accuracy: 0.9821, batch_loss: 0.0103, loss: 0.0533 ||:  83%|########3 | 54/65 [10:17<02:05, 11.45s/it]
2022-04-09 19:16:31,752 - INFO - tqdm - accuracy: 0.9824, batch_loss: 0.0126, loss: 0.0526 ||:  85%|########4 | 55/65 [10:28<01:54, 11.45s/it]
2022-04-09 19:16:43,170 - INFO - tqdm - accuracy: 0.9827, batch_loss: 0.0370, loss: 0.0523 ||:  86%|########6 | 56/65 [10:40<01:42, 11.44s/it]
2022-04-09 19:16:54,574 - INFO - tqdm - accuracy: 0.9830, batch_loss: 0.0044, loss: 0.0514 ||:  88%|########7 | 57/65 [10:51<01:31, 11.43s/it]
2022-04-09 19:17:05,994 - INFO - tqdm - accuracy: 0.9828, batch_loss: 0.1581, loss: 0.0533 ||:  89%|########9 | 58/65 [11:02<01:19, 11.42s/it]
2022-04-09 19:17:16,796 - INFO - tqdm - accuracy: 0.9825, batch_loss: 0.0510, loss: 0.0532 ||:  91%|######### | 59/65 [11:13<01:07, 11.24s/it]
2022-04-09 19:17:28,187 - INFO - tqdm - accuracy: 0.9828, batch_loss: 0.0219, loss: 0.0527 ||:  92%|#########2| 60/65 [11:25<00:56, 11.28s/it]
2022-04-09 19:17:39,602 - INFO - tqdm - accuracy: 0.9831, batch_loss: 0.0140, loss: 0.0521 ||:  94%|#########3| 61/65 [11:36<00:45, 11.32s/it]
2022-04-09 19:17:50,989 - INFO - tqdm - accuracy: 0.9834, batch_loss: 0.0195, loss: 0.0516 ||:  95%|#########5| 62/65 [11:47<00:34, 11.34s/it]
2022-04-09 19:18:02,397 - INFO - tqdm - accuracy: 0.9836, batch_loss: 0.0191, loss: 0.0511 ||:  97%|#########6| 63/65 [11:59<00:22, 11.36s/it]
2022-04-09 19:18:13,791 - INFO - tqdm - accuracy: 0.9834, batch_loss: 0.0994, loss: 0.0518 ||:  98%|#########8| 64/65 [12:10<00:11, 11.37s/it]
2022-04-09 19:18:18,853 - INFO - tqdm - accuracy: 0.9835, batch_loss: 0.0052, loss: 0.0511 ||: 100%|##########| 65/65 [12:15<00:00,  9.48s/it]
2022-04-09 19:18:18,854 - INFO - tqdm - accuracy: 0.9835, batch_loss: 0.0052, loss: 0.0511 ||: 100%|##########| 65/65 [12:15<00:00, 11.32s/it]
2022-04-09 19:18:21,722 - INFO - allennlp.training.trainer - Validating
2022-04-09 19:18:21,723 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-09 19:18:31,895 - INFO - tqdm - accuracy: 0.7073, batch_loss: 3.8836, loss: 1.4782 ||:  36%|###5      | 41/115 [00:10<00:18,  4.02it/s]
2022-04-09 19:18:42,142 - INFO - tqdm - accuracy: 0.6564, batch_loss: 0.0026, loss: 1.6752 ||:  71%|#######1  | 82/115 [00:20<00:08,  3.93it/s]
2022-04-09 19:18:50,595 - INFO - tqdm - accuracy: 0.6507, batch_loss: 3.5182, loss: 1.7047 ||: 100%|##########| 115/115 [00:28<00:00,  3.91it/s]
2022-04-09 19:18:50,595 - INFO - tqdm - accuracy: 0.6507, batch_loss: 3.5182, loss: 1.7047 ||: 100%|##########| 115/115 [00:28<00:00,  3.98it/s]
2022-04-09 19:18:50,595 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-09 19:18:50,596 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.984  |     0.651
2022-04-09 19:18:50,596 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-09 19:18:50,596 - INFO - allennlp.training.tensorboard_writer - loss               |     0.051  |     1.705
2022-04-09 19:18:50,597 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5806.574  |       N/A
2022-04-09 19:18:56,610 - INFO - allennlp.training.trainer - Epoch duration: 0:12:53.486491
2022-04-09 19:18:56,610 - INFO - allennlp.training.trainer - Estimated training time remaining: 0:25:52
2022-04-09 19:18:56,610 - INFO - allennlp.training.trainer - Epoch 8/9
2022-04-09 19:18:56,610 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-09 19:18:56,611 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-09 19:18:56,612 - INFO - allennlp.training.trainer - Training
2022-04-09 19:18:56,612 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-09 19:19:07,845 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.1442, loss: 0.1442 ||:   2%|1         | 1/65 [00:11<11:58, 11.23s/it]
2022-04-09 19:19:19,305 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0068, loss: 0.0755 ||:   3%|3         | 2/65 [00:22<11:56, 11.37s/it]
2022-04-09 19:19:30,952 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.0070, loss: 0.0527 ||:   5%|4         | 3/65 [00:34<11:52, 11.49s/it]
2022-04-09 19:19:42,412 - INFO - tqdm - accuracy: 0.9766, batch_loss: 0.1398, loss: 0.0744 ||:   6%|6         | 4/65 [00:45<11:40, 11.48s/it]
2022-04-09 19:19:53,780 - INFO - tqdm - accuracy: 0.9812, batch_loss: 0.0195, loss: 0.0635 ||:   8%|7         | 5/65 [00:57<11:26, 11.44s/it]
2022-04-09 19:20:05,159 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0053, loss: 0.0538 ||:   9%|9         | 6/65 [01:08<11:13, 11.42s/it]
2022-04-09 19:20:16,536 - INFO - tqdm - accuracy: 0.9821, batch_loss: 0.0832, loss: 0.0580 ||:  11%|#         | 7/65 [01:19<11:01, 11.41s/it]
2022-04-09 19:20:27,988 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0053, loss: 0.0514 ||:  12%|#2        | 8/65 [01:31<10:50, 11.42s/it]
2022-04-09 19:20:39,468 - INFO - tqdm - accuracy: 0.9792, batch_loss: 0.1799, loss: 0.0657 ||:  14%|#3        | 9/65 [01:42<10:40, 11.44s/it]
2022-04-09 19:20:50,877 - INFO - tqdm - accuracy: 0.9812, batch_loss: 0.0144, loss: 0.0605 ||:  15%|#5        | 10/65 [01:54<10:28, 11.43s/it]
2022-04-09 19:21:02,228 - INFO - tqdm - accuracy: 0.9801, batch_loss: 0.0575, loss: 0.0603 ||:  17%|#6        | 11/65 [02:05<10:15, 11.41s/it]
2022-04-09 19:21:13,640 - INFO - tqdm - accuracy: 0.9792, batch_loss: 0.0473, loss: 0.0592 ||:  18%|#8        | 12/65 [02:17<10:04, 11.41s/it]
2022-04-09 19:21:25,104 - INFO - tqdm - accuracy: 0.9808, batch_loss: 0.0213, loss: 0.0563 ||:  20%|##        | 13/65 [02:28<09:54, 11.42s/it]
2022-04-09 19:21:36,568 - INFO - tqdm - accuracy: 0.9821, batch_loss: 0.0248, loss: 0.0540 ||:  22%|##1       | 14/65 [02:39<09:43, 11.44s/it]
2022-04-09 19:21:48,009 - INFO - tqdm - accuracy: 0.9833, batch_loss: 0.0147, loss: 0.0514 ||:  23%|##3       | 15/65 [02:51<09:31, 11.44s/it]
2022-04-09 19:21:59,405 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0215, loss: 0.0495 ||:  25%|##4       | 16/65 [03:02<09:19, 11.43s/it]
2022-04-09 19:22:10,764 - INFO - tqdm - accuracy: 0.9853, batch_loss: 0.0096, loss: 0.0472 ||:  26%|##6       | 17/65 [03:14<09:07, 11.41s/it]
2022-04-09 19:22:22,184 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.1408, loss: 0.0524 ||:  28%|##7       | 18/65 [03:25<08:56, 11.41s/it]
2022-04-09 19:22:33,617 - INFO - tqdm - accuracy: 0.9836, batch_loss: 0.0519, loss: 0.0524 ||:  29%|##9       | 19/65 [03:37<08:45, 11.42s/it]
2022-04-09 19:22:45,097 - INFO - tqdm - accuracy: 0.9797, batch_loss: 0.1832, loss: 0.0589 ||:  31%|###       | 20/65 [03:48<08:34, 11.44s/it]
2022-04-09 19:22:56,532 - INFO - tqdm - accuracy: 0.9792, batch_loss: 0.0921, loss: 0.0605 ||:  32%|###2      | 21/65 [03:59<08:23, 11.44s/it]
2022-04-09 19:23:07,942 - INFO - tqdm - accuracy: 0.9801, batch_loss: 0.0147, loss: 0.0584 ||:  34%|###3      | 22/65 [04:11<08:11, 11.43s/it]
2022-04-09 19:23:19,347 - INFO - tqdm - accuracy: 0.9810, batch_loss: 0.0112, loss: 0.0564 ||:  35%|###5      | 23/65 [04:22<07:59, 11.42s/it]
2022-04-09 19:23:30,716 - INFO - tqdm - accuracy: 0.9805, batch_loss: 0.0479, loss: 0.0560 ||:  37%|###6      | 24/65 [04:34<07:47, 11.41s/it]
2022-04-09 19:23:42,110 - INFO - tqdm - accuracy: 0.9812, batch_loss: 0.0345, loss: 0.0551 ||:  38%|###8      | 25/65 [04:45<07:36, 11.40s/it]
2022-04-09 19:23:53,499 - INFO - tqdm - accuracy: 0.9820, batch_loss: 0.0089, loss: 0.0534 ||:  40%|####      | 26/65 [04:56<07:24, 11.40s/it]
2022-04-09 19:24:04,624 - INFO - tqdm - accuracy: 0.9815, batch_loss: 0.0335, loss: 0.0526 ||:  42%|####1     | 27/65 [05:08<07:10, 11.32s/it]
2022-04-09 19:24:16,016 - INFO - tqdm - accuracy: 0.9821, batch_loss: 0.0073, loss: 0.0510 ||:  43%|####3     | 28/65 [05:19<06:59, 11.34s/it]
2022-04-09 19:24:27,403 - INFO - tqdm - accuracy: 0.9828, batch_loss: 0.0052, loss: 0.0494 ||:  45%|####4     | 29/65 [05:30<06:48, 11.35s/it]
2022-04-09 19:24:38,778 - INFO - tqdm - accuracy: 0.9833, batch_loss: 0.0151, loss: 0.0483 ||:  46%|####6     | 30/65 [05:42<06:37, 11.36s/it]
2022-04-09 19:24:50,161 - INFO - tqdm - accuracy: 0.9839, batch_loss: 0.0066, loss: 0.0469 ||:  48%|####7     | 31/65 [05:53<06:26, 11.37s/it]
2022-04-09 19:25:01,570 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0092, loss: 0.0458 ||:  49%|####9     | 32/65 [06:04<06:15, 11.38s/it]
2022-04-09 19:25:12,949 - INFO - tqdm - accuracy: 0.9848, batch_loss: 0.0269, loss: 0.0452 ||:  51%|#####     | 33/65 [06:16<06:04, 11.38s/it]
2022-04-09 19:25:24,316 - INFO - tqdm - accuracy: 0.9853, batch_loss: 0.0593, loss: 0.0456 ||:  52%|#####2    | 34/65 [06:27<05:52, 11.38s/it]
2022-04-09 19:25:35,721 - INFO - tqdm - accuracy: 0.9857, batch_loss: 0.0079, loss: 0.0445 ||:  54%|#####3    | 35/65 [06:39<05:41, 11.38s/it]
2022-04-09 19:25:47,133 - INFO - tqdm - accuracy: 0.9861, batch_loss: 0.0078, loss: 0.0435 ||:  55%|#####5    | 36/65 [06:50<05:30, 11.39s/it]
2022-04-09 19:25:58,548 - INFO - tqdm - accuracy: 0.9856, batch_loss: 0.0599, loss: 0.0440 ||:  57%|#####6    | 37/65 [07:01<05:19, 11.40s/it]
2022-04-09 19:26:09,987 - INFO - tqdm - accuracy: 0.9860, batch_loss: 0.0046, loss: 0.0429 ||:  58%|#####8    | 38/65 [07:13<05:08, 11.41s/it]
2022-04-09 19:26:21,170 - INFO - tqdm - accuracy: 0.9848, batch_loss: 0.0888, loss: 0.0441 ||:  60%|######    | 39/65 [07:24<04:54, 11.34s/it]
2022-04-09 19:26:32,618 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0791, loss: 0.0450 ||:  62%|######1   | 40/65 [07:36<04:44, 11.37s/it]
2022-04-09 19:26:44,064 - INFO - tqdm - accuracy: 0.9840, batch_loss: 0.0740, loss: 0.0457 ||:  63%|######3   | 41/65 [07:47<04:33, 11.40s/it]
2022-04-09 19:26:55,527 - INFO - tqdm - accuracy: 0.9829, batch_loss: 0.1295, loss: 0.0477 ||:  65%|######4   | 42/65 [07:58<04:22, 11.42s/it]
2022-04-09 19:27:06,990 - INFO - tqdm - accuracy: 0.9833, batch_loss: 0.0298, loss: 0.0473 ||:  66%|######6   | 43/65 [08:10<04:11, 11.43s/it]
2022-04-09 19:27:18,447 - INFO - tqdm - accuracy: 0.9830, batch_loss: 0.0799, loss: 0.0480 ||:  68%|######7   | 44/65 [08:21<04:00, 11.44s/it]
2022-04-09 19:27:29,911 - INFO - tqdm - accuracy: 0.9833, batch_loss: 0.0067, loss: 0.0471 ||:  69%|######9   | 45/65 [08:33<03:48, 11.45s/it]
2022-04-09 19:27:41,352 - INFO - tqdm - accuracy: 0.9837, batch_loss: 0.0053, loss: 0.0462 ||:  71%|#######   | 46/65 [08:44<03:37, 11.44s/it]
2022-04-09 19:27:52,805 - INFO - tqdm - accuracy: 0.9840, batch_loss: 0.0043, loss: 0.0453 ||:  72%|#######2  | 47/65 [08:56<03:26, 11.45s/it]
2022-04-09 19:28:04,235 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0208, loss: 0.0448 ||:  74%|#######3  | 48/65 [09:07<03:14, 11.44s/it]
2022-04-09 19:28:15,645 - INFO - tqdm - accuracy: 0.9847, batch_loss: 0.0108, loss: 0.0441 ||:  75%|#######5  | 49/65 [09:19<03:02, 11.43s/it]
2022-04-09 19:28:27,071 - INFO - tqdm - accuracy: 0.9850, batch_loss: 0.0104, loss: 0.0434 ||:  77%|#######6  | 50/65 [09:30<02:51, 11.43s/it]
2022-04-09 19:28:38,477 - INFO - tqdm - accuracy: 0.9853, batch_loss: 0.0075, loss: 0.0427 ||:  78%|#######8  | 51/65 [09:41<02:39, 11.42s/it]
2022-04-09 19:28:49,868 - INFO - tqdm - accuracy: 0.9850, batch_loss: 0.0416, loss: 0.0427 ||:  80%|########  | 52/65 [09:53<02:28, 11.41s/it]
2022-04-09 19:29:01,247 - INFO - tqdm - accuracy: 0.9853, batch_loss: 0.0082, loss: 0.0420 ||:  82%|########1 | 53/65 [10:04<02:16, 11.40s/it]
2022-04-09 19:29:12,651 - INFO - tqdm - accuracy: 0.9855, batch_loss: 0.0035, loss: 0.0413 ||:  83%|########3 | 54/65 [10:16<02:05, 11.40s/it]
2022-04-09 19:29:24,034 - INFO - tqdm - accuracy: 0.9858, batch_loss: 0.0085, loss: 0.0407 ||:  85%|########4 | 55/65 [10:27<01:53, 11.40s/it]
2022-04-09 19:29:35,432 - INFO - tqdm - accuracy: 0.9855, batch_loss: 0.0677, loss: 0.0412 ||:  86%|########6 | 56/65 [10:38<01:42, 11.40s/it]
2022-04-09 19:29:46,817 - INFO - tqdm - accuracy: 0.9857, batch_loss: 0.0104, loss: 0.0407 ||:  88%|########7 | 57/65 [10:50<01:31, 11.39s/it]
2022-04-09 19:29:58,210 - INFO - tqdm - accuracy: 0.9860, batch_loss: 0.0095, loss: 0.0401 ||:  89%|########9 | 58/65 [11:01<01:19, 11.39s/it]
2022-04-09 19:30:09,588 - INFO - tqdm - accuracy: 0.9857, batch_loss: 0.0324, loss: 0.0400 ||:  91%|######### | 59/65 [11:12<01:08, 11.39s/it]
2022-04-09 19:30:20,648 - INFO - tqdm - accuracy: 0.9859, batch_loss: 0.0088, loss: 0.0395 ||:  92%|#########2| 60/65 [11:24<00:56, 11.29s/it]
2022-04-09 19:30:32,021 - INFO - tqdm - accuracy: 0.9856, batch_loss: 0.0724, loss: 0.0400 ||:  94%|#########3| 61/65 [11:35<00:45, 11.31s/it]
2022-04-09 19:30:43,393 - INFO - tqdm - accuracy: 0.9849, batch_loss: 0.1963, loss: 0.0425 ||:  95%|#########5| 62/65 [11:46<00:33, 11.33s/it]
2022-04-09 19:30:54,762 - INFO - tqdm - accuracy: 0.9846, batch_loss: 0.0737, loss: 0.0430 ||:  97%|#########6| 63/65 [11:58<00:22, 11.34s/it]
2022-04-09 19:31:06,137 - INFO - tqdm - accuracy: 0.9849, batch_loss: 0.0340, loss: 0.0429 ||:  98%|#########8| 64/65 [12:09<00:11, 11.35s/it]
2022-04-09 19:31:11,056 - INFO - tqdm - accuracy: 0.9850, batch_loss: 0.0026, loss: 0.0423 ||: 100%|##########| 65/65 [12:14<00:00,  9.42s/it]
2022-04-09 19:31:11,056 - INFO - tqdm - accuracy: 0.9850, batch_loss: 0.0026, loss: 0.0423 ||: 100%|##########| 65/65 [12:14<00:00, 11.30s/it]
2022-04-09 19:31:13,928 - INFO - allennlp.training.trainer - Validating
2022-04-09 19:31:13,930 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-09 19:31:24,116 - INFO - tqdm - accuracy: 0.6463, batch_loss: 3.4073, loss: 1.8233 ||:  36%|###5      | 41/115 [00:10<00:18,  4.01it/s]
2022-04-09 19:31:34,216 - INFO - tqdm - accuracy: 0.6605, batch_loss: 0.0003, loss: 1.7647 ||:  70%|#######   | 81/115 [00:20<00:08,  3.94it/s]
2022-04-09 19:31:42,720 - INFO - tqdm - accuracy: 0.6594, batch_loss: 0.0019, loss: 1.7203 ||: 100%|##########| 115/115 [00:28<00:00,  3.97it/s]
2022-04-09 19:31:42,720 - INFO - tqdm - accuracy: 0.6594, batch_loss: 0.0019, loss: 1.7203 ||: 100%|##########| 115/115 [00:28<00:00,  3.99it/s]
2022-04-09 19:31:42,720 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-09 19:31:42,721 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.985  |     0.659
2022-04-09 19:31:42,722 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-09 19:31:42,722 - INFO - allennlp.training.tensorboard_writer - loss               |     0.042  |     1.720
2022-04-09 19:31:42,723 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5806.574  |       N/A
2022-04-09 19:31:48,721 - INFO - allennlp.training.trainer - Epoch duration: 0:12:52.110372
2022-04-09 19:31:48,721 - INFO - allennlp.training.trainer - Estimated training time remaining: 0:12:55
2022-04-09 19:31:48,721 - INFO - allennlp.training.trainer - Epoch 9/9
2022-04-09 19:31:48,721 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-09 19:31:48,722 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-09 19:31:48,723 - INFO - allennlp.training.trainer - Training
2022-04-09 19:31:48,724 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-09 19:31:59,995 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0347, loss: 0.0347 ||:   2%|1         | 1/65 [00:11<12:01, 11.27s/it]
2022-04-09 19:32:11,427 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0132, loss: 0.0239 ||:   3%|3         | 2/65 [00:22<11:56, 11.37s/it]
2022-04-09 19:32:23,092 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0237, loss: 0.0238 ||:   5%|4         | 3/65 [00:34<11:53, 11.50s/it]
2022-04-09 19:32:34,436 - INFO - tqdm - accuracy: 0.9922, batch_loss: 0.0848, loss: 0.0391 ||:   6%|6         | 4/65 [00:45<11:37, 11.44s/it]
2022-04-09 19:32:45,798 - INFO - tqdm - accuracy: 0.9875, batch_loss: 0.0278, loss: 0.0368 ||:   8%|7         | 5/65 [00:57<11:24, 11.41s/it]
2022-04-09 19:32:57,189 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.0181, loss: 0.0337 ||:   9%|9         | 6/65 [01:08<11:12, 11.40s/it]
2022-04-09 19:33:08,539 - INFO - tqdm - accuracy: 0.9911, batch_loss: 0.0093, loss: 0.0302 ||:  11%|#         | 7/65 [01:19<11:00, 11.39s/it]
2022-04-09 19:33:19,961 - INFO - tqdm - accuracy: 0.9922, batch_loss: 0.0189, loss: 0.0288 ||:  12%|#2        | 8/65 [01:31<10:49, 11.40s/it]
2022-04-09 19:33:31,416 - INFO - tqdm - accuracy: 0.9931, batch_loss: 0.0048, loss: 0.0261 ||:  14%|#3        | 9/65 [01:42<10:39, 11.42s/it]
2022-04-09 19:33:42,876 - INFO - tqdm - accuracy: 0.9938, batch_loss: 0.0035, loss: 0.0239 ||:  15%|#5        | 10/65 [01:54<10:28, 11.43s/it]
2022-04-09 19:33:54,282 - INFO - tqdm - accuracy: 0.9943, batch_loss: 0.0404, loss: 0.0254 ||:  17%|#6        | 11/65 [02:05<10:16, 11.42s/it]
2022-04-09 19:34:05,669 - INFO - tqdm - accuracy: 0.9948, batch_loss: 0.0051, loss: 0.0237 ||:  18%|#8        | 12/65 [02:16<10:04, 11.41s/it]
2022-04-09 19:34:17,042 - INFO - tqdm - accuracy: 0.9952, batch_loss: 0.0113, loss: 0.0227 ||:  20%|##        | 13/65 [02:28<09:52, 11.40s/it]
2022-04-09 19:34:28,451 - INFO - tqdm - accuracy: 0.9933, batch_loss: 0.0507, loss: 0.0247 ||:  22%|##1       | 14/65 [02:39<09:41, 11.40s/it]
2022-04-09 19:34:40,345 - INFO - tqdm - accuracy: 0.9938, batch_loss: 0.0108, loss: 0.0238 ||:  23%|##3       | 15/65 [02:51<09:37, 11.55s/it]
2022-04-09 19:34:51,739 - INFO - tqdm - accuracy: 0.9941, batch_loss: 0.0036, loss: 0.0225 ||:  25%|##4       | 16/65 [03:03<09:23, 11.50s/it]
2022-04-09 19:35:03,203 - INFO - tqdm - accuracy: 0.9945, batch_loss: 0.0053, loss: 0.0215 ||:  26%|##6       | 17/65 [03:14<09:11, 11.49s/it]
2022-04-09 19:35:14,652 - INFO - tqdm - accuracy: 0.9948, batch_loss: 0.0074, loss: 0.0207 ||:  28%|##7       | 18/65 [03:25<08:59, 11.48s/it]
2022-04-09 19:35:26,072 - INFO - tqdm - accuracy: 0.9951, batch_loss: 0.0366, loss: 0.0216 ||:  29%|##9       | 19/65 [03:37<08:47, 11.46s/it]
2022-04-09 19:35:37,462 - INFO - tqdm - accuracy: 0.9922, batch_loss: 0.1770, loss: 0.0293 ||:  31%|###       | 20/65 [03:48<08:34, 11.44s/it]
2022-04-09 19:35:48,828 - INFO - tqdm - accuracy: 0.9926, batch_loss: 0.0036, loss: 0.0281 ||:  32%|###2      | 21/65 [04:00<08:22, 11.42s/it]
2022-04-09 19:36:00,186 - INFO - tqdm - accuracy: 0.9929, batch_loss: 0.0102, loss: 0.0273 ||:  34%|###3      | 22/65 [04:11<08:10, 11.40s/it]
2022-04-09 19:36:11,540 - INFO - tqdm - accuracy: 0.9932, batch_loss: 0.0087, loss: 0.0265 ||:  35%|###5      | 23/65 [04:22<07:58, 11.39s/it]
2022-04-09 19:36:22,924 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.1198, loss: 0.0304 ||:  37%|###6      | 24/65 [04:34<07:46, 11.39s/it]
2022-04-09 19:36:34,312 - INFO - tqdm - accuracy: 0.9900, batch_loss: 0.0099, loss: 0.0296 ||:  38%|###8      | 25/65 [04:45<07:35, 11.39s/it]
2022-04-09 19:36:45,735 - INFO - tqdm - accuracy: 0.9904, batch_loss: 0.0116, loss: 0.0289 ||:  40%|####      | 26/65 [04:57<07:24, 11.40s/it]
2022-04-09 19:36:57,134 - INFO - tqdm - accuracy: 0.9907, batch_loss: 0.0135, loss: 0.0283 ||:  42%|####1     | 27/65 [05:08<07:13, 11.40s/it]
2022-04-09 19:37:08,567 - INFO - tqdm - accuracy: 0.9900, batch_loss: 0.2166, loss: 0.0350 ||:  43%|####3     | 28/65 [05:19<07:02, 11.41s/it]
2022-04-09 19:37:20,011 - INFO - tqdm - accuracy: 0.9903, batch_loss: 0.0087, loss: 0.0341 ||:  45%|####4     | 29/65 [05:31<06:51, 11.42s/it]
2022-04-09 19:37:31,332 - INFO - tqdm - accuracy: 0.9906, batch_loss: 0.0060, loss: 0.0332 ||:  46%|####6     | 30/65 [05:42<06:38, 11.39s/it]
2022-04-09 19:37:42,772 - INFO - tqdm - accuracy: 0.9899, batch_loss: 0.0432, loss: 0.0335 ||:  48%|####7     | 31/65 [05:54<06:27, 11.40s/it]
2022-04-09 19:37:54,221 - INFO - tqdm - accuracy: 0.9902, batch_loss: 0.0073, loss: 0.0327 ||:  49%|####9     | 32/65 [06:05<06:16, 11.42s/it]
2022-04-09 19:38:05,669 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.0532, loss: 0.0333 ||:  51%|#####     | 33/65 [06:16<06:05, 11.43s/it]
2022-04-09 19:38:17,105 - INFO - tqdm - accuracy: 0.9899, batch_loss: 0.0088, loss: 0.0326 ||:  52%|#####2    | 34/65 [06:28<05:54, 11.43s/it]
2022-04-09 19:38:28,185 - INFO - tqdm - accuracy: 0.9902, batch_loss: 0.0041, loss: 0.0318 ||:  54%|#####3    | 35/65 [06:39<05:39, 11.32s/it]
2022-04-09 19:38:39,594 - INFO - tqdm - accuracy: 0.9904, batch_loss: 0.0111, loss: 0.0312 ||:  55%|#####5    | 36/65 [06:50<05:29, 11.35s/it]
2022-04-09 19:38:50,994 - INFO - tqdm - accuracy: 0.9899, batch_loss: 0.0649, loss: 0.0321 ||:  57%|#####6    | 37/65 [07:02<05:18, 11.36s/it]
2022-04-09 19:39:02,386 - INFO - tqdm - accuracy: 0.9901, batch_loss: 0.0045, loss: 0.0314 ||:  58%|#####8    | 38/65 [07:13<05:07, 11.37s/it]
2022-04-09 19:39:13,788 - INFO - tqdm - accuracy: 0.9904, batch_loss: 0.0043, loss: 0.0307 ||:  60%|######    | 39/65 [07:25<04:55, 11.38s/it]
2022-04-09 19:39:25,167 - INFO - tqdm - accuracy: 0.9906, batch_loss: 0.0054, loss: 0.0301 ||:  62%|######1   | 40/65 [07:36<04:44, 11.38s/it]
2022-04-09 19:39:36,570 - INFO - tqdm - accuracy: 0.9908, batch_loss: 0.0202, loss: 0.0298 ||:  63%|######3   | 41/65 [07:47<04:33, 11.39s/it]
2022-04-09 19:39:47,972 - INFO - tqdm - accuracy: 0.9903, batch_loss: 0.0277, loss: 0.0298 ||:  65%|######4   | 42/65 [07:59<04:22, 11.39s/it]
2022-04-09 19:39:59,356 - INFO - tqdm - accuracy: 0.9905, batch_loss: 0.0089, loss: 0.0293 ||:  66%|######6   | 43/65 [08:10<04:10, 11.39s/it]
2022-04-09 19:40:10,750 - INFO - tqdm - accuracy: 0.9908, batch_loss: 0.0084, loss: 0.0288 ||:  68%|######7   | 44/65 [08:22<03:59, 11.39s/it]
2022-04-09 19:40:22,142 - INFO - tqdm - accuracy: 0.9903, batch_loss: 0.0970, loss: 0.0303 ||:  69%|######9   | 45/65 [08:33<03:47, 11.39s/it]
2022-04-09 19:40:33,530 - INFO - tqdm - accuracy: 0.9898, batch_loss: 0.1241, loss: 0.0324 ||:  71%|#######   | 46/65 [08:44<03:36, 11.39s/it]
2022-04-09 19:40:44,909 - INFO - tqdm - accuracy: 0.9894, batch_loss: 0.0322, loss: 0.0324 ||:  72%|#######2  | 47/65 [08:56<03:24, 11.39s/it]
2022-04-09 19:40:56,286 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.0109, loss: 0.0319 ||:  74%|#######3  | 48/65 [09:07<03:13, 11.38s/it]
2022-04-09 19:41:07,705 - INFO - tqdm - accuracy: 0.9898, batch_loss: 0.0108, loss: 0.0315 ||:  75%|#######5  | 49/65 [09:18<03:02, 11.39s/it]
2022-04-09 19:41:18,873 - INFO - tqdm - accuracy: 0.9900, batch_loss: 0.0102, loss: 0.0311 ||:  77%|#######6  | 50/65 [09:30<02:49, 11.33s/it]
2022-04-09 19:41:30,308 - INFO - tqdm - accuracy: 0.9902, batch_loss: 0.0261, loss: 0.0310 ||:  78%|#######8  | 51/65 [09:41<02:39, 11.36s/it]
2022-04-09 19:41:41,744 - INFO - tqdm - accuracy: 0.9904, batch_loss: 0.0058, loss: 0.0305 ||:  80%|########  | 52/65 [09:53<02:27, 11.38s/it]
2022-04-09 19:41:53,185 - INFO - tqdm - accuracy: 0.9900, batch_loss: 0.0607, loss: 0.0310 ||:  82%|########1 | 53/65 [10:04<02:16, 11.40s/it]
2022-04-09 19:42:04,633 - INFO - tqdm - accuracy: 0.9902, batch_loss: 0.0262, loss: 0.0310 ||:  83%|########3 | 54/65 [10:15<02:05, 11.41s/it]
2022-04-09 19:42:16,082 - INFO - tqdm - accuracy: 0.9903, batch_loss: 0.0039, loss: 0.0305 ||:  85%|########4 | 55/65 [10:27<01:54, 11.42s/it]
2022-04-09 19:42:27,563 - INFO - tqdm - accuracy: 0.9899, batch_loss: 0.1007, loss: 0.0317 ||:  86%|########6 | 56/65 [10:38<01:42, 11.44s/it]
2022-04-09 19:42:39,026 - INFO - tqdm - accuracy: 0.9901, batch_loss: 0.0144, loss: 0.0314 ||:  88%|########7 | 57/65 [10:50<01:31, 11.45s/it]
2022-04-09 19:42:50,470 - INFO - tqdm - accuracy: 0.9903, batch_loss: 0.0154, loss: 0.0311 ||:  89%|########9 | 58/65 [11:01<01:20, 11.45s/it]
2022-04-09 19:43:01,928 - INFO - tqdm - accuracy: 0.9905, batch_loss: 0.0110, loss: 0.0308 ||:  91%|######### | 59/65 [11:13<01:08, 11.45s/it]
2022-04-09 19:43:13,362 - INFO - tqdm - accuracy: 0.9906, batch_loss: 0.0039, loss: 0.0303 ||:  92%|#########2| 60/65 [11:24<00:57, 11.45s/it]
2022-04-09 19:43:24,772 - INFO - tqdm - accuracy: 0.9908, batch_loss: 0.0083, loss: 0.0300 ||:  94%|#########3| 61/65 [11:36<00:45, 11.43s/it]
2022-04-09 19:43:36,186 - INFO - tqdm - accuracy: 0.9909, batch_loss: 0.0248, loss: 0.0299 ||:  95%|#########5| 62/65 [11:47<00:34, 11.43s/it]
2022-04-09 19:43:47,626 - INFO - tqdm - accuracy: 0.9906, batch_loss: 0.0244, loss: 0.0298 ||:  97%|#########6| 63/65 [11:58<00:22, 11.43s/it]
2022-04-09 19:43:59,051 - INFO - tqdm - accuracy: 0.9907, batch_loss: 0.0217, loss: 0.0297 ||:  98%|#########8| 64/65 [12:10<00:11, 11.43s/it]
2022-04-09 19:44:04,118 - INFO - tqdm - accuracy: 0.9908, batch_loss: 0.0135, loss: 0.0294 ||: 100%|##########| 65/65 [12:15<00:00,  9.52s/it]
2022-04-09 19:44:04,119 - INFO - tqdm - accuracy: 0.9908, batch_loss: 0.0135, loss: 0.0294 ||: 100%|##########| 65/65 [12:15<00:00, 11.31s/it]
2022-04-09 19:44:06,958 - INFO - allennlp.training.trainer - Validating
2022-04-09 19:44:06,960 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-09 19:44:17,022 - INFO - tqdm - accuracy: 0.6173, batch_loss: 2.7751, loss: 1.9176 ||:  36%|###5      | 41/115 [00:10<00:18,  3.98it/s]
2022-04-09 19:44:27,144 - INFO - tqdm - accuracy: 0.6273, batch_loss: 6.1950, loss: 1.9595 ||:  70%|#######   | 81/115 [00:20<00:08,  3.92it/s]
2022-04-09 19:44:35,783 - INFO - tqdm - accuracy: 0.6463, batch_loss: 0.3389, loss: 1.7658 ||: 100%|##########| 115/115 [00:28<00:00,  3.94it/s]
2022-04-09 19:44:35,783 - INFO - tqdm - accuracy: 0.6463, batch_loss: 0.3389, loss: 1.7658 ||: 100%|##########| 115/115 [00:28<00:00,  3.99it/s]
2022-04-09 19:44:35,784 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-09 19:44:35,784 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.991  |     0.646
2022-04-09 19:44:35,785 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-09 19:44:35,786 - INFO - allennlp.training.tensorboard_writer - loss               |     0.029  |     1.766
2022-04-09 19:44:35,786 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5806.574  |       N/A
2022-04-09 19:44:41,844 - INFO - allennlp.training.trainer - Epoch duration: 0:12:53.122776
2022-04-09 19:44:41,844 - INFO - allennlp.training.checkpointer - loading best weights
2022-04-09 19:44:42,826 - INFO - allennlp.common.util - Metrics: {
  "best_epoch": 3,
  "peak_worker_0_memory_MB": 5806.57421875,
  "peak_gpu_0_memory_MB": 9580.5712890625,
  "training_duration": "2:09:08.821914",
  "training_start_epoch": 0,
  "training_epochs": 9,
  "epoch": 9,
  "training_accuracy": 0.9907811741872877,
  "training_loss": 0.029437253328475122,
  "training_worker_0_memory_MB": 5806.57421875,
  "training_gpu_0_memory_MB": 9580.5712890625,
  "validation_accuracy": 0.6462882096069869,
  "validation_loss": 1.765817790620697,
  "best_validation_accuracy": 0.6637554585152838,
  "best_validation_loss": 0.8003474241365557
}
2022-04-09 19:44:42,826 - INFO - allennlp.models.archival - archiving weights and vocabulary to ./output_ir-ora-d_hyper1/model.tar.gz
