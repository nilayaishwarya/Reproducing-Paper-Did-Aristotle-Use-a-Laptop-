2022-04-10 18:32:11,214 - INFO - allennlp.common.params - random_seed = 42
2022-04-10 18:32:11,215 - INFO - allennlp.common.params - numpy_seed = 42
2022-04-10 18:32:11,215 - INFO - allennlp.common.params - pytorch_seed = 42
2022-04-10 18:32:11,898 - INFO - allennlp.common.checks - Pytorch version: 1.7.1
2022-04-10 18:32:11,898 - INFO - allennlp.common.params - type = default
2022-04-10 18:32:11,899 - INFO - allennlp.common.params - dataset_reader.type = strategy_qa_reader
2022-04-10 18:32:11,899 - INFO - allennlp.common.params - dataset_reader.lazy = False
2022-04-10 18:32:11,899 - INFO - allennlp.common.params - dataset_reader.cache_directory = None
2022-04-10 18:32:11,899 - INFO - allennlp.common.params - dataset_reader.max_instances = None
2022-04-10 18:32:11,899 - INFO - allennlp.common.params - dataset_reader.manual_distributed_sharding = False
2022-04-10 18:32:11,899 - INFO - allennlp.common.params - dataset_reader.manual_multi_process_sharding = False
2022-04-10 18:32:11,899 - INFO - allennlp.common.params - dataset_reader.tokenizer_wrapper.type = default
2022-04-10 18:32:11,900 - INFO - allennlp.common.params - dataset_reader.tokenizer_wrapper.pretrained_model = roberta-large
2022-04-10 18:32:11,900 - INFO - allennlp.common.params - dataset_reader.tokenizer_wrapper.call_kwargs.return_offsets_mapping = True
2022-04-10 18:32:11,900 - INFO - allennlp.common.params - dataset_reader.tokenizer_wrapper.call_kwargs.return_special_tokens_mask = True
2022-04-10 18:32:11,900 - INFO - allennlp.common.params - dataset_reader.tokenizer_wrapper.call_kwargs.truncation = only_first
2022-04-10 18:32:13,153 - INFO - allennlp.common.params - dataset_reader.save_tokenizer = True
2022-04-10 18:32:13,154 - INFO - allennlp.common.params - dataset_reader.is_training = True
2022-04-10 18:32:13,154 - INFO - allennlp.common.params - dataset_reader.pickle.action = None
2022-04-10 18:32:13,154 - INFO - allennlp.common.params - dataset_reader.pickle.file_name = None
2022-04-10 18:32:13,154 - INFO - allennlp.common.params - dataset_reader.pickle.path = ../pickle
2022-04-10 18:32:13,154 - INFO - allennlp.common.params - dataset_reader.pickle.save_even_when_max_instances = False
2022-04-10 18:32:13,154 - INFO - allennlp.common.params - dataset_reader.paragraphs_source = IR-ORA-D
2022-04-10 18:32:13,154 - INFO - allennlp.common.params - dataset_reader.answer_last_decomposition_step = False
2022-04-10 18:32:13,154 - INFO - allennlp.common.params - dataset_reader.skip_if_context_missing = True
2022-04-10 18:32:13,154 - INFO - allennlp.common.params - dataset_reader.paragraphs_limit = 10
2022-04-10 18:32:13,154 - INFO - allennlp.common.params - dataset_reader.generated_decompositions_paths = None
2022-04-10 18:32:13,154 - INFO - allennlp.common.params - dataset_reader.save_elasticsearch_cache = True
2022-04-10 18:32:13,604 - INFO - filelock - Lock 140347805752848 acquired on cache.lock
2022-04-10 18:32:14,292 - INFO - filelock - Lock 140347805752848 released on cache.lock
2022-04-10 18:32:14,292 - INFO - allennlp.common.params - train_data_path = data/strategyqa/train.json
2022-04-10 18:32:14,293 - INFO - allennlp.common.params - vocabulary = <allennlp.common.lazy.Lazy object at 0x7fa54cdedf10>
2022-04-10 18:32:14,294 - INFO - allennlp.common.params - datasets_for_vocab_creation = None
2022-04-10 18:32:14,294 - INFO - allennlp.common.params - validation_dataset_reader.type = strategy_qa_reader
2022-04-10 18:32:14,294 - INFO - allennlp.common.params - validation_dataset_reader.lazy = False
2022-04-10 18:32:14,294 - INFO - allennlp.common.params - validation_dataset_reader.cache_directory = None
2022-04-10 18:32:14,294 - INFO - allennlp.common.params - validation_dataset_reader.max_instances = None
2022-04-10 18:32:14,294 - INFO - allennlp.common.params - validation_dataset_reader.manual_distributed_sharding = False
2022-04-10 18:32:14,295 - INFO - allennlp.common.params - validation_dataset_reader.manual_multi_process_sharding = False
2022-04-10 18:32:14,295 - INFO - allennlp.common.params - validation_dataset_reader.tokenizer_wrapper.type = default
2022-04-10 18:32:14,295 - INFO - allennlp.common.params - validation_dataset_reader.tokenizer_wrapper.pretrained_model = roberta-large
2022-04-10 18:32:14,295 - INFO - allennlp.common.params - validation_dataset_reader.tokenizer_wrapper.call_kwargs.return_offsets_mapping = True
2022-04-10 18:32:14,295 - INFO - allennlp.common.params - validation_dataset_reader.tokenizer_wrapper.call_kwargs.return_special_tokens_mask = True
2022-04-10 18:32:14,295 - INFO - allennlp.common.params - validation_dataset_reader.tokenizer_wrapper.call_kwargs.truncation = only_first
2022-04-10 18:32:15,493 - INFO - allennlp.common.params - validation_dataset_reader.save_tokenizer = False
2022-04-10 18:32:15,494 - INFO - allennlp.common.params - validation_dataset_reader.is_training = False
2022-04-10 18:32:15,494 - INFO - allennlp.common.params - validation_dataset_reader.pickle.action = None
2022-04-10 18:32:15,494 - INFO - allennlp.common.params - validation_dataset_reader.pickle.file_name = None
2022-04-10 18:32:15,494 - INFO - allennlp.common.params - validation_dataset_reader.pickle.path = ../pickle
2022-04-10 18:32:15,494 - INFO - allennlp.common.params - validation_dataset_reader.pickle.save_even_when_max_instances = False
2022-04-10 18:32:15,494 - INFO - allennlp.common.params - validation_dataset_reader.paragraphs_source = IR-ORA-D
2022-04-10 18:32:15,494 - INFO - allennlp.common.params - validation_dataset_reader.answer_last_decomposition_step = False
2022-04-10 18:32:15,494 - INFO - allennlp.common.params - validation_dataset_reader.skip_if_context_missing = True
2022-04-10 18:32:15,494 - INFO - allennlp.common.params - validation_dataset_reader.paragraphs_limit = 10
2022-04-10 18:32:15,495 - INFO - allennlp.common.params - validation_dataset_reader.generated_decompositions_paths = None
2022-04-10 18:32:15,495 - INFO - allennlp.common.params - validation_dataset_reader.save_elasticsearch_cache = True
2022-04-10 18:32:15,495 - INFO - filelock - Lock 140347755424976 acquired on cache.lock
2022-04-10 18:32:16,319 - INFO - filelock - Lock 140347755424976 released on cache.lock
2022-04-10 18:32:16,319 - INFO - allennlp.common.params - validation_data_path = data/strategyqa/dev.json
2022-04-10 18:32:16,320 - INFO - allennlp.common.params - validation_data_loader = None
2022-04-10 18:32:16,320 - INFO - allennlp.common.params - test_data_path = None
2022-04-10 18:32:16,320 - INFO - allennlp.common.params - evaluate_on_test = True
2022-04-10 18:32:16,320 - INFO - allennlp.common.params - batch_weight_key = 
2022-04-10 18:32:16,320 - INFO - allennlp.training.util - Reading training data from data/strategyqa/train.json
2022-04-10 18:32:16,321 - INFO - tqdm - reading instances: 0it [00:00, ?it/s]
2022-04-10 18:32:16,321 - INFO - src.data.dataset_readers.base.base_dataset_reader - Reading the dataset from scratch
2022-04-10 18:32:16,321 - INFO - src.data.dataset_readers.strategy_qa_reader - Reading the dataset:
2022-04-10 18:32:16,321 - INFO - src.data.dataset_readers.strategy_qa_reader - Reading file at data/strategyqa/train.json
2022-04-10 18:32:23,614 - INFO - filelock - Lock 140347107145360 acquired on cache.lock
2022-04-10 18:32:24,402 - INFO - filelock - Lock 140347107145360 released on cache.lock
2022-04-10 18:32:24,440 - INFO - allennlp.training.util - Reading validation data from data/strategyqa/dev.json
2022-04-10 18:32:24,441 - INFO - tqdm - reading instances: 0it [00:00, ?it/s]
2022-04-10 18:32:24,441 - INFO - src.data.dataset_readers.base.base_dataset_reader - Reading the dataset from scratch
2022-04-10 18:32:24,441 - INFO - src.data.dataset_readers.strategy_qa_reader - Reading the dataset:
2022-04-10 18:32:24,441 - INFO - src.data.dataset_readers.strategy_qa_reader - Reading file at data/strategyqa/dev.json
2022-04-10 18:32:24,880 - INFO - src.data.dataset_readers.utils.elasticsearch_utils - Empty results from Elasticsearch for: actions commercial pilots take concerning engine noise arriving departing Orange County, California?
2022-04-10 18:32:25,199 - INFO - filelock - Lock 140347114562960 acquired on cache.lock
2022-04-10 18:32:25,993 - INFO - filelock - Lock 140347114562960 released on cache.lock
2022-04-10 18:32:26,028 - INFO - allennlp.common.params - type = from_instances
2022-04-10 18:32:26,028 - INFO - allennlp.common.params - min_count = None
2022-04-10 18:32:26,029 - INFO - allennlp.common.params - max_vocab_size = None
2022-04-10 18:32:26,029 - INFO - allennlp.common.params - non_padded_namespaces = ('*tags', '*labels')
2022-04-10 18:32:26,029 - INFO - allennlp.common.params - pretrained_files = None
2022-04-10 18:32:26,029 - INFO - allennlp.common.params - only_include_pretrained_words = False
2022-04-10 18:32:26,029 - INFO - allennlp.common.params - tokens_to_add = None
2022-04-10 18:32:26,029 - INFO - allennlp.common.params - min_pretrained_embeddings = None
2022-04-10 18:32:26,029 - INFO - allennlp.common.params - padding_token = @@PADDING@@
2022-04-10 18:32:26,029 - INFO - allennlp.common.params - oov_token = @@UNKNOWN@@
2022-04-10 18:32:26,029 - INFO - allennlp.data.vocabulary - Fitting token dictionary from dataset.
2022-04-10 18:32:26,029 - INFO - tqdm - building vocab: 0it [00:00, ?it/s]
2022-04-10 18:32:26,035 - INFO - allennlp.common.params - model.type = hf_classifier
2022-04-10 18:32:26,035 - INFO - allennlp.common.params - model.regularizer = None
2022-04-10 18:32:26,035 - INFO - allennlp.common.params - model.pretrained_model = roberta-large
2022-04-10 18:32:26,036 - INFO - allennlp.common.params - model.tokenizer_wrapper.type = default
2022-04-10 18:32:26,036 - INFO - allennlp.common.params - model.tokenizer_wrapper.pretrained_model = roberta-large
2022-04-10 18:32:26,036 - INFO - allennlp.common.params - model.tokenizer_wrapper.call_kwargs.return_offsets_mapping = True
2022-04-10 18:32:26,036 - INFO - allennlp.common.params - model.tokenizer_wrapper.call_kwargs.return_special_tokens_mask = True
2022-04-10 18:32:26,036 - INFO - allennlp.common.params - model.tokenizer_wrapper.call_kwargs.truncation = only_first
2022-04-10 18:32:27,223 - INFO - allennlp.common.params - model.num_labels = 2
2022-04-10 18:32:27,223 - INFO - allennlp.common.params - model.label_namespace = labels
2022-04-10 18:32:27,223 - INFO - allennlp.common.params - model.transformer_weights_path = None
2022-04-10 18:32:27,223 - INFO - allennlp.common.params - model.initializer.regexes.0.1.type = pretrained
2022-04-10 18:32:27,224 - INFO - allennlp.common.params - model.initializer.regexes.0.1.weights_file_path = output_roberta-star/model.tar.gz
2022-04-10 18:32:27,224 - INFO - allennlp.models.archival - extracting archive file output_roberta-star/model.tar.gz to temp dir /tmp/tmpjr983hq8
2022-04-10 18:32:38,695 - INFO - allennlp.models.archival - removing temporary unarchived model dir at /tmp/tmpjr983hq8
2022-04-10 18:32:38,870 - INFO - allennlp.common.params - model.initializer.prevent_regexes = None
2022-04-10 18:32:53,349 - INFO - allennlp.nn.initializers - Initializing parameters
2022-04-10 18:32:53,349 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.embeddings.word_embeddings.weight using .* initializer
2022-04-10 18:32:53,359 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.embeddings.position_embeddings.weight using .* initializer
2022-04-10 18:32:53,359 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.embeddings.token_type_embeddings.weight using .* initializer
2022-04-10 18:32:53,359 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.embeddings.LayerNorm.weight using .* initializer
2022-04-10 18:32:53,359 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.embeddings.LayerNorm.bias using .* initializer
2022-04-10 18:32:53,360 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.self.query.weight using .* initializer
2022-04-10 18:32:53,360 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.self.query.bias using .* initializer
2022-04-10 18:32:53,360 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.self.key.weight using .* initializer
2022-04-10 18:32:53,360 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.self.key.bias using .* initializer
2022-04-10 18:32:53,361 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.self.value.weight using .* initializer
2022-04-10 18:32:53,361 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.self.value.bias using .* initializer
2022-04-10 18:32:53,361 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.output.dense.weight using .* initializer
2022-04-10 18:32:53,362 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.output.dense.bias using .* initializer
2022-04-10 18:32:53,362 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.output.LayerNorm.weight using .* initializer
2022-04-10 18:32:53,362 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.output.LayerNorm.bias using .* initializer
2022-04-10 18:32:53,362 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.intermediate.dense.weight using .* initializer
2022-04-10 18:32:53,363 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.intermediate.dense.bias using .* initializer
2022-04-10 18:32:53,363 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.output.dense.weight using .* initializer
2022-04-10 18:32:53,364 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.output.dense.bias using .* initializer
2022-04-10 18:32:53,364 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.output.LayerNorm.weight using .* initializer
2022-04-10 18:32:53,365 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.output.LayerNorm.bias using .* initializer
2022-04-10 18:32:53,365 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.self.query.weight using .* initializer
2022-04-10 18:32:53,365 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.self.query.bias using .* initializer
2022-04-10 18:32:53,366 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.self.key.weight using .* initializer
2022-04-10 18:32:53,366 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.self.key.bias using .* initializer
2022-04-10 18:32:53,366 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.self.value.weight using .* initializer
2022-04-10 18:32:53,367 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.self.value.bias using .* initializer
2022-04-10 18:32:53,367 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.output.dense.weight using .* initializer
2022-04-10 18:32:53,367 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.output.dense.bias using .* initializer
2022-04-10 18:32:53,367 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.output.LayerNorm.weight using .* initializer
2022-04-10 18:32:53,368 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.output.LayerNorm.bias using .* initializer
2022-04-10 18:32:53,368 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.intermediate.dense.weight using .* initializer
2022-04-10 18:32:53,369 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.intermediate.dense.bias using .* initializer
2022-04-10 18:32:53,369 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.output.dense.weight using .* initializer
2022-04-10 18:32:53,370 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.output.dense.bias using .* initializer
2022-04-10 18:32:53,370 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.output.LayerNorm.weight using .* initializer
2022-04-10 18:32:53,370 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.output.LayerNorm.bias using .* initializer
2022-04-10 18:32:53,371 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.self.query.weight using .* initializer
2022-04-10 18:32:53,371 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.self.query.bias using .* initializer
2022-04-10 18:32:53,371 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.self.key.weight using .* initializer
2022-04-10 18:32:53,371 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.self.key.bias using .* initializer
2022-04-10 18:32:53,372 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.self.value.weight using .* initializer
2022-04-10 18:32:53,372 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.self.value.bias using .* initializer
2022-04-10 18:32:53,372 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.output.dense.weight using .* initializer
2022-04-10 18:32:53,373 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.output.dense.bias using .* initializer
2022-04-10 18:32:53,373 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.output.LayerNorm.weight using .* initializer
2022-04-10 18:32:53,373 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.output.LayerNorm.bias using .* initializer
2022-04-10 18:32:53,373 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.intermediate.dense.weight using .* initializer
2022-04-10 18:32:53,374 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.intermediate.dense.bias using .* initializer
2022-04-10 18:32:53,375 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.output.dense.weight using .* initializer
2022-04-10 18:32:53,376 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.output.dense.bias using .* initializer
2022-04-10 18:32:53,376 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.output.LayerNorm.weight using .* initializer
2022-04-10 18:32:53,376 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.output.LayerNorm.bias using .* initializer
2022-04-10 18:32:53,376 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.self.query.weight using .* initializer
2022-04-10 18:32:53,377 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.self.query.bias using .* initializer
2022-04-10 18:32:53,377 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.self.key.weight using .* initializer
2022-04-10 18:32:53,377 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.self.key.bias using .* initializer
2022-04-10 18:32:53,377 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.self.value.weight using .* initializer
2022-04-10 18:32:53,378 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.self.value.bias using .* initializer
2022-04-10 18:32:53,378 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.output.dense.weight using .* initializer
2022-04-10 18:32:53,378 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.output.dense.bias using .* initializer
2022-04-10 18:32:53,378 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.output.LayerNorm.weight using .* initializer
2022-04-10 18:32:53,379 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.output.LayerNorm.bias using .* initializer
2022-04-10 18:32:53,379 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.intermediate.dense.weight using .* initializer
2022-04-10 18:32:53,380 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.intermediate.dense.bias using .* initializer
2022-04-10 18:32:53,380 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.output.dense.weight using .* initializer
2022-04-10 18:32:53,381 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.output.dense.bias using .* initializer
2022-04-10 18:32:53,381 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.output.LayerNorm.weight using .* initializer
2022-04-10 18:32:53,381 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.output.LayerNorm.bias using .* initializer
2022-04-10 18:32:53,381 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.self.query.weight using .* initializer
2022-04-10 18:32:53,381 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.self.query.bias using .* initializer
2022-04-10 18:32:53,382 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.self.key.weight using .* initializer
2022-04-10 18:32:53,382 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.self.key.bias using .* initializer
2022-04-10 18:32:53,382 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.self.value.weight using .* initializer
2022-04-10 18:32:53,382 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.self.value.bias using .* initializer
2022-04-10 18:32:53,383 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.output.dense.weight using .* initializer
2022-04-10 18:32:53,383 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.output.dense.bias using .* initializer
2022-04-10 18:32:53,383 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.output.LayerNorm.weight using .* initializer
2022-04-10 18:32:53,383 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.output.LayerNorm.bias using .* initializer
2022-04-10 18:32:53,383 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.intermediate.dense.weight using .* initializer
2022-04-10 18:32:53,384 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.intermediate.dense.bias using .* initializer
2022-04-10 18:32:53,384 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.output.dense.weight using .* initializer
2022-04-10 18:32:53,385 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.output.dense.bias using .* initializer
2022-04-10 18:32:53,385 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.output.LayerNorm.weight using .* initializer
2022-04-10 18:32:53,386 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.output.LayerNorm.bias using .* initializer
2022-04-10 18:32:53,386 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.self.query.weight using .* initializer
2022-04-10 18:32:53,386 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.self.query.bias using .* initializer
2022-04-10 18:32:53,386 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.self.key.weight using .* initializer
2022-04-10 18:32:53,386 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.self.key.bias using .* initializer
2022-04-10 18:32:53,387 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.self.value.weight using .* initializer
2022-04-10 18:32:53,387 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.self.value.bias using .* initializer
2022-04-10 18:32:53,387 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.output.dense.weight using .* initializer
2022-04-10 18:32:53,387 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.output.dense.bias using .* initializer
2022-04-10 18:32:53,388 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.output.LayerNorm.weight using .* initializer
2022-04-10 18:32:53,388 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.output.LayerNorm.bias using .* initializer
2022-04-10 18:32:53,388 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.intermediate.dense.weight using .* initializer
2022-04-10 18:32:53,389 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.intermediate.dense.bias using .* initializer
2022-04-10 18:32:53,389 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.output.dense.weight using .* initializer
2022-04-10 18:32:53,390 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.output.dense.bias using .* initializer
2022-04-10 18:32:53,390 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.output.LayerNorm.weight using .* initializer
2022-04-10 18:32:53,390 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.output.LayerNorm.bias using .* initializer
2022-04-10 18:32:53,390 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.self.query.weight using .* initializer
2022-04-10 18:32:53,391 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.self.query.bias using .* initializer
2022-04-10 18:32:53,391 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.self.key.weight using .* initializer
2022-04-10 18:32:53,391 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.self.key.bias using .* initializer
2022-04-10 18:32:53,391 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.self.value.weight using .* initializer
2022-04-10 18:32:53,392 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.self.value.bias using .* initializer
2022-04-10 18:32:53,392 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.output.dense.weight using .* initializer
2022-04-10 18:32:53,392 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.output.dense.bias using .* initializer
2022-04-10 18:32:53,392 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.output.LayerNorm.weight using .* initializer
2022-04-10 18:32:53,392 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.output.LayerNorm.bias using .* initializer
2022-04-10 18:32:53,392 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.intermediate.dense.weight using .* initializer
2022-04-10 18:32:53,393 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.intermediate.dense.bias using .* initializer
2022-04-10 18:32:53,393 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.output.dense.weight using .* initializer
2022-04-10 18:32:53,394 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.output.dense.bias using .* initializer
2022-04-10 18:32:53,395 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.output.LayerNorm.weight using .* initializer
2022-04-10 18:32:53,395 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.output.LayerNorm.bias using .* initializer
2022-04-10 18:32:53,395 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.self.query.weight using .* initializer
2022-04-10 18:32:53,395 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.self.query.bias using .* initializer
2022-04-10 18:32:53,395 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.self.key.weight using .* initializer
2022-04-10 18:32:53,396 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.self.key.bias using .* initializer
2022-04-10 18:32:53,396 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.self.value.weight using .* initializer
2022-04-10 18:32:53,396 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.self.value.bias using .* initializer
2022-04-10 18:32:53,396 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.output.dense.weight using .* initializer
2022-04-10 18:32:53,397 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.output.dense.bias using .* initializer
2022-04-10 18:32:53,397 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.output.LayerNorm.weight using .* initializer
2022-04-10 18:32:53,397 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.output.LayerNorm.bias using .* initializer
2022-04-10 18:32:53,397 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.intermediate.dense.weight using .* initializer
2022-04-10 18:32:53,398 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.intermediate.dense.bias using .* initializer
2022-04-10 18:32:53,398 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.output.dense.weight using .* initializer
2022-04-10 18:32:53,399 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.output.dense.bias using .* initializer
2022-04-10 18:32:53,399 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.output.LayerNorm.weight using .* initializer
2022-04-10 18:32:53,399 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.output.LayerNorm.bias using .* initializer
2022-04-10 18:32:53,399 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.self.query.weight using .* initializer
2022-04-10 18:32:53,400 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.self.query.bias using .* initializer
2022-04-10 18:32:53,400 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.self.key.weight using .* initializer
2022-04-10 18:32:53,400 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.self.key.bias using .* initializer
2022-04-10 18:32:53,400 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.self.value.weight using .* initializer
2022-04-10 18:32:53,400 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.self.value.bias using .* initializer
2022-04-10 18:32:53,401 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.output.dense.weight using .* initializer
2022-04-10 18:32:53,401 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.output.dense.bias using .* initializer
2022-04-10 18:32:53,401 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.output.LayerNorm.weight using .* initializer
2022-04-10 18:32:53,401 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.output.LayerNorm.bias using .* initializer
2022-04-10 18:32:53,401 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.intermediate.dense.weight using .* initializer
2022-04-10 18:32:53,402 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.intermediate.dense.bias using .* initializer
2022-04-10 18:32:53,402 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.output.dense.weight using .* initializer
2022-04-10 18:32:53,403 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.output.dense.bias using .* initializer
2022-04-10 18:32:53,403 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.output.LayerNorm.weight using .* initializer
2022-04-10 18:32:53,403 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.output.LayerNorm.bias using .* initializer
2022-04-10 18:32:53,404 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.self.query.weight using .* initializer
2022-04-10 18:32:53,404 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.self.query.bias using .* initializer
2022-04-10 18:32:53,404 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.self.key.weight using .* initializer
2022-04-10 18:32:53,404 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.self.key.bias using .* initializer
2022-04-10 18:32:53,405 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.self.value.weight using .* initializer
2022-04-10 18:32:53,405 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.self.value.bias using .* initializer
2022-04-10 18:32:53,405 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.output.dense.weight using .* initializer
2022-04-10 18:32:53,405 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.output.dense.bias using .* initializer
2022-04-10 18:32:53,406 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.output.LayerNorm.weight using .* initializer
2022-04-10 18:32:53,406 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.output.LayerNorm.bias using .* initializer
2022-04-10 18:32:53,406 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.intermediate.dense.weight using .* initializer
2022-04-10 18:32:53,407 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.intermediate.dense.bias using .* initializer
2022-04-10 18:32:53,407 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.output.dense.weight using .* initializer
2022-04-10 18:32:53,408 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.output.dense.bias using .* initializer
2022-04-10 18:32:53,408 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.output.LayerNorm.weight using .* initializer
2022-04-10 18:32:53,408 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.output.LayerNorm.bias using .* initializer
2022-04-10 18:32:53,408 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.self.query.weight using .* initializer
2022-04-10 18:32:53,409 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.self.query.bias using .* initializer
2022-04-10 18:32:53,409 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.self.key.weight using .* initializer
2022-04-10 18:32:53,409 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.self.key.bias using .* initializer
2022-04-10 18:32:53,409 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.self.value.weight using .* initializer
2022-04-10 18:32:53,410 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.self.value.bias using .* initializer
2022-04-10 18:32:53,410 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.output.dense.weight using .* initializer
2022-04-10 18:32:53,410 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.output.dense.bias using .* initializer
2022-04-10 18:32:53,410 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.output.LayerNorm.weight using .* initializer
2022-04-10 18:32:53,410 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.output.LayerNorm.bias using .* initializer
2022-04-10 18:32:53,410 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.intermediate.dense.weight using .* initializer
2022-04-10 18:32:53,411 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.intermediate.dense.bias using .* initializer
2022-04-10 18:32:53,411 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.output.dense.weight using .* initializer
2022-04-10 18:32:53,412 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.output.dense.bias using .* initializer
2022-04-10 18:32:53,412 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.output.LayerNorm.weight using .* initializer
2022-04-10 18:32:53,413 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.output.LayerNorm.bias using .* initializer
2022-04-10 18:32:53,413 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.self.query.weight using .* initializer
2022-04-10 18:32:53,413 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.self.query.bias using .* initializer
2022-04-10 18:32:53,413 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.self.key.weight using .* initializer
2022-04-10 18:32:53,413 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.self.key.bias using .* initializer
2022-04-10 18:32:53,414 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.self.value.weight using .* initializer
2022-04-10 18:32:53,414 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.self.value.bias using .* initializer
2022-04-10 18:32:53,414 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.output.dense.weight using .* initializer
2022-04-10 18:32:53,414 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.output.dense.bias using .* initializer
2022-04-10 18:32:53,414 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.output.LayerNorm.weight using .* initializer
2022-04-10 18:32:53,415 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.output.LayerNorm.bias using .* initializer
2022-04-10 18:32:53,415 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.intermediate.dense.weight using .* initializer
2022-04-10 18:32:53,416 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.intermediate.dense.bias using .* initializer
2022-04-10 18:32:53,416 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.output.dense.weight using .* initializer
2022-04-10 18:32:53,417 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.output.dense.bias using .* initializer
2022-04-10 18:32:53,417 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.output.LayerNorm.weight using .* initializer
2022-04-10 18:32:53,417 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.output.LayerNorm.bias using .* initializer
2022-04-10 18:32:53,417 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.self.query.weight using .* initializer
2022-04-10 18:32:53,417 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.self.query.bias using .* initializer
2022-04-10 18:32:53,417 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.self.key.weight using .* initializer
2022-04-10 18:32:53,418 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.self.key.bias using .* initializer
2022-04-10 18:32:53,418 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.self.value.weight using .* initializer
2022-04-10 18:32:53,418 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.self.value.bias using .* initializer
2022-04-10 18:32:53,418 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.output.dense.weight using .* initializer
2022-04-10 18:32:53,419 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.output.dense.bias using .* initializer
2022-04-10 18:32:53,419 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.output.LayerNorm.weight using .* initializer
2022-04-10 18:32:53,419 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.output.LayerNorm.bias using .* initializer
2022-04-10 18:32:53,419 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.intermediate.dense.weight using .* initializer
2022-04-10 18:32:53,420 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.intermediate.dense.bias using .* initializer
2022-04-10 18:32:53,420 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.output.dense.weight using .* initializer
2022-04-10 18:32:53,421 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.output.dense.bias using .* initializer
2022-04-10 18:32:53,421 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.output.LayerNorm.weight using .* initializer
2022-04-10 18:32:53,421 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.output.LayerNorm.bias using .* initializer
2022-04-10 18:32:53,421 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.self.query.weight using .* initializer
2022-04-10 18:32:53,422 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.self.query.bias using .* initializer
2022-04-10 18:32:53,422 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.self.key.weight using .* initializer
2022-04-10 18:32:53,422 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.self.key.bias using .* initializer
2022-04-10 18:32:53,422 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.self.value.weight using .* initializer
2022-04-10 18:32:53,422 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.self.value.bias using .* initializer
2022-04-10 18:32:53,423 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.output.dense.weight using .* initializer
2022-04-10 18:32:53,423 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.output.dense.bias using .* initializer
2022-04-10 18:32:53,423 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.output.LayerNorm.weight using .* initializer
2022-04-10 18:32:53,423 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.output.LayerNorm.bias using .* initializer
2022-04-10 18:32:53,423 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.intermediate.dense.weight using .* initializer
2022-04-10 18:32:53,424 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.intermediate.dense.bias using .* initializer
2022-04-10 18:32:53,424 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.output.dense.weight using .* initializer
2022-04-10 18:32:53,425 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.output.dense.bias using .* initializer
2022-04-10 18:32:53,425 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.output.LayerNorm.weight using .* initializer
2022-04-10 18:32:53,425 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.output.LayerNorm.bias using .* initializer
2022-04-10 18:32:53,426 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.self.query.weight using .* initializer
2022-04-10 18:32:53,426 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.self.query.bias using .* initializer
2022-04-10 18:32:53,426 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.self.key.weight using .* initializer
2022-04-10 18:32:53,426 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.self.key.bias using .* initializer
2022-04-10 18:32:53,427 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.self.value.weight using .* initializer
2022-04-10 18:32:53,427 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.self.value.bias using .* initializer
2022-04-10 18:32:53,427 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.output.dense.weight using .* initializer
2022-04-10 18:32:53,427 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.output.dense.bias using .* initializer
2022-04-10 18:32:53,427 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.output.LayerNorm.weight using .* initializer
2022-04-10 18:32:53,427 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.output.LayerNorm.bias using .* initializer
2022-04-10 18:32:53,428 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.intermediate.dense.weight using .* initializer
2022-04-10 18:32:53,429 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.intermediate.dense.bias using .* initializer
2022-04-10 18:32:53,429 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.output.dense.weight using .* initializer
2022-04-10 18:32:53,430 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.output.dense.bias using .* initializer
2022-04-10 18:32:53,430 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.output.LayerNorm.weight using .* initializer
2022-04-10 18:32:53,430 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.output.LayerNorm.bias using .* initializer
2022-04-10 18:32:53,430 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.self.query.weight using .* initializer
2022-04-10 18:32:53,430 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.self.query.bias using .* initializer
2022-04-10 18:32:53,430 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.self.key.weight using .* initializer
2022-04-10 18:32:53,431 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.self.key.bias using .* initializer
2022-04-10 18:32:53,431 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.self.value.weight using .* initializer
2022-04-10 18:32:53,431 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.self.value.bias using .* initializer
2022-04-10 18:32:53,431 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.output.dense.weight using .* initializer
2022-04-10 18:32:53,432 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.output.dense.bias using .* initializer
2022-04-10 18:32:53,432 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.output.LayerNorm.weight using .* initializer
2022-04-10 18:32:53,432 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.output.LayerNorm.bias using .* initializer
2022-04-10 18:32:53,432 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.intermediate.dense.weight using .* initializer
2022-04-10 18:32:53,433 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.intermediate.dense.bias using .* initializer
2022-04-10 18:32:53,433 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.output.dense.weight using .* initializer
2022-04-10 18:32:53,434 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.output.dense.bias using .* initializer
2022-04-10 18:32:53,434 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.output.LayerNorm.weight using .* initializer
2022-04-10 18:32:53,434 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.output.LayerNorm.bias using .* initializer
2022-04-10 18:32:53,434 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.self.query.weight using .* initializer
2022-04-10 18:32:53,435 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.self.query.bias using .* initializer
2022-04-10 18:32:53,435 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.self.key.weight using .* initializer
2022-04-10 18:32:53,435 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.self.key.bias using .* initializer
2022-04-10 18:32:53,435 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.self.value.weight using .* initializer
2022-04-10 18:32:53,436 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.self.value.bias using .* initializer
2022-04-10 18:32:53,436 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.output.dense.weight using .* initializer
2022-04-10 18:32:53,436 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.output.dense.bias using .* initializer
2022-04-10 18:32:53,436 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.output.LayerNorm.weight using .* initializer
2022-04-10 18:32:53,436 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.output.LayerNorm.bias using .* initializer
2022-04-10 18:32:53,436 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.intermediate.dense.weight using .* initializer
2022-04-10 18:32:53,437 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.intermediate.dense.bias using .* initializer
2022-04-10 18:32:53,437 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.output.dense.weight using .* initializer
2022-04-10 18:32:53,438 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.output.dense.bias using .* initializer
2022-04-10 18:32:53,438 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.output.LayerNorm.weight using .* initializer
2022-04-10 18:32:53,439 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.output.LayerNorm.bias using .* initializer
2022-04-10 18:32:53,439 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.self.query.weight using .* initializer
2022-04-10 18:32:53,439 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.self.query.bias using .* initializer
2022-04-10 18:32:53,439 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.self.key.weight using .* initializer
2022-04-10 18:32:53,439 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.self.key.bias using .* initializer
2022-04-10 18:32:53,440 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.self.value.weight using .* initializer
2022-04-10 18:32:53,440 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.self.value.bias using .* initializer
2022-04-10 18:32:53,440 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.output.dense.weight using .* initializer
2022-04-10 18:32:53,440 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.output.dense.bias using .* initializer
2022-04-10 18:32:53,440 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.output.LayerNorm.weight using .* initializer
2022-04-10 18:32:53,441 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.output.LayerNorm.bias using .* initializer
2022-04-10 18:32:53,441 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.intermediate.dense.weight using .* initializer
2022-04-10 18:32:53,442 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.intermediate.dense.bias using .* initializer
2022-04-10 18:32:53,442 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.output.dense.weight using .* initializer
2022-04-10 18:32:53,443 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.output.dense.bias using .* initializer
2022-04-10 18:32:53,443 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.output.LayerNorm.weight using .* initializer
2022-04-10 18:32:53,443 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.output.LayerNorm.bias using .* initializer
2022-04-10 18:32:53,443 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.self.query.weight using .* initializer
2022-04-10 18:32:53,443 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.self.query.bias using .* initializer
2022-04-10 18:32:53,443 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.self.key.weight using .* initializer
2022-04-10 18:32:53,444 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.self.key.bias using .* initializer
2022-04-10 18:32:53,444 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.self.value.weight using .* initializer
2022-04-10 18:32:53,444 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.self.value.bias using .* initializer
2022-04-10 18:32:53,444 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.output.dense.weight using .* initializer
2022-04-10 18:32:53,445 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.output.dense.bias using .* initializer
2022-04-10 18:32:53,445 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.output.LayerNorm.weight using .* initializer
2022-04-10 18:32:53,445 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.output.LayerNorm.bias using .* initializer
2022-04-10 18:32:53,445 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.intermediate.dense.weight using .* initializer
2022-04-10 18:32:53,446 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.intermediate.dense.bias using .* initializer
2022-04-10 18:32:53,446 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.output.dense.weight using .* initializer
2022-04-10 18:32:53,447 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.output.dense.bias using .* initializer
2022-04-10 18:32:53,447 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.output.LayerNorm.weight using .* initializer
2022-04-10 18:32:53,447 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.output.LayerNorm.bias using .* initializer
2022-04-10 18:32:53,447 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.self.query.weight using .* initializer
2022-04-10 18:32:53,448 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.self.query.bias using .* initializer
2022-04-10 18:32:53,448 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.self.key.weight using .* initializer
2022-04-10 18:32:53,448 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.self.key.bias using .* initializer
2022-04-10 18:32:53,448 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.self.value.weight using .* initializer
2022-04-10 18:32:53,449 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.self.value.bias using .* initializer
2022-04-10 18:32:53,449 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.output.dense.weight using .* initializer
2022-04-10 18:32:53,449 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.output.dense.bias using .* initializer
2022-04-10 18:32:53,449 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.output.LayerNorm.weight using .* initializer
2022-04-10 18:32:53,449 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.output.LayerNorm.bias using .* initializer
2022-04-10 18:32:53,449 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.intermediate.dense.weight using .* initializer
2022-04-10 18:32:53,450 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.intermediate.dense.bias using .* initializer
2022-04-10 18:32:53,450 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.output.dense.weight using .* initializer
2022-04-10 18:32:53,451 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.output.dense.bias using .* initializer
2022-04-10 18:32:53,452 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.output.LayerNorm.weight using .* initializer
2022-04-10 18:32:53,452 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.output.LayerNorm.bias using .* initializer
2022-04-10 18:32:53,452 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.self.query.weight using .* initializer
2022-04-10 18:32:53,452 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.self.query.bias using .* initializer
2022-04-10 18:32:53,452 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.self.key.weight using .* initializer
2022-04-10 18:32:53,452 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.self.key.bias using .* initializer
2022-04-10 18:32:53,453 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.self.value.weight using .* initializer
2022-04-10 18:32:53,453 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.self.value.bias using .* initializer
2022-04-10 18:32:53,453 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.output.dense.weight using .* initializer
2022-04-10 18:32:53,453 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.output.dense.bias using .* initializer
2022-04-10 18:32:53,453 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.output.LayerNorm.weight using .* initializer
2022-04-10 18:32:53,454 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.output.LayerNorm.bias using .* initializer
2022-04-10 18:32:53,454 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.intermediate.dense.weight using .* initializer
2022-04-10 18:32:53,455 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.intermediate.dense.bias using .* initializer
2022-04-10 18:32:53,455 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.output.dense.weight using .* initializer
2022-04-10 18:32:53,456 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.output.dense.bias using .* initializer
2022-04-10 18:32:53,456 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.output.LayerNorm.weight using .* initializer
2022-04-10 18:32:53,456 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.output.LayerNorm.bias using .* initializer
2022-04-10 18:32:53,456 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.self.query.weight using .* initializer
2022-04-10 18:32:53,456 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.self.query.bias using .* initializer
2022-04-10 18:32:53,457 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.self.key.weight using .* initializer
2022-04-10 18:32:53,457 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.self.key.bias using .* initializer
2022-04-10 18:32:53,457 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.self.value.weight using .* initializer
2022-04-10 18:32:53,457 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.self.value.bias using .* initializer
2022-04-10 18:32:53,457 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.output.dense.weight using .* initializer
2022-04-10 18:32:53,458 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.output.dense.bias using .* initializer
2022-04-10 18:32:53,458 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.output.LayerNorm.weight using .* initializer
2022-04-10 18:32:53,458 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.output.LayerNorm.bias using .* initializer
2022-04-10 18:32:53,458 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.intermediate.dense.weight using .* initializer
2022-04-10 18:32:53,459 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.intermediate.dense.bias using .* initializer
2022-04-10 18:32:53,459 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.output.dense.weight using .* initializer
2022-04-10 18:32:53,460 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.output.dense.bias using .* initializer
2022-04-10 18:32:53,460 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.output.LayerNorm.weight using .* initializer
2022-04-10 18:32:53,460 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.output.LayerNorm.bias using .* initializer
2022-04-10 18:32:53,460 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.self.query.weight using .* initializer
2022-04-10 18:32:53,461 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.self.query.bias using .* initializer
2022-04-10 18:32:53,461 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.self.key.weight using .* initializer
2022-04-10 18:32:53,461 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.self.key.bias using .* initializer
2022-04-10 18:32:53,461 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.self.value.weight using .* initializer
2022-04-10 18:32:53,462 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.self.value.bias using .* initializer
2022-04-10 18:32:53,462 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.output.dense.weight using .* initializer
2022-04-10 18:32:53,462 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.output.dense.bias using .* initializer
2022-04-10 18:32:53,462 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.output.LayerNorm.weight using .* initializer
2022-04-10 18:32:53,462 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.output.LayerNorm.bias using .* initializer
2022-04-10 18:32:53,462 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.intermediate.dense.weight using .* initializer
2022-04-10 18:32:53,463 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.intermediate.dense.bias using .* initializer
2022-04-10 18:32:53,463 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.output.dense.weight using .* initializer
2022-04-10 18:32:53,464 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.output.dense.bias using .* initializer
2022-04-10 18:32:53,465 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.output.LayerNorm.weight using .* initializer
2022-04-10 18:32:53,465 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.output.LayerNorm.bias using .* initializer
2022-04-10 18:32:53,465 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.self.query.weight using .* initializer
2022-04-10 18:32:53,465 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.self.query.bias using .* initializer
2022-04-10 18:32:53,465 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.self.key.weight using .* initializer
2022-04-10 18:32:53,466 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.self.key.bias using .* initializer
2022-04-10 18:32:53,466 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.self.value.weight using .* initializer
2022-04-10 18:32:53,466 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.self.value.bias using .* initializer
2022-04-10 18:32:53,466 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.output.dense.weight using .* initializer
2022-04-10 18:32:53,467 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.output.dense.bias using .* initializer
2022-04-10 18:32:53,467 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.output.LayerNorm.weight using .* initializer
2022-04-10 18:32:53,467 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.output.LayerNorm.bias using .* initializer
2022-04-10 18:32:53,467 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.intermediate.dense.weight using .* initializer
2022-04-10 18:32:53,468 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.intermediate.dense.bias using .* initializer
2022-04-10 18:32:53,468 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.output.dense.weight using .* initializer
2022-04-10 18:32:53,469 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.output.dense.bias using .* initializer
2022-04-10 18:32:53,469 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.output.LayerNorm.weight using .* initializer
2022-04-10 18:32:53,469 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.output.LayerNorm.bias using .* initializer
2022-04-10 18:32:53,469 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.pooler.dense.weight using .* initializer
2022-04-10 18:32:53,470 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.pooler.dense.bias using .* initializer
2022-04-10 18:32:53,470 - INFO - allennlp.nn.initializers - Initializing _classifier.classifier.dense.weight using .* initializer
2022-04-10 18:32:53,470 - INFO - allennlp.nn.initializers - Initializing _classifier.classifier.dense.bias using .* initializer
2022-04-10 18:32:53,470 - INFO - allennlp.nn.initializers - Initializing _classifier.classifier.out_proj.weight using .* initializer
2022-04-10 18:32:53,470 - INFO - allennlp.nn.initializers - Initializing _classifier.classifier.out_proj.bias using .* initializer
2022-04-10 18:32:53,470 - INFO - allennlp.nn.initializers - Done initializing parameters; the following parameters are using their default initialization from their code
2022-04-10 18:32:54,020 - INFO - filelock - Lock 140347107145232 acquired on ./output_ir-ora-d_hyper5/vocabulary/.lock
2022-04-10 18:32:54,021 - INFO - filelock - Lock 140347107145232 released on ./output_ir-ora-d_hyper5/vocabulary/.lock
2022-04-10 18:32:54,021 - INFO - allennlp.common.params - data_loader.type = pytorch_dataloader
2022-04-10 18:32:54,021 - INFO - allennlp.common.params - data_loader.batch_size = 1
2022-04-10 18:32:54,021 - INFO - allennlp.common.params - data_loader.shuffle = False
2022-04-10 18:32:54,021 - INFO - allennlp.common.params - data_loader.sampler = None
2022-04-10 18:32:54,022 - INFO - allennlp.common.params - data_loader.num_workers = 0
2022-04-10 18:32:54,022 - INFO - allennlp.common.params - data_loader.pin_memory = False
2022-04-10 18:32:54,022 - INFO - allennlp.common.params - data_loader.drop_last = False
2022-04-10 18:32:54,022 - INFO - allennlp.common.params - data_loader.timeout = 0
2022-04-10 18:32:54,022 - INFO - allennlp.common.params - data_loader.worker_init_fn = None
2022-04-10 18:32:54,022 - INFO - allennlp.common.params - data_loader.multiprocessing_context = None
2022-04-10 18:32:54,022 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None
2022-04-10 18:32:54,022 - INFO - allennlp.common.params - data_loader.batch_sampler.type = bucket
2022-04-10 18:32:54,023 - INFO - allennlp.common.params - data_loader.batch_sampler.batch_size = 2
2022-04-10 18:32:54,023 - INFO - allennlp.common.params - data_loader.batch_sampler.sorting_keys = None
2022-04-10 18:32:54,023 - INFO - allennlp.common.params - data_loader.batch_sampler.padding_noise = 0.1
2022-04-10 18:32:54,023 - INFO - allennlp.common.params - data_loader.batch_sampler.drop_last = False
2022-04-10 18:32:54,023 - INFO - allennlp.common.params - data_loader.type = pytorch_dataloader
2022-04-10 18:32:54,024 - INFO - allennlp.common.params - data_loader.batch_size = 1
2022-04-10 18:32:54,024 - INFO - allennlp.common.params - data_loader.shuffle = False
2022-04-10 18:32:54,024 - INFO - allennlp.common.params - data_loader.sampler = None
2022-04-10 18:32:54,024 - INFO - allennlp.common.params - data_loader.num_workers = 0
2022-04-10 18:32:54,024 - INFO - allennlp.common.params - data_loader.pin_memory = False
2022-04-10 18:32:54,024 - INFO - allennlp.common.params - data_loader.drop_last = False
2022-04-10 18:32:54,024 - INFO - allennlp.common.params - data_loader.timeout = 0
2022-04-10 18:32:54,024 - INFO - allennlp.common.params - data_loader.worker_init_fn = None
2022-04-10 18:32:54,024 - INFO - allennlp.common.params - data_loader.multiprocessing_context = None
2022-04-10 18:32:54,024 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None
2022-04-10 18:32:54,025 - INFO - allennlp.common.params - data_loader.batch_sampler.type = bucket
2022-04-10 18:32:54,025 - INFO - allennlp.common.params - data_loader.batch_sampler.batch_size = 2
2022-04-10 18:32:54,025 - INFO - allennlp.common.params - data_loader.batch_sampler.sorting_keys = None
2022-04-10 18:32:54,025 - INFO - allennlp.common.params - data_loader.batch_sampler.padding_noise = 0.1
2022-04-10 18:32:54,025 - INFO - allennlp.common.params - data_loader.batch_sampler.drop_last = False
2022-04-10 18:32:54,025 - INFO - allennlp.common.params - trainer.type = gradient_descent
2022-04-10 18:32:54,026 - INFO - allennlp.common.params - trainer.patience = None
2022-04-10 18:32:54,026 - INFO - allennlp.common.params - trainer.validation_metric = +accuracy
2022-04-10 18:32:54,026 - INFO - allennlp.common.params - trainer.num_epochs = 15
2022-04-10 18:32:54,026 - INFO - allennlp.common.params - trainer.cuda_device = 0
2022-04-10 18:32:54,026 - INFO - allennlp.common.params - trainer.grad_norm = None
2022-04-10 18:32:54,026 - INFO - allennlp.common.params - trainer.grad_clipping = None
2022-04-10 18:32:54,026 - INFO - allennlp.common.params - trainer.distributed = False
2022-04-10 18:32:54,026 - INFO - allennlp.common.params - trainer.world_size = 1
2022-04-10 18:32:54,026 - INFO - allennlp.common.params - trainer.num_gradient_accumulation_steps = 16
2022-04-10 18:32:54,027 - INFO - allennlp.common.params - trainer.use_amp = False
2022-04-10 18:32:54,027 - INFO - allennlp.common.params - trainer.no_grad = None
2022-04-10 18:32:54,027 - INFO - allennlp.common.params - trainer.momentum_scheduler = None
2022-04-10 18:32:54,027 - INFO - allennlp.common.params - trainer.tensorboard_writer = <allennlp.common.lazy.Lazy object at 0x7fa5429a3210>
2022-04-10 18:32:54,027 - INFO - allennlp.common.params - trainer.moving_average = None
2022-04-10 18:32:54,027 - INFO - allennlp.common.params - trainer.checkpointer = <allennlp.common.lazy.Lazy object at 0x7fa5429a3290>
2022-04-10 18:32:54,027 - INFO - allennlp.common.params - trainer.batch_callbacks = None
2022-04-10 18:32:54,027 - INFO - allennlp.common.params - trainer.epoch_callbacks = None
2022-04-10 18:32:54,027 - INFO - allennlp.common.params - trainer.end_callbacks = None
2022-04-10 18:32:54,027 - INFO - allennlp.common.params - trainer.trainer_callbacks = None
2022-04-10 18:32:56,815 - INFO - allennlp.common.params - trainer.optimizer.type = huggingface_adamw
2022-04-10 18:32:56,816 - INFO - allennlp.common.params - trainer.optimizer.parameter_groups = None
2022-04-10 18:32:56,816 - INFO - allennlp.common.params - trainer.optimizer.lr = 1e-06
2022-04-10 18:32:56,816 - INFO - allennlp.common.params - trainer.optimizer.betas = [0.9, 0.98]
2022-04-10 18:32:56,816 - INFO - allennlp.common.params - trainer.optimizer.eps = 1e-06
2022-04-10 18:32:56,816 - INFO - allennlp.common.params - trainer.optimizer.weight_decay = 0.1
2022-04-10 18:32:56,816 - INFO - allennlp.common.params - trainer.optimizer.correct_bias = True
2022-04-10 18:32:56,817 - INFO - allennlp.training.optimizers - Number of trainable parameters: 356411394
2022-04-10 18:32:56,818 - INFO - allennlp.common.util - The following parameters are Frozen (without gradient):
2022-04-10 18:32:56,821 - INFO - allennlp.common.util - The following parameters are Tunable (with gradient):
2022-04-10 18:32:56,821 - INFO - allennlp.common.util - _classifier.roberta.embeddings.word_embeddings.weight
2022-04-10 18:32:56,821 - INFO - allennlp.common.util - _classifier.roberta.embeddings.position_embeddings.weight
2022-04-10 18:32:56,821 - INFO - allennlp.common.util - _classifier.roberta.embeddings.token_type_embeddings.weight
2022-04-10 18:32:56,821 - INFO - allennlp.common.util - _classifier.roberta.embeddings.LayerNorm.weight
2022-04-10 18:32:56,821 - INFO - allennlp.common.util - _classifier.roberta.embeddings.LayerNorm.bias
2022-04-10 18:32:56,821 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.self.query.weight
2022-04-10 18:32:56,822 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.self.query.bias
2022-04-10 18:32:56,822 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.self.key.weight
2022-04-10 18:32:56,822 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.self.key.bias
2022-04-10 18:32:56,822 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.self.value.weight
2022-04-10 18:32:56,822 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.self.value.bias
2022-04-10 18:32:56,822 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.output.dense.weight
2022-04-10 18:32:56,822 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.output.dense.bias
2022-04-10 18:32:56,822 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.output.LayerNorm.weight
2022-04-10 18:32:56,822 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.output.LayerNorm.bias
2022-04-10 18:32:56,822 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.intermediate.dense.weight
2022-04-10 18:32:56,822 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.intermediate.dense.bias
2022-04-10 18:32:56,822 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.output.dense.weight
2022-04-10 18:32:56,822 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.output.dense.bias
2022-04-10 18:32:56,823 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.output.LayerNorm.weight
2022-04-10 18:32:56,823 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.output.LayerNorm.bias
2022-04-10 18:32:56,823 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.self.query.weight
2022-04-10 18:32:56,823 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.self.query.bias
2022-04-10 18:32:56,823 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.self.key.weight
2022-04-10 18:32:56,823 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.self.key.bias
2022-04-10 18:32:56,823 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.self.value.weight
2022-04-10 18:32:56,823 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.self.value.bias
2022-04-10 18:32:56,823 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.output.dense.weight
2022-04-10 18:32:56,823 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.output.dense.bias
2022-04-10 18:32:56,823 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.output.LayerNorm.weight
2022-04-10 18:32:56,823 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.output.LayerNorm.bias
2022-04-10 18:32:56,823 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.intermediate.dense.weight
2022-04-10 18:32:56,824 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.intermediate.dense.bias
2022-04-10 18:32:56,824 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.output.dense.weight
2022-04-10 18:32:56,824 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.output.dense.bias
2022-04-10 18:32:56,824 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.output.LayerNorm.weight
2022-04-10 18:32:56,824 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.output.LayerNorm.bias
2022-04-10 18:32:56,824 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.self.query.weight
2022-04-10 18:32:56,824 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.self.query.bias
2022-04-10 18:32:56,824 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.self.key.weight
2022-04-10 18:32:56,824 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.self.key.bias
2022-04-10 18:32:56,824 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.self.value.weight
2022-04-10 18:32:56,824 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.self.value.bias
2022-04-10 18:32:56,824 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.output.dense.weight
2022-04-10 18:32:56,824 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.output.dense.bias
2022-04-10 18:32:56,825 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.output.LayerNorm.weight
2022-04-10 18:32:56,825 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.output.LayerNorm.bias
2022-04-10 18:32:56,825 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.intermediate.dense.weight
2022-04-10 18:32:56,825 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.intermediate.dense.bias
2022-04-10 18:32:56,825 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.output.dense.weight
2022-04-10 18:32:56,825 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.output.dense.bias
2022-04-10 18:32:56,825 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.output.LayerNorm.weight
2022-04-10 18:32:56,825 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.output.LayerNorm.bias
2022-04-10 18:32:56,825 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.self.query.weight
2022-04-10 18:32:56,825 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.self.query.bias
2022-04-10 18:32:56,825 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.self.key.weight
2022-04-10 18:32:56,826 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.self.key.bias
2022-04-10 18:32:56,826 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.self.value.weight
2022-04-10 18:32:56,826 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.self.value.bias
2022-04-10 18:32:56,826 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.output.dense.weight
2022-04-10 18:32:56,826 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.output.dense.bias
2022-04-10 18:32:56,826 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.output.LayerNorm.weight
2022-04-10 18:32:56,826 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.output.LayerNorm.bias
2022-04-10 18:32:56,826 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.intermediate.dense.weight
2022-04-10 18:32:56,826 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.intermediate.dense.bias
2022-04-10 18:32:56,826 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.output.dense.weight
2022-04-10 18:32:56,826 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.output.dense.bias
2022-04-10 18:32:56,826 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.output.LayerNorm.weight
2022-04-10 18:32:56,826 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.output.LayerNorm.bias
2022-04-10 18:32:56,827 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.self.query.weight
2022-04-10 18:32:56,827 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.self.query.bias
2022-04-10 18:32:56,827 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.self.key.weight
2022-04-10 18:32:56,827 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.self.key.bias
2022-04-10 18:32:56,827 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.self.value.weight
2022-04-10 18:32:56,827 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.self.value.bias
2022-04-10 18:32:56,827 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.output.dense.weight
2022-04-10 18:32:56,827 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.output.dense.bias
2022-04-10 18:32:56,827 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.output.LayerNorm.weight
2022-04-10 18:32:56,827 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.output.LayerNorm.bias
2022-04-10 18:32:56,827 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.intermediate.dense.weight
2022-04-10 18:32:56,827 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.intermediate.dense.bias
2022-04-10 18:32:56,827 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.output.dense.weight
2022-04-10 18:32:56,828 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.output.dense.bias
2022-04-10 18:32:56,828 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.output.LayerNorm.weight
2022-04-10 18:32:56,828 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.output.LayerNorm.bias
2022-04-10 18:32:56,828 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.self.query.weight
2022-04-10 18:32:56,828 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.self.query.bias
2022-04-10 18:32:56,828 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.self.key.weight
2022-04-10 18:32:56,828 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.self.key.bias
2022-04-10 18:32:56,828 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.self.value.weight
2022-04-10 18:32:56,828 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.self.value.bias
2022-04-10 18:32:56,828 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.output.dense.weight
2022-04-10 18:32:56,828 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.output.dense.bias
2022-04-10 18:32:56,829 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.output.LayerNorm.weight
2022-04-10 18:32:56,829 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.output.LayerNorm.bias
2022-04-10 18:32:56,829 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.intermediate.dense.weight
2022-04-10 18:32:56,829 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.intermediate.dense.bias
2022-04-10 18:32:56,829 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.output.dense.weight
2022-04-10 18:32:56,829 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.output.dense.bias
2022-04-10 18:32:56,829 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.output.LayerNorm.weight
2022-04-10 18:32:56,829 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.output.LayerNorm.bias
2022-04-10 18:32:56,829 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.self.query.weight
2022-04-10 18:32:56,829 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.self.query.bias
2022-04-10 18:32:56,829 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.self.key.weight
2022-04-10 18:32:56,829 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.self.key.bias
2022-04-10 18:32:56,830 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.self.value.weight
2022-04-10 18:32:56,830 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.self.value.bias
2022-04-10 18:32:56,830 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.output.dense.weight
2022-04-10 18:32:56,830 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.output.dense.bias
2022-04-10 18:32:56,830 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.output.LayerNorm.weight
2022-04-10 18:32:56,830 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.output.LayerNorm.bias
2022-04-10 18:32:56,830 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.intermediate.dense.weight
2022-04-10 18:32:56,830 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.intermediate.dense.bias
2022-04-10 18:32:56,830 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.output.dense.weight
2022-04-10 18:32:56,830 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.output.dense.bias
2022-04-10 18:32:56,830 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.output.LayerNorm.weight
2022-04-10 18:32:56,830 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.output.LayerNorm.bias
2022-04-10 18:32:56,831 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.self.query.weight
2022-04-10 18:32:56,831 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.self.query.bias
2022-04-10 18:32:56,831 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.self.key.weight
2022-04-10 18:32:56,831 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.self.key.bias
2022-04-10 18:32:56,831 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.self.value.weight
2022-04-10 18:32:56,831 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.self.value.bias
2022-04-10 18:32:56,831 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.output.dense.weight
2022-04-10 18:32:56,831 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.output.dense.bias
2022-04-10 18:32:56,831 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.output.LayerNorm.weight
2022-04-10 18:32:56,831 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.output.LayerNorm.bias
2022-04-10 18:32:56,831 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.intermediate.dense.weight
2022-04-10 18:32:56,831 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.intermediate.dense.bias
2022-04-10 18:32:56,831 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.output.dense.weight
2022-04-10 18:32:56,832 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.output.dense.bias
2022-04-10 18:32:56,832 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.output.LayerNorm.weight
2022-04-10 18:32:56,832 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.output.LayerNorm.bias
2022-04-10 18:32:56,832 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.self.query.weight
2022-04-10 18:32:56,832 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.self.query.bias
2022-04-10 18:32:56,832 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.self.key.weight
2022-04-10 18:32:56,832 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.self.key.bias
2022-04-10 18:32:56,832 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.self.value.weight
2022-04-10 18:32:56,832 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.self.value.bias
2022-04-10 18:32:56,832 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.output.dense.weight
2022-04-10 18:32:56,832 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.output.dense.bias
2022-04-10 18:32:56,832 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.output.LayerNorm.weight
2022-04-10 18:32:56,832 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.output.LayerNorm.bias
2022-04-10 18:32:56,832 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.intermediate.dense.weight
2022-04-10 18:32:56,833 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.intermediate.dense.bias
2022-04-10 18:32:56,833 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.output.dense.weight
2022-04-10 18:32:56,833 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.output.dense.bias
2022-04-10 18:32:56,833 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.output.LayerNorm.weight
2022-04-10 18:32:56,833 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.output.LayerNorm.bias
2022-04-10 18:32:56,833 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.self.query.weight
2022-04-10 18:32:56,833 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.self.query.bias
2022-04-10 18:32:56,833 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.self.key.weight
2022-04-10 18:32:56,833 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.self.key.bias
2022-04-10 18:32:56,833 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.self.value.weight
2022-04-10 18:32:56,833 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.self.value.bias
2022-04-10 18:32:56,833 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.output.dense.weight
2022-04-10 18:32:56,834 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.output.dense.bias
2022-04-10 18:32:56,834 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.output.LayerNorm.weight
2022-04-10 18:32:56,834 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.output.LayerNorm.bias
2022-04-10 18:32:56,834 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.intermediate.dense.weight
2022-04-10 18:32:56,834 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.intermediate.dense.bias
2022-04-10 18:32:56,834 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.output.dense.weight
2022-04-10 18:32:56,834 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.output.dense.bias
2022-04-10 18:32:56,834 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.output.LayerNorm.weight
2022-04-10 18:32:56,834 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.output.LayerNorm.bias
2022-04-10 18:32:56,834 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.self.query.weight
2022-04-10 18:32:56,834 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.self.query.bias
2022-04-10 18:32:56,834 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.self.key.weight
2022-04-10 18:32:56,835 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.self.key.bias
2022-04-10 18:32:56,835 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.self.value.weight
2022-04-10 18:32:56,835 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.self.value.bias
2022-04-10 18:32:56,835 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.output.dense.weight
2022-04-10 18:32:56,835 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.output.dense.bias
2022-04-10 18:32:56,835 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.output.LayerNorm.weight
2022-04-10 18:32:56,835 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.output.LayerNorm.bias
2022-04-10 18:32:56,835 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.intermediate.dense.weight
2022-04-10 18:32:56,835 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.intermediate.dense.bias
2022-04-10 18:32:56,835 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.output.dense.weight
2022-04-10 18:32:56,835 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.output.dense.bias
2022-04-10 18:32:56,836 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.output.LayerNorm.weight
2022-04-10 18:32:56,836 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.output.LayerNorm.bias
2022-04-10 18:32:56,836 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.self.query.weight
2022-04-10 18:32:56,836 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.self.query.bias
2022-04-10 18:32:56,836 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.self.key.weight
2022-04-10 18:32:56,836 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.self.key.bias
2022-04-10 18:32:56,836 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.self.value.weight
2022-04-10 18:32:56,836 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.self.value.bias
2022-04-10 18:32:56,836 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.output.dense.weight
2022-04-10 18:32:56,836 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.output.dense.bias
2022-04-10 18:32:56,836 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.output.LayerNorm.weight
2022-04-10 18:32:56,836 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.output.LayerNorm.bias
2022-04-10 18:32:56,836 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.intermediate.dense.weight
2022-04-10 18:32:56,837 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.intermediate.dense.bias
2022-04-10 18:32:56,837 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.output.dense.weight
2022-04-10 18:32:56,837 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.output.dense.bias
2022-04-10 18:32:56,837 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.output.LayerNorm.weight
2022-04-10 18:32:56,837 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.output.LayerNorm.bias
2022-04-10 18:32:56,837 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.self.query.weight
2022-04-10 18:32:56,837 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.self.query.bias
2022-04-10 18:32:56,837 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.self.key.weight
2022-04-10 18:32:56,837 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.self.key.bias
2022-04-10 18:32:56,837 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.self.value.weight
2022-04-10 18:32:56,837 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.self.value.bias
2022-04-10 18:32:56,837 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.output.dense.weight
2022-04-10 18:32:56,837 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.output.dense.bias
2022-04-10 18:32:56,838 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.output.LayerNorm.weight
2022-04-10 18:32:56,838 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.output.LayerNorm.bias
2022-04-10 18:32:56,838 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.intermediate.dense.weight
2022-04-10 18:32:56,838 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.intermediate.dense.bias
2022-04-10 18:32:56,838 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.output.dense.weight
2022-04-10 18:32:56,838 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.output.dense.bias
2022-04-10 18:32:56,838 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.output.LayerNorm.weight
2022-04-10 18:32:56,838 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.output.LayerNorm.bias
2022-04-10 18:32:56,838 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.self.query.weight
2022-04-10 18:32:56,838 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.self.query.bias
2022-04-10 18:32:56,838 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.self.key.weight
2022-04-10 18:32:56,838 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.self.key.bias
2022-04-10 18:32:56,838 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.self.value.weight
2022-04-10 18:32:56,839 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.self.value.bias
2022-04-10 18:32:56,839 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.output.dense.weight
2022-04-10 18:32:56,839 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.output.dense.bias
2022-04-10 18:32:56,839 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.output.LayerNorm.weight
2022-04-10 18:32:56,839 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.output.LayerNorm.bias
2022-04-10 18:32:56,839 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.intermediate.dense.weight
2022-04-10 18:32:56,839 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.intermediate.dense.bias
2022-04-10 18:32:56,839 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.output.dense.weight
2022-04-10 18:32:56,839 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.output.dense.bias
2022-04-10 18:32:56,839 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.output.LayerNorm.weight
2022-04-10 18:32:56,839 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.output.LayerNorm.bias
2022-04-10 18:32:56,839 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.self.query.weight
2022-04-10 18:32:56,839 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.self.query.bias
2022-04-10 18:32:56,840 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.self.key.weight
2022-04-10 18:32:56,840 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.self.key.bias
2022-04-10 18:32:56,840 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.self.value.weight
2022-04-10 18:32:56,840 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.self.value.bias
2022-04-10 18:32:56,840 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.output.dense.weight
2022-04-10 18:32:56,840 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.output.dense.bias
2022-04-10 18:32:56,840 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.output.LayerNorm.weight
2022-04-10 18:32:56,840 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.output.LayerNorm.bias
2022-04-10 18:32:56,840 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.intermediate.dense.weight
2022-04-10 18:32:56,840 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.intermediate.dense.bias
2022-04-10 18:32:56,840 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.output.dense.weight
2022-04-10 18:32:56,840 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.output.dense.bias
2022-04-10 18:32:56,840 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.output.LayerNorm.weight
2022-04-10 18:32:56,841 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.output.LayerNorm.bias
2022-04-10 18:32:56,841 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.self.query.weight
2022-04-10 18:32:56,841 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.self.query.bias
2022-04-10 18:32:56,841 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.self.key.weight
2022-04-10 18:32:56,841 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.self.key.bias
2022-04-10 18:32:56,841 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.self.value.weight
2022-04-10 18:32:56,841 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.self.value.bias
2022-04-10 18:32:56,841 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.output.dense.weight
2022-04-10 18:32:56,841 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.output.dense.bias
2022-04-10 18:32:56,841 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.output.LayerNorm.weight
2022-04-10 18:32:56,841 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.output.LayerNorm.bias
2022-04-10 18:32:56,841 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.intermediate.dense.weight
2022-04-10 18:32:56,841 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.intermediate.dense.bias
2022-04-10 18:32:56,842 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.output.dense.weight
2022-04-10 18:32:56,842 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.output.dense.bias
2022-04-10 18:32:56,842 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.output.LayerNorm.weight
2022-04-10 18:32:56,842 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.output.LayerNorm.bias
2022-04-10 18:32:56,842 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.self.query.weight
2022-04-10 18:32:56,842 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.self.query.bias
2022-04-10 18:32:56,842 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.self.key.weight
2022-04-10 18:32:56,842 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.self.key.bias
2022-04-10 18:32:56,842 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.self.value.weight
2022-04-10 18:32:56,842 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.self.value.bias
2022-04-10 18:32:56,842 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.output.dense.weight
2022-04-10 18:32:56,842 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.output.dense.bias
2022-04-10 18:32:56,842 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.output.LayerNorm.weight
2022-04-10 18:32:56,843 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.output.LayerNorm.bias
2022-04-10 18:32:56,843 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.intermediate.dense.weight
2022-04-10 18:32:56,843 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.intermediate.dense.bias
2022-04-10 18:32:56,843 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.output.dense.weight
2022-04-10 18:32:56,843 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.output.dense.bias
2022-04-10 18:32:56,843 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.output.LayerNorm.weight
2022-04-10 18:32:56,843 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.output.LayerNorm.bias
2022-04-10 18:32:56,843 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.self.query.weight
2022-04-10 18:32:56,843 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.self.query.bias
2022-04-10 18:32:56,843 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.self.key.weight
2022-04-10 18:32:56,843 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.self.key.bias
2022-04-10 18:32:56,843 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.self.value.weight
2022-04-10 18:32:56,844 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.self.value.bias
2022-04-10 18:32:56,844 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.output.dense.weight
2022-04-10 18:32:56,844 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.output.dense.bias
2022-04-10 18:32:56,844 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.output.LayerNorm.weight
2022-04-10 18:32:56,844 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.output.LayerNorm.bias
2022-04-10 18:32:56,844 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.intermediate.dense.weight
2022-04-10 18:32:56,844 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.intermediate.dense.bias
2022-04-10 18:32:56,844 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.output.dense.weight
2022-04-10 18:32:56,844 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.output.dense.bias
2022-04-10 18:32:56,844 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.output.LayerNorm.weight
2022-04-10 18:32:56,844 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.output.LayerNorm.bias
2022-04-10 18:32:56,844 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.self.query.weight
2022-04-10 18:32:56,844 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.self.query.bias
2022-04-10 18:32:56,845 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.self.key.weight
2022-04-10 18:32:56,845 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.self.key.bias
2022-04-10 18:32:56,845 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.self.value.weight
2022-04-10 18:32:56,845 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.self.value.bias
2022-04-10 18:32:56,845 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.output.dense.weight
2022-04-10 18:32:56,845 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.output.dense.bias
2022-04-10 18:32:56,845 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.output.LayerNorm.weight
2022-04-10 18:32:56,845 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.output.LayerNorm.bias
2022-04-10 18:32:56,845 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.intermediate.dense.weight
2022-04-10 18:32:56,845 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.intermediate.dense.bias
2022-04-10 18:32:56,845 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.output.dense.weight
2022-04-10 18:32:56,846 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.output.dense.bias
2022-04-10 18:32:56,846 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.output.LayerNorm.weight
2022-04-10 18:32:56,846 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.output.LayerNorm.bias
2022-04-10 18:32:56,846 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.self.query.weight
2022-04-10 18:32:56,846 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.self.query.bias
2022-04-10 18:32:56,846 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.self.key.weight
2022-04-10 18:32:56,846 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.self.key.bias
2022-04-10 18:32:56,846 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.self.value.weight
2022-04-10 18:32:56,846 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.self.value.bias
2022-04-10 18:32:56,846 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.output.dense.weight
2022-04-10 18:32:56,846 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.output.dense.bias
2022-04-10 18:32:56,846 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.output.LayerNorm.weight
2022-04-10 18:32:56,847 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.output.LayerNorm.bias
2022-04-10 18:32:56,847 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.intermediate.dense.weight
2022-04-10 18:32:56,847 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.intermediate.dense.bias
2022-04-10 18:32:56,847 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.output.dense.weight
2022-04-10 18:32:56,847 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.output.dense.bias
2022-04-10 18:32:56,847 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.output.LayerNorm.weight
2022-04-10 18:32:56,847 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.output.LayerNorm.bias
2022-04-10 18:32:56,847 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.self.query.weight
2022-04-10 18:32:56,847 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.self.query.bias
2022-04-10 18:32:56,847 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.self.key.weight
2022-04-10 18:32:56,847 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.self.key.bias
2022-04-10 18:32:56,847 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.self.value.weight
2022-04-10 18:32:56,847 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.self.value.bias
2022-04-10 18:32:56,848 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.output.dense.weight
2022-04-10 18:32:56,848 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.output.dense.bias
2022-04-10 18:32:56,848 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.output.LayerNorm.weight
2022-04-10 18:32:56,848 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.output.LayerNorm.bias
2022-04-10 18:32:56,848 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.intermediate.dense.weight
2022-04-10 18:32:56,848 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.intermediate.dense.bias
2022-04-10 18:32:56,848 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.output.dense.weight
2022-04-10 18:32:56,848 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.output.dense.bias
2022-04-10 18:32:56,848 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.output.LayerNorm.weight
2022-04-10 18:32:56,848 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.output.LayerNorm.bias
2022-04-10 18:32:56,848 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.self.query.weight
2022-04-10 18:32:56,849 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.self.query.bias
2022-04-10 18:32:56,849 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.self.key.weight
2022-04-10 18:32:56,849 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.self.key.bias
2022-04-10 18:32:56,849 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.self.value.weight
2022-04-10 18:32:56,849 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.self.value.bias
2022-04-10 18:32:56,849 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.output.dense.weight
2022-04-10 18:32:56,849 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.output.dense.bias
2022-04-10 18:32:56,849 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.output.LayerNorm.weight
2022-04-10 18:32:56,849 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.output.LayerNorm.bias
2022-04-10 18:32:56,849 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.intermediate.dense.weight
2022-04-10 18:32:56,849 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.intermediate.dense.bias
2022-04-10 18:32:56,849 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.output.dense.weight
2022-04-10 18:32:56,850 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.output.dense.bias
2022-04-10 18:32:56,850 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.output.LayerNorm.weight
2022-04-10 18:32:56,850 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.output.LayerNorm.bias
2022-04-10 18:32:56,850 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.self.query.weight
2022-04-10 18:32:56,850 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.self.query.bias
2022-04-10 18:32:56,850 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.self.key.weight
2022-04-10 18:32:56,850 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.self.key.bias
2022-04-10 18:32:56,850 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.self.value.weight
2022-04-10 18:32:56,850 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.self.value.bias
2022-04-10 18:32:56,850 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.output.dense.weight
2022-04-10 18:32:56,850 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.output.dense.bias
2022-04-10 18:32:56,850 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.output.LayerNorm.weight
2022-04-10 18:32:56,850 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.output.LayerNorm.bias
2022-04-10 18:32:56,851 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.intermediate.dense.weight
2022-04-10 18:32:56,851 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.intermediate.dense.bias
2022-04-10 18:32:56,851 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.output.dense.weight
2022-04-10 18:32:56,851 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.output.dense.bias
2022-04-10 18:32:56,851 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.output.LayerNorm.weight
2022-04-10 18:32:56,851 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.output.LayerNorm.bias
2022-04-10 18:32:56,851 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.self.query.weight
2022-04-10 18:32:56,851 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.self.query.bias
2022-04-10 18:32:56,851 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.self.key.weight
2022-04-10 18:32:56,851 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.self.key.bias
2022-04-10 18:32:56,851 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.self.value.weight
2022-04-10 18:32:56,851 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.self.value.bias
2022-04-10 18:32:56,851 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.output.dense.weight
2022-04-10 18:32:56,852 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.output.dense.bias
2022-04-10 18:32:56,852 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.output.LayerNorm.weight
2022-04-10 18:32:56,852 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.output.LayerNorm.bias
2022-04-10 18:32:56,852 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.intermediate.dense.weight
2022-04-10 18:32:56,852 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.intermediate.dense.bias
2022-04-10 18:32:56,852 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.output.dense.weight
2022-04-10 18:32:56,852 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.output.dense.bias
2022-04-10 18:32:56,852 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.output.LayerNorm.weight
2022-04-10 18:32:56,852 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.output.LayerNorm.bias
2022-04-10 18:32:56,852 - INFO - allennlp.common.util - _classifier.roberta.pooler.dense.weight
2022-04-10 18:32:56,852 - INFO - allennlp.common.util - _classifier.roberta.pooler.dense.bias
2022-04-10 18:32:56,852 - INFO - allennlp.common.util - _classifier.classifier.dense.weight
2022-04-10 18:32:56,852 - INFO - allennlp.common.util - _classifier.classifier.dense.bias
2022-04-10 18:32:56,853 - INFO - allennlp.common.util - _classifier.classifier.out_proj.weight
2022-04-10 18:32:56,853 - INFO - allennlp.common.util - _classifier.classifier.out_proj.bias
2022-04-10 18:32:56,853 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.type = slanted_triangular
2022-04-10 18:32:56,853 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.cut_frac = 0.06
2022-04-10 18:32:56,853 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.ratio = 32
2022-04-10 18:32:56,853 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.last_epoch = -1
2022-04-10 18:32:56,853 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.gradual_unfreezing = False
2022-04-10 18:32:56,853 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.discriminative_fine_tuning = False
2022-04-10 18:32:56,854 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.decay_factor = 0.38
2022-04-10 18:32:56,854 - INFO - allennlp.common.params - type = default
2022-04-10 18:32:56,854 - INFO - allennlp.common.params - keep_serialized_model_every_num_seconds = None
2022-04-10 18:32:56,854 - INFO - allennlp.common.params - num_serialized_models_to_keep = 2
2022-04-10 18:32:56,854 - INFO - allennlp.common.params - model_save_interval = None
2022-04-10 18:32:56,854 - INFO - allennlp.common.params - summary_interval = 100
2022-04-10 18:32:56,854 - INFO - allennlp.common.params - histogram_interval = None
2022-04-10 18:32:56,854 - INFO - allennlp.common.params - batch_size_interval = None
2022-04-10 18:32:56,855 - INFO - allennlp.common.params - should_log_parameter_statistics = True
2022-04-10 18:32:56,855 - INFO - allennlp.common.params - should_log_learning_rate = False
2022-04-10 18:32:56,855 - INFO - allennlp.common.params - get_batch_num_total = None
2022-04-10 18:32:56,857 - WARNING - allennlp.training.trainer - You provided a validation dataset but patience was set to None, meaning that early stopping is disabled
2022-04-10 18:32:56,950 - INFO - allennlp.training.trainer - Beginning training.
2022-04-10 18:32:56,950 - INFO - allennlp.training.trainer - Epoch 0/14
2022-04-10 18:32:56,950 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-10 18:32:56,950 - INFO - allennlp.training.trainer - GPU 0 memory usage: 1.3G
2022-04-10 18:32:56,951 - INFO - allennlp.training.trainer - Training
2022-04-10 18:32:56,952 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-10 18:32:56,952 - INFO - allennlp.data.samplers.bucket_batch_sampler - No sorting keys given; trying to guess a good one
2022-04-10 18:32:56,952 - INFO - allennlp.data.samplers.bucket_batch_sampler - Using ['tokens'] as the sorting keys
2022-04-10 18:33:07,118 - INFO - tqdm - accuracy: 0.4688, batch_loss: 1.5490, loss: 1.5490 ||:   2%|1         | 1/65 [00:10<10:50, 10.17s/it]
2022-04-10 18:33:17,326 - INFO - tqdm - accuracy: 0.5156, batch_loss: 1.5697, loss: 1.5594 ||:   3%|3         | 2/65 [00:20<10:42, 10.19s/it]
2022-04-10 18:33:27,675 - INFO - tqdm - accuracy: 0.5208, batch_loss: 2.0361, loss: 1.7183 ||:   5%|4         | 3/65 [00:30<10:36, 10.26s/it]
2022-04-10 18:33:38,155 - INFO - tqdm - accuracy: 0.5469, batch_loss: 1.3760, loss: 1.6327 ||:   6%|6         | 4/65 [00:41<10:31, 10.35s/it]
2022-04-10 18:33:48,722 - INFO - tqdm - accuracy: 0.5312, batch_loss: 1.2925, loss: 1.5647 ||:   8%|7         | 5/65 [00:51<10:25, 10.43s/it]
2022-04-10 18:33:59,421 - INFO - tqdm - accuracy: 0.5208, batch_loss: 1.8016, loss: 1.6042 ||:   9%|9         | 6/65 [01:02<10:20, 10.52s/it]
2022-04-10 18:34:10,275 - INFO - tqdm - accuracy: 0.5223, batch_loss: 1.5824, loss: 1.6011 ||:  11%|#         | 7/65 [01:13<10:16, 10.63s/it]
2022-04-10 18:34:21,283 - INFO - tqdm - accuracy: 0.5117, batch_loss: 1.6985, loss: 1.6132 ||:  12%|#2        | 8/65 [01:24<10:12, 10.75s/it]
2022-04-10 18:34:32,469 - INFO - tqdm - accuracy: 0.5035, batch_loss: 1.9615, loss: 1.6519 ||:  14%|#3        | 9/65 [01:35<10:09, 10.89s/it]
2022-04-10 18:34:43,747 - INFO - tqdm - accuracy: 0.4938, batch_loss: 1.7884, loss: 1.6656 ||:  15%|#5        | 10/65 [01:46<10:05, 11.01s/it]
2022-04-10 18:34:55,382 - INFO - tqdm - accuracy: 0.4972, batch_loss: 1.6963, loss: 1.6684 ||:  17%|#6        | 11/65 [01:58<10:04, 11.20s/it]
2022-04-10 18:35:07,187 - INFO - tqdm - accuracy: 0.5052, batch_loss: 1.2639, loss: 1.6347 ||:  18%|#8        | 12/65 [02:10<10:03, 11.38s/it]
2022-04-10 18:35:18,756 - INFO - tqdm - accuracy: 0.5144, batch_loss: 1.2499, loss: 1.6051 ||:  20%|##        | 13/65 [02:21<09:54, 11.44s/it]
2022-04-10 18:35:30,029 - INFO - tqdm - accuracy: 0.5112, batch_loss: 1.9052, loss: 1.6265 ||:  22%|##1       | 14/65 [02:33<09:40, 11.39s/it]
2022-04-10 18:35:41,411 - INFO - tqdm - accuracy: 0.5167, batch_loss: 1.5537, loss: 1.6216 ||:  23%|##3       | 15/65 [02:44<09:29, 11.39s/it]
2022-04-10 18:35:52,841 - INFO - tqdm - accuracy: 0.5137, batch_loss: 1.5479, loss: 1.6170 ||:  25%|##4       | 16/65 [02:55<09:18, 11.40s/it]
2022-04-10 18:36:04,428 - INFO - tqdm - accuracy: 0.5202, batch_loss: 0.9483, loss: 1.5777 ||:  26%|##6       | 17/65 [03:07<09:09, 11.46s/it]
2022-04-10 18:36:16,056 - INFO - tqdm - accuracy: 0.5243, batch_loss: 1.1897, loss: 1.5562 ||:  28%|##7       | 18/65 [03:19<09:00, 11.51s/it]
2022-04-10 18:36:27,643 - INFO - tqdm - accuracy: 0.5214, batch_loss: 1.3161, loss: 1.5435 ||:  29%|##9       | 19/65 [03:30<08:50, 11.53s/it]
2022-04-10 18:36:38,923 - INFO - tqdm - accuracy: 0.5203, batch_loss: 1.2758, loss: 1.5301 ||:  31%|###       | 20/65 [03:41<08:35, 11.46s/it]
2022-04-10 18:36:50,418 - INFO - tqdm - accuracy: 0.5149, batch_loss: 2.1311, loss: 1.5588 ||:  32%|###2      | 21/65 [03:53<08:24, 11.47s/it]
2022-04-10 18:37:01,851 - INFO - tqdm - accuracy: 0.5128, batch_loss: 1.4495, loss: 1.5538 ||:  34%|###3      | 22/65 [04:04<08:12, 11.46s/it]
2022-04-10 18:37:13,397 - INFO - tqdm - accuracy: 0.5177, batch_loss: 1.4719, loss: 1.5502 ||:  35%|###5      | 23/65 [04:16<08:02, 11.48s/it]
2022-04-10 18:37:24,961 - INFO - tqdm - accuracy: 0.5195, batch_loss: 1.4451, loss: 1.5458 ||:  37%|###6      | 24/65 [04:28<07:51, 11.51s/it]
2022-04-10 18:37:36,541 - INFO - tqdm - accuracy: 0.5175, batch_loss: 1.6103, loss: 1.5484 ||:  38%|###8      | 25/65 [04:39<07:41, 11.53s/it]
2022-04-10 18:37:48,135 - INFO - tqdm - accuracy: 0.5108, batch_loss: 1.4838, loss: 1.5459 ||:  40%|####      | 26/65 [04:51<07:30, 11.55s/it]
2022-04-10 18:37:59,743 - INFO - tqdm - accuracy: 0.5093, batch_loss: 1.3587, loss: 1.5390 ||:  42%|####1     | 27/65 [05:02<07:19, 11.57s/it]
2022-04-10 18:38:11,342 - INFO - tqdm - accuracy: 0.5112, batch_loss: 1.1120, loss: 1.5238 ||:  43%|####3     | 28/65 [05:14<07:08, 11.58s/it]
2022-04-10 18:38:22,950 - INFO - tqdm - accuracy: 0.5075, batch_loss: 1.8381, loss: 1.5346 ||:  45%|####4     | 29/65 [05:25<06:57, 11.59s/it]
2022-04-10 18:38:34,550 - INFO - tqdm - accuracy: 0.5073, batch_loss: 1.9155, loss: 1.5473 ||:  46%|####6     | 30/65 [05:37<06:45, 11.59s/it]
2022-04-10 18:38:46,140 - INFO - tqdm - accuracy: 0.5111, batch_loss: 0.9911, loss: 1.5293 ||:  48%|####7     | 31/65 [05:49<06:34, 11.59s/it]
2022-04-10 18:38:57,709 - INFO - tqdm - accuracy: 0.5059, batch_loss: 2.1655, loss: 1.5492 ||:  49%|####9     | 32/65 [06:00<06:22, 11.58s/it]
2022-04-10 18:39:09,263 - INFO - tqdm - accuracy: 0.5085, batch_loss: 0.8722, loss: 1.5287 ||:  51%|#####     | 33/65 [06:12<06:10, 11.57s/it]
2022-04-10 18:39:20,835 - INFO - tqdm - accuracy: 0.5055, batch_loss: 1.5865, loss: 1.5304 ||:  52%|#####2    | 34/65 [06:23<05:58, 11.57s/it]
2022-04-10 18:39:32,384 - INFO - tqdm - accuracy: 0.5036, batch_loss: 1.5259, loss: 1.5303 ||:  54%|#####3    | 35/65 [06:35<05:46, 11.57s/it]
2022-04-10 18:39:43,945 - INFO - tqdm - accuracy: 0.5035, batch_loss: 1.3754, loss: 1.5260 ||:  55%|#####5    | 36/65 [06:46<05:35, 11.56s/it]
2022-04-10 18:39:55,499 - INFO - tqdm - accuracy: 0.5008, batch_loss: 1.2841, loss: 1.5194 ||:  57%|#####6    | 37/65 [06:58<05:23, 11.56s/it]
2022-04-10 18:40:07,039 - INFO - tqdm - accuracy: 0.5016, batch_loss: 0.9762, loss: 1.5051 ||:  58%|#####8    | 38/65 [07:10<05:11, 11.56s/it]
2022-04-10 18:40:18,576 - INFO - tqdm - accuracy: 0.4984, batch_loss: 1.4298, loss: 1.5032 ||:  60%|######    | 39/65 [07:21<05:00, 11.55s/it]
2022-04-10 18:40:30,100 - INFO - tqdm - accuracy: 0.4984, batch_loss: 1.0630, loss: 1.4922 ||:  62%|######1   | 40/65 [07:33<04:48, 11.54s/it]
2022-04-10 18:40:41,626 - INFO - tqdm - accuracy: 0.4992, batch_loss: 1.1168, loss: 1.4831 ||:  63%|######3   | 41/65 [07:44<04:36, 11.54s/it]
2022-04-10 18:40:53,163 - INFO - tqdm - accuracy: 0.5022, batch_loss: 0.8496, loss: 1.4680 ||:  65%|######4   | 42/65 [07:56<04:25, 11.54s/it]
2022-04-10 18:41:04,685 - INFO - tqdm - accuracy: 0.5015, batch_loss: 1.1931, loss: 1.4616 ||:  66%|######6   | 43/65 [08:07<04:13, 11.53s/it]
2022-04-10 18:41:16,200 - INFO - tqdm - accuracy: 0.5036, batch_loss: 0.9254, loss: 1.4494 ||:  68%|######7   | 44/65 [08:19<04:02, 11.53s/it]
2022-04-10 18:41:27,716 - INFO - tqdm - accuracy: 0.5062, batch_loss: 1.0250, loss: 1.4400 ||:  69%|######9   | 45/65 [08:30<03:50, 11.52s/it]
2022-04-10 18:41:39,244 - INFO - tqdm - accuracy: 0.5068, batch_loss: 0.9775, loss: 1.4299 ||:  71%|#######   | 46/65 [08:42<03:38, 11.53s/it]
2022-04-10 18:41:50,765 - INFO - tqdm - accuracy: 0.5073, batch_loss: 1.1213, loss: 1.4233 ||:  72%|#######2  | 47/65 [08:53<03:27, 11.52s/it]
2022-04-10 18:42:02,321 - INFO - tqdm - accuracy: 0.5026, batch_loss: 1.1503, loss: 1.4177 ||:  74%|#######3  | 48/65 [09:05<03:16, 11.53s/it]
2022-04-10 18:42:13,848 - INFO - tqdm - accuracy: 0.5013, batch_loss: 1.1710, loss: 1.4126 ||:  75%|#######5  | 49/65 [09:16<03:04, 11.53s/it]
2022-04-10 18:42:25,374 - INFO - tqdm - accuracy: 0.5038, batch_loss: 0.7193, loss: 1.3988 ||:  77%|#######6  | 50/65 [09:28<02:52, 11.53s/it]
2022-04-10 18:42:36,919 - INFO - tqdm - accuracy: 0.5055, batch_loss: 0.9271, loss: 1.3895 ||:  78%|#######8  | 51/65 [09:39<02:41, 11.53s/it]
2022-04-10 18:42:48,462 - INFO - tqdm - accuracy: 0.5048, batch_loss: 0.8064, loss: 1.3783 ||:  80%|########  | 52/65 [09:51<02:29, 11.54s/it]
2022-04-10 18:43:00,004 - INFO - tqdm - accuracy: 0.5041, batch_loss: 1.0823, loss: 1.3727 ||:  82%|########1 | 53/65 [10:03<02:18, 11.54s/it]
2022-04-10 18:43:11,208 - INFO - tqdm - accuracy: 0.5078, batch_loss: 0.7134, loss: 1.3605 ||:  83%|########3 | 54/65 [10:14<02:05, 11.44s/it]
2022-04-10 18:43:22,731 - INFO - tqdm - accuracy: 0.5082, batch_loss: 0.8735, loss: 1.3516 ||:  85%|########4 | 55/65 [10:25<01:54, 11.46s/it]
2022-04-10 18:43:34,253 - INFO - tqdm - accuracy: 0.5075, batch_loss: 0.8230, loss: 1.3422 ||:  86%|########6 | 56/65 [10:37<01:43, 11.48s/it]
2022-04-10 18:43:45,771 - INFO - tqdm - accuracy: 0.5085, batch_loss: 0.7503, loss: 1.3318 ||:  88%|########7 | 57/65 [10:48<01:31, 11.49s/it]
2022-04-10 18:43:57,318 - INFO - tqdm - accuracy: 0.5100, batch_loss: 0.8488, loss: 1.3235 ||:  89%|########9 | 58/65 [11:00<01:20, 11.51s/it]
2022-04-10 18:44:08,854 - INFO - tqdm - accuracy: 0.5130, batch_loss: 0.7682, loss: 1.3141 ||:  91%|######### | 59/65 [11:11<01:09, 11.52s/it]
2022-04-10 18:44:20,391 - INFO - tqdm - accuracy: 0.5122, batch_loss: 0.9017, loss: 1.3072 ||:  92%|#########2| 60/65 [11:23<00:57, 11.52s/it]
2022-04-10 18:44:31,934 - INFO - tqdm - accuracy: 0.5146, batch_loss: 0.7221, loss: 1.2976 ||:  94%|#########3| 61/65 [11:34<00:46, 11.53s/it]
2022-04-10 18:44:43,460 - INFO - tqdm - accuracy: 0.5154, batch_loss: 0.9196, loss: 1.2915 ||:  95%|#########5| 62/65 [11:46<00:34, 11.53s/it]
2022-04-10 18:44:55,002 - INFO - tqdm - accuracy: 0.5166, batch_loss: 0.7553, loss: 1.2830 ||:  97%|#########6| 63/65 [11:58<00:23, 11.53s/it]
2022-04-10 18:45:06,539 - INFO - tqdm - accuracy: 0.5159, batch_loss: 0.9522, loss: 1.2778 ||:  98%|#########8| 64/65 [12:09<00:11, 11.53s/it]
2022-04-10 18:45:11,668 - INFO - tqdm - accuracy: 0.5172, batch_loss: 1.1544, loss: 1.2759 ||: 100%|##########| 65/65 [12:14<00:00,  9.61s/it]
2022-04-10 18:45:11,668 - INFO - tqdm - accuracy: 0.5172, batch_loss: 1.1544, loss: 1.2759 ||: 100%|##########| 65/65 [12:14<00:00, 11.30s/it]
2022-04-10 18:45:14,560 - INFO - allennlp.training.trainer - Validating
2022-04-10 18:45:14,562 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-10 18:45:14,562 - INFO - allennlp.data.samplers.bucket_batch_sampler - No sorting keys given; trying to guess a good one
2022-04-10 18:45:14,563 - INFO - allennlp.data.samplers.bucket_batch_sampler - Using ['tokens'] as the sorting keys
2022-04-10 18:45:24,670 - INFO - tqdm - accuracy: 0.4750, batch_loss: 0.2599, loss: 1.0398 ||:  35%|###4      | 40/115 [00:10<00:19,  3.92it/s]
2022-04-10 18:45:34,806 - INFO - tqdm - accuracy: 0.5437, batch_loss: 0.6036, loss: 0.9466 ||:  70%|######9   | 80/115 [00:20<00:08,  3.95it/s]
2022-04-10 18:45:43,732 - INFO - tqdm - accuracy: 0.5590, batch_loss: 1.1558, loss: 0.9121 ||: 100%|##########| 115/115 [00:29<00:00,  3.87it/s]
2022-04-10 18:45:43,732 - INFO - tqdm - accuracy: 0.5590, batch_loss: 1.1558, loss: 0.9121 ||: 100%|##########| 115/115 [00:29<00:00,  3.94it/s]
2022-04-10 18:45:43,732 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-10 18:45:43,734 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.517  |     0.559
2022-04-10 18:45:43,734 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  1359.606  |       N/A
2022-04-10 18:45:43,735 - INFO - allennlp.training.tensorboard_writer - loss               |     1.276  |     0.912
2022-04-10 18:45:43,735 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5811.043  |       N/A
2022-04-10 18:45:49,494 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to './output_ir-ora-d_hyper5/best.th'.
2022-04-10 18:45:50,800 - INFO - allennlp.training.trainer - Epoch duration: 0:12:53.850493
2022-04-10 18:45:50,801 - INFO - allennlp.training.trainer - Estimated training time remaining: 3:00:33
2022-04-10 18:45:50,801 - INFO - allennlp.training.trainer - Epoch 1/14
2022-04-10 18:45:50,801 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-10 18:45:50,801 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-10 18:45:50,803 - INFO - allennlp.training.trainer - Training
2022-04-10 18:45:50,803 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-10 18:46:02,166 - INFO - tqdm - accuracy: 0.5312, batch_loss: 0.6999, loss: 0.6999 ||:   2%|1         | 1/65 [00:11<12:07, 11.36s/it]
2022-04-10 18:46:13,771 - INFO - tqdm - accuracy: 0.6250, batch_loss: 0.6009, loss: 0.6504 ||:   3%|3         | 2/65 [00:22<12:04, 11.51s/it]
2022-04-10 18:46:25,623 - INFO - tqdm - accuracy: 0.6354, batch_loss: 0.9314, loss: 0.7441 ||:   5%|4         | 3/65 [00:34<12:03, 11.66s/it]
2022-04-10 18:46:37,302 - INFO - tqdm - accuracy: 0.6094, batch_loss: 0.7250, loss: 0.7393 ||:   6%|6         | 4/65 [00:46<11:51, 11.67s/it]
2022-04-10 18:46:48,787 - INFO - tqdm - accuracy: 0.5938, batch_loss: 0.7759, loss: 0.7466 ||:   8%|7         | 5/65 [00:57<11:36, 11.60s/it]
2022-04-10 18:47:00,246 - INFO - tqdm - accuracy: 0.5990, batch_loss: 0.7732, loss: 0.7511 ||:   9%|9         | 6/65 [01:09<11:21, 11.55s/it]
2022-04-10 18:47:11,758 - INFO - tqdm - accuracy: 0.6116, batch_loss: 0.6169, loss: 0.7319 ||:  11%|#         | 7/65 [01:20<11:09, 11.54s/it]
2022-04-10 18:47:23,339 - INFO - tqdm - accuracy: 0.6094, batch_loss: 0.6161, loss: 0.7174 ||:  12%|#2        | 8/65 [01:32<10:58, 11.55s/it]
2022-04-10 18:47:34,950 - INFO - tqdm - accuracy: 0.6181, batch_loss: 0.5831, loss: 0.7025 ||:  14%|#3        | 9/65 [01:44<10:47, 11.57s/it]
2022-04-10 18:47:46,536 - INFO - tqdm - accuracy: 0.6125, batch_loss: 0.7051, loss: 0.7028 ||:  15%|#5        | 10/65 [01:55<10:36, 11.58s/it]
2022-04-10 18:47:58,112 - INFO - tqdm - accuracy: 0.6051, batch_loss: 0.8491, loss: 0.7161 ||:  17%|#6        | 11/65 [02:07<10:25, 11.58s/it]
2022-04-10 18:48:09,650 - INFO - tqdm - accuracy: 0.6042, batch_loss: 0.6835, loss: 0.7133 ||:  18%|#8        | 12/65 [02:18<10:12, 11.56s/it]
2022-04-10 18:48:21,213 - INFO - tqdm - accuracy: 0.5889, batch_loss: 0.9245, loss: 0.7296 ||:  20%|##        | 13/65 [02:30<10:01, 11.56s/it]
2022-04-10 18:48:32,626 - INFO - tqdm - accuracy: 0.5781, batch_loss: 0.7639, loss: 0.7320 ||:  22%|##1       | 14/65 [02:41<09:47, 11.52s/it]
2022-04-10 18:48:44,154 - INFO - tqdm - accuracy: 0.5833, batch_loss: 0.6764, loss: 0.7283 ||:  23%|##3       | 15/65 [02:53<09:36, 11.52s/it]
2022-04-10 18:48:55,667 - INFO - tqdm - accuracy: 0.5801, batch_loss: 0.8341, loss: 0.7349 ||:  25%|##4       | 16/65 [03:04<09:24, 11.52s/it]
2022-04-10 18:49:07,175 - INFO - tqdm - accuracy: 0.5662, batch_loss: 0.9047, loss: 0.7449 ||:  26%|##6       | 17/65 [03:16<09:12, 11.52s/it]
2022-04-10 18:49:18,685 - INFO - tqdm - accuracy: 0.5642, batch_loss: 0.6539, loss: 0.7399 ||:  28%|##7       | 18/65 [03:27<09:01, 11.51s/it]
2022-04-10 18:49:30,191 - INFO - tqdm - accuracy: 0.5625, batch_loss: 0.6596, loss: 0.7356 ||:  29%|##9       | 19/65 [03:39<08:49, 11.51s/it]
2022-04-10 18:49:41,719 - INFO - tqdm - accuracy: 0.5656, batch_loss: 0.6236, loss: 0.7300 ||:  31%|###       | 20/65 [03:50<08:38, 11.52s/it]
2022-04-10 18:49:53,258 - INFO - tqdm - accuracy: 0.5729, batch_loss: 0.5983, loss: 0.7238 ||:  32%|###2      | 21/65 [04:02<08:27, 11.52s/it]
2022-04-10 18:50:04,700 - INFO - tqdm - accuracy: 0.5739, batch_loss: 0.7049, loss: 0.7229 ||:  34%|###3      | 22/65 [04:13<08:14, 11.50s/it]
2022-04-10 18:50:16,307 - INFO - tqdm - accuracy: 0.5747, batch_loss: 0.7769, loss: 0.7253 ||:  35%|###5      | 23/65 [04:25<08:04, 11.53s/it]
2022-04-10 18:50:27,917 - INFO - tqdm - accuracy: 0.5768, batch_loss: 0.6964, loss: 0.7240 ||:  37%|###6      | 24/65 [04:37<07:53, 11.55s/it]
2022-04-10 18:50:39,528 - INFO - tqdm - accuracy: 0.5837, batch_loss: 0.6056, loss: 0.7193 ||:  38%|###8      | 25/65 [04:48<07:42, 11.57s/it]
2022-04-10 18:50:51,118 - INFO - tqdm - accuracy: 0.5853, batch_loss: 0.7059, loss: 0.7188 ||:  40%|####      | 26/65 [05:00<07:31, 11.58s/it]
2022-04-10 18:51:02,682 - INFO - tqdm - accuracy: 0.5845, batch_loss: 0.6735, loss: 0.7171 ||:  42%|####1     | 27/65 [05:11<07:19, 11.57s/it]
2022-04-10 18:51:14,214 - INFO - tqdm - accuracy: 0.5815, batch_loss: 0.7558, loss: 0.7185 ||:  43%|####3     | 28/65 [05:23<07:07, 11.56s/it]
2022-04-10 18:51:25,714 - INFO - tqdm - accuracy: 0.5797, batch_loss: 0.6763, loss: 0.7170 ||:  45%|####4     | 29/65 [05:34<06:55, 11.54s/it]
2022-04-10 18:51:37,211 - INFO - tqdm - accuracy: 0.5771, batch_loss: 0.7128, loss: 0.7169 ||:  46%|####6     | 30/65 [05:46<06:43, 11.53s/it]
2022-04-10 18:51:48,729 - INFO - tqdm - accuracy: 0.5736, batch_loss: 0.7172, loss: 0.7169 ||:  48%|####7     | 31/65 [05:57<06:31, 11.53s/it]
2022-04-10 18:52:00,268 - INFO - tqdm - accuracy: 0.5723, batch_loss: 0.6697, loss: 0.7154 ||:  49%|####9     | 32/65 [06:09<06:20, 11.53s/it]
2022-04-10 18:52:11,808 - INFO - tqdm - accuracy: 0.5710, batch_loss: 0.7737, loss: 0.7172 ||:  51%|#####     | 33/65 [06:21<06:09, 11.53s/it]
2022-04-10 18:52:23,362 - INFO - tqdm - accuracy: 0.5680, batch_loss: 0.6841, loss: 0.7162 ||:  52%|#####2    | 34/65 [06:32<05:57, 11.54s/it]
2022-04-10 18:52:35,197 - INFO - tqdm - accuracy: 0.5634, batch_loss: 0.7647, loss: 0.7176 ||:  54%|#####3    | 35/65 [06:44<05:48, 11.63s/it]
2022-04-10 18:52:46,719 - INFO - tqdm - accuracy: 0.5651, batch_loss: 0.6964, loss: 0.7170 ||:  55%|#####5    | 36/65 [06:55<05:36, 11.60s/it]
2022-04-10 18:52:58,271 - INFO - tqdm - accuracy: 0.5642, batch_loss: 0.6272, loss: 0.7146 ||:  57%|#####6    | 37/65 [07:07<05:24, 11.58s/it]
2022-04-10 18:53:09,841 - INFO - tqdm - accuracy: 0.5617, batch_loss: 0.7640, loss: 0.7159 ||:  58%|#####8    | 38/65 [07:19<05:12, 11.58s/it]
2022-04-10 18:53:21,407 - INFO - tqdm - accuracy: 0.5593, batch_loss: 0.7114, loss: 0.7158 ||:  60%|######    | 39/65 [07:30<05:00, 11.58s/it]
2022-04-10 18:53:33,000 - INFO - tqdm - accuracy: 0.5617, batch_loss: 0.6317, loss: 0.7137 ||:  62%|######1   | 40/65 [07:42<04:49, 11.58s/it]
2022-04-10 18:53:44,610 - INFO - tqdm - accuracy: 0.5579, batch_loss: 0.6937, loss: 0.7132 ||:  63%|######3   | 41/65 [07:53<04:38, 11.59s/it]
2022-04-10 18:53:56,199 - INFO - tqdm - accuracy: 0.5625, batch_loss: 0.6126, loss: 0.7108 ||:  65%|######4   | 42/65 [08:05<04:26, 11.59s/it]
2022-04-10 18:54:07,783 - INFO - tqdm - accuracy: 0.5581, batch_loss: 0.7696, loss: 0.7122 ||:  66%|######6   | 43/65 [08:16<04:14, 11.59s/it]
2022-04-10 18:54:19,381 - INFO - tqdm - accuracy: 0.5568, batch_loss: 0.6623, loss: 0.7110 ||:  68%|######7   | 44/65 [08:28<04:03, 11.59s/it]
2022-04-10 18:54:30,980 - INFO - tqdm - accuracy: 0.5576, batch_loss: 0.6464, loss: 0.7096 ||:  69%|######9   | 45/65 [08:40<03:51, 11.59s/it]
2022-04-10 18:54:42,579 - INFO - tqdm - accuracy: 0.5598, batch_loss: 0.6270, loss: 0.7078 ||:  71%|#######   | 46/65 [08:51<03:40, 11.60s/it]
2022-04-10 18:54:54,136 - INFO - tqdm - accuracy: 0.5578, batch_loss: 0.7634, loss: 0.7090 ||:  72%|#######2  | 47/65 [09:03<03:28, 11.58s/it]
2022-04-10 18:55:05,718 - INFO - tqdm - accuracy: 0.5573, batch_loss: 0.8038, loss: 0.7110 ||:  74%|#######3  | 48/65 [09:14<03:16, 11.58s/it]
2022-04-10 18:55:17,250 - INFO - tqdm - accuracy: 0.5536, batch_loss: 0.8106, loss: 0.7130 ||:  75%|#######5  | 49/65 [09:26<03:05, 11.57s/it]
2022-04-10 18:55:28,778 - INFO - tqdm - accuracy: 0.5544, batch_loss: 0.7233, loss: 0.7132 ||:  77%|#######6  | 50/65 [09:37<02:53, 11.56s/it]
2022-04-10 18:55:40,305 - INFO - tqdm - accuracy: 0.5551, batch_loss: 0.7143, loss: 0.7132 ||:  78%|#######8  | 51/65 [09:49<02:41, 11.55s/it]
2022-04-10 18:55:51,853 - INFO - tqdm - accuracy: 0.5529, batch_loss: 0.7455, loss: 0.7138 ||:  80%|########  | 52/65 [10:01<02:30, 11.55s/it]
2022-04-10 18:56:03,375 - INFO - tqdm - accuracy: 0.5554, batch_loss: 0.6188, loss: 0.7120 ||:  82%|########1 | 53/65 [10:12<02:18, 11.54s/it]
2022-04-10 18:56:14,883 - INFO - tqdm - accuracy: 0.5567, batch_loss: 0.7452, loss: 0.7127 ||:  83%|########3 | 54/65 [10:24<02:06, 11.53s/it]
2022-04-10 18:56:26,389 - INFO - tqdm - accuracy: 0.5585, batch_loss: 0.6125, loss: 0.7108 ||:  85%|########4 | 55/65 [10:35<01:55, 11.52s/it]
2022-04-10 18:56:37,898 - INFO - tqdm - accuracy: 0.5580, batch_loss: 0.7043, loss: 0.7107 ||:  86%|########6 | 56/65 [10:47<01:43, 11.52s/it]
2022-04-10 18:56:49,159 - INFO - tqdm - accuracy: 0.5587, batch_loss: 0.6408, loss: 0.7095 ||:  88%|########7 | 57/65 [10:58<01:31, 11.44s/it]
2022-04-10 18:57:00,656 - INFO - tqdm - accuracy: 0.5566, batch_loss: 0.7138, loss: 0.7096 ||:  89%|########9 | 58/65 [11:09<01:20, 11.46s/it]
2022-04-10 18:57:12,163 - INFO - tqdm - accuracy: 0.5535, batch_loss: 0.7240, loss: 0.7098 ||:  91%|######### | 59/65 [11:21<01:08, 11.47s/it]
2022-04-10 18:57:23,684 - INFO - tqdm - accuracy: 0.5542, batch_loss: 0.7116, loss: 0.7098 ||:  92%|#########2| 60/65 [11:32<00:57, 11.49s/it]
2022-04-10 18:57:35,232 - INFO - tqdm - accuracy: 0.5564, batch_loss: 0.6383, loss: 0.7087 ||:  94%|#########3| 61/65 [11:44<00:46, 11.51s/it]
2022-04-10 18:57:46,809 - INFO - tqdm - accuracy: 0.5580, batch_loss: 0.6386, loss: 0.7075 ||:  95%|#########5| 62/65 [11:56<00:34, 11.53s/it]
2022-04-10 18:57:58,373 - INFO - tqdm - accuracy: 0.5590, batch_loss: 0.6775, loss: 0.7071 ||:  97%|#########6| 63/65 [12:07<00:23, 11.54s/it]
2022-04-10 18:58:09,613 - INFO - tqdm - accuracy: 0.5579, batch_loss: 0.7442, loss: 0.7076 ||:  98%|#########8| 64/65 [12:18<00:11, 11.45s/it]
2022-04-10 18:58:14,748 - INFO - tqdm - accuracy: 0.5560, batch_loss: 0.7734, loss: 0.7087 ||: 100%|##########| 65/65 [12:23<00:00,  9.55s/it]
2022-04-10 18:58:14,749 - INFO - tqdm - accuracy: 0.5560, batch_loss: 0.7734, loss: 0.7087 ||: 100%|##########| 65/65 [12:23<00:00, 11.45s/it]
2022-04-10 18:58:17,563 - INFO - allennlp.training.trainer - Validating
2022-04-10 18:58:17,564 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-10 18:58:27,648 - INFO - tqdm - accuracy: 0.6250, batch_loss: 0.5312, loss: 0.7356 ||:  35%|###4      | 40/115 [00:10<00:19,  3.90it/s]
2022-04-10 18:58:37,664 - INFO - tqdm - accuracy: 0.6013, batch_loss: 0.7494, loss: 0.7013 ||:  69%|######8   | 79/115 [00:20<00:09,  3.89it/s]
2022-04-10 18:58:46,794 - INFO - tqdm - accuracy: 0.5808, batch_loss: 0.9498, loss: 0.7179 ||: 100%|##########| 115/115 [00:29<00:00,  3.89it/s]
2022-04-10 18:58:46,794 - INFO - tqdm - accuracy: 0.5808, batch_loss: 0.9498, loss: 0.7179 ||: 100%|##########| 115/115 [00:29<00:00,  3.93it/s]
2022-04-10 18:58:46,795 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-10 18:58:46,795 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.556  |     0.581
2022-04-10 18:58:46,795 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-10 18:58:46,796 - INFO - allennlp.training.tensorboard_writer - loss               |     0.709  |     0.718
2022-04-10 18:58:46,797 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5811.043  |       N/A
2022-04-10 18:58:52,421 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to './output_ir-ora-d_hyper5/best.th'.
2022-04-10 18:59:04,246 - INFO - allennlp.training.trainer - Epoch duration: 0:13:13.445114
2022-04-10 18:59:04,246 - INFO - allennlp.training.trainer - Estimated training time remaining: 2:49:47
2022-04-10 18:59:04,246 - INFO - allennlp.training.trainer - Epoch 2/14
2022-04-10 18:59:04,246 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-10 18:59:04,247 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-10 18:59:04,248 - INFO - allennlp.training.trainer - Training
2022-04-10 18:59:04,248 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-10 18:59:15,493 - INFO - tqdm - accuracy: 0.4375, batch_loss: 0.7462, loss: 0.7462 ||:   2%|1         | 1/65 [00:11<11:59, 11.24s/it]
2022-04-10 18:59:27,082 - INFO - tqdm - accuracy: 0.4844, batch_loss: 0.7096, loss: 0.7279 ||:   3%|3         | 2/65 [00:22<12:01, 11.45s/it]
2022-04-10 18:59:38,991 - INFO - tqdm - accuracy: 0.4896, batch_loss: 0.6757, loss: 0.7105 ||:   5%|4         | 3/65 [00:34<12:02, 11.66s/it]
2022-04-10 18:59:50,890 - INFO - tqdm - accuracy: 0.5234, batch_loss: 0.6106, loss: 0.6855 ||:   6%|6         | 4/65 [00:46<11:56, 11.75s/it]
2022-04-10 19:00:02,390 - INFO - tqdm - accuracy: 0.5312, batch_loss: 0.6192, loss: 0.6722 ||:   8%|7         | 5/65 [00:58<11:39, 11.66s/it]
2022-04-10 19:00:13,673 - INFO - tqdm - accuracy: 0.5417, batch_loss: 0.6528, loss: 0.6690 ||:   9%|9         | 6/65 [01:09<11:20, 11.53s/it]
2022-04-10 19:00:25,079 - INFO - tqdm - accuracy: 0.5312, batch_loss: 0.7447, loss: 0.6798 ||:  11%|#         | 7/65 [01:20<11:06, 11.49s/it]
2022-04-10 19:00:36,645 - INFO - tqdm - accuracy: 0.5391, batch_loss: 0.6535, loss: 0.6765 ||:  12%|#2        | 8/65 [01:32<10:56, 11.52s/it]
2022-04-10 19:00:48,335 - INFO - tqdm - accuracy: 0.5451, batch_loss: 0.6440, loss: 0.6729 ||:  14%|#3        | 9/65 [01:44<10:47, 11.57s/it]
2022-04-10 19:00:59,969 - INFO - tqdm - accuracy: 0.5469, batch_loss: 0.6801, loss: 0.6736 ||:  15%|#5        | 10/65 [01:55<10:37, 11.59s/it]
2022-04-10 19:01:11,506 - INFO - tqdm - accuracy: 0.5369, batch_loss: 0.7189, loss: 0.6777 ||:  17%|#6        | 11/65 [02:07<10:24, 11.57s/it]
2022-04-10 19:01:23,003 - INFO - tqdm - accuracy: 0.5391, batch_loss: 0.6599, loss: 0.6763 ||:  18%|#8        | 12/65 [02:18<10:12, 11.55s/it]
2022-04-10 19:01:34,183 - INFO - tqdm - accuracy: 0.5446, batch_loss: 0.7196, loss: 0.6796 ||:  20%|##        | 13/65 [02:29<09:54, 11.44s/it]
2022-04-10 19:01:45,714 - INFO - tqdm - accuracy: 0.5481, batch_loss: 0.7011, loss: 0.6811 ||:  22%|##1       | 14/65 [02:41<09:44, 11.47s/it]
2022-04-10 19:01:57,254 - INFO - tqdm - accuracy: 0.5532, batch_loss: 0.8041, loss: 0.6893 ||:  23%|##3       | 15/65 [02:53<09:34, 11.49s/it]
2022-04-10 19:02:08,822 - INFO - tqdm - accuracy: 0.5538, batch_loss: 0.6454, loss: 0.6866 ||:  25%|##4       | 16/65 [03:04<09:24, 11.51s/it]
2022-04-10 19:02:20,398 - INFO - tqdm - accuracy: 0.5470, batch_loss: 0.7633, loss: 0.6911 ||:  26%|##6       | 17/65 [03:16<09:13, 11.53s/it]
2022-04-10 19:02:31,983 - INFO - tqdm - accuracy: 0.5409, batch_loss: 0.7016, loss: 0.6917 ||:  28%|##7       | 18/65 [03:27<09:02, 11.55s/it]
2022-04-10 19:02:43,577 - INFO - tqdm - accuracy: 0.5387, batch_loss: 0.6779, loss: 0.6909 ||:  29%|##9       | 19/65 [03:39<08:51, 11.56s/it]
2022-04-10 19:02:55,200 - INFO - tqdm - accuracy: 0.5383, batch_loss: 0.7168, loss: 0.6922 ||:  31%|###       | 20/65 [03:50<08:41, 11.58s/it]
2022-04-10 19:03:06,801 - INFO - tqdm - accuracy: 0.5395, batch_loss: 0.7051, loss: 0.6928 ||:  32%|###2      | 21/65 [04:02<08:29, 11.59s/it]
2022-04-10 19:03:18,384 - INFO - tqdm - accuracy: 0.5405, batch_loss: 0.6409, loss: 0.6905 ||:  34%|###3      | 22/65 [04:14<08:18, 11.59s/it]
2022-04-10 19:03:29,943 - INFO - tqdm - accuracy: 0.5456, batch_loss: 0.6140, loss: 0.6872 ||:  35%|###5      | 23/65 [04:25<08:06, 11.58s/it]
2022-04-10 19:03:41,370 - INFO - tqdm - accuracy: 0.5437, batch_loss: 0.7350, loss: 0.6891 ||:  37%|###6      | 24/65 [04:37<07:52, 11.53s/it]
2022-04-10 19:03:52,927 - INFO - tqdm - accuracy: 0.5457, batch_loss: 0.7068, loss: 0.6899 ||:  38%|###8      | 25/65 [04:48<07:41, 11.54s/it]
2022-04-10 19:04:04,455 - INFO - tqdm - accuracy: 0.5415, batch_loss: 0.7219, loss: 0.6911 ||:  40%|####      | 26/65 [05:00<07:29, 11.54s/it]
2022-04-10 19:04:15,965 - INFO - tqdm - accuracy: 0.5388, batch_loss: 0.7099, loss: 0.6918 ||:  42%|####1     | 27/65 [05:11<07:18, 11.53s/it]
2022-04-10 19:04:27,500 - INFO - tqdm - accuracy: 0.5408, batch_loss: 0.6385, loss: 0.6899 ||:  43%|####3     | 28/65 [05:23<07:06, 11.53s/it]
2022-04-10 19:04:39,010 - INFO - tqdm - accuracy: 0.5437, batch_loss: 0.6520, loss: 0.6886 ||:  45%|####4     | 29/65 [05:34<06:54, 11.52s/it]
2022-04-10 19:04:50,517 - INFO - tqdm - accuracy: 0.5454, batch_loss: 0.6391, loss: 0.6869 ||:  46%|####6     | 30/65 [05:46<06:43, 11.52s/it]
2022-04-10 19:05:02,027 - INFO - tqdm - accuracy: 0.5429, batch_loss: 0.6687, loss: 0.6863 ||:  48%|####7     | 31/65 [05:57<06:31, 11.52s/it]
2022-04-10 19:05:13,553 - INFO - tqdm - accuracy: 0.5425, batch_loss: 0.6637, loss: 0.6856 ||:  49%|####9     | 32/65 [06:09<06:20, 11.52s/it]
2022-04-10 19:05:25,058 - INFO - tqdm - accuracy: 0.5393, batch_loss: 0.7815, loss: 0.6885 ||:  51%|#####     | 33/65 [06:20<06:08, 11.51s/it]
2022-04-10 19:05:36,592 - INFO - tqdm - accuracy: 0.5428, batch_loss: 0.6413, loss: 0.6871 ||:  52%|#####2    | 34/65 [06:32<05:57, 11.52s/it]
2022-04-10 19:05:48,127 - INFO - tqdm - accuracy: 0.5460, batch_loss: 0.6404, loss: 0.6858 ||:  54%|#####3    | 35/65 [06:43<05:45, 11.53s/it]
2022-04-10 19:05:59,692 - INFO - tqdm - accuracy: 0.5430, batch_loss: 0.7152, loss: 0.6866 ||:  55%|#####5    | 36/65 [06:55<05:34, 11.54s/it]
2022-04-10 19:06:11,243 - INFO - tqdm - accuracy: 0.5427, batch_loss: 0.6876, loss: 0.6867 ||:  57%|#####6    | 37/65 [07:06<05:23, 11.54s/it]
2022-04-10 19:06:22,799 - INFO - tqdm - accuracy: 0.5473, batch_loss: 0.5927, loss: 0.6842 ||:  58%|#####8    | 38/65 [07:18<05:11, 11.55s/it]
2022-04-10 19:06:34,374 - INFO - tqdm - accuracy: 0.5461, batch_loss: 0.6324, loss: 0.6829 ||:  60%|######    | 39/65 [07:30<05:00, 11.55s/it]
2022-04-10 19:06:45,937 - INFO - tqdm - accuracy: 0.5457, batch_loss: 0.7311, loss: 0.6841 ||:  62%|######1   | 40/65 [07:41<04:48, 11.56s/it]
2022-04-10 19:06:57,526 - INFO - tqdm - accuracy: 0.5439, batch_loss: 0.6717, loss: 0.6838 ||:  63%|######3   | 41/65 [07:53<04:37, 11.57s/it]
2022-04-10 19:07:08,991 - INFO - tqdm - accuracy: 0.5458, batch_loss: 0.6460, loss: 0.6829 ||:  65%|######4   | 42/65 [08:04<04:25, 11.54s/it]
2022-04-10 19:07:20,575 - INFO - tqdm - accuracy: 0.5462, batch_loss: 0.6562, loss: 0.6822 ||:  66%|######6   | 43/65 [08:16<04:14, 11.55s/it]
2022-04-10 19:07:32,152 - INFO - tqdm - accuracy: 0.5473, batch_loss: 0.6703, loss: 0.6820 ||:  68%|######7   | 44/65 [08:27<04:02, 11.56s/it]
2022-04-10 19:07:43,750 - INFO - tqdm - accuracy: 0.5476, batch_loss: 0.6599, loss: 0.6815 ||:  69%|######9   | 45/65 [08:39<03:51, 11.57s/it]
2022-04-10 19:07:55,313 - INFO - tqdm - accuracy: 0.5479, batch_loss: 0.7262, loss: 0.6825 ||:  71%|#######   | 46/65 [08:51<03:39, 11.57s/it]
2022-04-10 19:08:06,904 - INFO - tqdm - accuracy: 0.5489, batch_loss: 0.7653, loss: 0.6842 ||:  72%|#######2  | 47/65 [09:02<03:28, 11.57s/it]
2022-04-10 19:08:18,456 - INFO - tqdm - accuracy: 0.5505, batch_loss: 0.6449, loss: 0.6834 ||:  74%|#######3  | 48/65 [09:14<03:16, 11.57s/it]
2022-04-10 19:08:30,021 - INFO - tqdm - accuracy: 0.5514, batch_loss: 0.6564, loss: 0.6828 ||:  75%|#######5  | 49/65 [09:25<03:05, 11.57s/it]
2022-04-10 19:08:41,571 - INFO - tqdm - accuracy: 0.5522, batch_loss: 0.6552, loss: 0.6823 ||:  77%|#######6  | 50/65 [09:37<02:53, 11.56s/it]
2022-04-10 19:08:53,113 - INFO - tqdm - accuracy: 0.5530, batch_loss: 0.6452, loss: 0.6816 ||:  78%|#######8  | 51/65 [09:48<02:41, 11.56s/it]
2022-04-10 19:09:04,664 - INFO - tqdm - accuracy: 0.5550, batch_loss: 0.6524, loss: 0.6810 ||:  80%|########  | 52/65 [10:00<02:30, 11.55s/it]
2022-04-10 19:09:16,213 - INFO - tqdm - accuracy: 0.5581, batch_loss: 0.5785, loss: 0.6791 ||:  82%|########1 | 53/65 [10:11<02:18, 11.55s/it]
2022-04-10 19:09:27,751 - INFO - tqdm - accuracy: 0.5553, batch_loss: 0.7592, loss: 0.6806 ||:  83%|########3 | 54/65 [10:23<02:07, 11.55s/it]
2022-04-10 19:09:39,280 - INFO - tqdm - accuracy: 0.5560, batch_loss: 0.6820, loss: 0.6806 ||:  85%|########4 | 55/65 [10:35<01:55, 11.54s/it]
2022-04-10 19:09:50,816 - INFO - tqdm - accuracy: 0.5544, batch_loss: 0.6594, loss: 0.6802 ||:  86%|########6 | 56/65 [10:46<01:43, 11.54s/it]
2022-04-10 19:10:02,340 - INFO - tqdm - accuracy: 0.5529, batch_loss: 0.7708, loss: 0.6818 ||:  88%|########7 | 57/65 [10:58<01:32, 11.54s/it]
2022-04-10 19:10:13,639 - INFO - tqdm - accuracy: 0.5536, batch_loss: 0.6278, loss: 0.6809 ||:  89%|########9 | 58/65 [11:09<01:20, 11.46s/it]
2022-04-10 19:10:25,164 - INFO - tqdm - accuracy: 0.5554, batch_loss: 0.6297, loss: 0.6800 ||:  91%|######### | 59/65 [11:20<01:08, 11.48s/it]
2022-04-10 19:10:36,704 - INFO - tqdm - accuracy: 0.5565, batch_loss: 0.6932, loss: 0.6802 ||:  92%|#########2| 60/65 [11:32<00:57, 11.50s/it]
2022-04-10 19:10:48,221 - INFO - tqdm - accuracy: 0.5561, batch_loss: 0.6292, loss: 0.6794 ||:  94%|#########3| 61/65 [11:43<00:46, 11.51s/it]
2022-04-10 19:10:59,761 - INFO - tqdm - accuracy: 0.5542, batch_loss: 0.7488, loss: 0.6805 ||:  95%|#########5| 62/65 [11:55<00:34, 11.52s/it]
2022-04-10 19:11:11,293 - INFO - tqdm - accuracy: 0.5533, batch_loss: 0.7002, loss: 0.6808 ||:  97%|#########6| 63/65 [12:07<00:23, 11.52s/it]
2022-04-10 19:11:22,811 - INFO - tqdm - accuracy: 0.5501, batch_loss: 0.7510, loss: 0.6819 ||:  98%|#########8| 64/65 [12:18<00:11, 11.52s/it]
2022-04-10 19:11:27,920 - INFO - tqdm - accuracy: 0.5512, batch_loss: 0.6322, loss: 0.6811 ||: 100%|##########| 65/65 [12:23<00:00,  9.60s/it]
2022-04-10 19:11:27,921 - INFO - tqdm - accuracy: 0.5512, batch_loss: 0.6322, loss: 0.6811 ||: 100%|##########| 65/65 [12:23<00:00, 11.44s/it]
2022-04-10 19:11:30,771 - INFO - allennlp.training.trainer - Validating
2022-04-10 19:11:30,773 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-10 19:11:40,909 - INFO - tqdm - accuracy: 0.6375, batch_loss: 0.6889, loss: 0.7273 ||:  35%|###4      | 40/115 [00:10<00:19,  3.93it/s]
2022-04-10 19:11:50,940 - INFO - tqdm - accuracy: 0.6226, batch_loss: 0.7204, loss: 0.6932 ||:  70%|######9   | 80/115 [00:20<00:08,  3.97it/s]
2022-04-10 19:12:00,040 - INFO - tqdm - accuracy: 0.5983, batch_loss: 0.9915, loss: 0.7057 ||: 100%|##########| 115/115 [00:29<00:00,  3.85it/s]
2022-04-10 19:12:00,041 - INFO - tqdm - accuracy: 0.5983, batch_loss: 0.9915, loss: 0.7057 ||: 100%|##########| 115/115 [00:29<00:00,  3.93it/s]
2022-04-10 19:12:00,041 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-10 19:12:00,042 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.551  |     0.598
2022-04-10 19:12:00,043 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-10 19:12:00,044 - INFO - allennlp.training.tensorboard_writer - loss               |     0.681  |     0.706
2022-04-10 19:12:00,044 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5811.043  |       N/A
2022-04-10 19:12:05,914 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to './output_ir-ora-d_hyper5/best.th'.
2022-04-10 19:12:18,313 - INFO - allennlp.training.trainer - Epoch duration: 0:13:14.066963
2022-04-10 19:12:18,313 - INFO - allennlp.training.trainer - Estimated training time remaining: 2:37:25
2022-04-10 19:12:18,314 - INFO - allennlp.training.trainer - Epoch 3/14
2022-04-10 19:12:18,314 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-10 19:12:18,314 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-10 19:12:18,315 - INFO - allennlp.training.trainer - Training
2022-04-10 19:12:18,316 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-10 19:12:29,485 - INFO - tqdm - accuracy: 0.3750, batch_loss: 0.7202, loss: 0.7202 ||:   2%|1         | 1/65 [00:11<11:54, 11.17s/it]
2022-04-10 19:12:40,951 - INFO - tqdm - accuracy: 0.3906, batch_loss: 0.7458, loss: 0.7330 ||:   3%|3         | 2/65 [00:22<11:54, 11.34s/it]
2022-04-10 19:12:52,734 - INFO - tqdm - accuracy: 0.4271, batch_loss: 0.6902, loss: 0.7187 ||:   5%|4         | 3/65 [00:34<11:55, 11.54s/it]
2022-04-10 19:13:04,666 - INFO - tqdm - accuracy: 0.4688, batch_loss: 0.6770, loss: 0.7083 ||:   6%|6         | 4/65 [00:46<11:53, 11.70s/it]
2022-04-10 19:13:16,632 - INFO - tqdm - accuracy: 0.4813, batch_loss: 0.6536, loss: 0.6973 ||:   8%|7         | 5/65 [00:58<11:47, 11.79s/it]
2022-04-10 19:13:27,981 - INFO - tqdm - accuracy: 0.4896, batch_loss: 0.7294, loss: 0.7027 ||:   9%|9         | 6/65 [01:09<11:26, 11.64s/it]
2022-04-10 19:13:39,389 - INFO - tqdm - accuracy: 0.5089, batch_loss: 0.6133, loss: 0.6899 ||:  11%|#         | 7/65 [01:21<11:10, 11.57s/it]
2022-04-10 19:13:50,798 - INFO - tqdm - accuracy: 0.5273, batch_loss: 0.6541, loss: 0.6854 ||:  12%|#2        | 8/65 [01:32<10:56, 11.52s/it]
2022-04-10 19:14:02,424 - INFO - tqdm - accuracy: 0.5382, batch_loss: 0.5864, loss: 0.6744 ||:  14%|#3        | 9/65 [01:44<10:46, 11.55s/it]
2022-04-10 19:14:14,001 - INFO - tqdm - accuracy: 0.5563, batch_loss: 0.6384, loss: 0.6708 ||:  15%|#5        | 10/65 [01:55<10:35, 11.56s/it]
2022-04-10 19:14:25,404 - INFO - tqdm - accuracy: 0.5653, batch_loss: 0.6048, loss: 0.6648 ||:  17%|#6        | 11/65 [02:07<10:21, 11.51s/it]
2022-04-10 19:14:36,901 - INFO - tqdm - accuracy: 0.5703, batch_loss: 0.6813, loss: 0.6662 ||:  18%|#8        | 12/65 [02:18<10:09, 11.51s/it]
2022-04-10 19:14:48,405 - INFO - tqdm - accuracy: 0.5841, batch_loss: 0.6126, loss: 0.6621 ||:  20%|##        | 13/65 [02:30<09:58, 11.51s/it]
2022-04-10 19:14:59,906 - INFO - tqdm - accuracy: 0.5938, batch_loss: 0.6533, loss: 0.6615 ||:  22%|##1       | 14/65 [02:41<09:46, 11.50s/it]
2022-04-10 19:15:11,452 - INFO - tqdm - accuracy: 0.5917, batch_loss: 0.6680, loss: 0.6619 ||:  23%|##3       | 15/65 [02:53<09:35, 11.52s/it]
2022-04-10 19:15:23,016 - INFO - tqdm - accuracy: 0.5820, batch_loss: 0.7343, loss: 0.6664 ||:  25%|##4       | 16/65 [03:04<09:25, 11.53s/it]
2022-04-10 19:15:34,569 - INFO - tqdm - accuracy: 0.5809, batch_loss: 0.6303, loss: 0.6643 ||:  26%|##6       | 17/65 [03:16<09:13, 11.54s/it]
2022-04-10 19:15:46,148 - INFO - tqdm - accuracy: 0.5833, batch_loss: 0.6298, loss: 0.6624 ||:  28%|##7       | 18/65 [03:27<09:02, 11.55s/it]
2022-04-10 19:15:57,707 - INFO - tqdm - accuracy: 0.5757, batch_loss: 0.7191, loss: 0.6654 ||:  29%|##9       | 19/65 [03:39<08:51, 11.55s/it]
2022-04-10 19:16:09,270 - INFO - tqdm - accuracy: 0.5766, batch_loss: 0.6537, loss: 0.6648 ||:  31%|###       | 20/65 [03:50<08:40, 11.56s/it]
2022-04-10 19:16:20,867 - INFO - tqdm - accuracy: 0.5848, batch_loss: 0.6030, loss: 0.6618 ||:  32%|###2      | 21/65 [04:02<08:28, 11.57s/it]
2022-04-10 19:16:32,466 - INFO - tqdm - accuracy: 0.5767, batch_loss: 0.7089, loss: 0.6640 ||:  34%|###3      | 22/65 [04:14<08:17, 11.58s/it]
2022-04-10 19:16:44,035 - INFO - tqdm - accuracy: 0.5761, batch_loss: 0.6202, loss: 0.6621 ||:  35%|###5      | 23/65 [04:25<08:06, 11.58s/it]
2022-04-10 19:16:55,613 - INFO - tqdm - accuracy: 0.5794, batch_loss: 0.5879, loss: 0.6590 ||:  37%|###6      | 24/65 [04:37<07:54, 11.58s/it]
2022-04-10 19:17:07,180 - INFO - tqdm - accuracy: 0.5863, batch_loss: 0.6062, loss: 0.6569 ||:  38%|###8      | 25/65 [04:48<07:42, 11.57s/it]
2022-04-10 19:17:18,711 - INFO - tqdm - accuracy: 0.5865, batch_loss: 0.6424, loss: 0.6563 ||:  40%|####      | 26/65 [05:00<07:30, 11.56s/it]
2022-04-10 19:17:30,250 - INFO - tqdm - accuracy: 0.5856, batch_loss: 0.6896, loss: 0.6575 ||:  42%|####1     | 27/65 [05:11<07:19, 11.55s/it]
2022-04-10 19:17:41,776 - INFO - tqdm - accuracy: 0.5859, batch_loss: 0.6262, loss: 0.6564 ||:  43%|####3     | 28/65 [05:23<07:07, 11.55s/it]
2022-04-10 19:17:53,289 - INFO - tqdm - accuracy: 0.5884, batch_loss: 0.5912, loss: 0.6542 ||:  45%|####4     | 29/65 [05:34<06:55, 11.54s/it]
2022-04-10 19:18:04,810 - INFO - tqdm - accuracy: 0.5896, batch_loss: 0.6396, loss: 0.6537 ||:  46%|####6     | 30/65 [05:46<06:43, 11.53s/it]
2022-04-10 19:18:16,304 - INFO - tqdm - accuracy: 0.5917, batch_loss: 0.6546, loss: 0.6537 ||:  48%|####7     | 31/65 [05:57<06:31, 11.52s/it]
2022-04-10 19:18:27,682 - INFO - tqdm - accuracy: 0.5908, batch_loss: 0.7149, loss: 0.6556 ||:  49%|####9     | 32/65 [06:09<06:18, 11.48s/it]
2022-04-10 19:18:39,185 - INFO - tqdm - accuracy: 0.5852, batch_loss: 0.6674, loss: 0.6560 ||:  51%|#####     | 33/65 [06:20<06:07, 11.49s/it]
2022-04-10 19:18:50,673 - INFO - tqdm - accuracy: 0.5864, batch_loss: 0.6096, loss: 0.6546 ||:  52%|#####2    | 34/65 [06:32<05:56, 11.49s/it]
2022-04-10 19:19:02,184 - INFO - tqdm - accuracy: 0.5866, batch_loss: 0.7163, loss: 0.6564 ||:  54%|#####3    | 35/65 [06:43<05:44, 11.49s/it]
2022-04-10 19:19:13,726 - INFO - tqdm - accuracy: 0.5877, batch_loss: 0.6644, loss: 0.6566 ||:  55%|#####5    | 36/65 [06:55<05:33, 11.51s/it]
2022-04-10 19:19:25,259 - INFO - tqdm - accuracy: 0.5853, batch_loss: 0.6618, loss: 0.6568 ||:  57%|#####6    | 37/65 [07:06<05:22, 11.52s/it]
2022-04-10 19:19:36,804 - INFO - tqdm - accuracy: 0.5847, batch_loss: 0.6649, loss: 0.6570 ||:  58%|#####8    | 38/65 [07:18<05:11, 11.52s/it]
2022-04-10 19:19:48,358 - INFO - tqdm - accuracy: 0.5841, batch_loss: 0.6415, loss: 0.6566 ||:  60%|######    | 39/65 [07:30<04:59, 11.53s/it]
2022-04-10 19:19:59,915 - INFO - tqdm - accuracy: 0.5859, batch_loss: 0.6111, loss: 0.6554 ||:  62%|######1   | 40/65 [07:41<04:48, 11.54s/it]
2022-04-10 19:20:11,496 - INFO - tqdm - accuracy: 0.5869, batch_loss: 0.7261, loss: 0.6572 ||:  63%|######3   | 41/65 [07:53<04:37, 11.55s/it]
2022-04-10 19:20:23,059 - INFO - tqdm - accuracy: 0.5863, batch_loss: 0.6794, loss: 0.6577 ||:  65%|######4   | 42/65 [08:04<04:25, 11.56s/it]
2022-04-10 19:20:34,631 - INFO - tqdm - accuracy: 0.5843, batch_loss: 0.6154, loss: 0.6567 ||:  66%|######6   | 43/65 [08:16<04:14, 11.56s/it]
2022-04-10 19:20:46,186 - INFO - tqdm - accuracy: 0.5852, batch_loss: 0.6241, loss: 0.6560 ||:  68%|######7   | 44/65 [08:27<04:02, 11.56s/it]
2022-04-10 19:20:57,778 - INFO - tqdm - accuracy: 0.5833, batch_loss: 0.7502, loss: 0.6581 ||:  69%|######9   | 45/65 [08:39<03:51, 11.57s/it]
2022-04-10 19:21:09,372 - INFO - tqdm - accuracy: 0.5842, batch_loss: 0.6420, loss: 0.6577 ||:  71%|#######   | 46/65 [08:51<03:39, 11.58s/it]
2022-04-10 19:21:20,971 - INFO - tqdm - accuracy: 0.5844, batch_loss: 0.6438, loss: 0.6574 ||:  72%|#######2  | 47/65 [09:02<03:28, 11.58s/it]
2022-04-10 19:21:32,552 - INFO - tqdm - accuracy: 0.5859, batch_loss: 0.6504, loss: 0.6573 ||:  74%|#######3  | 48/65 [09:14<03:16, 11.58s/it]
2022-04-10 19:21:44,111 - INFO - tqdm - accuracy: 0.5842, batch_loss: 0.7508, loss: 0.6592 ||:  75%|#######5  | 49/65 [09:25<03:05, 11.58s/it]
2022-04-10 19:21:55,674 - INFO - tqdm - accuracy: 0.5844, batch_loss: 0.6432, loss: 0.6589 ||:  77%|#######6  | 50/65 [09:37<02:53, 11.57s/it]
2022-04-10 19:22:07,215 - INFO - tqdm - accuracy: 0.5821, batch_loss: 0.7128, loss: 0.6599 ||:  78%|#######8  | 51/65 [09:48<02:41, 11.56s/it]
2022-04-10 19:22:18,503 - INFO - tqdm - accuracy: 0.5835, batch_loss: 0.6260, loss: 0.6593 ||:  80%|########  | 52/65 [10:00<02:29, 11.48s/it]
2022-04-10 19:22:30,039 - INFO - tqdm - accuracy: 0.5861, batch_loss: 0.6347, loss: 0.6588 ||:  82%|########1 | 53/65 [10:11<02:17, 11.50s/it]
2022-04-10 19:22:41,563 - INFO - tqdm - accuracy: 0.5851, batch_loss: 0.6979, loss: 0.6595 ||:  83%|########3 | 54/65 [10:23<02:06, 11.51s/it]
2022-04-10 19:22:53,083 - INFO - tqdm - accuracy: 0.5841, batch_loss: 0.7454, loss: 0.6611 ||:  85%|########4 | 55/65 [10:34<01:55, 11.51s/it]
2022-04-10 19:23:04,575 - INFO - tqdm - accuracy: 0.5837, batch_loss: 0.7624, loss: 0.6629 ||:  86%|########6 | 56/65 [10:46<01:43, 11.50s/it]
2022-04-10 19:23:16,080 - INFO - tqdm - accuracy: 0.5839, batch_loss: 0.6959, loss: 0.6635 ||:  88%|########7 | 57/65 [10:57<01:32, 11.50s/it]
2022-04-10 19:23:27,575 - INFO - tqdm - accuracy: 0.5846, batch_loss: 0.6240, loss: 0.6628 ||:  89%|########9 | 58/65 [11:09<01:20, 11.50s/it]
2022-04-10 19:23:38,754 - INFO - tqdm - accuracy: 0.5835, batch_loss: 0.6574, loss: 0.6627 ||:  91%|######### | 59/65 [11:20<01:08, 11.40s/it]
2022-04-10 19:23:50,289 - INFO - tqdm - accuracy: 0.5847, batch_loss: 0.6076, loss: 0.6618 ||:  92%|#########2| 60/65 [11:31<00:57, 11.44s/it]
2022-04-10 19:24:01,888 - INFO - tqdm - accuracy: 0.5818, batch_loss: 0.6856, loss: 0.6622 ||:  94%|#########3| 61/65 [11:43<00:45, 11.49s/it]
2022-04-10 19:24:13,477 - INFO - tqdm - accuracy: 0.5814, batch_loss: 0.6661, loss: 0.6622 ||:  95%|#########5| 62/65 [11:55<00:34, 11.52s/it]
2022-04-10 19:24:25,044 - INFO - tqdm - accuracy: 0.5811, batch_loss: 0.6735, loss: 0.6624 ||:  97%|#########6| 63/65 [12:06<00:23, 11.53s/it]
2022-04-10 19:24:36,627 - INFO - tqdm - accuracy: 0.5818, batch_loss: 0.6656, loss: 0.6625 ||:  98%|#########8| 64/65 [12:18<00:11, 11.55s/it]
2022-04-10 19:24:41,771 - INFO - tqdm - accuracy: 0.5813, batch_loss: 0.6936, loss: 0.6629 ||: 100%|##########| 65/65 [12:23<00:00,  9.63s/it]
2022-04-10 19:24:41,773 - INFO - tqdm - accuracy: 0.5813, batch_loss: 0.6936, loss: 0.6629 ||: 100%|##########| 65/65 [12:23<00:00, 11.44s/it]
2022-04-10 19:24:44,628 - INFO - allennlp.training.trainer - Validating
2022-04-10 19:24:44,630 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-10 19:24:54,795 - INFO - tqdm - accuracy: 0.5125, batch_loss: 0.3771, loss: 0.7468 ||:  35%|###4      | 40/115 [00:10<00:19,  3.91it/s]
2022-04-10 19:25:04,953 - INFO - tqdm - accuracy: 0.5687, batch_loss: 1.0411, loss: 0.6960 ||:  70%|######9   | 80/115 [00:20<00:09,  3.88it/s]
2022-04-10 19:25:13,819 - INFO - tqdm - accuracy: 0.5895, batch_loss: 0.4931, loss: 0.7046 ||: 100%|##########| 115/115 [00:29<00:00,  3.98it/s]
2022-04-10 19:25:13,819 - INFO - tqdm - accuracy: 0.5895, batch_loss: 0.4931, loss: 0.7046 ||: 100%|##########| 115/115 [00:29<00:00,  3.94it/s]
2022-04-10 19:25:13,819 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-10 19:25:13,820 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.581  |     0.590
2022-04-10 19:25:13,821 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-10 19:25:13,822 - INFO - allennlp.training.tensorboard_writer - loss               |     0.663  |     0.705
2022-04-10 19:25:13,823 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5811.043  |       N/A
2022-04-10 19:25:19,940 - INFO - allennlp.training.trainer - Epoch duration: 0:13:01.625969
2022-04-10 19:25:19,940 - INFO - allennlp.training.trainer - Estimated training time remaining: 2:24:03
2022-04-10 19:25:19,940 - INFO - allennlp.training.trainer - Epoch 4/14
2022-04-10 19:25:19,940 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-10 19:25:19,940 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-10 19:25:19,942 - INFO - allennlp.training.trainer - Training
2022-04-10 19:25:19,942 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-10 19:25:31,314 - INFO - tqdm - accuracy: 0.5625, batch_loss: 0.6594, loss: 0.6594 ||:   2%|1         | 1/65 [00:11<12:07, 11.37s/it]
2022-04-10 19:25:42,936 - INFO - tqdm - accuracy: 0.5625, batch_loss: 0.6773, loss: 0.6684 ||:   3%|3         | 2/65 [00:22<12:05, 11.52s/it]
2022-04-10 19:25:54,812 - INFO - tqdm - accuracy: 0.5729, batch_loss: 0.6588, loss: 0.6652 ||:   5%|4         | 3/65 [00:34<12:04, 11.68s/it]
2022-04-10 19:26:06,430 - INFO - tqdm - accuracy: 0.5703, batch_loss: 0.6636, loss: 0.6648 ||:   6%|6         | 4/65 [00:46<11:51, 11.66s/it]
2022-04-10 19:26:17,915 - INFO - tqdm - accuracy: 0.5625, batch_loss: 0.6789, loss: 0.6676 ||:   8%|7         | 5/65 [00:57<11:35, 11.59s/it]
2022-04-10 19:26:29,378 - INFO - tqdm - accuracy: 0.5573, batch_loss: 0.6816, loss: 0.6700 ||:   9%|9         | 6/65 [01:09<11:21, 11.55s/it]
2022-04-10 19:26:40,928 - INFO - tqdm - accuracy: 0.5580, batch_loss: 0.5927, loss: 0.6589 ||:  11%|#         | 7/65 [01:20<11:09, 11.55s/it]
2022-04-10 19:26:52,540 - INFO - tqdm - accuracy: 0.5586, batch_loss: 0.6710, loss: 0.6604 ||:  12%|#2        | 8/65 [01:32<10:59, 11.57s/it]
2022-04-10 19:27:04,122 - INFO - tqdm - accuracy: 0.5729, batch_loss: 0.5647, loss: 0.6498 ||:  14%|#3        | 9/65 [01:44<10:48, 11.57s/it]
2022-04-10 19:27:15,691 - INFO - tqdm - accuracy: 0.5750, batch_loss: 0.6453, loss: 0.6493 ||:  15%|#5        | 10/65 [01:55<10:36, 11.57s/it]
2022-04-10 19:27:27,231 - INFO - tqdm - accuracy: 0.5767, batch_loss: 0.6371, loss: 0.6482 ||:  17%|#6        | 11/65 [02:07<10:24, 11.56s/it]
2022-04-10 19:27:38,684 - INFO - tqdm - accuracy: 0.5729, batch_loss: 0.6917, loss: 0.6519 ||:  18%|#8        | 12/65 [02:18<10:11, 11.53s/it]
2022-04-10 19:27:50,195 - INFO - tqdm - accuracy: 0.5769, batch_loss: 0.6411, loss: 0.6510 ||:  20%|##        | 13/65 [02:30<09:59, 11.52s/it]
2022-04-10 19:28:01,709 - INFO - tqdm - accuracy: 0.5804, batch_loss: 0.6933, loss: 0.6540 ||:  22%|##1       | 14/65 [02:41<09:47, 11.52s/it]
2022-04-10 19:28:13,208 - INFO - tqdm - accuracy: 0.5813, batch_loss: 0.6452, loss: 0.6535 ||:  23%|##3       | 15/65 [02:53<09:35, 11.51s/it]
2022-04-10 19:28:24,719 - INFO - tqdm - accuracy: 0.5820, batch_loss: 0.6371, loss: 0.6524 ||:  25%|##4       | 16/65 [03:04<09:24, 11.51s/it]
2022-04-10 19:28:36,278 - INFO - tqdm - accuracy: 0.5901, batch_loss: 0.5846, loss: 0.6484 ||:  26%|##6       | 17/65 [03:16<09:13, 11.53s/it]
2022-04-10 19:28:47,824 - INFO - tqdm - accuracy: 0.5938, batch_loss: 0.6153, loss: 0.6466 ||:  28%|##7       | 18/65 [03:27<09:02, 11.53s/it]
2022-04-10 19:28:59,387 - INFO - tqdm - accuracy: 0.5905, batch_loss: 0.6146, loss: 0.6449 ||:  29%|##9       | 19/65 [03:39<08:50, 11.54s/it]
2022-04-10 19:29:10,937 - INFO - tqdm - accuracy: 0.5875, batch_loss: 0.7106, loss: 0.6482 ||:  31%|###       | 20/65 [03:50<08:39, 11.54s/it]
2022-04-10 19:29:22,515 - INFO - tqdm - accuracy: 0.5818, batch_loss: 0.7253, loss: 0.6519 ||:  32%|###2      | 21/65 [04:02<08:28, 11.55s/it]
2022-04-10 19:29:34,091 - INFO - tqdm - accuracy: 0.5824, batch_loss: 0.6399, loss: 0.6513 ||:  34%|###3      | 22/65 [04:14<08:17, 11.56s/it]
2022-04-10 19:29:45,649 - INFO - tqdm - accuracy: 0.5815, batch_loss: 0.7917, loss: 0.6574 ||:  35%|###5      | 23/65 [04:25<08:05, 11.56s/it]
2022-04-10 19:29:57,227 - INFO - tqdm - accuracy: 0.5781, batch_loss: 0.6206, loss: 0.6559 ||:  37%|###6      | 24/65 [04:37<07:54, 11.57s/it]
2022-04-10 19:30:08,818 - INFO - tqdm - accuracy: 0.5775, batch_loss: 0.6260, loss: 0.6547 ||:  38%|###8      | 25/65 [04:48<07:42, 11.57s/it]
2022-04-10 19:30:20,409 - INFO - tqdm - accuracy: 0.5841, batch_loss: 0.5917, loss: 0.6523 ||:  40%|####      | 26/65 [05:00<07:31, 11.58s/it]
2022-04-10 19:30:31,985 - INFO - tqdm - accuracy: 0.5891, batch_loss: 0.5911, loss: 0.6500 ||:  42%|####1     | 27/65 [05:12<07:19, 11.58s/it]
2022-04-10 19:30:43,578 - INFO - tqdm - accuracy: 0.5882, batch_loss: 0.7028, loss: 0.6519 ||:  43%|####3     | 28/65 [05:23<07:08, 11.58s/it]
2022-04-10 19:30:55,139 - INFO - tqdm - accuracy: 0.5884, batch_loss: 0.6784, loss: 0.6528 ||:  45%|####4     | 29/65 [05:35<06:56, 11.58s/it]
2022-04-10 19:31:06,554 - INFO - tqdm - accuracy: 0.5865, batch_loss: 0.7174, loss: 0.6550 ||:  46%|####6     | 30/65 [05:46<06:43, 11.53s/it]
2022-04-10 19:31:18,113 - INFO - tqdm - accuracy: 0.5887, batch_loss: 0.7031, loss: 0.6565 ||:  48%|####7     | 31/65 [05:58<06:32, 11.54s/it]
2022-04-10 19:31:29,658 - INFO - tqdm - accuracy: 0.5928, batch_loss: 0.6127, loss: 0.6551 ||:  49%|####9     | 32/65 [06:09<06:20, 11.54s/it]
2022-04-10 19:31:41,198 - INFO - tqdm - accuracy: 0.5938, batch_loss: 0.6558, loss: 0.6552 ||:  51%|#####     | 33/65 [06:21<06:09, 11.54s/it]
2022-04-10 19:31:52,736 - INFO - tqdm - accuracy: 0.5974, batch_loss: 0.5584, loss: 0.6523 ||:  52%|#####2    | 34/65 [06:32<05:57, 11.54s/it]
2022-04-10 19:32:04,245 - INFO - tqdm - accuracy: 0.6027, batch_loss: 0.5956, loss: 0.6507 ||:  54%|#####3    | 35/65 [06:44<05:45, 11.53s/it]
2022-04-10 19:32:15,766 - INFO - tqdm - accuracy: 0.5998, batch_loss: 0.6452, loss: 0.6505 ||:  55%|#####5    | 36/65 [06:55<05:34, 11.53s/it]
2022-04-10 19:32:26,913 - INFO - tqdm - accuracy: 0.6002, batch_loss: 0.6722, loss: 0.6511 ||:  57%|#####6    | 37/65 [07:06<05:19, 11.41s/it]
2022-04-10 19:32:38,406 - INFO - tqdm - accuracy: 0.6025, batch_loss: 0.5572, loss: 0.6487 ||:  58%|#####8    | 38/65 [07:18<05:08, 11.44s/it]
2022-04-10 19:32:49,904 - INFO - tqdm - accuracy: 0.6063, batch_loss: 0.5819, loss: 0.6469 ||:  60%|######    | 39/65 [07:29<04:57, 11.46s/it]
2022-04-10 19:33:01,955 - INFO - tqdm - accuracy: 0.6059, batch_loss: 0.6014, loss: 0.6458 ||:  62%|######1   | 40/65 [07:42<04:50, 11.63s/it]
2022-04-10 19:33:13,418 - INFO - tqdm - accuracy: 0.6056, batch_loss: 0.6839, loss: 0.6467 ||:  63%|######3   | 41/65 [07:53<04:37, 11.58s/it]
2022-04-10 19:33:24,939 - INFO - tqdm - accuracy: 0.6046, batch_loss: 0.6900, loss: 0.6478 ||:  65%|######4   | 42/65 [08:04<04:25, 11.56s/it]
2022-04-10 19:33:36,427 - INFO - tqdm - accuracy: 0.6036, batch_loss: 0.7053, loss: 0.6491 ||:  66%|######6   | 43/65 [08:16<04:13, 11.54s/it]
2022-04-10 19:33:47,958 - INFO - tqdm - accuracy: 0.6070, batch_loss: 0.5565, loss: 0.6470 ||:  68%|######7   | 44/65 [08:28<04:02, 11.54s/it]
2022-04-10 19:33:59,352 - INFO - tqdm - accuracy: 0.6067, batch_loss: 0.5876, loss: 0.6457 ||:  69%|######9   | 45/65 [08:39<03:49, 11.50s/it]
2022-04-10 19:34:10,875 - INFO - tqdm - accuracy: 0.6064, batch_loss: 0.6668, loss: 0.6461 ||:  71%|#######   | 46/65 [08:50<03:38, 11.50s/it]
2022-04-10 19:34:22,405 - INFO - tqdm - accuracy: 0.6041, batch_loss: 0.6128, loss: 0.6454 ||:  72%|#######2  | 47/65 [09:02<03:27, 11.51s/it]
2022-04-10 19:34:33,950 - INFO - tqdm - accuracy: 0.6039, batch_loss: 0.6884, loss: 0.6463 ||:  74%|#######3  | 48/65 [09:14<03:15, 11.52s/it]
2022-04-10 19:34:45,503 - INFO - tqdm - accuracy: 0.6088, batch_loss: 0.6021, loss: 0.6454 ||:  75%|#######5  | 49/65 [09:25<03:04, 11.53s/it]
2022-04-10 19:34:57,065 - INFO - tqdm - accuracy: 0.6073, batch_loss: 0.6993, loss: 0.6465 ||:  77%|#######6  | 50/65 [09:37<02:53, 11.54s/it]
2022-04-10 19:35:08,637 - INFO - tqdm - accuracy: 0.6070, batch_loss: 0.6386, loss: 0.6463 ||:  78%|#######8  | 51/65 [09:48<02:41, 11.55s/it]
2022-04-10 19:35:20,214 - INFO - tqdm - accuracy: 0.6055, batch_loss: 0.6571, loss: 0.6466 ||:  80%|########  | 52/65 [10:00<02:30, 11.56s/it]
2022-04-10 19:35:31,796 - INFO - tqdm - accuracy: 0.6094, batch_loss: 0.5030, loss: 0.6438 ||:  82%|########1 | 53/65 [10:11<02:18, 11.57s/it]
2022-04-10 19:35:43,373 - INFO - tqdm - accuracy: 0.6097, batch_loss: 0.6650, loss: 0.6442 ||:  83%|########3 | 54/65 [10:23<02:07, 11.57s/it]
2022-04-10 19:35:54,966 - INFO - tqdm - accuracy: 0.6111, batch_loss: 0.5934, loss: 0.6433 ||:  85%|########4 | 55/65 [10:35<01:55, 11.58s/it]
2022-04-10 19:36:06,565 - INFO - tqdm - accuracy: 0.6119, batch_loss: 0.6042, loss: 0.6426 ||:  86%|########6 | 56/65 [10:46<01:44, 11.58s/it]
2022-04-10 19:36:18,152 - INFO - tqdm - accuracy: 0.6122, batch_loss: 0.6715, loss: 0.6431 ||:  88%|########7 | 57/65 [10:58<01:32, 11.58s/it]
2022-04-10 19:36:29,742 - INFO - tqdm - accuracy: 0.6129, batch_loss: 0.6011, loss: 0.6424 ||:  89%|########9 | 58/65 [11:09<01:21, 11.59s/it]
2022-04-10 19:36:41,327 - INFO - tqdm - accuracy: 0.6131, batch_loss: 0.6572, loss: 0.6426 ||:  91%|######### | 59/65 [11:21<01:09, 11.59s/it]
2022-04-10 19:36:52,892 - INFO - tqdm - accuracy: 0.6128, batch_loss: 0.6365, loss: 0.6425 ||:  92%|#########2| 60/65 [11:32<00:57, 11.58s/it]
2022-04-10 19:37:04,445 - INFO - tqdm - accuracy: 0.6115, batch_loss: 0.7070, loss: 0.6436 ||:  94%|#########3| 61/65 [11:44<00:46, 11.57s/it]
2022-04-10 19:37:15,696 - INFO - tqdm - accuracy: 0.6107, batch_loss: 0.6852, loss: 0.6443 ||:  95%|#########5| 62/65 [11:55<00:34, 11.48s/it]
2022-04-10 19:37:27,183 - INFO - tqdm - accuracy: 0.6114, batch_loss: 0.6095, loss: 0.6437 ||:  97%|#########6| 63/65 [12:07<00:22, 11.48s/it]
2022-04-10 19:37:38,685 - INFO - tqdm - accuracy: 0.6126, batch_loss: 0.6408, loss: 0.6437 ||:  98%|#########8| 64/65 [12:18<00:11, 11.49s/it]
2022-04-10 19:37:43,809 - INFO - tqdm - accuracy: 0.6123, batch_loss: 0.6293, loss: 0.6435 ||: 100%|##########| 65/65 [12:23<00:00,  9.58s/it]
2022-04-10 19:37:43,809 - INFO - tqdm - accuracy: 0.6123, batch_loss: 0.6293, loss: 0.6435 ||: 100%|##########| 65/65 [12:23<00:00, 11.44s/it]
2022-04-10 19:37:46,627 - INFO - allennlp.training.trainer - Validating
2022-04-10 19:37:46,629 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-10 19:37:56,761 - INFO - tqdm - accuracy: 0.6375, batch_loss: 0.0851, loss: 0.7373 ||:  35%|###4      | 40/115 [00:10<00:19,  3.93it/s]
2022-04-10 19:38:06,770 - INFO - tqdm - accuracy: 0.6266, batch_loss: 1.3455, loss: 0.7119 ||:  69%|######8   | 79/115 [00:20<00:09,  3.88it/s]
2022-04-10 19:38:15,841 - INFO - tqdm - accuracy: 0.6157, batch_loss: 0.3177, loss: 0.7160 ||: 100%|##########| 115/115 [00:29<00:00,  3.88it/s]
2022-04-10 19:38:15,841 - INFO - tqdm - accuracy: 0.6157, batch_loss: 0.3177, loss: 0.7160 ||: 100%|##########| 115/115 [00:29<00:00,  3.94it/s]
2022-04-10 19:38:15,841 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-10 19:38:15,842 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.612  |     0.616
2022-04-10 19:38:15,842 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-10 19:38:15,842 - INFO - allennlp.training.tensorboard_writer - loss               |     0.643  |     0.716
2022-04-10 19:38:15,843 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5811.043  |       N/A
2022-04-10 19:38:21,487 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to './output_ir-ora-d_hyper5/best.th'.
2022-04-10 19:38:34,202 - INFO - allennlp.training.trainer - Epoch duration: 0:13:14.262520
2022-04-10 19:38:34,203 - INFO - allennlp.training.trainer - Estimated training time remaining: 2:11:14
2022-04-10 19:38:34,203 - INFO - allennlp.training.trainer - Epoch 5/14
2022-04-10 19:38:34,203 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-10 19:38:34,203 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-10 19:38:34,205 - INFO - allennlp.training.trainer - Training
2022-04-10 19:38:34,205 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-10 19:38:45,331 - INFO - tqdm - accuracy: 0.6875, batch_loss: 0.5553, loss: 0.5553 ||:   2%|1         | 1/65 [00:11<11:52, 11.13s/it]
2022-04-10 19:38:56,945 - INFO - tqdm - accuracy: 0.6406, batch_loss: 0.7201, loss: 0.6377 ||:   3%|3         | 2/65 [00:22<11:59, 11.41s/it]
2022-04-10 19:39:08,902 - INFO - tqdm - accuracy: 0.6250, batch_loss: 0.7261, loss: 0.6672 ||:   5%|4         | 3/65 [00:34<12:02, 11.66s/it]
2022-04-10 19:39:20,703 - INFO - tqdm - accuracy: 0.5781, batch_loss: 0.7157, loss: 0.6793 ||:   6%|6         | 4/65 [00:46<11:54, 11.72s/it]
2022-04-10 19:39:32,148 - INFO - tqdm - accuracy: 0.5875, batch_loss: 0.7150, loss: 0.6865 ||:   8%|7         | 5/65 [00:57<11:37, 11.62s/it]
2022-04-10 19:39:43,536 - INFO - tqdm - accuracy: 0.5990, batch_loss: 0.6105, loss: 0.6738 ||:   9%|9         | 6/65 [01:09<11:20, 11.54s/it]
2022-04-10 19:39:54,855 - INFO - tqdm - accuracy: 0.6116, batch_loss: 0.5885, loss: 0.6616 ||:  11%|#         | 7/65 [01:20<11:05, 11.47s/it]
2022-04-10 19:40:06,413 - INFO - tqdm - accuracy: 0.6406, batch_loss: 0.5229, loss: 0.6443 ||:  12%|#2        | 8/65 [01:32<10:55, 11.50s/it]
2022-04-10 19:40:18,035 - INFO - tqdm - accuracy: 0.6424, batch_loss: 0.6005, loss: 0.6394 ||:  14%|#3        | 9/65 [01:43<10:45, 11.54s/it]
2022-04-10 19:40:29,623 - INFO - tqdm - accuracy: 0.6375, batch_loss: 0.6575, loss: 0.6412 ||:  15%|#5        | 10/65 [01:55<10:35, 11.55s/it]
2022-04-10 19:40:41,164 - INFO - tqdm - accuracy: 0.6307, batch_loss: 0.7294, loss: 0.6492 ||:  17%|#6        | 11/65 [02:06<10:23, 11.55s/it]
2022-04-10 19:40:52,704 - INFO - tqdm - accuracy: 0.6380, batch_loss: 0.6727, loss: 0.6512 ||:  18%|#8        | 12/65 [02:18<10:11, 11.55s/it]
2022-04-10 19:41:04,207 - INFO - tqdm - accuracy: 0.6394, batch_loss: 0.6254, loss: 0.6492 ||:  20%|##        | 13/65 [02:30<09:59, 11.53s/it]
2022-04-10 19:41:15,680 - INFO - tqdm - accuracy: 0.6317, batch_loss: 0.7398, loss: 0.6557 ||:  22%|##1       | 14/65 [02:41<09:47, 11.52s/it]
2022-04-10 19:41:27,161 - INFO - tqdm - accuracy: 0.6271, batch_loss: 0.6236, loss: 0.6535 ||:  23%|##3       | 15/65 [02:52<09:35, 11.50s/it]
2022-04-10 19:41:38,300 - INFO - tqdm - accuracy: 0.6262, batch_loss: 0.6427, loss: 0.6529 ||:  25%|##4       | 16/65 [03:04<09:18, 11.39s/it]
2022-04-10 19:41:49,843 - INFO - tqdm - accuracy: 0.6243, batch_loss: 0.7011, loss: 0.6557 ||:  26%|##6       | 17/65 [03:15<09:09, 11.44s/it]
2022-04-10 19:42:01,421 - INFO - tqdm - accuracy: 0.6243, batch_loss: 0.6620, loss: 0.6561 ||:  28%|##7       | 18/65 [03:27<08:59, 11.48s/it]
2022-04-10 19:42:13,023 - INFO - tqdm - accuracy: 0.6227, batch_loss: 0.6366, loss: 0.6550 ||:  29%|##9       | 19/65 [03:38<08:49, 11.52s/it]
2022-04-10 19:42:24,620 - INFO - tqdm - accuracy: 0.6260, batch_loss: 0.5756, loss: 0.6511 ||:  31%|###       | 20/65 [03:50<08:39, 11.54s/it]
2022-04-10 19:42:36,190 - INFO - tqdm - accuracy: 0.6319, batch_loss: 0.6148, loss: 0.6493 ||:  32%|###2      | 21/65 [04:01<08:28, 11.55s/it]
2022-04-10 19:42:47,739 - INFO - tqdm - accuracy: 0.6287, batch_loss: 0.6519, loss: 0.6494 ||:  34%|###3      | 22/65 [04:13<08:16, 11.55s/it]
2022-04-10 19:42:59,292 - INFO - tqdm - accuracy: 0.6286, batch_loss: 0.6295, loss: 0.6486 ||:  35%|###5      | 23/65 [04:25<08:05, 11.55s/it]
2022-04-10 19:43:10,834 - INFO - tqdm - accuracy: 0.6219, batch_loss: 0.6927, loss: 0.6504 ||:  37%|###6      | 24/65 [04:36<07:53, 11.55s/it]
2022-04-10 19:43:22,365 - INFO - tqdm - accuracy: 0.6208, batch_loss: 0.6646, loss: 0.6510 ||:  38%|###8      | 25/65 [04:48<07:41, 11.54s/it]
2022-04-10 19:43:33,881 - INFO - tqdm - accuracy: 0.6173, batch_loss: 0.5973, loss: 0.6489 ||:  40%|####      | 26/65 [04:59<07:29, 11.53s/it]
2022-04-10 19:43:45,382 - INFO - tqdm - accuracy: 0.6153, batch_loss: 0.6426, loss: 0.6487 ||:  42%|####1     | 27/65 [05:11<07:17, 11.52s/it]
2022-04-10 19:43:56,917 - INFO - tqdm - accuracy: 0.6145, batch_loss: 0.6473, loss: 0.6486 ||:  43%|####3     | 28/65 [05:22<07:06, 11.53s/it]
2022-04-10 19:44:08,428 - INFO - tqdm - accuracy: 0.6192, batch_loss: 0.5478, loss: 0.6452 ||:  45%|####4     | 29/65 [05:34<06:54, 11.52s/it]
2022-04-10 19:44:19,944 - INFO - tqdm - accuracy: 0.6225, batch_loss: 0.5151, loss: 0.6408 ||:  46%|####6     | 30/65 [05:45<06:43, 11.52s/it]
2022-04-10 19:44:31,468 - INFO - tqdm - accuracy: 0.6196, batch_loss: 0.6925, loss: 0.6425 ||:  48%|####7     | 31/65 [05:57<06:31, 11.52s/it]
2022-04-10 19:44:42,989 - INFO - tqdm - accuracy: 0.6149, batch_loss: 0.6427, loss: 0.6425 ||:  49%|####9     | 32/65 [06:08<06:20, 11.52s/it]
2022-04-10 19:44:54,495 - INFO - tqdm - accuracy: 0.6180, batch_loss: 0.5824, loss: 0.6407 ||:  51%|#####     | 33/65 [06:20<06:08, 11.52s/it]
2022-04-10 19:45:06,015 - INFO - tqdm - accuracy: 0.6182, batch_loss: 0.6529, loss: 0.6410 ||:  52%|#####2    | 34/65 [06:31<05:57, 11.52s/it]
2022-04-10 19:45:17,500 - INFO - tqdm - accuracy: 0.6175, batch_loss: 0.6510, loss: 0.6413 ||:  54%|#####3    | 35/65 [06:43<05:45, 11.51s/it]
2022-04-10 19:45:29,011 - INFO - tqdm - accuracy: 0.6160, batch_loss: 0.6042, loss: 0.6403 ||:  55%|#####5    | 36/65 [06:54<05:33, 11.51s/it]
2022-04-10 19:45:40,536 - INFO - tqdm - accuracy: 0.6162, batch_loss: 0.6446, loss: 0.6404 ||:  57%|#####6    | 37/65 [07:06<05:22, 11.51s/it]
2022-04-10 19:45:52,092 - INFO - tqdm - accuracy: 0.6173, batch_loss: 0.6104, loss: 0.6396 ||:  58%|#####8    | 38/65 [07:17<05:11, 11.53s/it]
2022-04-10 19:46:03,639 - INFO - tqdm - accuracy: 0.6183, batch_loss: 0.5669, loss: 0.6378 ||:  60%|######    | 39/65 [07:29<04:59, 11.53s/it]
2022-04-10 19:46:15,201 - INFO - tqdm - accuracy: 0.6153, batch_loss: 0.7448, loss: 0.6404 ||:  62%|######1   | 40/65 [07:40<04:48, 11.54s/it]
2022-04-10 19:46:26,760 - INFO - tqdm - accuracy: 0.6148, batch_loss: 0.5488, loss: 0.6382 ||:  63%|######3   | 41/65 [07:52<04:37, 11.55s/it]
2022-04-10 19:46:38,297 - INFO - tqdm - accuracy: 0.6128, batch_loss: 0.6278, loss: 0.6379 ||:  65%|######4   | 42/65 [08:04<04:25, 11.54s/it]
2022-04-10 19:46:49,826 - INFO - tqdm - accuracy: 0.6124, batch_loss: 0.6077, loss: 0.6372 ||:  66%|######6   | 43/65 [08:15<04:13, 11.54s/it]
2022-04-10 19:47:01,381 - INFO - tqdm - accuracy: 0.6105, batch_loss: 0.6374, loss: 0.6372 ||:  68%|######7   | 44/65 [08:27<04:02, 11.54s/it]
2022-04-10 19:47:12,666 - INFO - tqdm - accuracy: 0.6115, batch_loss: 0.5531, loss: 0.6354 ||:  69%|######9   | 45/65 [08:38<03:49, 11.47s/it]
2022-04-10 19:47:24,218 - INFO - tqdm - accuracy: 0.6111, batch_loss: 0.6046, loss: 0.6347 ||:  71%|#######   | 46/65 [08:50<03:38, 11.49s/it]
2022-04-10 19:47:35,761 - INFO - tqdm - accuracy: 0.6128, batch_loss: 0.6440, loss: 0.6349 ||:  72%|#######2  | 47/65 [09:01<03:27, 11.51s/it]
2022-04-10 19:47:47,304 - INFO - tqdm - accuracy: 0.6085, batch_loss: 0.7212, loss: 0.6367 ||:  74%|#######3  | 48/65 [09:13<03:15, 11.52s/it]
2022-04-10 19:47:58,880 - INFO - tqdm - accuracy: 0.6107, batch_loss: 0.5599, loss: 0.6351 ||:  75%|#######5  | 49/65 [09:24<03:04, 11.54s/it]
2022-04-10 19:48:10,446 - INFO - tqdm - accuracy: 0.6123, batch_loss: 0.5633, loss: 0.6337 ||:  77%|#######6  | 50/65 [09:36<02:53, 11.54s/it]
2022-04-10 19:48:21,999 - INFO - tqdm - accuracy: 0.6156, batch_loss: 0.5354, loss: 0.6318 ||:  78%|#######8  | 51/65 [09:47<02:41, 11.55s/it]
2022-04-10 19:48:33,548 - INFO - tqdm - accuracy: 0.6152, batch_loss: 0.6240, loss: 0.6316 ||:  80%|########  | 52/65 [09:59<02:30, 11.55s/it]
2022-04-10 19:48:45,103 - INFO - tqdm - accuracy: 0.6142, batch_loss: 0.6011, loss: 0.6310 ||:  82%|########1 | 53/65 [10:10<02:18, 11.55s/it]
2022-04-10 19:48:56,658 - INFO - tqdm - accuracy: 0.6132, batch_loss: 0.6989, loss: 0.6323 ||:  83%|########3 | 54/65 [10:22<02:07, 11.55s/it]
2022-04-10 19:49:08,206 - INFO - tqdm - accuracy: 0.6140, batch_loss: 0.5809, loss: 0.6314 ||:  85%|########4 | 55/65 [10:34<01:55, 11.55s/it]
2022-04-10 19:49:19,793 - INFO - tqdm - accuracy: 0.6131, batch_loss: 0.6875, loss: 0.6324 ||:  86%|########6 | 56/65 [10:45<01:44, 11.56s/it]
2022-04-10 19:49:31,365 - INFO - tqdm - accuracy: 0.6122, batch_loss: 0.5843, loss: 0.6315 ||:  88%|########7 | 57/65 [10:57<01:32, 11.56s/it]
2022-04-10 19:49:42,933 - INFO - tqdm - accuracy: 0.6146, batch_loss: 0.5643, loss: 0.6304 ||:  89%|########9 | 58/65 [11:08<01:20, 11.57s/it]
2022-04-10 19:49:54,366 - INFO - tqdm - accuracy: 0.6147, batch_loss: 0.5843, loss: 0.6296 ||:  91%|######### | 59/65 [11:20<01:09, 11.53s/it]
2022-04-10 19:50:05,916 - INFO - tqdm - accuracy: 0.6139, batch_loss: 0.6651, loss: 0.6302 ||:  92%|#########2| 60/65 [11:31<00:57, 11.53s/it]
2022-04-10 19:50:17,465 - INFO - tqdm - accuracy: 0.6156, batch_loss: 0.5894, loss: 0.6295 ||:  94%|#########3| 61/65 [11:43<00:46, 11.54s/it]
2022-04-10 19:50:29,026 - INFO - tqdm - accuracy: 0.6172, batch_loss: 0.5730, loss: 0.6286 ||:  95%|#########5| 62/65 [11:54<00:34, 11.54s/it]
2022-04-10 19:50:40,569 - INFO - tqdm - accuracy: 0.6174, batch_loss: 0.6203, loss: 0.6285 ||:  97%|#########6| 63/65 [12:06<00:23, 11.54s/it]
2022-04-10 19:50:52,119 - INFO - tqdm - accuracy: 0.6180, batch_loss: 0.6259, loss: 0.6284 ||:  98%|#########8| 64/65 [12:17<00:11, 11.55s/it]
2022-04-10 19:50:57,248 - INFO - tqdm - accuracy: 0.6177, batch_loss: 0.6264, loss: 0.6284 ||: 100%|##########| 65/65 [12:23<00:00,  9.62s/it]
2022-04-10 19:50:57,249 - INFO - tqdm - accuracy: 0.6177, batch_loss: 0.6264, loss: 0.6284 ||: 100%|##########| 65/65 [12:23<00:00, 11.43s/it]
2022-04-10 19:51:00,099 - INFO - allennlp.training.trainer - Validating
2022-04-10 19:51:00,101 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-10 19:51:10,239 - INFO - tqdm - accuracy: 0.6125, batch_loss: 1.4403, loss: 0.7897 ||:  35%|###4      | 40/115 [00:10<00:19,  3.91it/s]
2022-04-10 19:51:20,480 - INFO - tqdm - accuracy: 0.6149, batch_loss: 0.8010, loss: 0.7241 ||:  70%|#######   | 81/115 [00:20<00:08,  4.06it/s]
2022-04-10 19:51:29,186 - INFO - tqdm - accuracy: 0.6288, batch_loss: 0.5943, loss: 0.7200 ||: 100%|##########| 115/115 [00:29<00:00,  3.90it/s]
2022-04-10 19:51:29,186 - INFO - tqdm - accuracy: 0.6288, batch_loss: 0.5943, loss: 0.7200 ||: 100%|##########| 115/115 [00:29<00:00,  3.95it/s]
2022-04-10 19:51:29,186 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-10 19:51:29,187 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.618  |     0.629
2022-04-10 19:51:29,187 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-10 19:51:29,187 - INFO - allennlp.training.tensorboard_writer - loss               |     0.628  |     0.720
2022-04-10 19:51:29,187 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5811.043  |       N/A
2022-04-10 19:51:34,842 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to './output_ir-ora-d_hyper5/best.th'.
2022-04-10 19:51:46,991 - INFO - allennlp.training.trainer - Epoch duration: 0:13:12.787853
2022-04-10 19:51:46,991 - INFO - allennlp.training.trainer - Estimated training time remaining: 1:58:15
2022-04-10 19:51:46,991 - INFO - allennlp.training.trainer - Epoch 6/14
2022-04-10 19:51:46,991 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-10 19:51:46,992 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-10 19:51:46,993 - INFO - allennlp.training.trainer - Training
2022-04-10 19:51:46,994 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-10 19:51:58,222 - INFO - tqdm - accuracy: 0.6562, batch_loss: 0.5749, loss: 0.5749 ||:   2%|1         | 1/65 [00:11<11:58, 11.23s/it]
2022-04-10 19:52:09,837 - INFO - tqdm - accuracy: 0.6250, batch_loss: 0.5754, loss: 0.5751 ||:   3%|3         | 2/65 [00:22<12:01, 11.46s/it]
2022-04-10 19:52:21,755 - INFO - tqdm - accuracy: 0.6354, batch_loss: 0.7038, loss: 0.6180 ||:   5%|4         | 3/65 [00:34<12:03, 11.67s/it]
2022-04-10 19:52:33,532 - INFO - tqdm - accuracy: 0.6328, batch_loss: 0.5885, loss: 0.6106 ||:   6%|6         | 4/65 [00:46<11:54, 11.71s/it]
2022-04-10 19:52:44,958 - INFO - tqdm - accuracy: 0.6375, batch_loss: 0.5742, loss: 0.6034 ||:   8%|7         | 5/65 [00:57<11:36, 11.61s/it]
2022-04-10 19:52:56,322 - INFO - tqdm - accuracy: 0.6510, batch_loss: 0.4968, loss: 0.5856 ||:   9%|9         | 6/65 [01:09<11:19, 11.52s/it]
2022-04-10 19:53:07,751 - INFO - tqdm - accuracy: 0.6339, batch_loss: 0.6594, loss: 0.5961 ||:  11%|#         | 7/65 [01:20<11:06, 11.49s/it]
2022-04-10 19:53:19,309 - INFO - tqdm - accuracy: 0.6367, batch_loss: 0.5962, loss: 0.5961 ||:  12%|#2        | 8/65 [01:32<10:56, 11.51s/it]
2022-04-10 19:53:30,927 - INFO - tqdm - accuracy: 0.6319, batch_loss: 0.6012, loss: 0.5967 ||:  14%|#3        | 9/65 [01:43<10:46, 11.55s/it]
2022-04-10 19:53:42,921 - INFO - tqdm - accuracy: 0.6250, batch_loss: 0.5805, loss: 0.5951 ||:  15%|#5        | 10/65 [01:55<10:42, 11.68s/it]
2022-04-10 19:53:54,416 - INFO - tqdm - accuracy: 0.6193, batch_loss: 0.7100, loss: 0.6055 ||:  17%|#6        | 11/65 [02:07<10:27, 11.63s/it]
2022-04-10 19:54:05,882 - INFO - tqdm - accuracy: 0.6302, batch_loss: 0.5975, loss: 0.6049 ||:  18%|#8        | 12/65 [02:18<10:13, 11.58s/it]
2022-04-10 19:54:17,389 - INFO - tqdm - accuracy: 0.6298, batch_loss: 0.6301, loss: 0.6068 ||:  20%|##        | 13/65 [02:30<10:00, 11.56s/it]
2022-04-10 19:54:28,897 - INFO - tqdm - accuracy: 0.6317, batch_loss: 0.6727, loss: 0.6115 ||:  22%|##1       | 14/65 [02:41<09:48, 11.54s/it]
2022-04-10 19:54:40,441 - INFO - tqdm - accuracy: 0.6292, batch_loss: 0.6282, loss: 0.6126 ||:  23%|##3       | 15/65 [02:53<09:37, 11.54s/it]
2022-04-10 19:54:51,997 - INFO - tqdm - accuracy: 0.6309, batch_loss: 0.5789, loss: 0.6105 ||:  25%|##4       | 16/65 [03:05<09:25, 11.55s/it]
2022-04-10 19:55:03,599 - INFO - tqdm - accuracy: 0.6360, batch_loss: 0.6296, loss: 0.6116 ||:  26%|##6       | 17/65 [03:16<09:15, 11.56s/it]
2022-04-10 19:55:15,186 - INFO - tqdm - accuracy: 0.6319, batch_loss: 0.6376, loss: 0.6131 ||:  28%|##7       | 18/65 [03:28<09:03, 11.57s/it]
2022-04-10 19:55:26,756 - INFO - tqdm - accuracy: 0.6349, batch_loss: 0.6073, loss: 0.6128 ||:  29%|##9       | 19/65 [03:39<08:52, 11.57s/it]
2022-04-10 19:55:38,305 - INFO - tqdm - accuracy: 0.6375, batch_loss: 0.5963, loss: 0.6120 ||:  31%|###       | 20/65 [03:51<08:40, 11.56s/it]
2022-04-10 19:55:49,845 - INFO - tqdm - accuracy: 0.6384, batch_loss: 0.6003, loss: 0.6114 ||:  32%|###2      | 21/65 [04:02<08:28, 11.56s/it]
2022-04-10 19:56:01,382 - INFO - tqdm - accuracy: 0.6420, batch_loss: 0.6077, loss: 0.6112 ||:  34%|###3      | 22/65 [04:14<08:16, 11.55s/it]
2022-04-10 19:56:12,895 - INFO - tqdm - accuracy: 0.6359, batch_loss: 0.6924, loss: 0.6148 ||:  35%|###5      | 23/65 [04:25<08:04, 11.54s/it]
2022-04-10 19:56:24,379 - INFO - tqdm - accuracy: 0.6354, batch_loss: 0.6246, loss: 0.6152 ||:  37%|###6      | 24/65 [04:37<07:52, 11.52s/it]
2022-04-10 19:56:35,880 - INFO - tqdm - accuracy: 0.6362, batch_loss: 0.6331, loss: 0.6159 ||:  38%|###8      | 25/65 [04:48<07:40, 11.52s/it]
2022-04-10 19:56:47,401 - INFO - tqdm - accuracy: 0.6394, batch_loss: 0.6004, loss: 0.6153 ||:  40%|####      | 26/65 [05:00<07:29, 11.52s/it]
2022-04-10 19:56:58,950 - INFO - tqdm - accuracy: 0.6424, batch_loss: 0.5163, loss: 0.6116 ||:  42%|####1     | 27/65 [05:11<07:18, 11.53s/it]
2022-04-10 19:57:10,479 - INFO - tqdm - accuracy: 0.6417, batch_loss: 0.6326, loss: 0.6124 ||:  43%|####3     | 28/65 [05:23<07:06, 11.53s/it]
2022-04-10 19:57:22,066 - INFO - tqdm - accuracy: 0.6487, batch_loss: 0.4856, loss: 0.6080 ||:  45%|####4     | 29/65 [05:35<06:55, 11.55s/it]
2022-04-10 19:57:33,519 - INFO - tqdm - accuracy: 0.6500, batch_loss: 0.6158, loss: 0.6083 ||:  46%|####6     | 30/65 [05:46<06:43, 11.52s/it]
2022-04-10 19:57:45,090 - INFO - tqdm - accuracy: 0.6502, batch_loss: 0.6185, loss: 0.6086 ||:  48%|####7     | 31/65 [05:58<06:32, 11.53s/it]
2022-04-10 19:57:56,673 - INFO - tqdm - accuracy: 0.6465, batch_loss: 0.7076, loss: 0.6117 ||:  49%|####9     | 32/65 [06:09<06:21, 11.55s/it]
2022-04-10 19:58:08,229 - INFO - tqdm - accuracy: 0.6449, batch_loss: 0.5762, loss: 0.6106 ||:  51%|#####     | 33/65 [06:21<06:09, 11.55s/it]
2022-04-10 19:58:19,819 - INFO - tqdm - accuracy: 0.6480, batch_loss: 0.5803, loss: 0.6097 ||:  52%|#####2    | 34/65 [06:32<05:58, 11.56s/it]
2022-04-10 19:58:31,393 - INFO - tqdm - accuracy: 0.6491, batch_loss: 0.5978, loss: 0.6094 ||:  54%|#####3    | 35/65 [06:44<05:46, 11.57s/it]
2022-04-10 19:58:42,971 - INFO - tqdm - accuracy: 0.6519, batch_loss: 0.6499, loss: 0.6105 ||:  55%|#####5    | 36/65 [06:55<05:35, 11.57s/it]
2022-04-10 19:58:54,541 - INFO - tqdm - accuracy: 0.6495, batch_loss: 0.6144, loss: 0.6106 ||:  57%|#####6    | 37/65 [07:07<05:23, 11.57s/it]
2022-04-10 19:59:06,127 - INFO - tqdm - accuracy: 0.6480, batch_loss: 0.6006, loss: 0.6103 ||:  58%|#####8    | 38/65 [07:19<05:12, 11.57s/it]
2022-04-10 19:59:17,702 - INFO - tqdm - accuracy: 0.6490, batch_loss: 0.6194, loss: 0.6106 ||:  60%|######    | 39/65 [07:30<05:00, 11.57s/it]
2022-04-10 19:59:29,262 - INFO - tqdm - accuracy: 0.6469, batch_loss: 0.7601, loss: 0.6143 ||:  62%|######1   | 40/65 [07:42<04:49, 11.57s/it]
2022-04-10 19:59:40,481 - INFO - tqdm - accuracy: 0.6468, batch_loss: 0.6480, loss: 0.6151 ||:  63%|######3   | 41/65 [07:53<04:35, 11.47s/it]
2022-04-10 19:59:52,051 - INFO - tqdm - accuracy: 0.6463, batch_loss: 0.6445, loss: 0.6158 ||:  65%|######4   | 42/65 [08:05<04:24, 11.50s/it]
2022-04-10 20:00:03,387 - INFO - tqdm - accuracy: 0.6473, batch_loss: 0.5399, loss: 0.6141 ||:  66%|######6   | 43/65 [08:16<04:11, 11.45s/it]
2022-04-10 20:00:14,950 - INFO - tqdm - accuracy: 0.6475, batch_loss: 0.5580, loss: 0.6128 ||:  68%|######7   | 44/65 [08:27<04:01, 11.48s/it]
2022-04-10 20:00:26,533 - INFO - tqdm - accuracy: 0.6456, batch_loss: 0.5841, loss: 0.6122 ||:  69%|######9   | 45/65 [08:39<03:50, 11.51s/it]
2022-04-10 20:00:38,100 - INFO - tqdm - accuracy: 0.6458, batch_loss: 0.6130, loss: 0.6122 ||:  71%|#######   | 46/65 [08:51<03:39, 11.53s/it]
2022-04-10 20:00:49,663 - INFO - tqdm - accuracy: 0.6454, batch_loss: 0.5903, loss: 0.6117 ||:  72%|#######2  | 47/65 [09:02<03:27, 11.54s/it]
2022-04-10 20:01:01,219 - INFO - tqdm - accuracy: 0.6450, batch_loss: 0.6743, loss: 0.6130 ||:  74%|#######3  | 48/65 [09:14<03:16, 11.54s/it]
2022-04-10 20:01:12,802 - INFO - tqdm - accuracy: 0.6433, batch_loss: 0.6485, loss: 0.6137 ||:  75%|#######5  | 49/65 [09:25<03:04, 11.56s/it]
2022-04-10 20:01:24,392 - INFO - tqdm - accuracy: 0.6442, batch_loss: 0.5910, loss: 0.6133 ||:  77%|#######6  | 50/65 [09:37<02:53, 11.57s/it]
2022-04-10 20:01:35,961 - INFO - tqdm - accuracy: 0.6456, batch_loss: 0.5911, loss: 0.6129 ||:  78%|#######8  | 51/65 [09:48<02:41, 11.57s/it]
2022-04-10 20:01:47,398 - INFO - tqdm - accuracy: 0.6476, batch_loss: 0.5410, loss: 0.6115 ||:  80%|########  | 52/65 [10:00<02:29, 11.53s/it]
2022-04-10 20:01:58,953 - INFO - tqdm - accuracy: 0.6513, batch_loss: 0.4865, loss: 0.6091 ||:  82%|########1 | 53/65 [10:11<02:18, 11.54s/it]
2022-04-10 20:02:10,501 - INFO - tqdm - accuracy: 0.6520, batch_loss: 0.5863, loss: 0.6087 ||:  83%|########3 | 54/65 [10:23<02:06, 11.54s/it]
2022-04-10 20:02:22,063 - INFO - tqdm - accuracy: 0.6532, batch_loss: 0.5342, loss: 0.6073 ||:  85%|########4 | 55/65 [10:35<01:55, 11.55s/it]
2022-04-10 20:02:33,505 - INFO - tqdm - accuracy: 0.6544, batch_loss: 0.5164, loss: 0.6057 ||:  86%|########6 | 56/65 [10:46<01:43, 11.52s/it]
2022-04-10 20:02:45,061 - INFO - tqdm - accuracy: 0.6539, batch_loss: 0.6164, loss: 0.6059 ||:  88%|########7 | 57/65 [10:58<01:32, 11.53s/it]
2022-04-10 20:02:56,635 - INFO - tqdm - accuracy: 0.6555, batch_loss: 0.5655, loss: 0.6052 ||:  89%|########9 | 58/65 [11:09<01:20, 11.54s/it]
2022-04-10 20:03:08,202 - INFO - tqdm - accuracy: 0.6571, batch_loss: 0.4710, loss: 0.6029 ||:  91%|######### | 59/65 [11:21<01:09, 11.55s/it]
2022-04-10 20:03:19,768 - INFO - tqdm - accuracy: 0.6582, batch_loss: 0.5345, loss: 0.6018 ||:  92%|#########2| 60/65 [11:32<00:57, 11.55s/it]
2022-04-10 20:03:31,334 - INFO - tqdm - accuracy: 0.6586, batch_loss: 0.5487, loss: 0.6009 ||:  94%|#########3| 61/65 [11:44<00:46, 11.56s/it]
2022-04-10 20:03:42,939 - INFO - tqdm - accuracy: 0.6586, batch_loss: 0.5697, loss: 0.6004 ||:  95%|#########5| 62/65 [11:55<00:34, 11.57s/it]
2022-04-10 20:03:54,510 - INFO - tqdm - accuracy: 0.6605, batch_loss: 0.5108, loss: 0.5990 ||:  97%|#########6| 63/65 [12:07<00:23, 11.57s/it]
2022-04-10 20:04:06,084 - INFO - tqdm - accuracy: 0.6590, batch_loss: 0.6443, loss: 0.5997 ||:  98%|#########8| 64/65 [12:19<00:11, 11.57s/it]
2022-04-10 20:04:11,210 - INFO - tqdm - accuracy: 0.6594, batch_loss: 0.5296, loss: 0.5986 ||: 100%|##########| 65/65 [12:24<00:00,  9.64s/it]
2022-04-10 20:04:11,210 - INFO - tqdm - accuracy: 0.6594, batch_loss: 0.5296, loss: 0.5986 ||: 100%|##########| 65/65 [12:24<00:00, 11.45s/it]
2022-04-10 20:04:14,108 - INFO - allennlp.training.trainer - Validating
2022-04-10 20:04:14,109 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-10 20:04:24,365 - INFO - tqdm - accuracy: 0.6790, batch_loss: 1.5907, loss: 0.7216 ||:  36%|###5      | 41/115 [00:10<00:18,  3.92it/s]
2022-04-10 20:04:34,504 - INFO - tqdm - accuracy: 0.6522, batch_loss: 0.2679, loss: 0.7134 ||:  70%|#######   | 81/115 [00:20<00:07,  4.37it/s]
2022-04-10 20:04:43,210 - INFO - tqdm - accuracy: 0.6332, batch_loss: 0.8052, loss: 0.7377 ||: 100%|##########| 115/115 [00:29<00:00,  3.91it/s]
2022-04-10 20:04:43,211 - INFO - tqdm - accuracy: 0.6332, batch_loss: 0.8052, loss: 0.7377 ||: 100%|##########| 115/115 [00:29<00:00,  3.95it/s]
2022-04-10 20:04:43,211 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-10 20:04:43,212 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.659  |     0.633
2022-04-10 20:04:43,213 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-10 20:04:43,213 - INFO - allennlp.training.tensorboard_writer - loss               |     0.599  |     0.738
2022-04-10 20:04:43,214 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5811.043  |       N/A
2022-04-10 20:04:49,017 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to './output_ir-ora-d_hyper5/best.th'.
2022-04-10 20:05:03,963 - INFO - allennlp.training.trainer - Epoch duration: 0:13:16.971689
2022-04-10 20:05:03,963 - INFO - allennlp.training.trainer - Estimated training time remaining: 1:45:16
2022-04-10 20:05:03,963 - INFO - allennlp.training.trainer - Epoch 7/14
2022-04-10 20:05:03,964 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-10 20:05:03,964 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-10 20:05:03,965 - INFO - allennlp.training.trainer - Training
2022-04-10 20:05:03,966 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-10 20:05:15,239 - INFO - tqdm - accuracy: 0.8438, batch_loss: 0.4508, loss: 0.4508 ||:   2%|1         | 1/65 [00:11<12:01, 11.27s/it]
2022-04-10 20:05:26,878 - INFO - tqdm - accuracy: 0.7344, batch_loss: 0.5690, loss: 0.5099 ||:   3%|3         | 2/65 [00:22<12:03, 11.49s/it]
2022-04-10 20:05:38,848 - INFO - tqdm - accuracy: 0.7083, batch_loss: 0.5883, loss: 0.5361 ||:   5%|4         | 3/65 [00:34<12:05, 11.71s/it]
2022-04-10 20:05:50,547 - INFO - tqdm - accuracy: 0.7031, batch_loss: 0.5467, loss: 0.5387 ||:   6%|6         | 4/65 [00:46<11:53, 11.70s/it]
2022-04-10 20:06:01,981 - INFO - tqdm - accuracy: 0.7000, batch_loss: 0.6296, loss: 0.5569 ||:   8%|7         | 5/65 [00:58<11:36, 11.61s/it]
2022-04-10 20:06:13,366 - INFO - tqdm - accuracy: 0.6823, batch_loss: 0.6276, loss: 0.5687 ||:   9%|9         | 6/65 [01:09<11:20, 11.53s/it]
2022-04-10 20:06:24,797 - INFO - tqdm - accuracy: 0.6875, batch_loss: 0.5040, loss: 0.5595 ||:  11%|#         | 7/65 [01:20<11:06, 11.50s/it]
2022-04-10 20:06:36,338 - INFO - tqdm - accuracy: 0.6836, batch_loss: 0.6091, loss: 0.5657 ||:  12%|#2        | 8/65 [01:32<10:56, 11.51s/it]
2022-04-10 20:06:47,967 - INFO - tqdm - accuracy: 0.6875, batch_loss: 0.5840, loss: 0.5677 ||:  14%|#3        | 9/65 [01:44<10:46, 11.55s/it]
2022-04-10 20:06:59,566 - INFO - tqdm - accuracy: 0.6750, batch_loss: 0.6794, loss: 0.5789 ||:  15%|#5        | 10/65 [01:55<10:36, 11.56s/it]
2022-04-10 20:07:11,090 - INFO - tqdm - accuracy: 0.6761, batch_loss: 0.6122, loss: 0.5819 ||:  17%|#6        | 11/65 [02:07<10:23, 11.55s/it]
2022-04-10 20:07:22,591 - INFO - tqdm - accuracy: 0.6667, batch_loss: 0.6259, loss: 0.5856 ||:  18%|#8        | 12/65 [02:18<10:11, 11.54s/it]
2022-04-10 20:07:34,093 - INFO - tqdm - accuracy: 0.6587, batch_loss: 0.7480, loss: 0.5981 ||:  20%|##        | 13/65 [02:30<09:59, 11.53s/it]
2022-04-10 20:07:45,596 - INFO - tqdm - accuracy: 0.6674, batch_loss: 0.5236, loss: 0.5927 ||:  22%|##1       | 14/65 [02:41<09:47, 11.52s/it]
2022-04-10 20:07:56,976 - INFO - tqdm - accuracy: 0.6667, batch_loss: 0.5963, loss: 0.5930 ||:  23%|##3       | 15/65 [02:53<09:33, 11.48s/it]
2022-04-10 20:08:08,487 - INFO - tqdm - accuracy: 0.6660, batch_loss: 0.5642, loss: 0.5912 ||:  25%|##4       | 16/65 [03:04<09:22, 11.49s/it]
2022-04-10 20:08:19,877 - INFO - tqdm - accuracy: 0.6562, batch_loss: 0.7477, loss: 0.6004 ||:  26%|##6       | 17/65 [03:15<09:09, 11.46s/it]
2022-04-10 20:08:31,413 - INFO - tqdm - accuracy: 0.6615, batch_loss: 0.5291, loss: 0.5964 ||:  28%|##7       | 18/65 [03:27<08:59, 11.48s/it]
2022-04-10 20:08:42,958 - INFO - tqdm - accuracy: 0.6645, batch_loss: 0.5578, loss: 0.5944 ||:  29%|##9       | 19/65 [03:38<08:49, 11.50s/it]
2022-04-10 20:08:54,379 - INFO - tqdm - accuracy: 0.6672, batch_loss: 0.5840, loss: 0.5939 ||:  31%|###       | 20/65 [03:50<08:36, 11.48s/it]
2022-04-10 20:09:05,932 - INFO - tqdm - accuracy: 0.6682, batch_loss: 0.5544, loss: 0.5920 ||:  32%|###2      | 21/65 [04:01<08:25, 11.50s/it]
2022-04-10 20:09:17,489 - INFO - tqdm - accuracy: 0.6634, batch_loss: 0.7436, loss: 0.5989 ||:  34%|###3      | 22/65 [04:13<08:15, 11.52s/it]
2022-04-10 20:09:29,045 - INFO - tqdm - accuracy: 0.6671, batch_loss: 0.5506, loss: 0.5968 ||:  35%|###5      | 23/65 [04:25<08:04, 11.53s/it]
2022-04-10 20:09:40,605 - INFO - tqdm - accuracy: 0.6654, batch_loss: 0.5682, loss: 0.5956 ||:  37%|###6      | 24/65 [04:36<07:53, 11.54s/it]
2022-04-10 20:09:52,158 - INFO - tqdm - accuracy: 0.6687, batch_loss: 0.5035, loss: 0.5919 ||:  38%|###8      | 25/65 [04:48<07:41, 11.54s/it]
2022-04-10 20:10:03,685 - INFO - tqdm - accuracy: 0.6695, batch_loss: 0.4847, loss: 0.5878 ||:  40%|####      | 26/65 [04:59<07:29, 11.54s/it]
2022-04-10 20:10:15,227 - INFO - tqdm - accuracy: 0.6644, batch_loss: 0.6098, loss: 0.5886 ||:  42%|####1     | 27/65 [05:11<07:18, 11.54s/it]
2022-04-10 20:10:26,779 - INFO - tqdm - accuracy: 0.6685, batch_loss: 0.5184, loss: 0.5861 ||:  43%|####3     | 28/65 [05:22<07:07, 11.54s/it]
2022-04-10 20:10:38,315 - INFO - tqdm - accuracy: 0.6724, batch_loss: 0.5450, loss: 0.5847 ||:  45%|####4     | 29/65 [05:34<06:55, 11.54s/it]
2022-04-10 20:10:49,889 - INFO - tqdm - accuracy: 0.6740, batch_loss: 0.5752, loss: 0.5844 ||:  46%|####6     | 30/65 [05:45<06:44, 11.55s/it]
2022-04-10 20:11:01,490 - INFO - tqdm - accuracy: 0.6754, batch_loss: 0.5366, loss: 0.5828 ||:  48%|####7     | 31/65 [05:57<06:33, 11.57s/it]
2022-04-10 20:11:13,056 - INFO - tqdm - accuracy: 0.6738, batch_loss: 0.6218, loss: 0.5840 ||:  49%|####9     | 32/65 [06:09<06:21, 11.57s/it]
2022-04-10 20:11:24,608 - INFO - tqdm - accuracy: 0.6733, batch_loss: 0.5591, loss: 0.5833 ||:  51%|#####     | 33/65 [06:20<06:09, 11.56s/it]
2022-04-10 20:11:36,174 - INFO - tqdm - accuracy: 0.6710, batch_loss: 0.6872, loss: 0.5863 ||:  52%|#####2    | 34/65 [06:32<05:58, 11.56s/it]
2022-04-10 20:11:47,722 - INFO - tqdm - accuracy: 0.6625, batch_loss: 0.8260, loss: 0.5932 ||:  54%|#####3    | 35/65 [06:43<05:46, 11.56s/it]
2022-04-10 20:11:59,294 - INFO - tqdm - accuracy: 0.6615, batch_loss: 0.5939, loss: 0.5932 ||:  55%|#####5    | 36/65 [06:55<05:35, 11.56s/it]
2022-04-10 20:12:10,882 - INFO - tqdm - accuracy: 0.6605, batch_loss: 0.5283, loss: 0.5915 ||:  57%|#####6    | 37/65 [07:06<05:23, 11.57s/it]
2022-04-10 20:12:22,435 - INFO - tqdm - accuracy: 0.6628, batch_loss: 0.5567, loss: 0.5905 ||:  58%|#####8    | 38/65 [07:18<05:12, 11.57s/it]
2022-04-10 20:12:33,984 - INFO - tqdm - accuracy: 0.6619, batch_loss: 0.6361, loss: 0.5917 ||:  60%|######    | 39/65 [07:30<05:00, 11.56s/it]
2022-04-10 20:12:45,542 - INFO - tqdm - accuracy: 0.6648, batch_loss: 0.5298, loss: 0.5902 ||:  62%|######1   | 40/65 [07:41<04:48, 11.56s/it]
2022-04-10 20:12:57,094 - INFO - tqdm - accuracy: 0.6669, batch_loss: 0.5845, loss: 0.5900 ||:  63%|######3   | 41/65 [07:53<04:37, 11.56s/it]
2022-04-10 20:13:08,621 - INFO - tqdm - accuracy: 0.6667, batch_loss: 0.5929, loss: 0.5901 ||:  65%|######4   | 42/65 [08:04<04:25, 11.55s/it]
2022-04-10 20:13:20,169 - INFO - tqdm - accuracy: 0.6693, batch_loss: 0.5745, loss: 0.5897 ||:  66%|######6   | 43/65 [08:16<04:14, 11.55s/it]
2022-04-10 20:13:31,719 - INFO - tqdm - accuracy: 0.6676, batch_loss: 0.6433, loss: 0.5909 ||:  68%|######7   | 44/65 [08:27<04:02, 11.55s/it]
2022-04-10 20:13:43,652 - INFO - tqdm - accuracy: 0.6701, batch_loss: 0.4824, loss: 0.5885 ||:  69%|######9   | 45/65 [08:39<03:53, 11.66s/it]
2022-04-10 20:13:55,141 - INFO - tqdm - accuracy: 0.6692, batch_loss: 0.6498, loss: 0.5899 ||:  71%|#######   | 46/65 [08:51<03:40, 11.61s/it]
2022-04-10 20:14:06,644 - INFO - tqdm - accuracy: 0.6695, batch_loss: 0.5504, loss: 0.5890 ||:  72%|#######2  | 47/65 [09:02<03:28, 11.58s/it]
2022-04-10 20:14:18,163 - INFO - tqdm - accuracy: 0.6680, batch_loss: 0.6493, loss: 0.5903 ||:  74%|#######3  | 48/65 [09:14<03:16, 11.56s/it]
2022-04-10 20:14:29,684 - INFO - tqdm - accuracy: 0.6690, batch_loss: 0.5485, loss: 0.5894 ||:  75%|#######5  | 49/65 [09:25<03:04, 11.55s/it]
2022-04-10 20:14:41,199 - INFO - tqdm - accuracy: 0.6700, batch_loss: 0.6129, loss: 0.5899 ||:  77%|#######6  | 50/65 [09:37<02:53, 11.54s/it]
2022-04-10 20:14:52,711 - INFO - tqdm - accuracy: 0.6697, batch_loss: 0.5623, loss: 0.5894 ||:  78%|#######8  | 51/65 [09:48<02:41, 11.53s/it]
2022-04-10 20:15:04,225 - INFO - tqdm - accuracy: 0.6707, batch_loss: 0.6203, loss: 0.5900 ||:  80%|########  | 52/65 [10:00<02:29, 11.53s/it]
2022-04-10 20:15:15,737 - INFO - tqdm - accuracy: 0.6728, batch_loss: 0.5047, loss: 0.5883 ||:  82%|########1 | 53/65 [10:11<02:18, 11.52s/it]
2022-04-10 20:15:27,247 - INFO - tqdm - accuracy: 0.6713, batch_loss: 0.5919, loss: 0.5884 ||:  83%|########3 | 54/65 [10:23<02:06, 11.52s/it]
2022-04-10 20:15:38,758 - INFO - tqdm - accuracy: 0.6722, batch_loss: 0.5386, loss: 0.5875 ||:  85%|########4 | 55/65 [10:34<01:55, 11.52s/it]
2022-04-10 20:15:50,276 - INFO - tqdm - accuracy: 0.6735, batch_loss: 0.5660, loss: 0.5871 ||:  86%|########6 | 56/65 [10:46<01:43, 11.52s/it]
2022-04-10 20:16:01,816 - INFO - tqdm - accuracy: 0.6721, batch_loss: 0.6056, loss: 0.5874 ||:  88%|########7 | 57/65 [10:57<01:32, 11.52s/it]
2022-04-10 20:16:13,326 - INFO - tqdm - accuracy: 0.6730, batch_loss: 0.5077, loss: 0.5861 ||:  89%|########9 | 58/65 [11:09<01:20, 11.52s/it]
2022-04-10 20:16:24,229 - INFO - tqdm - accuracy: 0.6746, batch_loss: 0.4841, loss: 0.5843 ||:  91%|######### | 59/65 [11:20<01:08, 11.33s/it]
2022-04-10 20:16:35,742 - INFO - tqdm - accuracy: 0.6759, batch_loss: 0.4475, loss: 0.5821 ||:  92%|#########2| 60/65 [11:31<00:56, 11.39s/it]
2022-04-10 20:16:47,254 - INFO - tqdm - accuracy: 0.6766, batch_loss: 0.5427, loss: 0.5814 ||:  94%|#########3| 61/65 [11:43<00:45, 11.43s/it]
2022-04-10 20:16:58,787 - INFO - tqdm - accuracy: 0.6773, batch_loss: 0.5568, loss: 0.5810 ||:  95%|#########5| 62/65 [11:54<00:34, 11.46s/it]
2022-04-10 20:17:10,305 - INFO - tqdm - accuracy: 0.6759, batch_loss: 0.5908, loss: 0.5812 ||:  97%|#########6| 63/65 [12:06<00:22, 11.48s/it]
2022-04-10 20:17:21,840 - INFO - tqdm - accuracy: 0.6756, batch_loss: 0.5698, loss: 0.5810 ||:  98%|#########8| 64/65 [12:17<00:11, 11.49s/it]
2022-04-10 20:17:26,968 - INFO - tqdm - accuracy: 0.6764, batch_loss: 0.5706, loss: 0.5808 ||: 100%|##########| 65/65 [12:23<00:00,  9.58s/it]
2022-04-10 20:17:26,969 - INFO - tqdm - accuracy: 0.6764, batch_loss: 0.5706, loss: 0.5808 ||: 100%|##########| 65/65 [12:23<00:00, 11.43s/it]
2022-04-10 20:17:29,789 - INFO - allennlp.training.trainer - Validating
2022-04-10 20:17:29,791 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-10 20:17:39,837 - INFO - tqdm - accuracy: 0.7125, batch_loss: 0.5713, loss: 0.6846 ||:  35%|###4      | 40/115 [00:10<00:18,  4.01it/s]
2022-04-10 20:17:49,937 - INFO - tqdm - accuracy: 0.6792, batch_loss: 0.7524, loss: 0.7342 ||:  70%|######9   | 80/115 [00:20<00:08,  3.91it/s]
2022-04-10 20:17:58,912 - INFO - tqdm - accuracy: 0.6507, batch_loss: 1.0676, loss: 0.7514 ||: 100%|##########| 115/115 [00:29<00:00,  3.90it/s]
2022-04-10 20:17:58,913 - INFO - tqdm - accuracy: 0.6507, batch_loss: 1.0676, loss: 0.7514 ||: 100%|##########| 115/115 [00:29<00:00,  3.95it/s]
2022-04-10 20:17:58,913 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-10 20:17:58,913 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.676  |     0.651
2022-04-10 20:17:58,914 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-10 20:17:58,915 - INFO - allennlp.training.tensorboard_writer - loss               |     0.581  |     0.751
2022-04-10 20:17:58,915 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5811.043  |       N/A
2022-04-10 20:18:04,606 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to './output_ir-ora-d_hyper5/best.th'.
2022-04-10 20:18:15,970 - INFO - allennlp.training.trainer - Epoch duration: 0:13:12.006368
2022-04-10 20:18:15,970 - INFO - allennlp.training.trainer - Estimated training time remaining: 1:32:09
2022-04-10 20:18:15,970 - INFO - allennlp.training.trainer - Epoch 8/14
2022-04-10 20:18:15,970 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-10 20:18:15,971 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-10 20:18:15,972 - INFO - allennlp.training.trainer - Training
2022-04-10 20:18:15,972 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-10 20:18:27,212 - INFO - tqdm - accuracy: 0.7188, batch_loss: 0.5678, loss: 0.5678 ||:   2%|1         | 1/65 [00:11<11:59, 11.24s/it]
2022-04-10 20:18:38,774 - INFO - tqdm - accuracy: 0.7344, batch_loss: 0.6084, loss: 0.5881 ||:   3%|3         | 2/65 [00:22<12:00, 11.43s/it]
2022-04-10 20:18:50,676 - INFO - tqdm - accuracy: 0.7500, batch_loss: 0.4584, loss: 0.5449 ||:   5%|4         | 3/65 [00:34<12:01, 11.64s/it]
2022-04-10 20:19:02,462 - INFO - tqdm - accuracy: 0.7422, batch_loss: 0.4975, loss: 0.5330 ||:   6%|6         | 4/65 [00:46<11:53, 11.70s/it]
2022-04-10 20:19:13,932 - INFO - tqdm - accuracy: 0.7188, batch_loss: 0.6041, loss: 0.5472 ||:   8%|7         | 5/65 [00:57<11:37, 11.62s/it]
2022-04-10 20:19:25,348 - INFO - tqdm - accuracy: 0.7344, batch_loss: 0.4997, loss: 0.5393 ||:   9%|9         | 6/65 [01:09<11:21, 11.55s/it]
2022-04-10 20:19:36,746 - INFO - tqdm - accuracy: 0.7321, batch_loss: 0.6204, loss: 0.5509 ||:  11%|#         | 7/65 [01:20<11:06, 11.50s/it]
2022-04-10 20:19:48,206 - INFO - tqdm - accuracy: 0.7266, batch_loss: 0.5337, loss: 0.5487 ||:  12%|#2        | 8/65 [01:32<10:54, 11.49s/it]
2022-04-10 20:19:59,782 - INFO - tqdm - accuracy: 0.7222, batch_loss: 0.5145, loss: 0.5449 ||:  14%|#3        | 9/65 [01:43<10:44, 11.51s/it]
2022-04-10 20:20:11,402 - INFO - tqdm - accuracy: 0.7219, batch_loss: 0.4808, loss: 0.5385 ||:  15%|#5        | 10/65 [01:55<10:35, 11.55s/it]
2022-04-10 20:20:22,984 - INFO - tqdm - accuracy: 0.7131, batch_loss: 0.6557, loss: 0.5492 ||:  17%|#6        | 11/65 [02:07<10:24, 11.56s/it]
2022-04-10 20:20:34,512 - INFO - tqdm - accuracy: 0.7109, batch_loss: 0.5390, loss: 0.5483 ||:  18%|#8        | 12/65 [02:18<10:12, 11.55s/it]
2022-04-10 20:20:45,986 - INFO - tqdm - accuracy: 0.7212, batch_loss: 0.4324, loss: 0.5394 ||:  20%|##        | 13/65 [02:30<09:59, 11.53s/it]
2022-04-10 20:20:57,510 - INFO - tqdm - accuracy: 0.7210, batch_loss: 0.5156, loss: 0.5377 ||:  22%|##1       | 14/65 [02:41<09:47, 11.53s/it]
2022-04-10 20:21:09,032 - INFO - tqdm - accuracy: 0.7229, batch_loss: 0.5254, loss: 0.5369 ||:  23%|##3       | 15/65 [02:53<09:36, 11.52s/it]
2022-04-10 20:21:20,538 - INFO - tqdm - accuracy: 0.7188, batch_loss: 0.6471, loss: 0.5438 ||:  25%|##4       | 16/65 [03:04<09:24, 11.52s/it]
2022-04-10 20:21:32,079 - INFO - tqdm - accuracy: 0.7151, batch_loss: 0.5538, loss: 0.5444 ||:  26%|##6       | 17/65 [03:16<09:13, 11.53s/it]
2022-04-10 20:21:43,637 - INFO - tqdm - accuracy: 0.7066, batch_loss: 0.6751, loss: 0.5516 ||:  28%|##7       | 18/65 [03:27<09:02, 11.54s/it]
2022-04-10 20:21:55,197 - INFO - tqdm - accuracy: 0.7138, batch_loss: 0.5231, loss: 0.5501 ||:  29%|##9       | 19/65 [03:39<08:50, 11.54s/it]
2022-04-10 20:22:06,791 - INFO - tqdm - accuracy: 0.7094, batch_loss: 0.6848, loss: 0.5569 ||:  31%|###       | 20/65 [03:50<08:40, 11.56s/it]
2022-04-10 20:22:18,368 - INFO - tqdm - accuracy: 0.7068, batch_loss: 0.6675, loss: 0.5621 ||:  32%|###2      | 21/65 [04:02<08:28, 11.56s/it]
2022-04-10 20:22:29,909 - INFO - tqdm - accuracy: 0.7116, batch_loss: 0.4623, loss: 0.5576 ||:  34%|###3      | 22/65 [04:13<08:16, 11.56s/it]
2022-04-10 20:22:41,420 - INFO - tqdm - accuracy: 0.7120, batch_loss: 0.5398, loss: 0.5568 ||:  35%|###5      | 23/65 [04:25<08:04, 11.54s/it]
2022-04-10 20:22:52,934 - INFO - tqdm - accuracy: 0.7096, batch_loss: 0.5701, loss: 0.5574 ||:  37%|###6      | 24/65 [04:36<07:52, 11.53s/it]
2022-04-10 20:23:04,448 - INFO - tqdm - accuracy: 0.7125, batch_loss: 0.4711, loss: 0.5539 ||:  38%|###8      | 25/65 [04:48<07:41, 11.53s/it]
2022-04-10 20:23:15,994 - INFO - tqdm - accuracy: 0.7151, batch_loss: 0.5358, loss: 0.5532 ||:  40%|####      | 26/65 [05:00<07:29, 11.53s/it]
2022-04-10 20:23:27,324 - INFO - tqdm - accuracy: 0.7141, batch_loss: 0.5378, loss: 0.5527 ||:  42%|####1     | 27/65 [05:11<07:15, 11.47s/it]
2022-04-10 20:23:38,914 - INFO - tqdm - accuracy: 0.7143, batch_loss: 0.6095, loss: 0.5547 ||:  43%|####3     | 28/65 [05:22<07:05, 11.51s/it]
2022-04-10 20:23:50,482 - INFO - tqdm - accuracy: 0.7155, batch_loss: 0.5267, loss: 0.5537 ||:  45%|####4     | 29/65 [05:34<06:54, 11.53s/it]
2022-04-10 20:24:02,052 - INFO - tqdm - accuracy: 0.7167, batch_loss: 0.5462, loss: 0.5535 ||:  46%|####6     | 30/65 [05:46<06:43, 11.54s/it]
2022-04-10 20:24:13,586 - INFO - tqdm - accuracy: 0.7157, batch_loss: 0.5785, loss: 0.5543 ||:  48%|####7     | 31/65 [05:57<06:32, 11.54s/it]
2022-04-10 20:24:25,097 - INFO - tqdm - accuracy: 0.7139, batch_loss: 0.5019, loss: 0.5526 ||:  49%|####9     | 32/65 [06:09<06:20, 11.53s/it]
2022-04-10 20:24:36,630 - INFO - tqdm - accuracy: 0.7140, batch_loss: 0.5448, loss: 0.5524 ||:  51%|#####     | 33/65 [06:20<06:08, 11.53s/it]
2022-04-10 20:24:48,184 - INFO - tqdm - accuracy: 0.7114, batch_loss: 0.6130, loss: 0.5542 ||:  52%|#####2    | 34/65 [06:32<05:57, 11.54s/it]
2022-04-10 20:24:59,715 - INFO - tqdm - accuracy: 0.7116, batch_loss: 0.5430, loss: 0.5539 ||:  54%|#####3    | 35/65 [06:43<05:46, 11.54s/it]
2022-04-10 20:25:11,250 - INFO - tqdm - accuracy: 0.7092, batch_loss: 0.6347, loss: 0.5561 ||:  55%|#####5    | 36/65 [06:55<05:34, 11.54s/it]
2022-04-10 20:25:22,779 - INFO - tqdm - accuracy: 0.7078, batch_loss: 0.5999, loss: 0.5573 ||:  57%|#####6    | 37/65 [07:06<05:22, 11.53s/it]
2022-04-10 20:25:34,308 - INFO - tqdm - accuracy: 0.7081, batch_loss: 0.5722, loss: 0.5577 ||:  58%|#####8    | 38/65 [07:18<05:11, 11.53s/it]
2022-04-10 20:25:45,559 - INFO - tqdm - accuracy: 0.7043, batch_loss: 0.6233, loss: 0.5594 ||:  60%|######    | 39/65 [07:29<04:57, 11.45s/it]
2022-04-10 20:25:57,087 - INFO - tqdm - accuracy: 0.7039, batch_loss: 0.5392, loss: 0.5589 ||:  62%|######1   | 40/65 [07:41<04:46, 11.47s/it]
2022-04-10 20:26:08,606 - INFO - tqdm - accuracy: 0.7027, batch_loss: 0.5877, loss: 0.5596 ||:  63%|######3   | 41/65 [07:52<04:35, 11.49s/it]
2022-04-10 20:26:20,106 - INFO - tqdm - accuracy: 0.6987, batch_loss: 0.7276, loss: 0.5636 ||:  65%|######4   | 42/65 [08:04<04:24, 11.49s/it]
2022-04-10 20:26:31,633 - INFO - tqdm - accuracy: 0.6969, batch_loss: 0.5946, loss: 0.5643 ||:  66%|######6   | 43/65 [08:15<04:13, 11.50s/it]
2022-04-10 20:26:43,120 - INFO - tqdm - accuracy: 0.6953, batch_loss: 0.6559, loss: 0.5664 ||:  68%|######7   | 44/65 [08:27<04:01, 11.50s/it]
2022-04-10 20:26:54,604 - INFO - tqdm - accuracy: 0.6972, batch_loss: 0.5216, loss: 0.5654 ||:  69%|######9   | 45/65 [08:38<03:49, 11.49s/it]
2022-04-10 20:27:06,102 - INFO - tqdm - accuracy: 0.6997, batch_loss: 0.4694, loss: 0.5633 ||:  71%|#######   | 46/65 [08:50<03:38, 11.49s/it]
2022-04-10 20:27:17,592 - INFO - tqdm - accuracy: 0.7001, batch_loss: 0.5516, loss: 0.5630 ||:  72%|#######2  | 47/65 [09:01<03:26, 11.49s/it]
2022-04-10 20:27:29,089 - INFO - tqdm - accuracy: 0.6999, batch_loss: 0.6212, loss: 0.5643 ||:  74%|#######3  | 48/65 [09:13<03:15, 11.49s/it]
2022-04-10 20:27:40,569 - INFO - tqdm - accuracy: 0.7003, batch_loss: 0.4289, loss: 0.5615 ||:  75%|#######5  | 49/65 [09:24<03:03, 11.49s/it]
2022-04-10 20:27:52,037 - INFO - tqdm - accuracy: 0.6981, batch_loss: 0.5341, loss: 0.5610 ||:  77%|#######6  | 50/65 [09:36<02:52, 11.48s/it]
2022-04-10 20:28:03,523 - INFO - tqdm - accuracy: 0.6949, batch_loss: 0.6558, loss: 0.5628 ||:  78%|#######8  | 51/65 [09:47<02:40, 11.48s/it]
2022-04-10 20:28:15,003 - INFO - tqdm - accuracy: 0.6905, batch_loss: 0.7366, loss: 0.5662 ||:  80%|########  | 52/65 [09:59<02:29, 11.48s/it]
2022-04-10 20:28:26,488 - INFO - tqdm - accuracy: 0.6910, batch_loss: 0.5137, loss: 0.5652 ||:  82%|########1 | 53/65 [10:10<02:17, 11.48s/it]
2022-04-10 20:28:37,983 - INFO - tqdm - accuracy: 0.6921, batch_loss: 0.5521, loss: 0.5649 ||:  83%|########3 | 54/65 [10:22<02:06, 11.49s/it]
2022-04-10 20:28:49,473 - INFO - tqdm - accuracy: 0.6943, batch_loss: 0.4484, loss: 0.5628 ||:  85%|########4 | 55/65 [10:33<01:54, 11.49s/it]
2022-04-10 20:29:00,980 - INFO - tqdm - accuracy: 0.6925, batch_loss: 0.6020, loss: 0.5635 ||:  86%|########6 | 56/65 [10:45<01:43, 11.49s/it]
2022-04-10 20:29:12,496 - INFO - tqdm - accuracy: 0.6924, batch_loss: 0.6109, loss: 0.5643 ||:  88%|########7 | 57/65 [10:56<01:32, 11.50s/it]
2022-04-10 20:29:24,022 - INFO - tqdm - accuracy: 0.6913, batch_loss: 0.6443, loss: 0.5657 ||:  89%|########9 | 58/65 [11:08<01:20, 11.51s/it]
2022-04-10 20:29:35,551 - INFO - tqdm - accuracy: 0.6907, batch_loss: 0.5914, loss: 0.5661 ||:  91%|######### | 59/65 [11:19<01:09, 11.51s/it]
2022-04-10 20:29:46,743 - INFO - tqdm - accuracy: 0.6910, batch_loss: 0.5132, loss: 0.5653 ||:  92%|#########2| 60/65 [11:30<00:57, 11.42s/it]
2022-04-10 20:29:58,280 - INFO - tqdm - accuracy: 0.6914, batch_loss: 0.5601, loss: 0.5652 ||:  94%|#########3| 61/65 [11:42<00:45, 11.45s/it]
2022-04-10 20:30:09,834 - INFO - tqdm - accuracy: 0.6919, batch_loss: 0.5700, loss: 0.5653 ||:  95%|#########5| 62/65 [11:53<00:34, 11.48s/it]
2022-04-10 20:30:21,408 - INFO - tqdm - accuracy: 0.6933, batch_loss: 0.4779, loss: 0.5639 ||:  97%|#########6| 63/65 [12:05<00:23, 11.51s/it]
2022-04-10 20:30:32,938 - INFO - tqdm - accuracy: 0.6952, batch_loss: 0.4712, loss: 0.5624 ||:  98%|#########8| 64/65 [12:16<00:11, 11.52s/it]
2022-04-10 20:30:37,929 - INFO - tqdm - accuracy: 0.6958, batch_loss: 0.4903, loss: 0.5613 ||: 100%|##########| 65/65 [12:21<00:00,  9.56s/it]
2022-04-10 20:30:37,929 - INFO - tqdm - accuracy: 0.6958, batch_loss: 0.4903, loss: 0.5613 ||: 100%|##########| 65/65 [12:21<00:00, 11.41s/it]
2022-04-10 20:30:40,764 - INFO - allennlp.training.trainer - Validating
2022-04-10 20:30:40,765 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-10 20:30:50,893 - INFO - tqdm - accuracy: 0.6125, batch_loss: 0.5884, loss: 0.7479 ||:  35%|###4      | 40/115 [00:10<00:19,  3.92it/s]
2022-04-10 20:31:01,110 - INFO - tqdm - accuracy: 0.6188, batch_loss: 0.7023, loss: 0.7429 ||:  70%|######9   | 80/115 [00:20<00:08,  3.90it/s]
2022-04-10 20:31:09,868 - INFO - tqdm - accuracy: 0.6245, batch_loss: 0.9764, loss: 0.7494 ||: 100%|##########| 115/115 [00:29<00:00,  3.97it/s]
2022-04-10 20:31:09,868 - INFO - tqdm - accuracy: 0.6245, batch_loss: 0.9764, loss: 0.7494 ||: 100%|##########| 115/115 [00:29<00:00,  3.95it/s]
2022-04-10 20:31:09,869 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-10 20:31:09,869 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.696  |     0.624
2022-04-10 20:31:09,870 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-10 20:31:09,871 - INFO - allennlp.training.tensorboard_writer - loss               |     0.561  |     0.749
2022-04-10 20:31:09,871 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5811.043  |       N/A
2022-04-10 20:31:15,989 - INFO - allennlp.training.trainer - Epoch duration: 0:13:00.019345
2022-04-10 20:31:15,990 - INFO - allennlp.training.trainer - Estimated training time remaining: 1:18:52
2022-04-10 20:31:15,990 - INFO - allennlp.training.trainer - Epoch 9/14
2022-04-10 20:31:15,990 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-10 20:31:15,990 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-10 20:31:15,991 - INFO - allennlp.training.trainer - Training
2022-04-10 20:31:15,992 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-10 20:31:27,436 - INFO - tqdm - accuracy: 0.7188, batch_loss: 0.4829, loss: 0.4829 ||:   2%|1         | 1/65 [00:11<12:12, 11.44s/it]
2022-04-10 20:31:39,141 - INFO - tqdm - accuracy: 0.7031, batch_loss: 0.5878, loss: 0.5354 ||:   3%|3         | 2/65 [00:23<12:10, 11.60s/it]
2022-04-10 20:31:50,976 - INFO - tqdm - accuracy: 0.6667, batch_loss: 0.6054, loss: 0.5587 ||:   5%|4         | 3/65 [00:34<12:05, 11.71s/it]
2022-04-10 20:32:02,360 - INFO - tqdm - accuracy: 0.6875, batch_loss: 0.5567, loss: 0.5582 ||:   6%|6         | 4/65 [00:46<11:46, 11.58s/it]
2022-04-10 20:32:13,809 - INFO - tqdm - accuracy: 0.6625, batch_loss: 0.5766, loss: 0.5619 ||:   8%|7         | 5/65 [00:57<11:31, 11.53s/it]
2022-04-10 20:32:25,278 - INFO - tqdm - accuracy: 0.6719, batch_loss: 0.5828, loss: 0.5654 ||:   9%|9         | 6/65 [01:09<11:19, 11.51s/it]
2022-04-10 20:32:36,839 - INFO - tqdm - accuracy: 0.6696, batch_loss: 0.5655, loss: 0.5654 ||:  11%|#         | 7/65 [01:20<11:08, 11.53s/it]
2022-04-10 20:32:48,441 - INFO - tqdm - accuracy: 0.6680, batch_loss: 0.6412, loss: 0.5749 ||:  12%|#2        | 8/65 [01:32<10:58, 11.55s/it]
2022-04-10 20:33:00,020 - INFO - tqdm - accuracy: 0.6771, batch_loss: 0.4811, loss: 0.5644 ||:  14%|#3        | 9/65 [01:44<10:47, 11.56s/it]
2022-04-10 20:33:11,563 - INFO - tqdm - accuracy: 0.6875, batch_loss: 0.5069, loss: 0.5587 ||:  15%|#5        | 10/65 [01:55<10:35, 11.55s/it]
2022-04-10 20:33:23,089 - INFO - tqdm - accuracy: 0.6989, batch_loss: 0.5152, loss: 0.5547 ||:  17%|#6        | 11/65 [02:07<10:23, 11.55s/it]
2022-04-10 20:33:34,608 - INFO - tqdm - accuracy: 0.7031, batch_loss: 0.5781, loss: 0.5567 ||:  18%|#8        | 12/65 [02:18<10:11, 11.54s/it]
2022-04-10 20:33:46,105 - INFO - tqdm - accuracy: 0.6995, batch_loss: 0.5646, loss: 0.5573 ||:  20%|##        | 13/65 [02:30<09:59, 11.53s/it]
2022-04-10 20:33:57,597 - INFO - tqdm - accuracy: 0.7054, batch_loss: 0.4409, loss: 0.5490 ||:  22%|##1       | 14/65 [02:41<09:47, 11.52s/it]
2022-04-10 20:34:09,462 - INFO - tqdm - accuracy: 0.7042, batch_loss: 0.5405, loss: 0.5484 ||:  23%|##3       | 15/65 [02:53<09:41, 11.62s/it]
2022-04-10 20:34:20,927 - INFO - tqdm - accuracy: 0.7070, batch_loss: 0.5273, loss: 0.5471 ||:  25%|##4       | 16/65 [03:04<09:27, 11.57s/it]
2022-04-10 20:34:32,425 - INFO - tqdm - accuracy: 0.7169, batch_loss: 0.4149, loss: 0.5393 ||:  26%|##6       | 17/65 [03:16<09:14, 11.55s/it]
2022-04-10 20:34:43,967 - INFO - tqdm - accuracy: 0.7205, batch_loss: 0.4909, loss: 0.5366 ||:  28%|##7       | 18/65 [03:27<09:02, 11.55s/it]
2022-04-10 20:34:55,508 - INFO - tqdm - accuracy: 0.7122, batch_loss: 0.6654, loss: 0.5434 ||:  29%|##9       | 19/65 [03:39<08:51, 11.55s/it]
2022-04-10 20:35:07,054 - INFO - tqdm - accuracy: 0.7078, batch_loss: 0.7237, loss: 0.5524 ||:  31%|###       | 20/65 [03:51<08:39, 11.55s/it]
2022-04-10 20:35:18,599 - INFO - tqdm - accuracy: 0.7068, batch_loss: 0.6203, loss: 0.5556 ||:  32%|###2      | 21/65 [04:02<08:28, 11.55s/it]
2022-04-10 20:35:30,126 - INFO - tqdm - accuracy: 0.7031, batch_loss: 0.6109, loss: 0.5582 ||:  34%|###3      | 22/65 [04:14<08:16, 11.54s/it]
2022-04-10 20:35:41,652 - INFO - tqdm - accuracy: 0.6997, batch_loss: 0.6044, loss: 0.5602 ||:  35%|###5      | 23/65 [04:25<08:04, 11.54s/it]
2022-04-10 20:35:53,156 - INFO - tqdm - accuracy: 0.6966, batch_loss: 0.7258, loss: 0.5671 ||:  37%|###6      | 24/65 [04:37<07:52, 11.53s/it]
2022-04-10 20:36:04,690 - INFO - tqdm - accuracy: 0.6987, batch_loss: 0.4930, loss: 0.5641 ||:  38%|###8      | 25/65 [04:48<07:41, 11.53s/it]
2022-04-10 20:36:16,234 - INFO - tqdm - accuracy: 0.6971, batch_loss: 0.5892, loss: 0.5651 ||:  40%|####      | 26/65 [05:00<07:29, 11.53s/it]
2022-04-10 20:36:27,777 - INFO - tqdm - accuracy: 0.6956, batch_loss: 0.6430, loss: 0.5680 ||:  42%|####1     | 27/65 [05:11<07:18, 11.54s/it]
2022-04-10 20:36:39,332 - INFO - tqdm - accuracy: 0.6975, batch_loss: 0.5376, loss: 0.5669 ||:  43%|####3     | 28/65 [05:23<07:07, 11.54s/it]
2022-04-10 20:36:50,890 - INFO - tqdm - accuracy: 0.6994, batch_loss: 0.4831, loss: 0.5640 ||:  45%|####4     | 29/65 [05:34<06:55, 11.55s/it]
2022-04-10 20:37:02,342 - INFO - tqdm - accuracy: 0.6990, batch_loss: 0.4236, loss: 0.5593 ||:  46%|####6     | 30/65 [05:46<06:43, 11.52s/it]
2022-04-10 20:37:13,892 - INFO - tqdm - accuracy: 0.7026, batch_loss: 0.4319, loss: 0.5552 ||:  48%|####7     | 31/65 [05:57<06:31, 11.53s/it]
2022-04-10 20:37:25,448 - INFO - tqdm - accuracy: 0.7051, batch_loss: 0.5200, loss: 0.5541 ||:  49%|####9     | 32/65 [06:09<06:20, 11.54s/it]
2022-04-10 20:37:36,992 - INFO - tqdm - accuracy: 0.7074, batch_loss: 0.5153, loss: 0.5529 ||:  51%|#####     | 33/65 [06:20<06:09, 11.54s/it]
2022-04-10 20:37:48,528 - INFO - tqdm - accuracy: 0.7022, batch_loss: 0.7678, loss: 0.5592 ||:  52%|#####2    | 34/65 [06:32<05:57, 11.54s/it]
2022-04-10 20:37:59,737 - INFO - tqdm - accuracy: 0.7024, batch_loss: 0.5173, loss: 0.5580 ||:  54%|#####3    | 35/65 [06:43<05:43, 11.44s/it]
2022-04-10 20:38:11,280 - INFO - tqdm - accuracy: 0.7020, batch_loss: 0.5723, loss: 0.5584 ||:  55%|#####5    | 36/65 [06:55<05:32, 11.47s/it]
2022-04-10 20:38:22,841 - INFO - tqdm - accuracy: 0.7008, batch_loss: 0.5514, loss: 0.5582 ||:  57%|#####6    | 37/65 [07:06<05:21, 11.50s/it]
2022-04-10 20:38:34,404 - INFO - tqdm - accuracy: 0.7004, batch_loss: 0.5338, loss: 0.5576 ||:  58%|#####8    | 38/65 [07:18<05:10, 11.52s/it]
2022-04-10 20:38:45,960 - INFO - tqdm - accuracy: 0.7001, batch_loss: 0.4958, loss: 0.5560 ||:  60%|######    | 39/65 [07:29<04:59, 11.53s/it]
2022-04-10 20:38:57,528 - INFO - tqdm - accuracy: 0.7045, batch_loss: 0.3904, loss: 0.5519 ||:  62%|######1   | 40/65 [07:41<04:48, 11.54s/it]
2022-04-10 20:39:09,065 - INFO - tqdm - accuracy: 0.7040, batch_loss: 0.5365, loss: 0.5515 ||:  63%|######3   | 41/65 [07:53<04:36, 11.54s/it]
2022-04-10 20:39:20,614 - INFO - tqdm - accuracy: 0.7066, batch_loss: 0.5643, loss: 0.5518 ||:  65%|######4   | 42/65 [08:04<04:25, 11.54s/it]
2022-04-10 20:39:32,166 - INFO - tqdm - accuracy: 0.7105, batch_loss: 0.3534, loss: 0.5472 ||:  66%|######6   | 43/65 [08:16<04:13, 11.55s/it]
2022-04-10 20:39:43,720 - INFO - tqdm - accuracy: 0.7086, batch_loss: 0.6164, loss: 0.5488 ||:  68%|######7   | 44/65 [08:27<04:02, 11.55s/it]
2022-04-10 20:39:55,259 - INFO - tqdm - accuracy: 0.7067, batch_loss: 0.6108, loss: 0.5501 ||:  69%|######9   | 45/65 [08:39<03:50, 11.55s/it]
2022-04-10 20:40:06,815 - INFO - tqdm - accuracy: 0.7056, batch_loss: 0.6066, loss: 0.5514 ||:  71%|#######   | 46/65 [08:50<03:39, 11.55s/it]
2022-04-10 20:40:18,383 - INFO - tqdm - accuracy: 0.7059, batch_loss: 0.5529, loss: 0.5514 ||:  72%|#######2  | 47/65 [09:02<03:27, 11.55s/it]
2022-04-10 20:40:29,945 - INFO - tqdm - accuracy: 0.7068, batch_loss: 0.5418, loss: 0.5512 ||:  74%|#######3  | 48/65 [09:13<03:16, 11.56s/it]
2022-04-10 20:40:41,501 - INFO - tqdm - accuracy: 0.7045, batch_loss: 0.6407, loss: 0.5530 ||:  75%|#######5  | 49/65 [09:25<03:04, 11.56s/it]
2022-04-10 20:40:52,814 - INFO - tqdm - accuracy: 0.7048, batch_loss: 0.5180, loss: 0.5523 ||:  77%|#######6  | 50/65 [09:36<02:52, 11.48s/it]
2022-04-10 20:41:04,365 - INFO - tqdm - accuracy: 0.7045, batch_loss: 0.5425, loss: 0.5521 ||:  78%|#######8  | 51/65 [09:48<02:41, 11.50s/it]
2022-04-10 20:41:15,917 - INFO - tqdm - accuracy: 0.7029, batch_loss: 0.5676, loss: 0.5524 ||:  80%|########  | 52/65 [09:59<02:29, 11.52s/it]
2022-04-10 20:41:27,462 - INFO - tqdm - accuracy: 0.7015, batch_loss: 0.6403, loss: 0.5541 ||:  82%|########1 | 53/65 [10:11<02:18, 11.53s/it]
2022-04-10 20:41:39,003 - INFO - tqdm - accuracy: 0.7035, batch_loss: 0.4668, loss: 0.5525 ||:  83%|########3 | 54/65 [10:23<02:06, 11.53s/it]
2022-04-10 20:41:50,551 - INFO - tqdm - accuracy: 0.7055, batch_loss: 0.4596, loss: 0.5508 ||:  85%|########4 | 55/65 [10:34<01:55, 11.54s/it]
2022-04-10 20:42:02,114 - INFO - tqdm - accuracy: 0.7052, batch_loss: 0.5430, loss: 0.5507 ||:  86%|########6 | 56/65 [10:46<01:43, 11.54s/it]
2022-04-10 20:42:13,672 - INFO - tqdm - accuracy: 0.7049, batch_loss: 0.6030, loss: 0.5516 ||:  88%|########7 | 57/65 [10:57<01:32, 11.55s/it]
2022-04-10 20:42:25,214 - INFO - tqdm - accuracy: 0.7057, batch_loss: 0.5571, loss: 0.5517 ||:  89%|########9 | 58/65 [11:09<01:20, 11.55s/it]
2022-04-10 20:42:36,767 - INFO - tqdm - accuracy: 0.7048, batch_loss: 0.5583, loss: 0.5518 ||:  91%|######### | 59/65 [11:20<01:09, 11.55s/it]
2022-04-10 20:42:48,323 - INFO - tqdm - accuracy: 0.7045, batch_loss: 0.5518, loss: 0.5518 ||:  92%|#########2| 60/65 [11:32<00:57, 11.55s/it]
2022-04-10 20:42:59,869 - INFO - tqdm - accuracy: 0.7068, batch_loss: 0.5218, loss: 0.5513 ||:  94%|#########3| 61/65 [11:43<00:46, 11.55s/it]
2022-04-10 20:43:11,410 - INFO - tqdm - accuracy: 0.7085, batch_loss: 0.4814, loss: 0.5502 ||:  95%|#########5| 62/65 [11:55<00:34, 11.55s/it]
2022-04-10 20:43:22,962 - INFO - tqdm - accuracy: 0.7107, batch_loss: 0.4229, loss: 0.5481 ||:  97%|#########6| 63/65 [12:06<00:23, 11.55s/it]
2022-04-10 20:43:34,488 - INFO - tqdm - accuracy: 0.7118, batch_loss: 0.4648, loss: 0.5468 ||:  98%|#########8| 64/65 [12:18<00:11, 11.54s/it]
2022-04-10 20:43:39,620 - INFO - tqdm - accuracy: 0.7132, batch_loss: 0.3777, loss: 0.5442 ||: 100%|##########| 65/65 [12:23<00:00,  9.62s/it]
2022-04-10 20:43:39,621 - INFO - tqdm - accuracy: 0.7132, batch_loss: 0.3777, loss: 0.5442 ||: 100%|##########| 65/65 [12:23<00:00, 11.44s/it]
2022-04-10 20:43:42,473 - INFO - allennlp.training.trainer - Validating
2022-04-10 20:43:42,475 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-10 20:43:52,672 - INFO - tqdm - accuracy: 0.6049, batch_loss: 0.6111, loss: 0.8577 ||:  36%|###5      | 41/115 [00:10<00:18,  3.92it/s]
2022-04-10 20:44:02,896 - INFO - tqdm - accuracy: 0.6087, batch_loss: 1.0915, loss: 0.8172 ||:  70%|#######   | 81/115 [00:20<00:08,  3.90it/s]
2022-04-10 20:44:11,629 - INFO - tqdm - accuracy: 0.6288, batch_loss: 0.6161, loss: 0.7673 ||: 100%|##########| 115/115 [00:29<00:00,  3.90it/s]
2022-04-10 20:44:11,630 - INFO - tqdm - accuracy: 0.6288, batch_loss: 0.6161, loss: 0.7673 ||: 100%|##########| 115/115 [00:29<00:00,  3.94it/s]
2022-04-10 20:44:11,630 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-10 20:44:11,630 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.713  |     0.629
2022-04-10 20:44:11,631 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-10 20:44:11,632 - INFO - allennlp.training.tensorboard_writer - loss               |     0.544  |     0.767
2022-04-10 20:44:11,632 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5811.043  |       N/A
2022-04-10 20:44:17,638 - INFO - allennlp.training.trainer - Epoch duration: 0:13:01.647956
2022-04-10 20:44:17,638 - INFO - allennlp.training.trainer - Estimated training time remaining: 1:05:40
2022-04-10 20:44:17,638 - INFO - allennlp.training.trainer - Epoch 10/14
2022-04-10 20:44:17,638 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-10 20:44:17,639 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-10 20:44:17,640 - INFO - allennlp.training.trainer - Training
2022-04-10 20:44:17,641 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-10 20:44:29,022 - INFO - tqdm - accuracy: 0.7188, batch_loss: 0.5351, loss: 0.5351 ||:   2%|1         | 1/65 [00:11<12:08, 11.38s/it]
2022-04-10 20:44:40,403 - INFO - tqdm - accuracy: 0.7188, batch_loss: 0.5096, loss: 0.5224 ||:   3%|3         | 2/65 [00:22<11:57, 11.38s/it]
2022-04-10 20:44:52,242 - INFO - tqdm - accuracy: 0.7292, batch_loss: 0.4423, loss: 0.4957 ||:   5%|4         | 3/65 [00:34<11:58, 11.59s/it]
2022-04-10 20:45:03,761 - INFO - tqdm - accuracy: 0.7266, batch_loss: 0.5775, loss: 0.5161 ||:   6%|6         | 4/65 [00:46<11:45, 11.56s/it]
2022-04-10 20:45:15,062 - INFO - tqdm - accuracy: 0.7500, batch_loss: 0.4322, loss: 0.4993 ||:   8%|7         | 5/65 [00:57<11:28, 11.47s/it]
2022-04-10 20:45:26,518 - INFO - tqdm - accuracy: 0.7500, batch_loss: 0.4566, loss: 0.4922 ||:   9%|9         | 6/65 [01:08<11:16, 11.46s/it]
2022-04-10 20:45:38,036 - INFO - tqdm - accuracy: 0.7411, batch_loss: 0.5712, loss: 0.5035 ||:  11%|#         | 7/65 [01:20<11:05, 11.48s/it]
2022-04-10 20:45:49,604 - INFO - tqdm - accuracy: 0.7383, batch_loss: 0.5022, loss: 0.5033 ||:  12%|#2        | 8/65 [01:31<10:56, 11.51s/it]
2022-04-10 20:46:01,221 - INFO - tqdm - accuracy: 0.7361, batch_loss: 0.4707, loss: 0.4997 ||:  14%|#3        | 9/65 [01:43<10:46, 11.54s/it]
2022-04-10 20:46:12,786 - INFO - tqdm - accuracy: 0.7469, batch_loss: 0.4261, loss: 0.4923 ||:  15%|#5        | 10/65 [01:55<10:35, 11.55s/it]
2022-04-10 20:46:24,329 - INFO - tqdm - accuracy: 0.7358, batch_loss: 0.6416, loss: 0.5059 ||:  17%|#6        | 11/65 [02:06<10:23, 11.55s/it]
2022-04-10 20:46:35,854 - INFO - tqdm - accuracy: 0.7292, batch_loss: 0.6225, loss: 0.5156 ||:  18%|#8        | 12/65 [02:18<10:11, 11.54s/it]
2022-04-10 20:46:47,334 - INFO - tqdm - accuracy: 0.7188, batch_loss: 0.6555, loss: 0.5264 ||:  20%|##        | 13/65 [02:29<09:59, 11.52s/it]
2022-04-10 20:46:58,831 - INFO - tqdm - accuracy: 0.7098, batch_loss: 0.6121, loss: 0.5325 ||:  22%|##1       | 14/65 [02:41<09:47, 11.51s/it]
2022-04-10 20:47:10,340 - INFO - tqdm - accuracy: 0.7208, batch_loss: 0.3261, loss: 0.5188 ||:  23%|##3       | 15/65 [02:52<09:35, 11.51s/it]
2022-04-10 20:47:21,877 - INFO - tqdm - accuracy: 0.7227, batch_loss: 0.4963, loss: 0.5174 ||:  25%|##4       | 16/65 [03:04<09:24, 11.52s/it]
2022-04-10 20:47:33,427 - INFO - tqdm - accuracy: 0.7335, batch_loss: 0.4304, loss: 0.5122 ||:  26%|##6       | 17/65 [03:15<09:13, 11.53s/it]
2022-04-10 20:47:44,959 - INFO - tqdm - accuracy: 0.7274, batch_loss: 0.5588, loss: 0.5148 ||:  28%|##7       | 18/65 [03:27<09:01, 11.53s/it]
2022-04-10 20:47:56,497 - INFO - tqdm - accuracy: 0.7319, batch_loss: 0.6075, loss: 0.5197 ||:  29%|##9       | 19/65 [03:38<08:50, 11.53s/it]
2022-04-10 20:48:08,029 - INFO - tqdm - accuracy: 0.7219, batch_loss: 0.6594, loss: 0.5267 ||:  31%|###       | 20/65 [03:50<08:38, 11.53s/it]
2022-04-10 20:48:19,557 - INFO - tqdm - accuracy: 0.7292, batch_loss: 0.5038, loss: 0.5256 ||:  32%|###2      | 21/65 [04:01<08:27, 11.53s/it]
2022-04-10 20:48:31,096 - INFO - tqdm - accuracy: 0.7259, batch_loss: 0.6129, loss: 0.5296 ||:  34%|###3      | 22/65 [04:13<08:15, 11.53s/it]
2022-04-10 20:48:42,293 - INFO - tqdm - accuracy: 0.7252, batch_loss: 0.4962, loss: 0.5281 ||:  35%|###5      | 23/65 [04:24<08:00, 11.43s/it]
2022-04-10 20:48:53,790 - INFO - tqdm - accuracy: 0.7236, batch_loss: 0.5785, loss: 0.5302 ||:  37%|###6      | 24/65 [04:36<07:49, 11.45s/it]
2022-04-10 20:49:05,318 - INFO - tqdm - accuracy: 0.7209, batch_loss: 0.4862, loss: 0.5284 ||:  38%|###8      | 25/65 [04:47<07:38, 11.47s/it]
2022-04-10 20:49:16,710 - INFO - tqdm - accuracy: 0.7220, batch_loss: 0.4552, loss: 0.5256 ||:  40%|####      | 26/65 [04:59<07:26, 11.45s/it]
2022-04-10 20:49:28,258 - INFO - tqdm - accuracy: 0.7149, batch_loss: 0.6905, loss: 0.5317 ||:  42%|####1     | 27/65 [05:10<07:16, 11.48s/it]
2022-04-10 20:49:39,816 - INFO - tqdm - accuracy: 0.7173, batch_loss: 0.5285, loss: 0.5316 ||:  43%|####3     | 28/65 [05:22<07:05, 11.50s/it]
2022-04-10 20:49:51,366 - INFO - tqdm - accuracy: 0.7152, batch_loss: 0.5187, loss: 0.5312 ||:  45%|####4     | 29/65 [05:33<06:54, 11.52s/it]
2022-04-10 20:50:02,927 - INFO - tqdm - accuracy: 0.7143, batch_loss: 0.5647, loss: 0.5323 ||:  46%|####6     | 30/65 [05:45<06:43, 11.53s/it]
2022-04-10 20:50:14,479 - INFO - tqdm - accuracy: 0.7134, batch_loss: 0.6064, loss: 0.5347 ||:  48%|####7     | 31/65 [05:56<06:32, 11.54s/it]
2022-04-10 20:50:26,008 - INFO - tqdm - accuracy: 0.7146, batch_loss: 0.5139, loss: 0.5340 ||:  49%|####9     | 32/65 [06:08<06:20, 11.53s/it]
2022-04-10 20:50:37,537 - INFO - tqdm - accuracy: 0.7137, batch_loss: 0.5586, loss: 0.5348 ||:  51%|#####     | 33/65 [06:19<06:09, 11.53s/it]
2022-04-10 20:50:49,097 - INFO - tqdm - accuracy: 0.7121, batch_loss: 0.5565, loss: 0.5354 ||:  52%|#####2    | 34/65 [06:31<05:57, 11.54s/it]
2022-04-10 20:51:00,652 - INFO - tqdm - accuracy: 0.7149, batch_loss: 0.4014, loss: 0.5316 ||:  54%|#####3    | 35/65 [06:43<05:46, 11.55s/it]
2022-04-10 20:51:12,234 - INFO - tqdm - accuracy: 0.7159, batch_loss: 0.4651, loss: 0.5297 ||:  55%|#####5    | 36/65 [06:54<05:35, 11.56s/it]
2022-04-10 20:51:23,800 - INFO - tqdm - accuracy: 0.7202, batch_loss: 0.3751, loss: 0.5256 ||:  57%|#####6    | 37/65 [07:06<05:23, 11.56s/it]
2022-04-10 20:51:35,354 - INFO - tqdm - accuracy: 0.7218, batch_loss: 0.5132, loss: 0.5252 ||:  58%|#####8    | 38/65 [07:17<05:12, 11.56s/it]
2022-04-10 20:51:46,900 - INFO - tqdm - accuracy: 0.7225, batch_loss: 0.4874, loss: 0.5243 ||:  60%|######    | 39/65 [07:29<05:00, 11.55s/it]
2022-04-10 20:51:58,439 - INFO - tqdm - accuracy: 0.7193, batch_loss: 0.5789, loss: 0.5256 ||:  62%|######1   | 40/65 [07:40<04:48, 11.55s/it]
2022-04-10 20:52:09,985 - INFO - tqdm - accuracy: 0.7201, batch_loss: 0.5129, loss: 0.5253 ||:  63%|######3   | 41/65 [07:52<04:37, 11.55s/it]
2022-04-10 20:52:21,530 - INFO - tqdm - accuracy: 0.7185, batch_loss: 0.5858, loss: 0.5268 ||:  65%|######4   | 42/65 [08:03<04:25, 11.55s/it]
2022-04-10 20:52:33,075 - INFO - tqdm - accuracy: 0.7193, batch_loss: 0.5551, loss: 0.5274 ||:  66%|######6   | 43/65 [08:15<04:14, 11.55s/it]
2022-04-10 20:52:44,621 - INFO - tqdm - accuracy: 0.7249, batch_loss: 0.3814, loss: 0.5241 ||:  68%|######7   | 44/65 [08:26<04:02, 11.55s/it]
2022-04-10 20:52:56,198 - INFO - tqdm - accuracy: 0.7276, batch_loss: 0.4511, loss: 0.5225 ||:  69%|######9   | 45/65 [08:38<03:51, 11.56s/it]
2022-04-10 20:53:07,763 - INFO - tqdm - accuracy: 0.7294, batch_loss: 0.4558, loss: 0.5210 ||:  71%|#######   | 46/65 [08:50<03:39, 11.56s/it]
2022-04-10 20:53:19,338 - INFO - tqdm - accuracy: 0.7279, batch_loss: 0.6552, loss: 0.5239 ||:  72%|#######2  | 47/65 [09:01<03:28, 11.56s/it]
2022-04-10 20:53:30,888 - INFO - tqdm - accuracy: 0.7290, batch_loss: 0.4319, loss: 0.5220 ||:  74%|#######3  | 48/65 [09:13<03:16, 11.56s/it]
2022-04-10 20:53:42,449 - INFO - tqdm - accuracy: 0.7301, batch_loss: 0.4660, loss: 0.5208 ||:  75%|#######5  | 49/65 [09:24<03:04, 11.56s/it]
2022-04-10 20:53:54,572 - INFO - tqdm - accuracy: 0.7305, batch_loss: 0.5222, loss: 0.5209 ||:  77%|#######6  | 50/65 [09:36<02:55, 11.73s/it]
2022-04-10 20:54:06,082 - INFO - tqdm - accuracy: 0.7278, batch_loss: 0.6358, loss: 0.5231 ||:  78%|#######8  | 51/65 [09:48<02:43, 11.66s/it]
2022-04-10 20:54:17,609 - INFO - tqdm - accuracy: 0.7288, batch_loss: 0.4674, loss: 0.5220 ||:  80%|########  | 52/65 [09:59<02:31, 11.62s/it]
2022-04-10 20:54:29,152 - INFO - tqdm - accuracy: 0.7268, batch_loss: 0.5625, loss: 0.5228 ||:  82%|########1 | 53/65 [10:11<02:19, 11.60s/it]
2022-04-10 20:54:40,692 - INFO - tqdm - accuracy: 0.7244, batch_loss: 0.6371, loss: 0.5249 ||:  83%|########3 | 54/65 [10:23<02:07, 11.58s/it]
2022-04-10 20:54:52,237 - INFO - tqdm - accuracy: 0.7243, batch_loss: 0.4884, loss: 0.5243 ||:  85%|########4 | 55/65 [10:34<01:55, 11.57s/it]
2022-04-10 20:55:03,807 - INFO - tqdm - accuracy: 0.7219, batch_loss: 0.5382, loss: 0.5245 ||:  86%|########6 | 56/65 [10:46<01:44, 11.57s/it]
2022-04-10 20:55:15,364 - INFO - tqdm - accuracy: 0.7208, batch_loss: 0.4685, loss: 0.5235 ||:  88%|########7 | 57/65 [10:57<01:32, 11.57s/it]
2022-04-10 20:55:26,922 - INFO - tqdm - accuracy: 0.7218, batch_loss: 0.4723, loss: 0.5226 ||:  89%|########9 | 58/65 [11:09<01:20, 11.56s/it]
2022-04-10 20:55:38,467 - INFO - tqdm - accuracy: 0.7202, batch_loss: 0.6971, loss: 0.5256 ||:  91%|######### | 59/65 [11:20<01:09, 11.56s/it]
2022-04-10 20:55:50,023 - INFO - tqdm - accuracy: 0.7196, batch_loss: 0.5775, loss: 0.5265 ||:  92%|#########2| 60/65 [11:32<00:57, 11.56s/it]
2022-04-10 20:56:01,570 - INFO - tqdm - accuracy: 0.7191, batch_loss: 0.5705, loss: 0.5272 ||:  94%|#########3| 61/65 [11:43<00:46, 11.55s/it]
2022-04-10 20:56:13,108 - INFO - tqdm - accuracy: 0.7201, batch_loss: 0.4481, loss: 0.5259 ||:  95%|#########5| 62/65 [11:55<00:34, 11.55s/it]
2022-04-10 20:56:24,646 - INFO - tqdm - accuracy: 0.7211, batch_loss: 0.4530, loss: 0.5247 ||:  97%|#########6| 63/65 [12:07<00:23, 11.55s/it]
2022-04-10 20:56:36,197 - INFO - tqdm - accuracy: 0.7206, batch_loss: 0.6140, loss: 0.5261 ||:  98%|#########8| 64/65 [12:18<00:11, 11.55s/it]
2022-04-10 20:56:41,324 - INFO - tqdm - accuracy: 0.7210, batch_loss: 0.4650, loss: 0.5252 ||: 100%|##########| 65/65 [12:23<00:00,  9.62s/it]
2022-04-10 20:56:41,324 - INFO - tqdm - accuracy: 0.7210, batch_loss: 0.4650, loss: 0.5252 ||: 100%|##########| 65/65 [12:23<00:00, 11.44s/it]
2022-04-10 20:56:44,163 - INFO - allennlp.training.trainer - Validating
2022-04-10 20:56:44,165 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-10 20:56:54,311 - INFO - tqdm - accuracy: 0.6750, batch_loss: 0.0243, loss: 0.6473 ||:  35%|###4      | 40/115 [00:10<00:19,  3.92it/s]
2022-04-10 20:57:04,328 - INFO - tqdm - accuracy: 0.6101, batch_loss: 2.1408, loss: 0.7623 ||:  70%|######9   | 80/115 [00:20<00:08,  3.93it/s]
2022-04-10 20:57:13,288 - INFO - tqdm - accuracy: 0.6376, batch_loss: 2.0119, loss: 0.7881 ||: 100%|##########| 115/115 [00:29<00:00,  3.91it/s]
2022-04-10 20:57:13,288 - INFO - tqdm - accuracy: 0.6376, batch_loss: 2.0119, loss: 0.7881 ||: 100%|##########| 115/115 [00:29<00:00,  3.95it/s]
2022-04-10 20:57:13,289 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-10 20:57:13,289 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.721  |     0.638
2022-04-10 20:57:13,289 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-10 20:57:13,289 - INFO - allennlp.training.tensorboard_writer - loss               |     0.525  |     0.788
2022-04-10 20:57:13,289 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5811.043  |       N/A
2022-04-10 20:57:19,380 - INFO - allennlp.training.trainer - Epoch duration: 0:13:01.741513
2022-04-10 20:57:19,380 - INFO - allennlp.training.trainer - Estimated training time remaining: 0:52:29
2022-04-10 20:57:19,380 - INFO - allennlp.training.trainer - Epoch 11/14
2022-04-10 20:57:19,380 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-10 20:57:19,381 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-10 20:57:19,382 - INFO - allennlp.training.trainer - Training
2022-04-10 20:57:19,382 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-10 20:57:30,779 - INFO - tqdm - accuracy: 0.7812, batch_loss: 0.4550, loss: 0.4550 ||:   2%|1         | 1/65 [00:11<12:09, 11.40s/it]
2022-04-10 20:57:42,476 - INFO - tqdm - accuracy: 0.7812, batch_loss: 0.4072, loss: 0.4311 ||:   3%|3         | 2/65 [00:23<12:09, 11.57s/it]
2022-04-10 20:57:54,265 - INFO - tqdm - accuracy: 0.7812, batch_loss: 0.4781, loss: 0.4468 ||:   5%|4         | 3/65 [00:34<12:03, 11.67s/it]
2022-04-10 20:58:05,759 - INFO - tqdm - accuracy: 0.7500, batch_loss: 0.5703, loss: 0.4776 ||:   6%|6         | 4/65 [00:46<11:47, 11.60s/it]
2022-04-10 20:58:17,197 - INFO - tqdm - accuracy: 0.7375, batch_loss: 0.5164, loss: 0.4854 ||:   8%|7         | 5/65 [00:57<11:32, 11.54s/it]
2022-04-10 20:58:28,689 - INFO - tqdm - accuracy: 0.7240, batch_loss: 0.5539, loss: 0.4968 ||:   9%|9         | 6/65 [01:09<11:19, 11.53s/it]
2022-04-10 20:58:40,252 - INFO - tqdm - accuracy: 0.7277, batch_loss: 0.5809, loss: 0.5088 ||:  11%|#         | 7/65 [01:20<11:09, 11.54s/it]
2022-04-10 20:58:51,855 - INFO - tqdm - accuracy: 0.7266, batch_loss: 0.5133, loss: 0.5094 ||:  12%|#2        | 8/65 [01:32<10:58, 11.56s/it]
2022-04-10 20:59:03,465 - INFO - tqdm - accuracy: 0.7257, batch_loss: 0.5250, loss: 0.5111 ||:  14%|#3        | 9/65 [01:44<10:48, 11.57s/it]
2022-04-10 20:59:14,986 - INFO - tqdm - accuracy: 0.7250, batch_loss: 0.5647, loss: 0.5165 ||:  15%|#5        | 10/65 [01:55<10:35, 11.56s/it]
2022-04-10 20:59:26,466 - INFO - tqdm - accuracy: 0.7301, batch_loss: 0.4580, loss: 0.5112 ||:  17%|#6        | 11/65 [02:07<10:22, 11.53s/it]
2022-04-10 20:59:37,928 - INFO - tqdm - accuracy: 0.7344, batch_loss: 0.4681, loss: 0.5076 ||:  18%|#8        | 12/65 [02:18<10:10, 11.51s/it]
2022-04-10 20:59:49,434 - INFO - tqdm - accuracy: 0.7332, batch_loss: 0.5059, loss: 0.5075 ||:  20%|##        | 13/65 [02:30<09:58, 11.51s/it]
2022-04-10 21:00:00,979 - INFO - tqdm - accuracy: 0.7254, batch_loss: 0.6193, loss: 0.5154 ||:  22%|##1       | 14/65 [02:41<09:47, 11.52s/it]
2022-04-10 21:00:12,558 - INFO - tqdm - accuracy: 0.7250, batch_loss: 0.4897, loss: 0.5137 ||:  23%|##3       | 15/65 [02:53<09:36, 11.54s/it]
2022-04-10 21:00:24,032 - INFO - tqdm - accuracy: 0.7324, batch_loss: 0.4882, loss: 0.5121 ||:  25%|##4       | 16/65 [03:04<09:24, 11.52s/it]
2022-04-10 21:00:35,615 - INFO - tqdm - accuracy: 0.7353, batch_loss: 0.4766, loss: 0.5100 ||:  26%|##6       | 17/65 [03:16<09:13, 11.54s/it]
2022-04-10 21:00:47,173 - INFO - tqdm - accuracy: 0.7274, batch_loss: 0.6167, loss: 0.5160 ||:  28%|##7       | 18/65 [03:27<09:02, 11.54s/it]
2022-04-10 21:00:58,753 - INFO - tqdm - accuracy: 0.7286, batch_loss: 0.4873, loss: 0.5145 ||:  29%|##9       | 19/65 [03:39<08:51, 11.55s/it]
2022-04-10 21:01:10,311 - INFO - tqdm - accuracy: 0.7297, batch_loss: 0.5317, loss: 0.5153 ||:  31%|###       | 20/65 [03:50<08:40, 11.56s/it]
2022-04-10 21:01:21,856 - INFO - tqdm - accuracy: 0.7292, batch_loss: 0.4842, loss: 0.5138 ||:  32%|###2      | 21/65 [04:02<08:28, 11.55s/it]
2022-04-10 21:01:33,388 - INFO - tqdm - accuracy: 0.7315, batch_loss: 0.4754, loss: 0.5121 ||:  34%|###3      | 22/65 [04:14<08:16, 11.55s/it]
2022-04-10 21:01:44,891 - INFO - tqdm - accuracy: 0.7269, batch_loss: 0.4954, loss: 0.5114 ||:  35%|###5      | 23/65 [04:25<08:04, 11.53s/it]
2022-04-10 21:01:56,372 - INFO - tqdm - accuracy: 0.7266, batch_loss: 0.5610, loss: 0.5134 ||:  37%|###6      | 24/65 [04:36<07:52, 11.52s/it]
2022-04-10 21:02:07,857 - INFO - tqdm - accuracy: 0.7262, batch_loss: 0.5205, loss: 0.5137 ||:  38%|###8      | 25/65 [04:48<07:40, 11.51s/it]
2022-04-10 21:02:19,342 - INFO - tqdm - accuracy: 0.7248, batch_loss: 0.5013, loss: 0.5132 ||:  40%|####      | 26/65 [04:59<07:28, 11.50s/it]
2022-04-10 21:02:30,852 - INFO - tqdm - accuracy: 0.7164, batch_loss: 0.5995, loss: 0.5164 ||:  42%|####1     | 27/65 [05:11<07:17, 11.50s/it]
2022-04-10 21:02:42,335 - INFO - tqdm - accuracy: 0.7199, batch_loss: 0.4433, loss: 0.5138 ||:  43%|####3     | 28/65 [05:22<07:05, 11.50s/it]
2022-04-10 21:02:53,853 - INFO - tqdm - accuracy: 0.7220, batch_loss: 0.4374, loss: 0.5112 ||:  45%|####4     | 29/65 [05:34<06:54, 11.50s/it]
2022-04-10 21:03:05,370 - INFO - tqdm - accuracy: 0.7229, batch_loss: 0.5174, loss: 0.5114 ||:  46%|####6     | 30/65 [05:45<06:42, 11.51s/it]
2022-04-10 21:03:16,883 - INFO - tqdm - accuracy: 0.7258, batch_loss: 0.4448, loss: 0.5092 ||:  48%|####7     | 31/65 [05:57<06:31, 11.51s/it]
2022-04-10 21:03:28,439 - INFO - tqdm - accuracy: 0.7266, batch_loss: 0.5185, loss: 0.5095 ||:  49%|####9     | 32/65 [06:09<06:20, 11.52s/it]
2022-04-10 21:03:39,990 - INFO - tqdm - accuracy: 0.7282, batch_loss: 0.4627, loss: 0.5081 ||:  51%|#####     | 33/65 [06:20<06:09, 11.53s/it]
2022-04-10 21:03:51,536 - INFO - tqdm - accuracy: 0.7298, batch_loss: 0.5347, loss: 0.5089 ||:  52%|#####2    | 34/65 [06:32<05:57, 11.54s/it]
2022-04-10 21:04:03,085 - INFO - tqdm - accuracy: 0.7321, batch_loss: 0.4282, loss: 0.5066 ||:  54%|#####3    | 35/65 [06:43<05:46, 11.54s/it]
2022-04-10 21:04:14,638 - INFO - tqdm - accuracy: 0.7309, batch_loss: 0.4596, loss: 0.5053 ||:  55%|#####5    | 36/65 [06:55<05:34, 11.54s/it]
2022-04-10 21:04:26,185 - INFO - tqdm - accuracy: 0.7314, batch_loss: 0.5447, loss: 0.5064 ||:  57%|#####6    | 37/65 [07:06<05:23, 11.54s/it]
2022-04-10 21:04:37,726 - INFO - tqdm - accuracy: 0.7319, batch_loss: 0.5351, loss: 0.5071 ||:  58%|#####8    | 38/65 [07:18<05:11, 11.54s/it]
2022-04-10 21:04:49,281 - INFO - tqdm - accuracy: 0.7300, batch_loss: 0.5859, loss: 0.5091 ||:  60%|######    | 39/65 [07:29<05:00, 11.55s/it]
2022-04-10 21:05:00,814 - INFO - tqdm - accuracy: 0.7312, batch_loss: 0.5661, loss: 0.5106 ||:  62%|######1   | 40/65 [07:41<04:48, 11.54s/it]
2022-04-10 21:05:12,358 - INFO - tqdm - accuracy: 0.7325, batch_loss: 0.5720, loss: 0.5121 ||:  63%|######3   | 41/65 [07:52<04:37, 11.54s/it]
2022-04-10 21:05:23,896 - INFO - tqdm - accuracy: 0.7321, batch_loss: 0.4545, loss: 0.5107 ||:  65%|######4   | 42/65 [08:04<04:25, 11.54s/it]
2022-04-10 21:05:35,319 - INFO - tqdm - accuracy: 0.7289, batch_loss: 0.6114, loss: 0.5130 ||:  66%|######6   | 43/65 [08:15<04:13, 11.51s/it]
2022-04-10 21:05:46,841 - INFO - tqdm - accuracy: 0.7287, batch_loss: 0.4945, loss: 0.5126 ||:  68%|######7   | 44/65 [08:27<04:01, 11.51s/it]
2022-04-10 21:05:58,053 - INFO - tqdm - accuracy: 0.7262, batch_loss: 0.4872, loss: 0.5120 ||:  69%|######9   | 45/65 [08:38<03:48, 11.42s/it]
2022-04-10 21:06:09,583 - INFO - tqdm - accuracy: 0.7240, batch_loss: 0.6430, loss: 0.5149 ||:  71%|#######   | 46/65 [08:50<03:37, 11.45s/it]
2022-04-10 21:06:21,113 - INFO - tqdm - accuracy: 0.7252, batch_loss: 0.5112, loss: 0.5148 ||:  72%|#######2  | 47/65 [09:01<03:26, 11.48s/it]
2022-04-10 21:06:32,649 - INFO - tqdm - accuracy: 0.7225, batch_loss: 0.6925, loss: 0.5185 ||:  74%|#######3  | 48/65 [09:13<03:15, 11.49s/it]
2022-04-10 21:06:44,159 - INFO - tqdm - accuracy: 0.7230, batch_loss: 0.4787, loss: 0.5177 ||:  75%|#######5  | 49/65 [09:24<03:03, 11.50s/it]
2022-04-10 21:06:55,714 - INFO - tqdm - accuracy: 0.7248, batch_loss: 0.3731, loss: 0.5148 ||:  77%|#######6  | 50/65 [09:36<02:52, 11.52s/it]
2022-04-10 21:07:07,246 - INFO - tqdm - accuracy: 0.7229, batch_loss: 0.5735, loss: 0.5160 ||:  78%|#######8  | 51/65 [09:47<02:41, 11.52s/it]
2022-04-10 21:07:18,777 - INFO - tqdm - accuracy: 0.7216, batch_loss: 0.7024, loss: 0.5195 ||:  80%|########  | 52/65 [09:59<02:29, 11.52s/it]
2022-04-10 21:07:30,315 - INFO - tqdm - accuracy: 0.7239, batch_loss: 0.3836, loss: 0.5170 ||:  82%|########1 | 53/65 [10:10<02:18, 11.53s/it]
2022-04-10 21:07:41,834 - INFO - tqdm - accuracy: 0.7250, batch_loss: 0.4078, loss: 0.5150 ||:  83%|########3 | 54/65 [10:22<02:06, 11.53s/it]
2022-04-10 21:07:53,354 - INFO - tqdm - accuracy: 0.7254, batch_loss: 0.4263, loss: 0.5133 ||:  85%|########4 | 55/65 [10:33<01:55, 11.52s/it]
2022-04-10 21:08:04,884 - INFO - tqdm - accuracy: 0.7247, batch_loss: 0.6428, loss: 0.5157 ||:  86%|########6 | 56/65 [10:45<01:43, 11.53s/it]
2022-04-10 21:08:16,152 - INFO - tqdm - accuracy: 0.7263, batch_loss: 0.4312, loss: 0.5142 ||:  88%|########7 | 57/65 [10:56<01:31, 11.45s/it]
2022-04-10 21:08:27,666 - INFO - tqdm - accuracy: 0.7267, batch_loss: 0.5413, loss: 0.5146 ||:  89%|########9 | 58/65 [11:08<01:20, 11.47s/it]
2022-04-10 21:08:39,175 - INFO - tqdm - accuracy: 0.7271, batch_loss: 0.5319, loss: 0.5149 ||:  91%|######### | 59/65 [11:19<01:08, 11.48s/it]
2022-04-10 21:08:50,701 - INFO - tqdm - accuracy: 0.7285, batch_loss: 0.4031, loss: 0.5131 ||:  92%|#########2| 60/65 [11:31<00:57, 11.49s/it]
2022-04-10 21:09:02,238 - INFO - tqdm - accuracy: 0.7283, batch_loss: 0.5393, loss: 0.5135 ||:  94%|#########3| 61/65 [11:42<00:46, 11.51s/it]
2022-04-10 21:09:13,749 - INFO - tqdm - accuracy: 0.7282, batch_loss: 0.5751, loss: 0.5145 ||:  95%|#########5| 62/65 [11:54<00:34, 11.51s/it]
2022-04-10 21:09:25,274 - INFO - tqdm - accuracy: 0.7285, batch_loss: 0.4726, loss: 0.5138 ||:  97%|#########6| 63/65 [12:05<00:23, 11.51s/it]
2022-04-10 21:09:36,827 - INFO - tqdm - accuracy: 0.7298, batch_loss: 0.4402, loss: 0.5127 ||:  98%|#########8| 64/65 [12:17<00:11, 11.53s/it]
2022-04-10 21:09:41,951 - INFO - tqdm - accuracy: 0.7302, batch_loss: 0.4816, loss: 0.5122 ||: 100%|##########| 65/65 [12:22<00:00,  9.60s/it]
2022-04-10 21:09:41,951 - INFO - tqdm - accuracy: 0.7302, batch_loss: 0.4816, loss: 0.5122 ||: 100%|##########| 65/65 [12:22<00:00, 11.42s/it]
2022-04-10 21:09:44,794 - INFO - allennlp.training.trainer - Validating
2022-04-10 21:09:44,796 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-10 21:09:54,929 - INFO - tqdm - accuracy: 0.5625, batch_loss: 0.2021, loss: 1.0039 ||:  35%|###4      | 40/115 [00:10<00:19,  3.93it/s]
2022-04-10 21:10:05,015 - INFO - tqdm - accuracy: 0.6226, batch_loss: 0.4809, loss: 0.8766 ||:  70%|######9   | 80/115 [00:20<00:07,  4.57it/s]
2022-04-10 21:10:13,895 - INFO - tqdm - accuracy: 0.6550, batch_loss: 0.3929, loss: 0.8057 ||: 100%|##########| 115/115 [00:29<00:00,  3.89it/s]
2022-04-10 21:10:13,895 - INFO - tqdm - accuracy: 0.6550, batch_loss: 0.3929, loss: 0.8057 ||: 100%|##########| 115/115 [00:29<00:00,  3.95it/s]
2022-04-10 21:10:13,895 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-10 21:10:13,896 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.730  |     0.655
2022-04-10 21:10:13,897 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-10 21:10:13,898 - INFO - allennlp.training.tensorboard_writer - loss               |     0.512  |     0.806
2022-04-10 21:10:13,898 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5811.043  |       N/A
2022-04-10 21:10:19,533 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to './output_ir-ora-d_hyper5/best.th'.
2022-04-10 21:10:35,049 - INFO - allennlp.training.trainer - Epoch duration: 0:13:15.668856
2022-04-10 21:10:35,049 - INFO - allennlp.training.trainer - Estimated training time remaining: 0:39:24
2022-04-10 21:10:35,049 - INFO - allennlp.training.trainer - Epoch 12/14
2022-04-10 21:10:35,050 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-10 21:10:35,050 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-10 21:10:35,051 - INFO - allennlp.training.trainer - Training
2022-04-10 21:10:35,052 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-10 21:10:46,311 - INFO - tqdm - accuracy: 0.7188, batch_loss: 0.5764, loss: 0.5764 ||:   2%|1         | 1/65 [00:11<12:00, 11.26s/it]
2022-04-10 21:10:57,921 - INFO - tqdm - accuracy: 0.7500, batch_loss: 0.4291, loss: 0.5027 ||:   3%|3         | 2/65 [00:22<12:02, 11.47s/it]
2022-04-10 21:11:09,721 - INFO - tqdm - accuracy: 0.7708, batch_loss: 0.4483, loss: 0.4846 ||:   5%|4         | 3/65 [00:34<12:00, 11.62s/it]
2022-04-10 21:11:21,507 - INFO - tqdm - accuracy: 0.7812, batch_loss: 0.4431, loss: 0.4742 ||:   6%|6         | 4/65 [00:46<11:52, 11.68s/it]
2022-04-10 21:11:32,946 - INFO - tqdm - accuracy: 0.7750, batch_loss: 0.5458, loss: 0.4885 ||:   8%|7         | 5/65 [00:57<11:35, 11.60s/it]
2022-04-10 21:11:44,325 - INFO - tqdm - accuracy: 0.7656, batch_loss: 0.6702, loss: 0.5188 ||:   9%|9         | 6/65 [01:09<11:19, 11.52s/it]
2022-04-10 21:11:55,764 - INFO - tqdm - accuracy: 0.7589, batch_loss: 0.4791, loss: 0.5131 ||:  11%|#         | 7/65 [01:20<11:06, 11.50s/it]
2022-04-10 21:12:07,317 - INFO - tqdm - accuracy: 0.7656, batch_loss: 0.4163, loss: 0.5010 ||:  12%|#2        | 8/65 [01:32<10:56, 11.51s/it]
2022-04-10 21:12:18,927 - INFO - tqdm - accuracy: 0.7743, batch_loss: 0.3708, loss: 0.4866 ||:  14%|#3        | 9/65 [01:43<10:46, 11.54s/it]
2022-04-10 21:12:30,523 - INFO - tqdm - accuracy: 0.7812, batch_loss: 0.4075, loss: 0.4787 ||:  15%|#5        | 10/65 [01:55<10:35, 11.56s/it]
2022-04-10 21:12:42,044 - INFO - tqdm - accuracy: 0.7784, batch_loss: 0.4257, loss: 0.4738 ||:  17%|#6        | 11/65 [02:06<10:23, 11.55s/it]
2022-04-10 21:12:53,523 - INFO - tqdm - accuracy: 0.7682, batch_loss: 0.5347, loss: 0.4789 ||:  18%|#8        | 12/65 [02:18<10:10, 11.53s/it]
2022-04-10 21:13:04,630 - INFO - tqdm - accuracy: 0.7759, batch_loss: 0.3688, loss: 0.4704 ||:  20%|##        | 13/65 [02:29<09:52, 11.40s/it]
2022-04-10 21:13:16,126 - INFO - tqdm - accuracy: 0.7696, batch_loss: 0.5752, loss: 0.4779 ||:  22%|##1       | 14/65 [02:41<09:42, 11.43s/it]
2022-04-10 21:13:27,663 - INFO - tqdm - accuracy: 0.7704, batch_loss: 0.4708, loss: 0.4774 ||:  23%|##3       | 15/65 [02:52<09:33, 11.46s/it]
2022-04-10 21:13:39,216 - INFO - tqdm - accuracy: 0.7691, batch_loss: 0.5509, loss: 0.4820 ||:  25%|##4       | 16/65 [03:04<09:22, 11.49s/it]
2022-04-10 21:13:50,791 - INFO - tqdm - accuracy: 0.7624, batch_loss: 0.5644, loss: 0.4869 ||:  26%|##6       | 17/65 [03:15<09:12, 11.51s/it]
2022-04-10 21:14:02,374 - INFO - tqdm - accuracy: 0.7635, batch_loss: 0.6146, loss: 0.4940 ||:  28%|##7       | 18/65 [03:27<09:02, 11.54s/it]
2022-04-10 21:14:13,956 - INFO - tqdm - accuracy: 0.7562, batch_loss: 0.5707, loss: 0.4980 ||:  29%|##9       | 19/65 [03:38<08:51, 11.55s/it]
2022-04-10 21:14:25,916 - INFO - tqdm - accuracy: 0.7496, batch_loss: 0.6013, loss: 0.5032 ||:  31%|###       | 20/65 [03:50<08:45, 11.67s/it]
2022-04-10 21:14:37,429 - INFO - tqdm - accuracy: 0.7452, batch_loss: 0.6003, loss: 0.5078 ||:  32%|###2      | 21/65 [04:02<08:31, 11.62s/it]
2022-04-10 21:14:48,979 - INFO - tqdm - accuracy: 0.7482, batch_loss: 0.5311, loss: 0.5089 ||:  34%|###3      | 22/65 [04:13<08:18, 11.60s/it]
2022-04-10 21:15:00,521 - INFO - tqdm - accuracy: 0.7524, batch_loss: 0.4341, loss: 0.5056 ||:  35%|###5      | 23/65 [04:25<08:06, 11.58s/it]
2022-04-10 21:15:12,071 - INFO - tqdm - accuracy: 0.7458, batch_loss: 0.5244, loss: 0.5064 ||:  37%|###6      | 24/65 [04:37<07:54, 11.57s/it]
2022-04-10 21:15:23,381 - INFO - tqdm - accuracy: 0.7472, batch_loss: 0.5682, loss: 0.5089 ||:  38%|###8      | 25/65 [04:48<07:39, 11.49s/it]
2022-04-10 21:15:34,938 - INFO - tqdm - accuracy: 0.7485, batch_loss: 0.4843, loss: 0.5079 ||:  40%|####      | 26/65 [04:59<07:29, 11.51s/it]
2022-04-10 21:15:46,466 - INFO - tqdm - accuracy: 0.7486, batch_loss: 0.5682, loss: 0.5102 ||:  42%|####1     | 27/65 [05:11<07:17, 11.52s/it]
2022-04-10 21:15:58,009 - INFO - tqdm - accuracy: 0.7408, batch_loss: 0.6695, loss: 0.5158 ||:  43%|####3     | 28/65 [05:22<07:06, 11.53s/it]
2022-04-10 21:16:09,542 - INFO - tqdm - accuracy: 0.7379, batch_loss: 0.6399, loss: 0.5201 ||:  45%|####4     | 29/65 [05:34<06:54, 11.53s/it]
2022-04-10 21:16:21,084 - INFO - tqdm - accuracy: 0.7414, batch_loss: 0.3995, loss: 0.5161 ||:  46%|####6     | 30/65 [05:46<06:43, 11.53s/it]
2022-04-10 21:16:32,608 - INFO - tqdm - accuracy: 0.7376, batch_loss: 0.4962, loss: 0.5155 ||:  48%|####7     | 31/65 [05:57<06:32, 11.53s/it]
2022-04-10 21:16:44,027 - INFO - tqdm - accuracy: 0.7380, batch_loss: 0.5185, loss: 0.5156 ||:  49%|####9     | 32/65 [06:08<06:19, 11.50s/it]
2022-04-10 21:16:55,569 - INFO - tqdm - accuracy: 0.7355, batch_loss: 0.5371, loss: 0.5162 ||:  51%|#####     | 33/65 [06:20<06:08, 11.51s/it]
2022-04-10 21:17:07,081 - INFO - tqdm - accuracy: 0.7387, batch_loss: 0.4090, loss: 0.5131 ||:  52%|#####2    | 34/65 [06:32<05:56, 11.51s/it]
2022-04-10 21:17:18,611 - INFO - tqdm - accuracy: 0.7382, batch_loss: 0.4718, loss: 0.5119 ||:  54%|#####3    | 35/65 [06:43<05:45, 11.52s/it]
2022-04-10 21:17:30,120 - INFO - tqdm - accuracy: 0.7368, batch_loss: 0.6555, loss: 0.5159 ||:  55%|#####5    | 36/65 [06:55<05:33, 11.51s/it]
2022-04-10 21:17:41,641 - INFO - tqdm - accuracy: 0.7363, batch_loss: 0.5450, loss: 0.5167 ||:  57%|#####6    | 37/65 [07:06<05:22, 11.52s/it]
2022-04-10 21:17:53,166 - INFO - tqdm - accuracy: 0.7350, batch_loss: 0.6745, loss: 0.5208 ||:  58%|#####8    | 38/65 [07:18<05:11, 11.52s/it]
2022-04-10 21:18:04,701 - INFO - tqdm - accuracy: 0.7354, batch_loss: 0.4276, loss: 0.5184 ||:  60%|######    | 39/65 [07:29<04:59, 11.52s/it]
2022-04-10 21:18:16,246 - INFO - tqdm - accuracy: 0.7373, batch_loss: 0.4524, loss: 0.5168 ||:  62%|######1   | 40/65 [07:41<04:48, 11.53s/it]
2022-04-10 21:18:27,799 - INFO - tqdm - accuracy: 0.7368, batch_loss: 0.5607, loss: 0.5178 ||:  63%|######3   | 41/65 [07:52<04:36, 11.54s/it]
2022-04-10 21:18:39,322 - INFO - tqdm - accuracy: 0.7364, batch_loss: 0.4252, loss: 0.5156 ||:  65%|######4   | 42/65 [08:04<04:25, 11.53s/it]
2022-04-10 21:18:50,884 - INFO - tqdm - accuracy: 0.7353, batch_loss: 0.5441, loss: 0.5163 ||:  66%|######6   | 43/65 [08:15<04:13, 11.54s/it]
2022-04-10 21:19:02,430 - INFO - tqdm - accuracy: 0.7342, batch_loss: 0.4787, loss: 0.5154 ||:  68%|######7   | 44/65 [08:27<04:02, 11.54s/it]
2022-04-10 21:19:13,971 - INFO - tqdm - accuracy: 0.7345, batch_loss: 0.4806, loss: 0.5147 ||:  69%|######9   | 45/65 [08:38<03:50, 11.54s/it]
2022-04-10 21:19:25,498 - INFO - tqdm - accuracy: 0.7335, batch_loss: 0.5434, loss: 0.5153 ||:  71%|#######   | 46/65 [08:50<03:39, 11.54s/it]
2022-04-10 21:19:37,030 - INFO - tqdm - accuracy: 0.7352, batch_loss: 0.4184, loss: 0.5132 ||:  72%|#######2  | 47/65 [09:01<03:27, 11.54s/it]
2022-04-10 21:19:48,571 - INFO - tqdm - accuracy: 0.7355, batch_loss: 0.4869, loss: 0.5127 ||:  74%|#######3  | 48/65 [09:13<03:16, 11.54s/it]
2022-04-10 21:20:00,112 - INFO - tqdm - accuracy: 0.7358, batch_loss: 0.4594, loss: 0.5116 ||:  75%|#######5  | 49/65 [09:25<03:04, 11.54s/it]
2022-04-10 21:20:11,626 - INFO - tqdm - accuracy: 0.7361, batch_loss: 0.5521, loss: 0.5124 ||:  77%|#######6  | 50/65 [09:36<02:52, 11.53s/it]
2022-04-10 21:20:23,164 - INFO - tqdm - accuracy: 0.7357, batch_loss: 0.4967, loss: 0.5121 ||:  78%|#######8  | 51/65 [09:48<02:41, 11.53s/it]
2022-04-10 21:20:34,691 - INFO - tqdm - accuracy: 0.7318, batch_loss: 0.6134, loss: 0.5140 ||:  80%|########  | 52/65 [09:59<02:29, 11.53s/it]
2022-04-10 21:20:46,236 - INFO - tqdm - accuracy: 0.7310, batch_loss: 0.5410, loss: 0.5146 ||:  82%|########1 | 53/65 [10:11<02:18, 11.54s/it]
2022-04-10 21:20:57,756 - INFO - tqdm - accuracy: 0.7331, batch_loss: 0.3719, loss: 0.5119 ||:  83%|########3 | 54/65 [10:22<02:06, 11.53s/it]
2022-04-10 21:21:09,283 - INFO - tqdm - accuracy: 0.7339, batch_loss: 0.4471, loss: 0.5107 ||:  85%|########4 | 55/65 [10:34<01:55, 11.53s/it]
2022-04-10 21:21:20,814 - INFO - tqdm - accuracy: 0.7326, batch_loss: 0.6049, loss: 0.5124 ||:  86%|########6 | 56/65 [10:45<01:43, 11.53s/it]
2022-04-10 21:21:32,329 - INFO - tqdm - accuracy: 0.7334, batch_loss: 0.4788, loss: 0.5118 ||:  88%|########7 | 57/65 [10:57<01:32, 11.53s/it]
2022-04-10 21:21:43,848 - INFO - tqdm - accuracy: 0.7348, batch_loss: 0.5038, loss: 0.5117 ||:  89%|########9 | 58/65 [11:08<01:20, 11.52s/it]
2022-04-10 21:21:55,386 - INFO - tqdm - accuracy: 0.7345, batch_loss: 0.6436, loss: 0.5139 ||:  91%|######### | 59/65 [11:20<01:09, 11.53s/it]
2022-04-10 21:22:06,927 - INFO - tqdm - accuracy: 0.7342, batch_loss: 0.5044, loss: 0.5138 ||:  92%|#########2| 60/65 [11:31<00:57, 11.53s/it]
2022-04-10 21:22:18,441 - INFO - tqdm - accuracy: 0.7335, batch_loss: 0.5084, loss: 0.5137 ||:  94%|#########3| 61/65 [11:43<00:46, 11.53s/it]
2022-04-10 21:22:29,956 - INFO - tqdm - accuracy: 0.7312, batch_loss: 0.6673, loss: 0.5162 ||:  95%|#########5| 62/65 [11:54<00:34, 11.52s/it]
2022-04-10 21:22:41,468 - INFO - tqdm - accuracy: 0.7320, batch_loss: 0.4996, loss: 0.5159 ||:  97%|#########6| 63/65 [12:06<00:23, 11.52s/it]
2022-04-10 21:22:52,859 - INFO - tqdm - accuracy: 0.7303, batch_loss: 0.5734, loss: 0.5168 ||:  98%|#########8| 64/65 [12:17<00:11, 11.48s/it]
2022-04-10 21:22:57,978 - INFO - tqdm - accuracy: 0.7312, batch_loss: 0.3121, loss: 0.5136 ||: 100%|##########| 65/65 [12:22<00:00,  9.57s/it]
2022-04-10 21:22:57,978 - INFO - tqdm - accuracy: 0.7312, batch_loss: 0.3121, loss: 0.5136 ||: 100%|##########| 65/65 [12:22<00:00, 11.43s/it]
2022-04-10 21:23:00,829 - INFO - allennlp.training.trainer - Validating
2022-04-10 21:23:00,830 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-10 21:23:10,967 - INFO - tqdm - accuracy: 0.7160, batch_loss: 1.7543, loss: 0.7875 ||:  36%|###5      | 41/115 [00:10<00:18,  3.97it/s]
2022-04-10 21:23:21,183 - INFO - tqdm - accuracy: 0.6646, batch_loss: 0.6049, loss: 0.8028 ||:  70%|#######   | 81/115 [00:20<00:08,  3.88it/s]
2022-04-10 21:23:29,981 - INFO - tqdm - accuracy: 0.6463, batch_loss: 0.5739, loss: 0.8117 ||: 100%|##########| 115/115 [00:29<00:00,  3.85it/s]
2022-04-10 21:23:29,981 - INFO - tqdm - accuracy: 0.6463, batch_loss: 0.5739, loss: 0.8117 ||: 100%|##########| 115/115 [00:29<00:00,  3.94it/s]
2022-04-10 21:23:29,982 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-10 21:23:29,983 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.731  |     0.646
2022-04-10 21:23:29,984 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-10 21:23:29,985 - INFO - allennlp.training.tensorboard_writer - loss               |     0.514  |     0.812
2022-04-10 21:23:29,985 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5811.043  |       N/A
2022-04-10 21:23:36,095 - INFO - allennlp.training.trainer - Epoch duration: 0:13:01.045266
2022-04-10 21:23:36,095 - INFO - allennlp.training.trainer - Estimated training time remaining: 0:26:15
2022-04-10 21:23:36,095 - INFO - allennlp.training.trainer - Epoch 13/14
2022-04-10 21:23:36,095 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-10 21:23:36,096 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-10 21:23:36,097 - INFO - allennlp.training.trainer - Training
2022-04-10 21:23:36,098 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-10 21:23:47,465 - INFO - tqdm - accuracy: 0.7500, batch_loss: 0.4941, loss: 0.4941 ||:   2%|1         | 1/65 [00:11<12:07, 11.37s/it]
2022-04-10 21:23:59,099 - INFO - tqdm - accuracy: 0.7344, batch_loss: 0.5026, loss: 0.4983 ||:   3%|3         | 2/65 [00:23<12:06, 11.52s/it]
2022-04-10 21:24:10,918 - INFO - tqdm - accuracy: 0.7396, batch_loss: 0.4851, loss: 0.4939 ||:   5%|4         | 3/65 [00:34<12:02, 11.66s/it]
2022-04-10 21:24:22,446 - INFO - tqdm - accuracy: 0.7188, batch_loss: 0.5669, loss: 0.5122 ||:   6%|6         | 4/65 [00:46<11:48, 11.61s/it]
2022-04-10 21:24:33,891 - INFO - tqdm - accuracy: 0.7063, batch_loss: 0.5501, loss: 0.5198 ||:   8%|7         | 5/65 [00:57<11:32, 11.55s/it]
2022-04-10 21:24:45,388 - INFO - tqdm - accuracy: 0.7188, batch_loss: 0.4338, loss: 0.5054 ||:   9%|9         | 6/65 [01:09<11:20, 11.53s/it]
2022-04-10 21:24:56,901 - INFO - tqdm - accuracy: 0.7143, batch_loss: 0.5550, loss: 0.5125 ||:  11%|#         | 7/65 [01:20<11:08, 11.53s/it]
2022-04-10 21:25:08,445 - INFO - tqdm - accuracy: 0.7148, batch_loss: 0.5591, loss: 0.5183 ||:  12%|#2        | 8/65 [01:32<10:57, 11.53s/it]
2022-04-10 21:25:20,010 - INFO - tqdm - accuracy: 0.7292, batch_loss: 0.4266, loss: 0.5081 ||:  14%|#3        | 9/65 [01:43<10:46, 11.54s/it]
2022-04-10 21:25:31,591 - INFO - tqdm - accuracy: 0.7219, batch_loss: 0.6029, loss: 0.5176 ||:  15%|#5        | 10/65 [01:55<10:35, 11.55s/it]
2022-04-10 21:25:43,172 - INFO - tqdm - accuracy: 0.7301, batch_loss: 0.3719, loss: 0.5044 ||:  17%|#6        | 11/65 [02:07<10:24, 11.56s/it]
2022-04-10 21:25:54,755 - INFO - tqdm - accuracy: 0.7240, batch_loss: 0.5161, loss: 0.5054 ||:  18%|#8        | 12/65 [02:18<10:13, 11.57s/it]
2022-04-10 21:26:06,338 - INFO - tqdm - accuracy: 0.7284, batch_loss: 0.4491, loss: 0.5010 ||:  20%|##        | 13/65 [02:30<10:01, 11.57s/it]
2022-04-10 21:26:17,886 - INFO - tqdm - accuracy: 0.7232, batch_loss: 0.6231, loss: 0.5097 ||:  22%|##1       | 14/65 [02:41<09:49, 11.57s/it]
2022-04-10 21:26:29,418 - INFO - tqdm - accuracy: 0.7250, batch_loss: 0.4807, loss: 0.5078 ||:  23%|##3       | 15/65 [02:53<09:37, 11.56s/it]
2022-04-10 21:26:40,926 - INFO - tqdm - accuracy: 0.7266, batch_loss: 0.4640, loss: 0.5051 ||:  25%|##4       | 16/65 [03:04<09:25, 11.54s/it]
2022-04-10 21:26:52,432 - INFO - tqdm - accuracy: 0.7316, batch_loss: 0.4500, loss: 0.5018 ||:  26%|##6       | 17/65 [03:16<09:13, 11.53s/it]
2022-04-10 21:27:03,572 - INFO - tqdm - accuracy: 0.7304, batch_loss: 0.5642, loss: 0.5053 ||:  28%|##7       | 18/65 [03:27<08:56, 11.41s/it]
2022-04-10 21:27:15,070 - INFO - tqdm - accuracy: 0.7331, batch_loss: 0.4557, loss: 0.5027 ||:  29%|##9       | 19/65 [03:38<08:46, 11.44s/it]
2022-04-10 21:27:26,631 - INFO - tqdm - accuracy: 0.7371, batch_loss: 0.4943, loss: 0.5023 ||:  31%|###       | 20/65 [03:50<08:36, 11.48s/it]
2022-04-10 21:27:38,223 - INFO - tqdm - accuracy: 0.7347, batch_loss: 0.5096, loss: 0.5026 ||:  32%|###2      | 21/65 [04:02<08:26, 11.51s/it]
2022-04-10 21:27:49,786 - INFO - tqdm - accuracy: 0.7326, batch_loss: 0.5489, loss: 0.5047 ||:  34%|###3      | 22/65 [04:13<08:15, 11.53s/it]
2022-04-10 21:28:01,310 - INFO - tqdm - accuracy: 0.7320, batch_loss: 0.4759, loss: 0.5035 ||:  35%|###5      | 23/65 [04:25<08:04, 11.53s/it]
2022-04-10 21:28:12,745 - INFO - tqdm - accuracy: 0.7314, batch_loss: 0.4808, loss: 0.5025 ||:  37%|###6      | 24/65 [04:36<07:51, 11.50s/it]
2022-04-10 21:28:24,224 - INFO - tqdm - accuracy: 0.7359, batch_loss: 0.3913, loss: 0.4981 ||:  38%|###8      | 25/65 [04:48<07:39, 11.49s/it]
2022-04-10 21:28:35,717 - INFO - tqdm - accuracy: 0.7365, batch_loss: 0.5209, loss: 0.4990 ||:  40%|####      | 26/65 [04:59<07:28, 11.49s/it]
2022-04-10 21:28:47,233 - INFO - tqdm - accuracy: 0.7346, batch_loss: 0.6055, loss: 0.5029 ||:  42%|####1     | 27/65 [05:11<07:16, 11.50s/it]
2022-04-10 21:28:58,764 - INFO - tqdm - accuracy: 0.7296, batch_loss: 0.6275, loss: 0.5073 ||:  43%|####3     | 28/65 [05:22<07:05, 11.51s/it]
2022-04-10 21:29:10,317 - INFO - tqdm - accuracy: 0.7238, batch_loss: 0.6101, loss: 0.5109 ||:  45%|####4     | 29/65 [05:34<06:54, 11.52s/it]
2022-04-10 21:29:21,888 - INFO - tqdm - accuracy: 0.7289, batch_loss: 0.3981, loss: 0.5071 ||:  46%|####6     | 30/65 [05:45<06:43, 11.54s/it]
2022-04-10 21:29:33,488 - INFO - tqdm - accuracy: 0.7306, batch_loss: 0.4230, loss: 0.5044 ||:  48%|####7     | 31/65 [05:57<06:32, 11.56s/it]
2022-04-10 21:29:45,062 - INFO - tqdm - accuracy: 0.7331, batch_loss: 0.4157, loss: 0.5016 ||:  49%|####9     | 32/65 [06:08<06:21, 11.56s/it]
2022-04-10 21:29:56,613 - INFO - tqdm - accuracy: 0.7327, batch_loss: 0.5349, loss: 0.5027 ||:  51%|#####     | 33/65 [06:20<06:09, 11.56s/it]
2022-04-10 21:30:07,990 - INFO - tqdm - accuracy: 0.7305, batch_loss: 0.6040, loss: 0.5056 ||:  52%|#####2    | 34/65 [06:31<05:56, 11.50s/it]
2022-04-10 21:30:19,474 - INFO - tqdm - accuracy: 0.7310, batch_loss: 0.5308, loss: 0.5064 ||:  54%|#####3    | 35/65 [06:43<05:44, 11.50s/it]
2022-04-10 21:30:30,952 - INFO - tqdm - accuracy: 0.7272, batch_loss: 0.5754, loss: 0.5083 ||:  55%|#####5    | 36/65 [06:54<05:33, 11.49s/it]
2022-04-10 21:30:42,434 - INFO - tqdm - accuracy: 0.7287, batch_loss: 0.5330, loss: 0.5089 ||:  57%|#####6    | 37/65 [07:06<05:21, 11.49s/it]
2022-04-10 21:30:53,910 - INFO - tqdm - accuracy: 0.7292, batch_loss: 0.4483, loss: 0.5073 ||:  58%|#####8    | 38/65 [07:17<05:10, 11.48s/it]
2022-04-10 21:31:05,399 - INFO - tqdm - accuracy: 0.7289, batch_loss: 0.6241, loss: 0.5103 ||:  60%|######    | 39/65 [07:29<04:58, 11.49s/it]
2022-04-10 21:31:16,896 - INFO - tqdm - accuracy: 0.7303, batch_loss: 0.5990, loss: 0.5126 ||:  62%|######1   | 40/65 [07:40<04:47, 11.49s/it]
2022-04-10 21:31:28,407 - INFO - tqdm - accuracy: 0.7307, batch_loss: 0.4569, loss: 0.5112 ||:  63%|######3   | 41/65 [07:52<04:35, 11.50s/it]
2022-04-10 21:31:39,913 - INFO - tqdm - accuracy: 0.7305, batch_loss: 0.5079, loss: 0.5111 ||:  65%|######4   | 42/65 [08:03<04:24, 11.50s/it]
2022-04-10 21:31:51,443 - INFO - tqdm - accuracy: 0.7316, batch_loss: 0.5386, loss: 0.5118 ||:  66%|######6   | 43/65 [08:15<04:13, 11.51s/it]
2022-04-10 21:32:02,959 - INFO - tqdm - accuracy: 0.7335, batch_loss: 0.4411, loss: 0.5102 ||:  68%|######7   | 44/65 [08:26<04:01, 11.51s/it]
2022-04-10 21:32:14,486 - INFO - tqdm - accuracy: 0.7311, batch_loss: 0.6753, loss: 0.5138 ||:  69%|######9   | 45/65 [08:38<03:50, 11.52s/it]
2022-04-10 21:32:25,786 - INFO - tqdm - accuracy: 0.7294, batch_loss: 0.5381, loss: 0.5143 ||:  71%|#######   | 46/65 [08:49<03:37, 11.45s/it]
2022-04-10 21:32:37,321 - INFO - tqdm - accuracy: 0.7259, batch_loss: 0.6410, loss: 0.5170 ||:  72%|#######2  | 47/65 [09:01<03:26, 11.48s/it]
2022-04-10 21:32:48,859 - INFO - tqdm - accuracy: 0.7244, batch_loss: 0.5711, loss: 0.5182 ||:  74%|#######3  | 48/65 [09:12<03:15, 11.49s/it]
2022-04-10 21:33:00,410 - INFO - tqdm - accuracy: 0.7230, batch_loss: 0.6448, loss: 0.5208 ||:  75%|#######5  | 49/65 [09:24<03:04, 11.51s/it]
2022-04-10 21:33:11,828 - INFO - tqdm - accuracy: 0.7230, batch_loss: 0.4812, loss: 0.5200 ||:  77%|#######6  | 50/65 [09:35<02:52, 11.48s/it]
2022-04-10 21:33:23,360 - INFO - tqdm - accuracy: 0.7223, batch_loss: 0.5323, loss: 0.5202 ||:  78%|#######8  | 51/65 [09:47<02:40, 11.50s/it]
2022-04-10 21:33:34,895 - INFO - tqdm - accuracy: 0.7252, batch_loss: 0.4014, loss: 0.5179 ||:  80%|########  | 52/65 [09:58<02:29, 11.51s/it]
2022-04-10 21:33:46,465 - INFO - tqdm - accuracy: 0.7227, batch_loss: 0.5397, loss: 0.5183 ||:  82%|########1 | 53/65 [10:10<02:18, 11.53s/it]
2022-04-10 21:33:58,019 - INFO - tqdm - accuracy: 0.7238, batch_loss: 0.3704, loss: 0.5156 ||:  83%|########3 | 54/65 [10:21<02:06, 11.54s/it]
2022-04-10 21:34:09,967 - INFO - tqdm - accuracy: 0.7237, batch_loss: 0.6571, loss: 0.5182 ||:  85%|########4 | 55/65 [10:33<01:56, 11.66s/it]
2022-04-10 21:34:21,480 - INFO - tqdm - accuracy: 0.7236, batch_loss: 0.5823, loss: 0.5193 ||:  86%|########6 | 56/65 [10:45<01:44, 11.62s/it]
2022-04-10 21:34:33,048 - INFO - tqdm - accuracy: 0.7246, batch_loss: 0.5107, loss: 0.5192 ||:  88%|########7 | 57/65 [10:56<01:32, 11.60s/it]
2022-04-10 21:34:44,614 - INFO - tqdm - accuracy: 0.7256, batch_loss: 0.4506, loss: 0.5180 ||:  89%|########9 | 58/65 [11:08<01:21, 11.59s/it]
2022-04-10 21:34:56,175 - INFO - tqdm - accuracy: 0.7271, batch_loss: 0.4855, loss: 0.5174 ||:  91%|######### | 59/65 [11:20<01:09, 11.58s/it]
2022-04-10 21:35:07,716 - INFO - tqdm - accuracy: 0.7285, batch_loss: 0.3646, loss: 0.5149 ||:  92%|#########2| 60/65 [11:31<00:57, 11.57s/it]
2022-04-10 21:35:19,282 - INFO - tqdm - accuracy: 0.7289, batch_loss: 0.4790, loss: 0.5143 ||:  94%|#########3| 61/65 [11:43<00:46, 11.57s/it]
2022-04-10 21:35:30,851 - INFO - tqdm - accuracy: 0.7267, batch_loss: 0.5743, loss: 0.5153 ||:  95%|#########5| 62/65 [11:54<00:34, 11.57s/it]
2022-04-10 21:35:42,409 - INFO - tqdm - accuracy: 0.7285, batch_loss: 0.3693, loss: 0.5129 ||:  97%|#########6| 63/65 [12:06<00:23, 11.57s/it]
2022-04-10 21:35:53,983 - INFO - tqdm - accuracy: 0.7294, batch_loss: 0.4671, loss: 0.5122 ||:  98%|#########8| 64/65 [12:17<00:11, 11.57s/it]
2022-04-10 21:35:59,119 - INFO - tqdm - accuracy: 0.7297, batch_loss: 0.5456, loss: 0.5127 ||: 100%|##########| 65/65 [12:23<00:00,  9.64s/it]
2022-04-10 21:35:59,119 - INFO - tqdm - accuracy: 0.7297, batch_loss: 0.5456, loss: 0.5127 ||: 100%|##########| 65/65 [12:23<00:00, 11.43s/it]
2022-04-10 21:36:02,127 - INFO - allennlp.training.trainer - Validating
2022-04-10 21:36:02,129 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-10 21:36:12,153 - INFO - tqdm - accuracy: 0.7215, batch_loss: 1.1114, loss: 0.7399 ||:  35%|###4      | 40/115 [00:10<00:19,  3.93it/s]
2022-04-10 21:36:22,379 - INFO - tqdm - accuracy: 0.6352, batch_loss: 0.6814, loss: 0.8499 ||:  70%|######9   | 80/115 [00:20<00:08,  3.90it/s]
2022-04-10 21:36:31,277 - INFO - tqdm - accuracy: 0.6463, batch_loss: 0.3561, loss: 0.8197 ||: 100%|##########| 115/115 [00:29<00:00,  3.92it/s]
2022-04-10 21:36:31,277 - INFO - tqdm - accuracy: 0.6463, batch_loss: 0.3561, loss: 0.8197 ||: 100%|##########| 115/115 [00:29<00:00,  3.95it/s]
2022-04-10 21:36:31,277 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-10 21:36:31,278 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.730  |     0.646
2022-04-10 21:36:31,278 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-10 21:36:31,278 - INFO - allennlp.training.tensorboard_writer - loss               |     0.513  |     0.820
2022-04-10 21:36:31,278 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5811.043  |       N/A
2022-04-10 21:36:37,262 - INFO - allennlp.training.trainer - Epoch duration: 0:13:01.166462
2022-04-10 21:36:37,262 - INFO - allennlp.training.trainer - Estimated training time remaining: 0:13:07
2022-04-10 21:36:37,262 - INFO - allennlp.training.trainer - Epoch 14/14
2022-04-10 21:36:37,262 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-10 21:36:37,262 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-10 21:36:37,264 - INFO - allennlp.training.trainer - Training
2022-04-10 21:36:37,264 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-10 21:36:48,608 - INFO - tqdm - accuracy: 0.7812, batch_loss: 0.3923, loss: 0.3923 ||:   2%|1         | 1/65 [00:11<12:05, 11.34s/it]
2022-04-10 21:37:00,235 - INFO - tqdm - accuracy: 0.7656, batch_loss: 0.5310, loss: 0.4616 ||:   3%|3         | 2/65 [00:22<12:05, 11.51s/it]
2022-04-10 21:37:12,066 - INFO - tqdm - accuracy: 0.7500, batch_loss: 0.5658, loss: 0.4964 ||:   5%|4         | 3/65 [00:34<12:02, 11.66s/it]
2022-04-10 21:37:23,624 - INFO - tqdm - accuracy: 0.7656, batch_loss: 0.3736, loss: 0.4657 ||:   6%|6         | 4/65 [00:46<11:48, 11.62s/it]
2022-04-10 21:37:35,074 - INFO - tqdm - accuracy: 0.7562, batch_loss: 0.4491, loss: 0.4624 ||:   8%|7         | 5/65 [00:57<11:33, 11.56s/it]
2022-04-10 21:37:46,542 - INFO - tqdm - accuracy: 0.7552, batch_loss: 0.5091, loss: 0.4701 ||:   9%|9         | 6/65 [01:09<11:20, 11.53s/it]
2022-04-10 21:37:58,085 - INFO - tqdm - accuracy: 0.7634, batch_loss: 0.4581, loss: 0.4684 ||:  11%|#         | 7/65 [01:20<11:08, 11.53s/it]
2022-04-10 21:38:09,645 - INFO - tqdm - accuracy: 0.7656, batch_loss: 0.4505, loss: 0.4662 ||:  12%|#2        | 8/65 [01:32<10:57, 11.54s/it]
2022-04-10 21:38:21,258 - INFO - tqdm - accuracy: 0.7639, batch_loss: 0.4381, loss: 0.4631 ||:  14%|#3        | 9/65 [01:43<10:47, 11.56s/it]
2022-04-10 21:38:32,824 - INFO - tqdm - accuracy: 0.7594, batch_loss: 0.4756, loss: 0.4643 ||:  15%|#5        | 10/65 [01:55<10:36, 11.56s/it]
2022-04-10 21:38:44,371 - INFO - tqdm - accuracy: 0.7557, batch_loss: 0.5382, loss: 0.4710 ||:  17%|#6        | 11/65 [02:07<10:24, 11.56s/it]
2022-04-10 21:38:55,887 - INFO - tqdm - accuracy: 0.7448, batch_loss: 0.5667, loss: 0.4790 ||:  18%|#8        | 12/65 [02:18<10:11, 11.55s/it]
2022-04-10 21:39:07,391 - INFO - tqdm - accuracy: 0.7404, batch_loss: 0.5302, loss: 0.4829 ||:  20%|##        | 13/65 [02:30<09:59, 11.53s/it]
2022-04-10 21:39:18,881 - INFO - tqdm - accuracy: 0.7433, batch_loss: 0.4154, loss: 0.4781 ||:  22%|##1       | 14/65 [02:41<09:47, 11.52s/it]
2022-04-10 21:39:30,400 - INFO - tqdm - accuracy: 0.7438, batch_loss: 0.5405, loss: 0.4823 ||:  23%|##3       | 15/65 [02:53<09:35, 11.52s/it]
2022-04-10 21:39:41,881 - INFO - tqdm - accuracy: 0.7402, batch_loss: 0.5450, loss: 0.4862 ||:  25%|##4       | 16/65 [03:04<09:23, 11.51s/it]
2022-04-10 21:39:53,366 - INFO - tqdm - accuracy: 0.7426, batch_loss: 0.4298, loss: 0.4829 ||:  26%|##6       | 17/65 [03:16<09:12, 11.50s/it]
2022-04-10 21:40:04,870 - INFO - tqdm - accuracy: 0.7378, batch_loss: 0.5830, loss: 0.4884 ||:  28%|##7       | 18/65 [03:27<09:00, 11.50s/it]
2022-04-10 21:40:16,382 - INFO - tqdm - accuracy: 0.7418, batch_loss: 0.4025, loss: 0.4839 ||:  29%|##9       | 19/65 [03:39<08:49, 11.50s/it]
2022-04-10 21:40:27,903 - INFO - tqdm - accuracy: 0.7359, batch_loss: 0.6279, loss: 0.4911 ||:  31%|###       | 20/65 [03:50<08:37, 11.51s/it]
2022-04-10 21:40:39,430 - INFO - tqdm - accuracy: 0.7411, batch_loss: 0.4172, loss: 0.4876 ||:  32%|###2      | 21/65 [04:02<08:26, 11.51s/it]
2022-04-10 21:40:50,834 - INFO - tqdm - accuracy: 0.7415, batch_loss: 0.4830, loss: 0.4874 ||:  34%|###3      | 22/65 [04:13<08:13, 11.48s/it]
2022-04-10 21:41:02,351 - INFO - tqdm - accuracy: 0.7473, batch_loss: 0.3950, loss: 0.4834 ||:  35%|###5      | 23/65 [04:25<08:02, 11.49s/it]
2022-04-10 21:41:13,892 - INFO - tqdm - accuracy: 0.7513, batch_loss: 0.4719, loss: 0.4829 ||:  37%|###6      | 24/65 [04:36<07:51, 11.51s/it]
2022-04-10 21:41:25,447 - INFO - tqdm - accuracy: 0.7488, batch_loss: 0.5377, loss: 0.4851 ||:  38%|###8      | 25/65 [04:48<07:40, 11.52s/it]
2022-04-10 21:41:36,989 - INFO - tqdm - accuracy: 0.7536, batch_loss: 0.3819, loss: 0.4811 ||:  40%|####      | 26/65 [04:59<07:29, 11.53s/it]
2022-04-10 21:41:48,534 - INFO - tqdm - accuracy: 0.7488, batch_loss: 0.5538, loss: 0.4838 ||:  42%|####1     | 27/65 [05:11<07:18, 11.53s/it]
2022-04-10 21:42:00,072 - INFO - tqdm - accuracy: 0.7478, batch_loss: 0.5474, loss: 0.4861 ||:  43%|####3     | 28/65 [05:22<07:06, 11.53s/it]
2022-04-10 21:42:11,616 - INFO - tqdm - accuracy: 0.7511, batch_loss: 0.4031, loss: 0.4832 ||:  45%|####4     | 29/65 [05:34<06:55, 11.54s/it]
2022-04-10 21:42:23,180 - INFO - tqdm - accuracy: 0.7458, batch_loss: 0.6851, loss: 0.4900 ||:  46%|####6     | 30/65 [05:45<06:44, 11.55s/it]
2022-04-10 21:42:34,719 - INFO - tqdm - accuracy: 0.7440, batch_loss: 0.6148, loss: 0.4940 ||:  48%|####7     | 31/65 [05:57<06:32, 11.54s/it]
2022-04-10 21:42:46,288 - INFO - tqdm - accuracy: 0.7422, batch_loss: 0.4651, loss: 0.4931 ||:  49%|####9     | 32/65 [06:09<06:21, 11.55s/it]
2022-04-10 21:42:57,834 - INFO - tqdm - accuracy: 0.7434, batch_loss: 0.3961, loss: 0.4901 ||:  51%|#####     | 33/65 [06:20<06:09, 11.55s/it]
2022-04-10 21:43:09,398 - INFO - tqdm - accuracy: 0.7445, batch_loss: 0.5110, loss: 0.4907 ||:  52%|#####2    | 34/65 [06:32<05:58, 11.55s/it]
2022-04-10 21:43:20,954 - INFO - tqdm - accuracy: 0.7402, batch_loss: 0.6699, loss: 0.4959 ||:  54%|#####3    | 35/65 [06:43<05:46, 11.55s/it]
2022-04-10 21:43:32,528 - INFO - tqdm - accuracy: 0.7361, batch_loss: 0.5790, loss: 0.4982 ||:  55%|#####5    | 36/65 [06:55<05:35, 11.56s/it]
2022-04-10 21:43:44,082 - INFO - tqdm - accuracy: 0.7348, batch_loss: 0.5375, loss: 0.4992 ||:  57%|#####6    | 37/65 [07:06<05:23, 11.56s/it]
2022-04-10 21:43:55,656 - INFO - tqdm - accuracy: 0.7368, batch_loss: 0.4264, loss: 0.4973 ||:  58%|#####8    | 38/65 [07:18<05:12, 11.56s/it]
2022-04-10 21:44:07,232 - INFO - tqdm - accuracy: 0.7332, batch_loss: 0.5747, loss: 0.4993 ||:  60%|######    | 39/65 [07:29<05:00, 11.57s/it]
2022-04-10 21:44:18,792 - INFO - tqdm - accuracy: 0.7312, batch_loss: 0.4751, loss: 0.4987 ||:  62%|######1   | 40/65 [07:41<04:49, 11.56s/it]
2022-04-10 21:44:30,365 - INFO - tqdm - accuracy: 0.7332, batch_loss: 0.4102, loss: 0.4965 ||:  63%|######3   | 41/65 [07:53<04:37, 11.57s/it]
2022-04-10 21:44:41,916 - INFO - tqdm - accuracy: 0.7336, batch_loss: 0.5712, loss: 0.4983 ||:  65%|######4   | 42/65 [08:04<04:25, 11.56s/it]
2022-04-10 21:44:53,480 - INFO - tqdm - accuracy: 0.7311, batch_loss: 0.5582, loss: 0.4997 ||:  66%|######6   | 43/65 [08:16<04:14, 11.56s/it]
2022-04-10 21:45:05,043 - INFO - tqdm - accuracy: 0.7294, batch_loss: 0.5003, loss: 0.4997 ||:  68%|######7   | 44/65 [08:27<04:02, 11.56s/it]
2022-04-10 21:45:16,604 - INFO - tqdm - accuracy: 0.7299, batch_loss: 0.4690, loss: 0.4990 ||:  69%|######9   | 45/65 [08:39<03:51, 11.56s/it]
2022-04-10 21:45:28,163 - INFO - tqdm - accuracy: 0.7323, batch_loss: 0.4043, loss: 0.4970 ||:  71%|#######   | 46/65 [08:50<03:39, 11.56s/it]
2022-04-10 21:45:39,711 - INFO - tqdm - accuracy: 0.7327, batch_loss: 0.4821, loss: 0.4967 ||:  72%|#######2  | 47/65 [09:02<03:28, 11.56s/it]
2022-04-10 21:45:51,258 - INFO - tqdm - accuracy: 0.7337, batch_loss: 0.4764, loss: 0.4962 ||:  74%|#######3  | 48/65 [09:13<03:16, 11.55s/it]
2022-04-10 21:46:02,800 - INFO - tqdm - accuracy: 0.7347, batch_loss: 0.4215, loss: 0.4947 ||:  75%|#######5  | 49/65 [09:25<03:04, 11.55s/it]
2022-04-10 21:46:14,343 - INFO - tqdm - accuracy: 0.7369, batch_loss: 0.4525, loss: 0.4939 ||:  77%|#######6  | 50/65 [09:37<02:53, 11.55s/it]
2022-04-10 21:46:25,899 - INFO - tqdm - accuracy: 0.7365, batch_loss: 0.4603, loss: 0.4932 ||:  78%|#######8  | 51/65 [09:48<02:41, 11.55s/it]
2022-04-10 21:46:36,950 - INFO - tqdm - accuracy: 0.7384, batch_loss: 0.4813, loss: 0.4930 ||:  80%|########  | 52/65 [09:59<02:28, 11.40s/it]
2022-04-10 21:46:48,479 - INFO - tqdm - accuracy: 0.7386, batch_loss: 0.4632, loss: 0.4924 ||:  82%|########1 | 53/65 [10:11<02:17, 11.44s/it]
2022-04-10 21:47:00,002 - INFO - tqdm - accuracy: 0.7389, batch_loss: 0.4523, loss: 0.4917 ||:  83%|########3 | 54/65 [10:22<02:06, 11.46s/it]
2022-04-10 21:47:11,532 - INFO - tqdm - accuracy: 0.7385, batch_loss: 0.4854, loss: 0.4916 ||:  85%|########4 | 55/65 [10:34<01:54, 11.48s/it]
2022-04-10 21:47:22,812 - INFO - tqdm - accuracy: 0.7370, batch_loss: 0.5837, loss: 0.4932 ||:  86%|########6 | 56/65 [10:45<01:42, 11.42s/it]
2022-04-10 21:47:34,276 - INFO - tqdm - accuracy: 0.7351, batch_loss: 0.5952, loss: 0.4950 ||:  88%|########7 | 57/65 [10:57<01:31, 11.44s/it]
2022-04-10 21:47:45,799 - INFO - tqdm - accuracy: 0.7337, batch_loss: 0.6590, loss: 0.4978 ||:  89%|########9 | 58/65 [11:08<01:20, 11.46s/it]
2022-04-10 21:47:57,323 - INFO - tqdm - accuracy: 0.7329, batch_loss: 0.4900, loss: 0.4977 ||:  91%|######### | 59/65 [11:20<01:08, 11.48s/it]
2022-04-10 21:48:08,813 - INFO - tqdm - accuracy: 0.7306, batch_loss: 0.6293, loss: 0.4999 ||:  92%|#########2| 60/65 [11:31<00:57, 11.48s/it]
2022-04-10 21:48:20,337 - INFO - tqdm - accuracy: 0.7314, batch_loss: 0.4281, loss: 0.4987 ||:  94%|#########3| 61/65 [11:43<00:45, 11.50s/it]
2022-04-10 21:48:31,828 - INFO - tqdm - accuracy: 0.7322, batch_loss: 0.4176, loss: 0.4974 ||:  95%|#########5| 62/65 [11:54<00:34, 11.49s/it]
2022-04-10 21:48:43,360 - INFO - tqdm - accuracy: 0.7295, batch_loss: 0.5964, loss: 0.4990 ||:  97%|#########6| 63/65 [12:06<00:23, 11.51s/it]
2022-04-10 21:48:54,879 - INFO - tqdm - accuracy: 0.7294, batch_loss: 0.4772, loss: 0.4986 ||:  98%|#########8| 64/65 [12:17<00:11, 11.51s/it]
2022-04-10 21:48:59,989 - INFO - tqdm - accuracy: 0.7293, batch_loss: 0.5423, loss: 0.4993 ||: 100%|##########| 65/65 [12:22<00:00,  9.59s/it]
2022-04-10 21:48:59,989 - INFO - tqdm - accuracy: 0.7293, batch_loss: 0.5423, loss: 0.4993 ||: 100%|##########| 65/65 [12:22<00:00, 11.43s/it]
2022-04-10 21:49:02,947 - INFO - allennlp.training.trainer - Validating
2022-04-10 21:49:02,949 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-10 21:49:13,050 - INFO - tqdm - accuracy: 0.7250, batch_loss: 1.1304, loss: 0.7175 ||:  35%|###4      | 40/115 [00:10<00:19,  3.92it/s]
2022-04-10 21:49:23,263 - INFO - tqdm - accuracy: 0.6750, batch_loss: 3.3157, loss: 0.7686 ||:  70%|######9   | 80/115 [00:20<00:08,  3.89it/s]
2022-04-10 21:49:32,050 - INFO - tqdm - accuracy: 0.6463, batch_loss: 0.9632, loss: 0.8236 ||: 100%|##########| 115/115 [00:29<00:00,  3.88it/s]
2022-04-10 21:49:32,050 - INFO - tqdm - accuracy: 0.6463, batch_loss: 0.9632, loss: 0.8236 ||: 100%|##########| 115/115 [00:29<00:00,  3.95it/s]
2022-04-10 21:49:32,050 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-10 21:49:32,051 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.729  |     0.646
2022-04-10 21:49:32,052 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-10 21:49:32,053 - INFO - allennlp.training.tensorboard_writer - loss               |     0.499  |     0.824
2022-04-10 21:49:32,053 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5811.043  |       N/A
2022-04-10 21:49:38,068 - INFO - allennlp.training.trainer - Epoch duration: 0:13:00.806199
2022-04-10 21:49:38,069 - INFO - allennlp.training.checkpointer - loading best weights
2022-04-10 21:49:38,882 - INFO - allennlp.common.util - Metrics: {
  "best_epoch": 11,
  "peak_worker_0_memory_MB": 5811.04296875,
  "peak_gpu_0_memory_MB": 9580.5712890625,
  "training_duration": "3:16:35.103806",
  "training_start_epoch": 0,
  "training_epochs": 14,
  "epoch": 14,
  "training_accuracy": 0.7292576419213974,
  "training_loss": 0.4993112120463943,
  "training_worker_0_memory_MB": 5811.04296875,
  "training_gpu_0_memory_MB": 9580.5712890625,
  "validation_accuracy": 0.6462882096069869,
  "validation_loss": 0.8236278979917583,
  "best_validation_accuracy": 0.6550218340611353,
  "best_validation_loss": 0.805672458316321
}
2022-04-10 21:49:38,883 - INFO - allennlp.models.archival - archiving weights and vocabulary to ./output_ir-ora-d_hyper5/model.tar.gz
