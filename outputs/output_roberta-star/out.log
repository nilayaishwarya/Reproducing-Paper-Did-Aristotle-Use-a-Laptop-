2022-03-30 15:26:27,037 - INFO - allennlp.common.params - random_seed = 42
2022-03-30 15:26:27,038 - INFO - allennlp.common.params - numpy_seed = 42
2022-03-30 15:26:27,038 - INFO - allennlp.common.params - pytorch_seed = 42
2022-03-30 15:26:27,038 - INFO - allennlp.common.checks - Pytorch version: 1.7.1
2022-03-30 15:26:27,038 - INFO - allennlp.common.params - type = default
2022-03-30 15:26:27,039 - INFO - allennlp.common.params - dataset_reader.type = boolean_qa_reader
2022-03-30 15:26:27,039 - INFO - allennlp.common.params - dataset_reader.lazy = False
2022-03-30 15:26:27,040 - INFO - allennlp.common.params - dataset_reader.cache_directory = None
2022-03-30 15:26:27,040 - INFO - allennlp.common.params - dataset_reader.max_instances = None
2022-03-30 15:26:27,040 - INFO - allennlp.common.params - dataset_reader.manual_distributed_sharding = False
2022-03-30 15:26:27,040 - INFO - allennlp.common.params - dataset_reader.manual_multi_process_sharding = False
2022-03-30 15:26:27,040 - INFO - allennlp.common.params - dataset_reader.tokenizer_wrapper.type = default
2022-03-30 15:26:27,041 - INFO - allennlp.common.params - dataset_reader.tokenizer_wrapper.pretrained_model = roberta-large
2022-03-30 15:26:27,041 - INFO - allennlp.common.params - dataset_reader.tokenizer_wrapper.call_kwargs.truncation = only_first
2022-03-30 15:26:28,257 - INFO - allennlp.common.params - dataset_reader.save_tokenizer = True
2022-03-30 15:26:28,258 - INFO - allennlp.common.params - dataset_reader.is_training = True
2022-03-30 15:26:28,258 - INFO - allennlp.common.params - dataset_reader.pickle.action = None
2022-03-30 15:26:28,258 - INFO - allennlp.common.params - dataset_reader.with_context = True
2022-03-30 15:26:28,258 - INFO - allennlp.common.params - dataset_reader.context_key = passage
2022-03-30 15:26:28,258 - INFO - allennlp.common.params - dataset_reader.answer_key = answer
2022-03-30 15:26:28,258 - INFO - allennlp.common.params - dataset_reader.is_twenty_questions = False
2022-03-30 15:26:28,712 - INFO - allennlp.common.params - train_data_path = data/boolq/train.jsonl
2022-03-30 15:26:28,713 - INFO - allennlp.common.params - vocabulary = <allennlp.common.lazy.Lazy object at 0x7fd5f01b8190>
2022-03-30 15:26:28,713 - INFO - allennlp.common.params - datasets_for_vocab_creation = None
2022-03-30 15:26:28,714 - INFO - allennlp.common.params - validation_dataset_reader.type = boolean_qa_reader
2022-03-30 15:26:28,714 - INFO - allennlp.common.params - validation_dataset_reader.lazy = False
2022-03-30 15:26:28,714 - INFO - allennlp.common.params - validation_dataset_reader.cache_directory = None
2022-03-30 15:26:28,714 - INFO - allennlp.common.params - validation_dataset_reader.max_instances = None
2022-03-30 15:26:28,715 - INFO - allennlp.common.params - validation_dataset_reader.manual_distributed_sharding = False
2022-03-30 15:26:28,715 - INFO - allennlp.common.params - validation_dataset_reader.manual_multi_process_sharding = False
2022-03-30 15:26:28,715 - INFO - allennlp.common.params - validation_dataset_reader.tokenizer_wrapper.type = default
2022-03-30 15:26:28,715 - INFO - allennlp.common.params - validation_dataset_reader.tokenizer_wrapper.pretrained_model = roberta-large
2022-03-30 15:26:28,716 - INFO - allennlp.common.params - validation_dataset_reader.tokenizer_wrapper.call_kwargs.truncation = only_first
2022-03-30 15:26:29,913 - INFO - allennlp.common.params - validation_dataset_reader.save_tokenizer = False
2022-03-30 15:26:29,913 - INFO - allennlp.common.params - validation_dataset_reader.is_training = False
2022-03-30 15:26:29,914 - INFO - allennlp.common.params - validation_dataset_reader.pickle.action = None
2022-03-30 15:26:29,914 - INFO - allennlp.common.params - validation_dataset_reader.with_context = True
2022-03-30 15:26:29,914 - INFO - allennlp.common.params - validation_dataset_reader.context_key = passage
2022-03-30 15:26:29,914 - INFO - allennlp.common.params - validation_dataset_reader.answer_key = answer
2022-03-30 15:26:29,914 - INFO - allennlp.common.params - validation_dataset_reader.is_twenty_questions = False
2022-03-30 15:26:29,914 - INFO - allennlp.common.params - validation_data_path = data/boolq/dev.jsonl
2022-03-30 15:26:29,914 - INFO - allennlp.common.params - validation_data_loader = None
2022-03-30 15:26:29,914 - INFO - allennlp.common.params - test_data_path = None
2022-03-30 15:26:29,915 - INFO - allennlp.common.params - evaluate_on_test = False
2022-03-30 15:26:29,915 - INFO - allennlp.common.params - batch_weight_key = 
2022-03-30 15:26:29,915 - INFO - allennlp.training.util - Reading training data from data/boolq/train.jsonl
2022-03-30 15:26:29,915 - INFO - tqdm - reading instances: 0it [00:00, ?it/s]
2022-03-30 15:26:29,915 - INFO - src.data.dataset_readers.boolean_qa_reader - Reading the dataset:
2022-03-30 15:26:29,916 - INFO - src.data.dataset_readers.boolean_qa_reader - Reading file at data/boolq/train.jsonl
2022-03-30 15:26:39,256 - INFO - allennlp.training.util - Reading validation data from data/boolq/dev.jsonl
2022-03-30 15:26:39,256 - INFO - tqdm - reading instances: 0it [00:00, ?it/s]
2022-03-30 15:26:39,256 - INFO - src.data.dataset_readers.boolean_qa_reader - Reading the dataset:
2022-03-30 15:26:39,256 - INFO - src.data.dataset_readers.boolean_qa_reader - Reading file at data/boolq/dev.jsonl
2022-03-30 15:26:42,308 - INFO - allennlp.common.params - type = from_instances
2022-03-30 15:26:42,308 - INFO - allennlp.common.params - min_count = None
2022-03-30 15:26:42,308 - INFO - allennlp.common.params - max_vocab_size = None
2022-03-30 15:26:42,308 - INFO - allennlp.common.params - non_padded_namespaces = ('*tags', '*labels')
2022-03-30 15:26:42,308 - INFO - allennlp.common.params - pretrained_files = None
2022-03-30 15:26:42,308 - INFO - allennlp.common.params - only_include_pretrained_words = False
2022-03-30 15:26:42,308 - INFO - allennlp.common.params - tokens_to_add = None
2022-03-30 15:26:42,308 - INFO - allennlp.common.params - min_pretrained_embeddings = None
2022-03-30 15:26:42,309 - INFO - allennlp.common.params - padding_token = @@PADDING@@
2022-03-30 15:26:42,309 - INFO - allennlp.common.params - oov_token = @@UNKNOWN@@
2022-03-30 15:26:42,309 - INFO - allennlp.data.vocabulary - Fitting token dictionary from dataset.
2022-03-30 15:26:42,309 - INFO - tqdm - building vocab: 0it [00:00, ?it/s]
2022-03-30 15:26:42,340 - INFO - allennlp.common.params - model.type = hf_classifier
2022-03-30 15:26:42,340 - INFO - allennlp.common.params - model.regularizer = None
2022-03-30 15:26:42,340 - INFO - allennlp.common.params - model.pretrained_model = roberta-large
2022-03-30 15:26:42,341 - INFO - allennlp.common.params - model.tokenizer_wrapper.type = default
2022-03-30 15:26:42,341 - INFO - allennlp.common.params - model.tokenizer_wrapper.pretrained_model = roberta-large
2022-03-30 15:26:42,341 - INFO - allennlp.common.params - model.tokenizer_wrapper.call_kwargs.truncation = only_first
2022-03-30 15:26:43,546 - INFO - allennlp.common.params - model.num_labels = 2
2022-03-30 15:26:43,547 - INFO - allennlp.common.params - model.label_namespace = labels
2022-03-30 15:26:43,547 - INFO - allennlp.common.params - model.transformer_weights_path = None
2022-03-30 15:26:43,547 - INFO - allennlp.common.params - model.initializer.regexes.0.1.type = pretrained
2022-03-30 15:26:43,547 - INFO - allennlp.common.params - model.initializer.regexes.0.1.weights_file_path = experiments/for_STAR_1_twentyquestions_0/model.tar.gz
2022-03-30 15:26:43,548 - INFO - allennlp.models.archival - extracting archive file experiments/for_STAR_1_twentyquestions_0/model.tar.gz to temp dir /tmp/tmpphfizy5p
2022-03-30 15:26:55,670 - INFO - allennlp.models.archival - removing temporary unarchived model dir at /tmp/tmpphfizy5p
2022-03-30 15:26:55,852 - INFO - allennlp.common.params - model.initializer.prevent_regexes = None
2022-03-30 15:27:10,979 - INFO - allennlp.nn.initializers - Initializing parameters
2022-03-30 15:27:10,980 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.embeddings.word_embeddings.weight using .* initializer
2022-03-30 15:27:10,989 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.embeddings.position_embeddings.weight using .* initializer
2022-03-30 15:27:10,990 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.embeddings.token_type_embeddings.weight using .* initializer
2022-03-30 15:27:10,990 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.embeddings.LayerNorm.weight using .* initializer
2022-03-30 15:27:10,990 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.embeddings.LayerNorm.bias using .* initializer
2022-03-30 15:27:10,990 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.self.query.weight using .* initializer
2022-03-30 15:27:10,991 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.self.query.bias using .* initializer
2022-03-30 15:27:10,991 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.self.key.weight using .* initializer
2022-03-30 15:27:10,991 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.self.key.bias using .* initializer
2022-03-30 15:27:10,991 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.self.value.weight using .* initializer
2022-03-30 15:27:10,992 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.self.value.bias using .* initializer
2022-03-30 15:27:10,992 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.output.dense.weight using .* initializer
2022-03-30 15:27:10,992 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.output.dense.bias using .* initializer
2022-03-30 15:27:10,993 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.output.LayerNorm.weight using .* initializer
2022-03-30 15:27:10,993 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.output.LayerNorm.bias using .* initializer
2022-03-30 15:27:10,993 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.intermediate.dense.weight using .* initializer
2022-03-30 15:27:10,994 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.intermediate.dense.bias using .* initializer
2022-03-30 15:27:10,994 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.output.dense.weight using .* initializer
2022-03-30 15:27:10,995 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.output.dense.bias using .* initializer
2022-03-30 15:27:10,995 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.output.LayerNorm.weight using .* initializer
2022-03-30 15:27:10,995 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.output.LayerNorm.bias using .* initializer
2022-03-30 15:27:10,995 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.self.query.weight using .* initializer
2022-03-30 15:27:10,996 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.self.query.bias using .* initializer
2022-03-30 15:27:10,996 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.self.key.weight using .* initializer
2022-03-30 15:27:10,996 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.self.key.bias using .* initializer
2022-03-30 15:27:10,997 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.self.value.weight using .* initializer
2022-03-30 15:27:10,997 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.self.value.bias using .* initializer
2022-03-30 15:27:10,997 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.output.dense.weight using .* initializer
2022-03-30 15:27:10,997 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.output.dense.bias using .* initializer
2022-03-30 15:27:10,998 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.output.LayerNorm.weight using .* initializer
2022-03-30 15:27:10,998 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.output.LayerNorm.bias using .* initializer
2022-03-30 15:27:10,998 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.intermediate.dense.weight using .* initializer
2022-03-30 15:27:10,999 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.intermediate.dense.bias using .* initializer
2022-03-30 15:27:10,999 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.output.dense.weight using .* initializer
2022-03-30 15:27:11,000 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.output.dense.bias using .* initializer
2022-03-30 15:27:11,000 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.output.LayerNorm.weight using .* initializer
2022-03-30 15:27:11,000 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.output.LayerNorm.bias using .* initializer
2022-03-30 15:27:11,001 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.self.query.weight using .* initializer
2022-03-30 15:27:11,001 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.self.query.bias using .* initializer
2022-03-30 15:27:11,001 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.self.key.weight using .* initializer
2022-03-30 15:27:11,002 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.self.key.bias using .* initializer
2022-03-30 15:27:11,002 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.self.value.weight using .* initializer
2022-03-30 15:27:11,002 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.self.value.bias using .* initializer
2022-03-30 15:27:11,002 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.output.dense.weight using .* initializer
2022-03-30 15:27:11,003 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.output.dense.bias using .* initializer
2022-03-30 15:27:11,003 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.output.LayerNorm.weight using .* initializer
2022-03-30 15:27:11,003 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.output.LayerNorm.bias using .* initializer
2022-03-30 15:27:11,003 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.intermediate.dense.weight using .* initializer
2022-03-30 15:27:11,004 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.intermediate.dense.bias using .* initializer
2022-03-30 15:27:11,004 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.output.dense.weight using .* initializer
2022-03-30 15:27:11,005 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.output.dense.bias using .* initializer
2022-03-30 15:27:11,006 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.output.LayerNorm.weight using .* initializer
2022-03-30 15:27:11,006 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.output.LayerNorm.bias using .* initializer
2022-03-30 15:27:11,006 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.self.query.weight using .* initializer
2022-03-30 15:27:11,006 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.self.query.bias using .* initializer
2022-03-30 15:27:11,006 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.self.key.weight using .* initializer
2022-03-30 15:27:11,007 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.self.key.bias using .* initializer
2022-03-30 15:27:11,007 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.self.value.weight using .* initializer
2022-03-30 15:27:11,007 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.self.value.bias using .* initializer
2022-03-30 15:27:11,008 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.output.dense.weight using .* initializer
2022-03-30 15:27:11,008 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.output.dense.bias using .* initializer
2022-03-30 15:27:11,008 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.output.LayerNorm.weight using .* initializer
2022-03-30 15:27:11,008 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.output.LayerNorm.bias using .* initializer
2022-03-30 15:27:11,008 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.intermediate.dense.weight using .* initializer
2022-03-30 15:27:11,009 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.intermediate.dense.bias using .* initializer
2022-03-30 15:27:11,010 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.output.dense.weight using .* initializer
2022-03-30 15:27:11,011 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.output.dense.bias using .* initializer
2022-03-30 15:27:11,011 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.output.LayerNorm.weight using .* initializer
2022-03-30 15:27:11,011 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.output.LayerNorm.bias using .* initializer
2022-03-30 15:27:11,011 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.self.query.weight using .* initializer
2022-03-30 15:27:11,011 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.self.query.bias using .* initializer
2022-03-30 15:27:11,012 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.self.key.weight using .* initializer
2022-03-30 15:27:11,012 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.self.key.bias using .* initializer
2022-03-30 15:27:11,012 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.self.value.weight using .* initializer
2022-03-30 15:27:11,013 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.self.value.bias using .* initializer
2022-03-30 15:27:11,013 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.output.dense.weight using .* initializer
2022-03-30 15:27:11,013 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.output.dense.bias using .* initializer
2022-03-30 15:27:11,013 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.output.LayerNorm.weight using .* initializer
2022-03-30 15:27:11,013 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.output.LayerNorm.bias using .* initializer
2022-03-30 15:27:11,014 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.intermediate.dense.weight using .* initializer
2022-03-30 15:27:11,015 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.intermediate.dense.bias using .* initializer
2022-03-30 15:27:11,015 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.output.dense.weight using .* initializer
2022-03-30 15:27:11,016 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.output.dense.bias using .* initializer
2022-03-30 15:27:11,016 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.output.LayerNorm.weight using .* initializer
2022-03-30 15:27:11,016 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.output.LayerNorm.bias using .* initializer
2022-03-30 15:27:11,016 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.self.query.weight using .* initializer
2022-03-30 15:27:11,017 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.self.query.bias using .* initializer
2022-03-30 15:27:11,017 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.self.key.weight using .* initializer
2022-03-30 15:27:11,017 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.self.key.bias using .* initializer
2022-03-30 15:27:11,017 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.self.value.weight using .* initializer
2022-03-30 15:27:11,018 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.self.value.bias using .* initializer
2022-03-30 15:27:11,018 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.output.dense.weight using .* initializer
2022-03-30 15:27:11,018 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.output.dense.bias using .* initializer
2022-03-30 15:27:11,018 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.output.LayerNorm.weight using .* initializer
2022-03-30 15:27:11,019 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.output.LayerNorm.bias using .* initializer
2022-03-30 15:27:11,019 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.intermediate.dense.weight using .* initializer
2022-03-30 15:27:11,020 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.intermediate.dense.bias using .* initializer
2022-03-30 15:27:11,020 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.output.dense.weight using .* initializer
2022-03-30 15:27:11,021 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.output.dense.bias using .* initializer
2022-03-30 15:27:11,021 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.output.LayerNorm.weight using .* initializer
2022-03-30 15:27:11,021 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.output.LayerNorm.bias using .* initializer
2022-03-30 15:27:11,021 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.self.query.weight using .* initializer
2022-03-30 15:27:11,022 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.self.query.bias using .* initializer
2022-03-30 15:27:11,022 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.self.key.weight using .* initializer
2022-03-30 15:27:11,022 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.self.key.bias using .* initializer
2022-03-30 15:27:11,022 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.self.value.weight using .* initializer
2022-03-30 15:27:11,023 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.self.value.bias using .* initializer
2022-03-30 15:27:11,023 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.output.dense.weight using .* initializer
2022-03-30 15:27:11,023 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.output.dense.bias using .* initializer
2022-03-30 15:27:11,023 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.output.LayerNorm.weight using .* initializer
2022-03-30 15:27:11,023 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.output.LayerNorm.bias using .* initializer
2022-03-30 15:27:11,024 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.intermediate.dense.weight using .* initializer
2022-03-30 15:27:11,025 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.intermediate.dense.bias using .* initializer
2022-03-30 15:27:11,025 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.output.dense.weight using .* initializer
2022-03-30 15:27:11,026 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.output.dense.bias using .* initializer
2022-03-30 15:27:11,026 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.output.LayerNorm.weight using .* initializer
2022-03-30 15:27:11,026 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.output.LayerNorm.bias using .* initializer
2022-03-30 15:27:11,026 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.self.query.weight using .* initializer
2022-03-30 15:27:11,027 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.self.query.bias using .* initializer
2022-03-30 15:27:11,027 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.self.key.weight using .* initializer
2022-03-30 15:27:11,027 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.self.key.bias using .* initializer
2022-03-30 15:27:11,027 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.self.value.weight using .* initializer
2022-03-30 15:27:11,028 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.self.value.bias using .* initializer
2022-03-30 15:27:11,028 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.output.dense.weight using .* initializer
2022-03-30 15:27:11,028 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.output.dense.bias using .* initializer
2022-03-30 15:27:11,028 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.output.LayerNorm.weight using .* initializer
2022-03-30 15:27:11,029 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.output.LayerNorm.bias using .* initializer
2022-03-30 15:27:11,029 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.intermediate.dense.weight using .* initializer
2022-03-30 15:27:11,030 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.intermediate.dense.bias using .* initializer
2022-03-30 15:27:11,030 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.output.dense.weight using .* initializer
2022-03-30 15:27:11,031 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.output.dense.bias using .* initializer
2022-03-30 15:27:11,031 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.output.LayerNorm.weight using .* initializer
2022-03-30 15:27:11,031 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.output.LayerNorm.bias using .* initializer
2022-03-30 15:27:11,031 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.self.query.weight using .* initializer
2022-03-30 15:27:11,032 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.self.query.bias using .* initializer
2022-03-30 15:27:11,032 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.self.key.weight using .* initializer
2022-03-30 15:27:11,032 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.self.key.bias using .* initializer
2022-03-30 15:27:11,032 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.self.value.weight using .* initializer
2022-03-30 15:27:11,033 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.self.value.bias using .* initializer
2022-03-30 15:27:11,033 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.output.dense.weight using .* initializer
2022-03-30 15:27:11,033 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.output.dense.bias using .* initializer
2022-03-30 15:27:11,033 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.output.LayerNorm.weight using .* initializer
2022-03-30 15:27:11,034 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.output.LayerNorm.bias using .* initializer
2022-03-30 15:27:11,034 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.intermediate.dense.weight using .* initializer
2022-03-30 15:27:11,035 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.intermediate.dense.bias using .* initializer
2022-03-30 15:27:11,035 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.output.dense.weight using .* initializer
2022-03-30 15:27:11,036 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.output.dense.bias using .* initializer
2022-03-30 15:27:11,036 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.output.LayerNorm.weight using .* initializer
2022-03-30 15:27:11,036 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.output.LayerNorm.bias using .* initializer
2022-03-30 15:27:11,036 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.self.query.weight using .* initializer
2022-03-30 15:27:11,037 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.self.query.bias using .* initializer
2022-03-30 15:27:11,037 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.self.key.weight using .* initializer
2022-03-30 15:27:11,037 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.self.key.bias using .* initializer
2022-03-30 15:27:11,037 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.self.value.weight using .* initializer
2022-03-30 15:27:11,038 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.self.value.bias using .* initializer
2022-03-30 15:27:11,038 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.output.dense.weight using .* initializer
2022-03-30 15:27:11,038 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.output.dense.bias using .* initializer
2022-03-30 15:27:11,039 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.output.LayerNorm.weight using .* initializer
2022-03-30 15:27:11,039 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.output.LayerNorm.bias using .* initializer
2022-03-30 15:27:11,039 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.intermediate.dense.weight using .* initializer
2022-03-30 15:27:11,040 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.intermediate.dense.bias using .* initializer
2022-03-30 15:27:11,040 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.output.dense.weight using .* initializer
2022-03-30 15:27:11,041 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.output.dense.bias using .* initializer
2022-03-30 15:27:11,041 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.output.LayerNorm.weight using .* initializer
2022-03-30 15:27:11,041 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.output.LayerNorm.bias using .* initializer
2022-03-30 15:27:11,041 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.self.query.weight using .* initializer
2022-03-30 15:27:11,042 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.self.query.bias using .* initializer
2022-03-30 15:27:11,042 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.self.key.weight using .* initializer
2022-03-30 15:27:11,042 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.self.key.bias using .* initializer
2022-03-30 15:27:11,043 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.self.value.weight using .* initializer
2022-03-30 15:27:11,043 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.self.value.bias using .* initializer
2022-03-30 15:27:11,043 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.output.dense.weight using .* initializer
2022-03-30 15:27:11,044 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.output.dense.bias using .* initializer
2022-03-30 15:27:11,044 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.output.LayerNorm.weight using .* initializer
2022-03-30 15:27:11,044 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.output.LayerNorm.bias using .* initializer
2022-03-30 15:27:11,044 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.intermediate.dense.weight using .* initializer
2022-03-30 15:27:11,045 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.intermediate.dense.bias using .* initializer
2022-03-30 15:27:11,045 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.output.dense.weight using .* initializer
2022-03-30 15:27:11,046 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.output.dense.bias using .* initializer
2022-03-30 15:27:11,046 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.output.LayerNorm.weight using .* initializer
2022-03-30 15:27:11,047 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.output.LayerNorm.bias using .* initializer
2022-03-30 15:27:11,047 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.self.query.weight using .* initializer
2022-03-30 15:27:11,047 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.self.query.bias using .* initializer
2022-03-30 15:27:11,047 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.self.key.weight using .* initializer
2022-03-30 15:27:11,048 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.self.key.bias using .* initializer
2022-03-30 15:27:11,048 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.self.value.weight using .* initializer
2022-03-30 15:27:11,048 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.self.value.bias using .* initializer
2022-03-30 15:27:11,049 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.output.dense.weight using .* initializer
2022-03-30 15:27:11,049 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.output.dense.bias using .* initializer
2022-03-30 15:27:11,049 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.output.LayerNorm.weight using .* initializer
2022-03-30 15:27:11,049 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.output.LayerNorm.bias using .* initializer
2022-03-30 15:27:11,049 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.intermediate.dense.weight using .* initializer
2022-03-30 15:27:11,050 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.intermediate.dense.bias using .* initializer
2022-03-30 15:27:11,051 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.output.dense.weight using .* initializer
2022-03-30 15:27:11,051 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.output.dense.bias using .* initializer
2022-03-30 15:27:11,052 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.output.LayerNorm.weight using .* initializer
2022-03-30 15:27:11,052 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.output.LayerNorm.bias using .* initializer
2022-03-30 15:27:11,052 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.self.query.weight using .* initializer
2022-03-30 15:27:11,052 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.self.query.bias using .* initializer
2022-03-30 15:27:11,053 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.self.key.weight using .* initializer
2022-03-30 15:27:11,053 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.self.key.bias using .* initializer
2022-03-30 15:27:11,053 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.self.value.weight using .* initializer
2022-03-30 15:27:11,053 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.self.value.bias using .* initializer
2022-03-30 15:27:11,054 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.output.dense.weight using .* initializer
2022-03-30 15:27:11,054 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.output.dense.bias using .* initializer
2022-03-30 15:27:11,054 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.output.LayerNorm.weight using .* initializer
2022-03-30 15:27:11,054 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.output.LayerNorm.bias using .* initializer
2022-03-30 15:27:11,054 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.intermediate.dense.weight using .* initializer
2022-03-30 15:27:11,055 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.intermediate.dense.bias using .* initializer
2022-03-30 15:27:11,056 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.output.dense.weight using .* initializer
2022-03-30 15:27:11,057 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.output.dense.bias using .* initializer
2022-03-30 15:27:11,057 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.output.LayerNorm.weight using .* initializer
2022-03-30 15:27:11,057 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.output.LayerNorm.bias using .* initializer
2022-03-30 15:27:11,057 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.self.query.weight using .* initializer
2022-03-30 15:27:11,057 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.self.query.bias using .* initializer
2022-03-30 15:27:11,058 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.self.key.weight using .* initializer
2022-03-30 15:27:11,058 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.self.key.bias using .* initializer
2022-03-30 15:27:11,058 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.self.value.weight using .* initializer
2022-03-30 15:27:11,058 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.self.value.bias using .* initializer
2022-03-30 15:27:11,059 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.output.dense.weight using .* initializer
2022-03-30 15:27:11,059 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.output.dense.bias using .* initializer
2022-03-30 15:27:11,059 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.output.LayerNorm.weight using .* initializer
2022-03-30 15:27:11,059 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.output.LayerNorm.bias using .* initializer
2022-03-30 15:27:11,060 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.intermediate.dense.weight using .* initializer
2022-03-30 15:27:11,061 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.intermediate.dense.bias using .* initializer
2022-03-30 15:27:11,061 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.output.dense.weight using .* initializer
2022-03-30 15:27:11,062 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.output.dense.bias using .* initializer
2022-03-30 15:27:11,062 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.output.LayerNorm.weight using .* initializer
2022-03-30 15:27:11,062 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.output.LayerNorm.bias using .* initializer
2022-03-30 15:27:11,062 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.self.query.weight using .* initializer
2022-03-30 15:27:11,063 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.self.query.bias using .* initializer
2022-03-30 15:27:11,063 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.self.key.weight using .* initializer
2022-03-30 15:27:11,063 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.self.key.bias using .* initializer
2022-03-30 15:27:11,063 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.self.value.weight using .* initializer
2022-03-30 15:27:11,064 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.self.value.bias using .* initializer
2022-03-30 15:27:11,064 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.output.dense.weight using .* initializer
2022-03-30 15:27:11,064 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.output.dense.bias using .* initializer
2022-03-30 15:27:11,064 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.output.LayerNorm.weight using .* initializer
2022-03-30 15:27:11,065 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.output.LayerNorm.bias using .* initializer
2022-03-30 15:27:11,065 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.intermediate.dense.weight using .* initializer
2022-03-30 15:27:11,066 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.intermediate.dense.bias using .* initializer
2022-03-30 15:27:11,066 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.output.dense.weight using .* initializer
2022-03-30 15:27:11,067 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.output.dense.bias using .* initializer
2022-03-30 15:27:11,067 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.output.LayerNorm.weight using .* initializer
2022-03-30 15:27:11,067 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.output.LayerNorm.bias using .* initializer
2022-03-30 15:27:11,067 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.self.query.weight using .* initializer
2022-03-30 15:27:11,068 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.self.query.bias using .* initializer
2022-03-30 15:27:11,068 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.self.key.weight using .* initializer
2022-03-30 15:27:11,068 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.self.key.bias using .* initializer
2022-03-30 15:27:11,068 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.self.value.weight using .* initializer
2022-03-30 15:27:11,069 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.self.value.bias using .* initializer
2022-03-30 15:27:11,069 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.output.dense.weight using .* initializer
2022-03-30 15:27:11,069 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.output.dense.bias using .* initializer
2022-03-30 15:27:11,070 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.output.LayerNorm.weight using .* initializer
2022-03-30 15:27:11,070 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.output.LayerNorm.bias using .* initializer
2022-03-30 15:27:11,070 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.intermediate.dense.weight using .* initializer
2022-03-30 15:27:11,071 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.intermediate.dense.bias using .* initializer
2022-03-30 15:27:11,071 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.output.dense.weight using .* initializer
2022-03-30 15:27:11,072 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.output.dense.bias using .* initializer
2022-03-30 15:27:11,072 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.output.LayerNorm.weight using .* initializer
2022-03-30 15:27:11,072 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.output.LayerNorm.bias using .* initializer
2022-03-30 15:27:11,073 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.self.query.weight using .* initializer
2022-03-30 15:27:11,073 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.self.query.bias using .* initializer
2022-03-30 15:27:11,073 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.self.key.weight using .* initializer
2022-03-30 15:27:11,073 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.self.key.bias using .* initializer
2022-03-30 15:27:11,074 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.self.value.weight using .* initializer
2022-03-30 15:27:11,074 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.self.value.bias using .* initializer
2022-03-30 15:27:11,074 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.output.dense.weight using .* initializer
2022-03-30 15:27:11,075 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.output.dense.bias using .* initializer
2022-03-30 15:27:11,075 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.output.LayerNorm.weight using .* initializer
2022-03-30 15:27:11,075 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.output.LayerNorm.bias using .* initializer
2022-03-30 15:27:11,075 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.intermediate.dense.weight using .* initializer
2022-03-30 15:27:11,076 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.intermediate.dense.bias using .* initializer
2022-03-30 15:27:11,076 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.output.dense.weight using .* initializer
2022-03-30 15:27:11,077 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.output.dense.bias using .* initializer
2022-03-30 15:27:11,077 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.output.LayerNorm.weight using .* initializer
2022-03-30 15:27:11,077 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.output.LayerNorm.bias using .* initializer
2022-03-30 15:27:11,078 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.self.query.weight using .* initializer
2022-03-30 15:27:11,078 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.self.query.bias using .* initializer
2022-03-30 15:27:11,078 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.self.key.weight using .* initializer
2022-03-30 15:27:11,079 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.self.key.bias using .* initializer
2022-03-30 15:27:11,079 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.self.value.weight using .* initializer
2022-03-30 15:27:11,079 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.self.value.bias using .* initializer
2022-03-30 15:27:11,079 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.output.dense.weight using .* initializer
2022-03-30 15:27:11,080 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.output.dense.bias using .* initializer
2022-03-30 15:27:11,080 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.output.LayerNorm.weight using .* initializer
2022-03-30 15:27:11,080 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.output.LayerNorm.bias using .* initializer
2022-03-30 15:27:11,080 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.intermediate.dense.weight using .* initializer
2022-03-30 15:27:11,081 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.intermediate.dense.bias using .* initializer
2022-03-30 15:27:11,081 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.output.dense.weight using .* initializer
2022-03-30 15:27:11,082 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.output.dense.bias using .* initializer
2022-03-30 15:27:11,083 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.output.LayerNorm.weight using .* initializer
2022-03-30 15:27:11,083 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.output.LayerNorm.bias using .* initializer
2022-03-30 15:27:11,083 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.self.query.weight using .* initializer
2022-03-30 15:27:11,083 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.self.query.bias using .* initializer
2022-03-30 15:27:11,083 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.self.key.weight using .* initializer
2022-03-30 15:27:11,084 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.self.key.bias using .* initializer
2022-03-30 15:27:11,084 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.self.value.weight using .* initializer
2022-03-30 15:27:11,084 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.self.value.bias using .* initializer
2022-03-30 15:27:11,084 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.output.dense.weight using .* initializer
2022-03-30 15:27:11,085 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.output.dense.bias using .* initializer
2022-03-30 15:27:11,085 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.output.LayerNorm.weight using .* initializer
2022-03-30 15:27:11,085 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.output.LayerNorm.bias using .* initializer
2022-03-30 15:27:11,085 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.intermediate.dense.weight using .* initializer
2022-03-30 15:27:11,086 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.intermediate.dense.bias using .* initializer
2022-03-30 15:27:11,086 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.output.dense.weight using .* initializer
2022-03-30 15:27:11,087 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.output.dense.bias using .* initializer
2022-03-30 15:27:11,087 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.output.LayerNorm.weight using .* initializer
2022-03-30 15:27:11,088 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.output.LayerNorm.bias using .* initializer
2022-03-30 15:27:11,088 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.self.query.weight using .* initializer
2022-03-30 15:27:11,088 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.self.query.bias using .* initializer
2022-03-30 15:27:11,088 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.self.key.weight using .* initializer
2022-03-30 15:27:11,089 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.self.key.bias using .* initializer
2022-03-30 15:27:11,089 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.self.value.weight using .* initializer
2022-03-30 15:27:11,089 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.self.value.bias using .* initializer
2022-03-30 15:27:11,089 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.output.dense.weight using .* initializer
2022-03-30 15:27:11,089 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.output.dense.bias using .* initializer
2022-03-30 15:27:11,090 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.output.LayerNorm.weight using .* initializer
2022-03-30 15:27:11,090 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.output.LayerNorm.bias using .* initializer
2022-03-30 15:27:11,090 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.intermediate.dense.weight using .* initializer
2022-03-30 15:27:11,091 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.intermediate.dense.bias using .* initializer
2022-03-30 15:27:11,091 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.output.dense.weight using .* initializer
2022-03-30 15:27:11,092 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.output.dense.bias using .* initializer
2022-03-30 15:27:11,092 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.output.LayerNorm.weight using .* initializer
2022-03-30 15:27:11,092 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.output.LayerNorm.bias using .* initializer
2022-03-30 15:27:11,092 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.self.query.weight using .* initializer
2022-03-30 15:27:11,093 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.self.query.bias using .* initializer
2022-03-30 15:27:11,093 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.self.key.weight using .* initializer
2022-03-30 15:27:11,093 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.self.key.bias using .* initializer
2022-03-30 15:27:11,093 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.self.value.weight using .* initializer
2022-03-30 15:27:11,093 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.self.value.bias using .* initializer
2022-03-30 15:27:11,094 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.output.dense.weight using .* initializer
2022-03-30 15:27:11,094 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.output.dense.bias using .* initializer
2022-03-30 15:27:11,094 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.output.LayerNorm.weight using .* initializer
2022-03-30 15:27:11,094 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.output.LayerNorm.bias using .* initializer
2022-03-30 15:27:11,094 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.intermediate.dense.weight using .* initializer
2022-03-30 15:27:11,095 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.intermediate.dense.bias using .* initializer
2022-03-30 15:27:11,095 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.output.dense.weight using .* initializer
2022-03-30 15:27:11,096 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.output.dense.bias using .* initializer
2022-03-30 15:27:11,096 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.output.LayerNorm.weight using .* initializer
2022-03-30 15:27:11,097 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.output.LayerNorm.bias using .* initializer
2022-03-30 15:27:11,097 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.self.query.weight using .* initializer
2022-03-30 15:27:11,097 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.self.query.bias using .* initializer
2022-03-30 15:27:11,097 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.self.key.weight using .* initializer
2022-03-30 15:27:11,098 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.self.key.bias using .* initializer
2022-03-30 15:27:11,098 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.self.value.weight using .* initializer
2022-03-30 15:27:11,098 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.self.value.bias using .* initializer
2022-03-30 15:27:11,098 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.output.dense.weight using .* initializer
2022-03-30 15:27:11,099 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.output.dense.bias using .* initializer
2022-03-30 15:27:11,099 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.output.LayerNorm.weight using .* initializer
2022-03-30 15:27:11,099 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.output.LayerNorm.bias using .* initializer
2022-03-30 15:27:11,099 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.intermediate.dense.weight using .* initializer
2022-03-30 15:27:11,100 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.intermediate.dense.bias using .* initializer
2022-03-30 15:27:11,100 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.output.dense.weight using .* initializer
2022-03-30 15:27:11,101 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.output.dense.bias using .* initializer
2022-03-30 15:27:11,101 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.output.LayerNorm.weight using .* initializer
2022-03-30 15:27:11,101 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.output.LayerNorm.bias using .* initializer
2022-03-30 15:27:11,101 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.self.query.weight using .* initializer
2022-03-30 15:27:11,102 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.self.query.bias using .* initializer
2022-03-30 15:27:11,102 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.self.key.weight using .* initializer
2022-03-30 15:27:11,102 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.self.key.bias using .* initializer
2022-03-30 15:27:11,102 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.self.value.weight using .* initializer
2022-03-30 15:27:11,103 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.self.value.bias using .* initializer
2022-03-30 15:27:11,103 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.output.dense.weight using .* initializer
2022-03-30 15:27:11,103 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.output.dense.bias using .* initializer
2022-03-30 15:27:11,103 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.output.LayerNorm.weight using .* initializer
2022-03-30 15:27:11,103 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.output.LayerNorm.bias using .* initializer
2022-03-30 15:27:11,103 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.intermediate.dense.weight using .* initializer
2022-03-30 15:27:11,104 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.intermediate.dense.bias using .* initializer
2022-03-30 15:27:11,105 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.output.dense.weight using .* initializer
2022-03-30 15:27:11,106 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.output.dense.bias using .* initializer
2022-03-30 15:27:11,106 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.output.LayerNorm.weight using .* initializer
2022-03-30 15:27:11,106 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.output.LayerNorm.bias using .* initializer
2022-03-30 15:27:11,106 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.self.query.weight using .* initializer
2022-03-30 15:27:11,106 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.self.query.bias using .* initializer
2022-03-30 15:27:11,107 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.self.key.weight using .* initializer
2022-03-30 15:27:11,107 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.self.key.bias using .* initializer
2022-03-30 15:27:11,107 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.self.value.weight using .* initializer
2022-03-30 15:27:11,107 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.self.value.bias using .* initializer
2022-03-30 15:27:11,108 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.output.dense.weight using .* initializer
2022-03-30 15:27:11,108 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.output.dense.bias using .* initializer
2022-03-30 15:27:11,108 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.output.LayerNorm.weight using .* initializer
2022-03-30 15:27:11,108 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.output.LayerNorm.bias using .* initializer
2022-03-30 15:27:11,108 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.intermediate.dense.weight using .* initializer
2022-03-30 15:27:11,109 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.intermediate.dense.bias using .* initializer
2022-03-30 15:27:11,109 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.output.dense.weight using .* initializer
2022-03-30 15:27:11,110 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.output.dense.bias using .* initializer
2022-03-30 15:27:11,111 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.output.LayerNorm.weight using .* initializer
2022-03-30 15:27:11,111 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.output.LayerNorm.bias using .* initializer
2022-03-30 15:27:11,111 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.pooler.dense.weight using .* initializer
2022-03-30 15:27:11,111 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.pooler.dense.bias using .* initializer
2022-03-30 15:27:11,111 - INFO - allennlp.nn.initializers - Initializing _classifier.classifier.dense.weight using .* initializer
2022-03-30 15:27:11,112 - INFO - allennlp.nn.initializers - Initializing _classifier.classifier.dense.bias using .* initializer
2022-03-30 15:27:11,112 - INFO - allennlp.nn.initializers - Initializing _classifier.classifier.out_proj.weight using .* initializer
2022-03-30 15:27:11,112 - INFO - allennlp.nn.initializers - Initializing _classifier.classifier.out_proj.bias using .* initializer
2022-03-30 15:27:11,112 - INFO - allennlp.nn.initializers - Done initializing parameters; the following parameters are using their default initialization from their code
2022-03-30 15:27:11,668 - INFO - filelock - Lock 140555466825424 acquired on ./output_roberta-star/vocabulary/.lock
2022-03-30 15:27:11,669 - INFO - filelock - Lock 140555466825424 released on ./output_roberta-star/vocabulary/.lock
2022-03-30 15:27:11,669 - INFO - allennlp.common.params - data_loader.type = pytorch_dataloader
2022-03-30 15:27:11,669 - INFO - allennlp.common.params - data_loader.batch_size = 1
2022-03-30 15:27:11,669 - INFO - allennlp.common.params - data_loader.shuffle = False
2022-03-30 15:27:11,669 - INFO - allennlp.common.params - data_loader.sampler = None
2022-03-30 15:27:11,670 - INFO - allennlp.common.params - data_loader.num_workers = 0
2022-03-30 15:27:11,670 - INFO - allennlp.common.params - data_loader.pin_memory = False
2022-03-30 15:27:11,670 - INFO - allennlp.common.params - data_loader.drop_last = False
2022-03-30 15:27:11,670 - INFO - allennlp.common.params - data_loader.timeout = 0
2022-03-30 15:27:11,670 - INFO - allennlp.common.params - data_loader.worker_init_fn = None
2022-03-30 15:27:11,670 - INFO - allennlp.common.params - data_loader.multiprocessing_context = None
2022-03-30 15:27:11,670 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None
2022-03-30 15:27:11,671 - INFO - allennlp.common.params - data_loader.batch_sampler.type = bucket
2022-03-30 15:27:11,671 - INFO - allennlp.common.params - data_loader.batch_sampler.batch_size = 2
2022-03-30 15:27:11,671 - INFO - allennlp.common.params - data_loader.batch_sampler.sorting_keys = None
2022-03-30 15:27:11,671 - INFO - allennlp.common.params - data_loader.batch_sampler.padding_noise = 0.1
2022-03-30 15:27:11,671 - INFO - allennlp.common.params - data_loader.batch_sampler.drop_last = False
2022-03-30 15:27:11,672 - INFO - allennlp.common.params - data_loader.type = pytorch_dataloader
2022-03-30 15:27:11,672 - INFO - allennlp.common.params - data_loader.batch_size = 1
2022-03-30 15:27:11,672 - INFO - allennlp.common.params - data_loader.shuffle = False
2022-03-30 15:27:11,672 - INFO - allennlp.common.params - data_loader.sampler = None
2022-03-30 15:27:11,672 - INFO - allennlp.common.params - data_loader.num_workers = 0
2022-03-30 15:27:11,673 - INFO - allennlp.common.params - data_loader.pin_memory = False
2022-03-30 15:27:11,673 - INFO - allennlp.common.params - data_loader.drop_last = False
2022-03-30 15:27:11,673 - INFO - allennlp.common.params - data_loader.timeout = 0
2022-03-30 15:27:11,673 - INFO - allennlp.common.params - data_loader.worker_init_fn = None
2022-03-30 15:27:11,673 - INFO - allennlp.common.params - data_loader.multiprocessing_context = None
2022-03-30 15:27:11,673 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None
2022-03-30 15:27:11,673 - INFO - allennlp.common.params - data_loader.batch_sampler.type = bucket
2022-03-30 15:27:11,674 - INFO - allennlp.common.params - data_loader.batch_sampler.batch_size = 2
2022-03-30 15:27:11,674 - INFO - allennlp.common.params - data_loader.batch_sampler.sorting_keys = None
2022-03-30 15:27:11,674 - INFO - allennlp.common.params - data_loader.batch_sampler.padding_noise = 0.1
2022-03-30 15:27:11,674 - INFO - allennlp.common.params - data_loader.batch_sampler.drop_last = False
2022-03-30 15:27:11,674 - INFO - allennlp.common.params - trainer.type = gradient_descent
2022-03-30 15:27:11,675 - INFO - allennlp.common.params - trainer.patience = None
2022-03-30 15:27:11,675 - INFO - allennlp.common.params - trainer.validation_metric = +accuracy
2022-03-30 15:27:11,675 - INFO - allennlp.common.params - trainer.num_epochs = 10
2022-03-30 15:27:11,675 - INFO - allennlp.common.params - trainer.cuda_device = 0
2022-03-30 15:27:11,675 - INFO - allennlp.common.params - trainer.grad_norm = None
2022-03-30 15:27:11,675 - INFO - allennlp.common.params - trainer.grad_clipping = None
2022-03-30 15:27:11,675 - INFO - allennlp.common.params - trainer.distributed = False
2022-03-30 15:27:11,676 - INFO - allennlp.common.params - trainer.world_size = 1
2022-03-30 15:27:11,676 - INFO - allennlp.common.params - trainer.num_gradient_accumulation_steps = 16
2022-03-30 15:27:11,676 - INFO - allennlp.common.params - trainer.use_amp = False
2022-03-30 15:27:11,676 - INFO - allennlp.common.params - trainer.no_grad = None
2022-03-30 15:27:11,676 - INFO - allennlp.common.params - trainer.momentum_scheduler = None
2022-03-30 15:27:11,676 - INFO - allennlp.common.params - trainer.tensorboard_writer = <allennlp.common.lazy.Lazy object at 0x7fd5f0194510>
2022-03-30 15:27:11,676 - INFO - allennlp.common.params - trainer.moving_average = None
2022-03-30 15:27:11,677 - INFO - allennlp.common.params - trainer.batch_callbacks = None
2022-03-30 15:27:11,677 - INFO - allennlp.common.params - trainer.epoch_callbacks = None
2022-03-30 15:27:11,677 - INFO - allennlp.common.params - trainer.end_callbacks = None
2022-03-30 15:27:11,677 - INFO - allennlp.common.params - trainer.trainer_callbacks = None
2022-03-30 15:27:12,017 - INFO - allennlp.common.params - trainer.optimizer.type = huggingface_adamw
2022-03-30 15:27:12,017 - INFO - allennlp.common.params - trainer.optimizer.parameter_groups = None
2022-03-30 15:27:12,017 - INFO - allennlp.common.params - trainer.optimizer.lr = 1e-05
2022-03-30 15:27:12,017 - INFO - allennlp.common.params - trainer.optimizer.betas = [0.9, 0.98]
2022-03-30 15:27:12,018 - INFO - allennlp.common.params - trainer.optimizer.eps = 1e-06
2022-03-30 15:27:12,018 - INFO - allennlp.common.params - trainer.optimizer.weight_decay = 0.1
2022-03-30 15:27:12,018 - INFO - allennlp.common.params - trainer.optimizer.correct_bias = True
2022-03-30 15:27:12,018 - INFO - allennlp.training.optimizers - Number of trainable parameters: 356411394
2022-03-30 15:27:12,020 - INFO - allennlp.common.util - The following parameters are Frozen (without gradient):
2022-03-30 15:27:12,023 - INFO - allennlp.common.util - The following parameters are Tunable (with gradient):
2022-03-30 15:27:12,023 - INFO - allennlp.common.util - _classifier.roberta.embeddings.word_embeddings.weight
2022-03-30 15:27:12,023 - INFO - allennlp.common.util - _classifier.roberta.embeddings.position_embeddings.weight
2022-03-30 15:27:12,023 - INFO - allennlp.common.util - _classifier.roberta.embeddings.token_type_embeddings.weight
2022-03-30 15:27:12,023 - INFO - allennlp.common.util - _classifier.roberta.embeddings.LayerNorm.weight
2022-03-30 15:27:12,023 - INFO - allennlp.common.util - _classifier.roberta.embeddings.LayerNorm.bias
2022-03-30 15:27:12,024 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.self.query.weight
2022-03-30 15:27:12,024 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.self.query.bias
2022-03-30 15:27:12,024 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.self.key.weight
2022-03-30 15:27:12,024 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.self.key.bias
2022-03-30 15:27:12,024 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.self.value.weight
2022-03-30 15:27:12,024 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.self.value.bias
2022-03-30 15:27:12,024 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.output.dense.weight
2022-03-30 15:27:12,024 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.output.dense.bias
2022-03-30 15:27:12,024 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.output.LayerNorm.weight
2022-03-30 15:27:12,024 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.output.LayerNorm.bias
2022-03-30 15:27:12,025 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.intermediate.dense.weight
2022-03-30 15:27:12,025 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.intermediate.dense.bias
2022-03-30 15:27:12,025 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.output.dense.weight
2022-03-30 15:27:12,025 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.output.dense.bias
2022-03-30 15:27:12,025 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.output.LayerNorm.weight
2022-03-30 15:27:12,025 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.output.LayerNorm.bias
2022-03-30 15:27:12,025 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.self.query.weight
2022-03-30 15:27:12,025 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.self.query.bias
2022-03-30 15:27:12,025 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.self.key.weight
2022-03-30 15:27:12,025 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.self.key.bias
2022-03-30 15:27:12,026 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.self.value.weight
2022-03-30 15:27:12,026 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.self.value.bias
2022-03-30 15:27:12,026 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.output.dense.weight
2022-03-30 15:27:12,026 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.output.dense.bias
2022-03-30 15:27:12,026 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.output.LayerNorm.weight
2022-03-30 15:27:12,026 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.output.LayerNorm.bias
2022-03-30 15:27:12,026 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.intermediate.dense.weight
2022-03-30 15:27:12,026 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.intermediate.dense.bias
2022-03-30 15:27:12,026 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.output.dense.weight
2022-03-30 15:27:12,026 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.output.dense.bias
2022-03-30 15:27:12,027 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.output.LayerNorm.weight
2022-03-30 15:27:12,027 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.output.LayerNorm.bias
2022-03-30 15:27:12,027 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.self.query.weight
2022-03-30 15:27:12,027 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.self.query.bias
2022-03-30 15:27:12,027 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.self.key.weight
2022-03-30 15:27:12,027 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.self.key.bias
2022-03-30 15:27:12,027 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.self.value.weight
2022-03-30 15:27:12,027 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.self.value.bias
2022-03-30 15:27:12,027 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.output.dense.weight
2022-03-30 15:27:12,027 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.output.dense.bias
2022-03-30 15:27:12,028 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.output.LayerNorm.weight
2022-03-30 15:27:12,028 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.output.LayerNorm.bias
2022-03-30 15:27:12,028 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.intermediate.dense.weight
2022-03-30 15:27:12,028 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.intermediate.dense.bias
2022-03-30 15:27:12,028 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.output.dense.weight
2022-03-30 15:27:12,028 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.output.dense.bias
2022-03-30 15:27:12,028 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.output.LayerNorm.weight
2022-03-30 15:27:12,028 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.output.LayerNorm.bias
2022-03-30 15:27:12,028 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.self.query.weight
2022-03-30 15:27:12,028 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.self.query.bias
2022-03-30 15:27:12,029 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.self.key.weight
2022-03-30 15:27:12,029 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.self.key.bias
2022-03-30 15:27:12,029 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.self.value.weight
2022-03-30 15:27:12,029 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.self.value.bias
2022-03-30 15:27:12,029 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.output.dense.weight
2022-03-30 15:27:12,029 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.output.dense.bias
2022-03-30 15:27:12,029 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.output.LayerNorm.weight
2022-03-30 15:27:12,029 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.output.LayerNorm.bias
2022-03-30 15:27:12,029 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.intermediate.dense.weight
2022-03-30 15:27:12,029 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.intermediate.dense.bias
2022-03-30 15:27:12,029 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.output.dense.weight
2022-03-30 15:27:12,030 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.output.dense.bias
2022-03-30 15:27:12,030 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.output.LayerNorm.weight
2022-03-30 15:27:12,030 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.output.LayerNorm.bias
2022-03-30 15:27:12,030 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.self.query.weight
2022-03-30 15:27:12,030 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.self.query.bias
2022-03-30 15:27:12,030 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.self.key.weight
2022-03-30 15:27:12,030 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.self.key.bias
2022-03-30 15:27:12,030 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.self.value.weight
2022-03-30 15:27:12,030 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.self.value.bias
2022-03-30 15:27:12,030 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.output.dense.weight
2022-03-30 15:27:12,031 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.output.dense.bias
2022-03-30 15:27:12,031 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.output.LayerNorm.weight
2022-03-30 15:27:12,031 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.output.LayerNorm.bias
2022-03-30 15:27:12,031 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.intermediate.dense.weight
2022-03-30 15:27:12,031 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.intermediate.dense.bias
2022-03-30 15:27:12,031 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.output.dense.weight
2022-03-30 15:27:12,031 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.output.dense.bias
2022-03-30 15:27:12,031 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.output.LayerNorm.weight
2022-03-30 15:27:12,031 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.output.LayerNorm.bias
2022-03-30 15:27:12,031 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.self.query.weight
2022-03-30 15:27:12,031 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.self.query.bias
2022-03-30 15:27:12,032 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.self.key.weight
2022-03-30 15:27:12,032 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.self.key.bias
2022-03-30 15:27:12,032 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.self.value.weight
2022-03-30 15:27:12,032 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.self.value.bias
2022-03-30 15:27:12,032 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.output.dense.weight
2022-03-30 15:27:12,032 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.output.dense.bias
2022-03-30 15:27:12,032 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.output.LayerNorm.weight
2022-03-30 15:27:12,032 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.output.LayerNorm.bias
2022-03-30 15:27:12,032 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.intermediate.dense.weight
2022-03-30 15:27:12,032 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.intermediate.dense.bias
2022-03-30 15:27:12,032 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.output.dense.weight
2022-03-30 15:27:12,033 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.output.dense.bias
2022-03-30 15:27:12,033 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.output.LayerNorm.weight
2022-03-30 15:27:12,033 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.output.LayerNorm.bias
2022-03-30 15:27:12,033 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.self.query.weight
2022-03-30 15:27:12,033 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.self.query.bias
2022-03-30 15:27:12,033 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.self.key.weight
2022-03-30 15:27:12,033 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.self.key.bias
2022-03-30 15:27:12,033 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.self.value.weight
2022-03-30 15:27:12,033 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.self.value.bias
2022-03-30 15:27:12,033 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.output.dense.weight
2022-03-30 15:27:12,033 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.output.dense.bias
2022-03-30 15:27:12,033 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.output.LayerNorm.weight
2022-03-30 15:27:12,034 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.output.LayerNorm.bias
2022-03-30 15:27:12,034 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.intermediate.dense.weight
2022-03-30 15:27:12,034 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.intermediate.dense.bias
2022-03-30 15:27:12,034 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.output.dense.weight
2022-03-30 15:27:12,034 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.output.dense.bias
2022-03-30 15:27:12,034 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.output.LayerNorm.weight
2022-03-30 15:27:12,034 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.output.LayerNorm.bias
2022-03-30 15:27:12,034 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.self.query.weight
2022-03-30 15:27:12,034 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.self.query.bias
2022-03-30 15:27:12,034 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.self.key.weight
2022-03-30 15:27:12,035 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.self.key.bias
2022-03-30 15:27:12,035 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.self.value.weight
2022-03-30 15:27:12,035 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.self.value.bias
2022-03-30 15:27:12,035 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.output.dense.weight
2022-03-30 15:27:12,035 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.output.dense.bias
2022-03-30 15:27:12,035 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.output.LayerNorm.weight
2022-03-30 15:27:12,035 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.output.LayerNorm.bias
2022-03-30 15:27:12,035 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.intermediate.dense.weight
2022-03-30 15:27:12,035 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.intermediate.dense.bias
2022-03-30 15:27:12,035 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.output.dense.weight
2022-03-30 15:27:12,036 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.output.dense.bias
2022-03-30 15:27:12,036 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.output.LayerNorm.weight
2022-03-30 15:27:12,036 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.output.LayerNorm.bias
2022-03-30 15:27:12,036 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.self.query.weight
2022-03-30 15:27:12,036 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.self.query.bias
2022-03-30 15:27:12,036 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.self.key.weight
2022-03-30 15:27:12,036 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.self.key.bias
2022-03-30 15:27:12,036 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.self.value.weight
2022-03-30 15:27:12,036 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.self.value.bias
2022-03-30 15:27:12,036 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.output.dense.weight
2022-03-30 15:27:12,036 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.output.dense.bias
2022-03-30 15:27:12,036 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.output.LayerNorm.weight
2022-03-30 15:27:12,037 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.output.LayerNorm.bias
2022-03-30 15:27:12,037 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.intermediate.dense.weight
2022-03-30 15:27:12,037 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.intermediate.dense.bias
2022-03-30 15:27:12,037 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.output.dense.weight
2022-03-30 15:27:12,037 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.output.dense.bias
2022-03-30 15:27:12,037 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.output.LayerNorm.weight
2022-03-30 15:27:12,037 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.output.LayerNorm.bias
2022-03-30 15:27:12,037 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.self.query.weight
2022-03-30 15:27:12,037 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.self.query.bias
2022-03-30 15:27:12,037 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.self.key.weight
2022-03-30 15:27:12,037 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.self.key.bias
2022-03-30 15:27:12,038 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.self.value.weight
2022-03-30 15:27:12,038 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.self.value.bias
2022-03-30 15:27:12,038 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.output.dense.weight
2022-03-30 15:27:12,038 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.output.dense.bias
2022-03-30 15:27:12,038 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.output.LayerNorm.weight
2022-03-30 15:27:12,038 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.output.LayerNorm.bias
2022-03-30 15:27:12,038 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.intermediate.dense.weight
2022-03-30 15:27:12,038 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.intermediate.dense.bias
2022-03-30 15:27:12,038 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.output.dense.weight
2022-03-30 15:27:12,038 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.output.dense.bias
2022-03-30 15:27:12,038 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.output.LayerNorm.weight
2022-03-30 15:27:12,039 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.output.LayerNorm.bias
2022-03-30 15:27:12,039 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.self.query.weight
2022-03-30 15:27:12,039 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.self.query.bias
2022-03-30 15:27:12,039 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.self.key.weight
2022-03-30 15:27:12,039 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.self.key.bias
2022-03-30 15:27:12,039 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.self.value.weight
2022-03-30 15:27:12,039 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.self.value.bias
2022-03-30 15:27:12,039 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.output.dense.weight
2022-03-30 15:27:12,039 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.output.dense.bias
2022-03-30 15:27:12,039 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.output.LayerNorm.weight
2022-03-30 15:27:12,040 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.output.LayerNorm.bias
2022-03-30 15:27:12,040 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.intermediate.dense.weight
2022-03-30 15:27:12,040 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.intermediate.dense.bias
2022-03-30 15:27:12,040 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.output.dense.weight
2022-03-30 15:27:12,040 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.output.dense.bias
2022-03-30 15:27:12,040 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.output.LayerNorm.weight
2022-03-30 15:27:12,040 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.output.LayerNorm.bias
2022-03-30 15:27:12,040 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.self.query.weight
2022-03-30 15:27:12,040 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.self.query.bias
2022-03-30 15:27:12,040 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.self.key.weight
2022-03-30 15:27:12,041 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.self.key.bias
2022-03-30 15:27:12,041 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.self.value.weight
2022-03-30 15:27:12,041 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.self.value.bias
2022-03-30 15:27:12,041 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.output.dense.weight
2022-03-30 15:27:12,041 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.output.dense.bias
2022-03-30 15:27:12,041 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.output.LayerNorm.weight
2022-03-30 15:27:12,041 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.output.LayerNorm.bias
2022-03-30 15:27:12,041 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.intermediate.dense.weight
2022-03-30 15:27:12,041 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.intermediate.dense.bias
2022-03-30 15:27:12,041 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.output.dense.weight
2022-03-30 15:27:12,041 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.output.dense.bias
2022-03-30 15:27:12,041 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.output.LayerNorm.weight
2022-03-30 15:27:12,041 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.output.LayerNorm.bias
2022-03-30 15:27:12,041 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.self.query.weight
2022-03-30 15:27:12,042 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.self.query.bias
2022-03-30 15:27:12,042 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.self.key.weight
2022-03-30 15:27:12,042 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.self.key.bias
2022-03-30 15:27:12,042 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.self.value.weight
2022-03-30 15:27:12,042 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.self.value.bias
2022-03-30 15:27:12,042 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.output.dense.weight
2022-03-30 15:27:12,042 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.output.dense.bias
2022-03-30 15:27:12,042 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.output.LayerNorm.weight
2022-03-30 15:27:12,042 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.output.LayerNorm.bias
2022-03-30 15:27:12,042 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.intermediate.dense.weight
2022-03-30 15:27:12,042 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.intermediate.dense.bias
2022-03-30 15:27:12,042 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.output.dense.weight
2022-03-30 15:27:12,042 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.output.dense.bias
2022-03-30 15:27:12,043 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.output.LayerNorm.weight
2022-03-30 15:27:12,043 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.output.LayerNorm.bias
2022-03-30 15:27:12,043 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.self.query.weight
2022-03-30 15:27:12,043 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.self.query.bias
2022-03-30 15:27:12,043 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.self.key.weight
2022-03-30 15:27:12,043 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.self.key.bias
2022-03-30 15:27:12,043 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.self.value.weight
2022-03-30 15:27:12,043 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.self.value.bias
2022-03-30 15:27:12,043 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.output.dense.weight
2022-03-30 15:27:12,043 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.output.dense.bias
2022-03-30 15:27:12,043 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.output.LayerNorm.weight
2022-03-30 15:27:12,043 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.output.LayerNorm.bias
2022-03-30 15:27:12,043 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.intermediate.dense.weight
2022-03-30 15:27:12,043 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.intermediate.dense.bias
2022-03-30 15:27:12,043 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.output.dense.weight
2022-03-30 15:27:12,044 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.output.dense.bias
2022-03-30 15:27:12,044 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.output.LayerNorm.weight
2022-03-30 15:27:12,044 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.output.LayerNorm.bias
2022-03-30 15:27:12,044 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.self.query.weight
2022-03-30 15:27:12,044 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.self.query.bias
2022-03-30 15:27:12,044 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.self.key.weight
2022-03-30 15:27:12,044 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.self.key.bias
2022-03-30 15:27:12,044 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.self.value.weight
2022-03-30 15:27:12,044 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.self.value.bias
2022-03-30 15:27:12,044 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.output.dense.weight
2022-03-30 15:27:12,044 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.output.dense.bias
2022-03-30 15:27:12,044 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.output.LayerNorm.weight
2022-03-30 15:27:12,044 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.output.LayerNorm.bias
2022-03-30 15:27:12,045 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.intermediate.dense.weight
2022-03-30 15:27:12,045 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.intermediate.dense.bias
2022-03-30 15:27:12,045 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.output.dense.weight
2022-03-30 15:27:12,045 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.output.dense.bias
2022-03-30 15:27:12,045 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.output.LayerNorm.weight
2022-03-30 15:27:12,045 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.output.LayerNorm.bias
2022-03-30 15:27:12,045 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.self.query.weight
2022-03-30 15:27:12,045 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.self.query.bias
2022-03-30 15:27:12,045 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.self.key.weight
2022-03-30 15:27:12,045 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.self.key.bias
2022-03-30 15:27:12,045 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.self.value.weight
2022-03-30 15:27:12,046 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.self.value.bias
2022-03-30 15:27:12,046 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.output.dense.weight
2022-03-30 15:27:12,046 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.output.dense.bias
2022-03-30 15:27:12,046 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.output.LayerNorm.weight
2022-03-30 15:27:12,046 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.output.LayerNorm.bias
2022-03-30 15:27:12,046 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.intermediate.dense.weight
2022-03-30 15:27:12,046 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.intermediate.dense.bias
2022-03-30 15:27:12,046 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.output.dense.weight
2022-03-30 15:27:12,046 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.output.dense.bias
2022-03-30 15:27:12,046 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.output.LayerNorm.weight
2022-03-30 15:27:12,047 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.output.LayerNorm.bias
2022-03-30 15:27:12,047 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.self.query.weight
2022-03-30 15:27:12,047 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.self.query.bias
2022-03-30 15:27:12,047 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.self.key.weight
2022-03-30 15:27:12,047 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.self.key.bias
2022-03-30 15:27:12,047 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.self.value.weight
2022-03-30 15:27:12,047 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.self.value.bias
2022-03-30 15:27:12,047 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.output.dense.weight
2022-03-30 15:27:12,048 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.output.dense.bias
2022-03-30 15:27:12,048 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.output.LayerNorm.weight
2022-03-30 15:27:12,048 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.output.LayerNorm.bias
2022-03-30 15:27:12,048 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.intermediate.dense.weight
2022-03-30 15:27:12,048 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.intermediate.dense.bias
2022-03-30 15:27:12,048 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.output.dense.weight
2022-03-30 15:27:12,048 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.output.dense.bias
2022-03-30 15:27:12,048 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.output.LayerNorm.weight
2022-03-30 15:27:12,048 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.output.LayerNorm.bias
2022-03-30 15:27:12,048 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.self.query.weight
2022-03-30 15:27:12,049 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.self.query.bias
2022-03-30 15:27:12,049 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.self.key.weight
2022-03-30 15:27:12,049 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.self.key.bias
2022-03-30 15:27:12,049 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.self.value.weight
2022-03-30 15:27:12,049 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.self.value.bias
2022-03-30 15:27:12,049 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.output.dense.weight
2022-03-30 15:27:12,049 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.output.dense.bias
2022-03-30 15:27:12,049 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.output.LayerNorm.weight
2022-03-30 15:27:12,049 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.output.LayerNorm.bias
2022-03-30 15:27:12,049 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.intermediate.dense.weight
2022-03-30 15:27:12,050 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.intermediate.dense.bias
2022-03-30 15:27:12,050 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.output.dense.weight
2022-03-30 15:27:12,050 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.output.dense.bias
2022-03-30 15:27:12,050 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.output.LayerNorm.weight
2022-03-30 15:27:12,050 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.output.LayerNorm.bias
2022-03-30 15:27:12,050 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.self.query.weight
2022-03-30 15:27:12,050 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.self.query.bias
2022-03-30 15:27:12,050 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.self.key.weight
2022-03-30 15:27:12,050 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.self.key.bias
2022-03-30 15:27:12,051 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.self.value.weight
2022-03-30 15:27:12,051 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.self.value.bias
2022-03-30 15:27:12,051 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.output.dense.weight
2022-03-30 15:27:12,051 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.output.dense.bias
2022-03-30 15:27:12,051 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.output.LayerNorm.weight
2022-03-30 15:27:12,051 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.output.LayerNorm.bias
2022-03-30 15:27:12,051 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.intermediate.dense.weight
2022-03-30 15:27:12,051 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.intermediate.dense.bias
2022-03-30 15:27:12,051 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.output.dense.weight
2022-03-30 15:27:12,052 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.output.dense.bias
2022-03-30 15:27:12,052 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.output.LayerNorm.weight
2022-03-30 15:27:12,052 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.output.LayerNorm.bias
2022-03-30 15:27:12,052 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.self.query.weight
2022-03-30 15:27:12,052 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.self.query.bias
2022-03-30 15:27:12,052 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.self.key.weight
2022-03-30 15:27:12,052 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.self.key.bias
2022-03-30 15:27:12,052 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.self.value.weight
2022-03-30 15:27:12,052 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.self.value.bias
2022-03-30 15:27:12,053 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.output.dense.weight
2022-03-30 15:27:12,053 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.output.dense.bias
2022-03-30 15:27:12,053 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.output.LayerNorm.weight
2022-03-30 15:27:12,053 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.output.LayerNorm.bias
2022-03-30 15:27:12,053 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.intermediate.dense.weight
2022-03-30 15:27:12,053 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.intermediate.dense.bias
2022-03-30 15:27:12,053 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.output.dense.weight
2022-03-30 15:27:12,053 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.output.dense.bias
2022-03-30 15:27:12,053 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.output.LayerNorm.weight
2022-03-30 15:27:12,053 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.output.LayerNorm.bias
2022-03-30 15:27:12,054 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.self.query.weight
2022-03-30 15:27:12,054 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.self.query.bias
2022-03-30 15:27:12,054 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.self.key.weight
2022-03-30 15:27:12,054 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.self.key.bias
2022-03-30 15:27:12,054 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.self.value.weight
2022-03-30 15:27:12,054 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.self.value.bias
2022-03-30 15:27:12,054 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.output.dense.weight
2022-03-30 15:27:12,054 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.output.dense.bias
2022-03-30 15:27:12,054 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.output.LayerNorm.weight
2022-03-30 15:27:12,055 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.output.LayerNorm.bias
2022-03-30 15:27:12,055 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.intermediate.dense.weight
2022-03-30 15:27:12,055 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.intermediate.dense.bias
2022-03-30 15:27:12,055 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.output.dense.weight
2022-03-30 15:27:12,055 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.output.dense.bias
2022-03-30 15:27:12,055 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.output.LayerNorm.weight
2022-03-30 15:27:12,055 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.output.LayerNorm.bias
2022-03-30 15:27:12,055 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.self.query.weight
2022-03-30 15:27:12,055 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.self.query.bias
2022-03-30 15:27:12,055 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.self.key.weight
2022-03-30 15:27:12,056 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.self.key.bias
2022-03-30 15:27:12,056 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.self.value.weight
2022-03-30 15:27:12,056 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.self.value.bias
2022-03-30 15:27:12,056 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.output.dense.weight
2022-03-30 15:27:12,056 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.output.dense.bias
2022-03-30 15:27:12,056 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.output.LayerNorm.weight
2022-03-30 15:27:12,056 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.output.LayerNorm.bias
2022-03-30 15:27:12,056 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.intermediate.dense.weight
2022-03-30 15:27:12,056 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.intermediate.dense.bias
2022-03-30 15:27:12,057 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.output.dense.weight
2022-03-30 15:27:12,057 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.output.dense.bias
2022-03-30 15:27:12,057 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.output.LayerNorm.weight
2022-03-30 15:27:12,057 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.output.LayerNorm.bias
2022-03-30 15:27:12,057 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.self.query.weight
2022-03-30 15:27:12,057 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.self.query.bias
2022-03-30 15:27:12,057 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.self.key.weight
2022-03-30 15:27:12,057 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.self.key.bias
2022-03-30 15:27:12,057 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.self.value.weight
2022-03-30 15:27:12,057 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.self.value.bias
2022-03-30 15:27:12,058 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.output.dense.weight
2022-03-30 15:27:12,058 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.output.dense.bias
2022-03-30 15:27:12,058 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.output.LayerNorm.weight
2022-03-30 15:27:12,058 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.output.LayerNorm.bias
2022-03-30 15:27:12,058 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.intermediate.dense.weight
2022-03-30 15:27:12,058 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.intermediate.dense.bias
2022-03-30 15:27:12,058 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.output.dense.weight
2022-03-30 15:27:12,058 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.output.dense.bias
2022-03-30 15:27:12,058 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.output.LayerNorm.weight
2022-03-30 15:27:12,058 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.output.LayerNorm.bias
2022-03-30 15:27:12,058 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.self.query.weight
2022-03-30 15:27:12,058 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.self.query.bias
2022-03-30 15:27:12,058 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.self.key.weight
2022-03-30 15:27:12,059 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.self.key.bias
2022-03-30 15:27:12,059 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.self.value.weight
2022-03-30 15:27:12,059 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.self.value.bias
2022-03-30 15:27:12,059 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.output.dense.weight
2022-03-30 15:27:12,059 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.output.dense.bias
2022-03-30 15:27:12,059 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.output.LayerNorm.weight
2022-03-30 15:27:12,059 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.output.LayerNorm.bias
2022-03-30 15:27:12,059 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.intermediate.dense.weight
2022-03-30 15:27:12,059 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.intermediate.dense.bias
2022-03-30 15:27:12,059 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.output.dense.weight
2022-03-30 15:27:12,059 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.output.dense.bias
2022-03-30 15:27:12,059 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.output.LayerNorm.weight
2022-03-30 15:27:12,059 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.output.LayerNorm.bias
2022-03-30 15:27:12,060 - INFO - allennlp.common.util - _classifier.roberta.pooler.dense.weight
2022-03-30 15:27:12,060 - INFO - allennlp.common.util - _classifier.roberta.pooler.dense.bias
2022-03-30 15:27:12,060 - INFO - allennlp.common.util - _classifier.classifier.dense.weight
2022-03-30 15:27:12,060 - INFO - allennlp.common.util - _classifier.classifier.dense.bias
2022-03-30 15:27:12,060 - INFO - allennlp.common.util - _classifier.classifier.out_proj.weight
2022-03-30 15:27:12,060 - INFO - allennlp.common.util - _classifier.classifier.out_proj.bias
2022-03-30 15:27:12,060 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.type = slanted_triangular
2022-03-30 15:27:12,061 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.cut_frac = 0.06
2022-03-30 15:27:12,061 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.ratio = 32
2022-03-30 15:27:12,061 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.last_epoch = -1
2022-03-30 15:27:12,061 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.gradual_unfreezing = False
2022-03-30 15:27:12,061 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.discriminative_fine_tuning = False
2022-03-30 15:27:12,061 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.decay_factor = 0.38
2022-03-30 15:27:12,061 - INFO - allennlp.common.params - trainer.checkpointer.type = default
2022-03-30 15:27:12,062 - INFO - allennlp.common.params - trainer.checkpointer.keep_serialized_model_every_num_seconds = None
2022-03-30 15:27:12,062 - INFO - allennlp.common.params - trainer.checkpointer.num_serialized_models_to_keep = -1
2022-03-30 15:27:12,062 - INFO - allennlp.common.params - trainer.checkpointer.model_save_interval = None
2022-03-30 15:27:12,062 - INFO - allennlp.common.params - summary_interval = 100
2022-03-30 15:27:12,062 - INFO - allennlp.common.params - histogram_interval = None
2022-03-30 15:27:12,063 - INFO - allennlp.common.params - batch_size_interval = None
2022-03-30 15:27:12,063 - INFO - allennlp.common.params - should_log_parameter_statistics = True
2022-03-30 15:27:12,063 - INFO - allennlp.common.params - should_log_learning_rate = False
2022-03-30 15:27:12,063 - INFO - allennlp.common.params - get_batch_num_total = None
2022-03-30 15:27:12,065 - WARNING - allennlp.training.trainer - You provided a validation dataset but patience was set to None, meaning that early stopping is disabled
2022-03-30 15:27:12,084 - INFO - allennlp.training.trainer - Beginning training.
2022-03-30 15:27:12,084 - INFO - allennlp.training.trainer - Epoch 0/9
2022-03-30 15:27:12,084 - INFO - allennlp.training.trainer - Worker 0 memory usage: 6.9G
2022-03-30 15:27:12,085 - INFO - allennlp.training.trainer - GPU 0 memory usage: 5.6G
2022-03-30 15:27:12,086 - INFO - allennlp.training.trainer - Training
2022-03-30 15:27:12,086 - INFO - tqdm - 0%|          | 0/295 [00:00<?, ?it/s]
2022-03-30 15:27:12,086 - INFO - allennlp.data.samplers.bucket_batch_sampler - No sorting keys given; trying to guess a good one
2022-03-30 15:27:12,087 - INFO - allennlp.data.samplers.bucket_batch_sampler - Using ['tokens'] as the sorting keys
2022-03-30 15:27:24,681 - INFO - tqdm - accuracy: 0.5391, batch_loss: 0.6221, loss: 0.6849 ||:   1%|1         | 4/295 [00:12<15:14,  3.14s/it]
2022-03-30 15:27:36,358 - INFO - tqdm - accuracy: 0.5273, batch_loss: 0.6550, loss: 0.6878 ||:   3%|2         | 8/295 [00:24<14:09,  2.96s/it]
2022-03-30 15:27:49,341 - INFO - tqdm - accuracy: 0.5573, batch_loss: 0.6123, loss: 0.6734 ||:   4%|4         | 12/295 [00:37<14:55,  3.16s/it]
2022-03-30 15:27:59,854 - INFO - tqdm - accuracy: 0.5604, batch_loss: 0.7181, loss: 0.6741 ||:   5%|5         | 15/295 [00:47<15:47,  3.38s/it]
2022-03-30 15:28:13,239 - INFO - tqdm - accuracy: 0.5773, batch_loss: 0.6270, loss: 0.6706 ||:   6%|6         | 19/295 [01:01<15:40,  3.41s/it]
2022-03-30 15:28:25,096 - INFO - tqdm - accuracy: 0.5734, batch_loss: 0.7198, loss: 0.6744 ||:   8%|7         | 23/295 [01:13<13:53,  3.06s/it]
2022-03-30 15:28:37,918 - INFO - tqdm - accuracy: 0.5764, batch_loss: 0.6912, loss: 0.6709 ||:   9%|9         | 27/295 [01:25<14:07,  3.16s/it]
2022-03-30 15:28:50,353 - INFO - tqdm - accuracy: 0.5776, batch_loss: 0.7764, loss: 0.6717 ||:  11%|#         | 31/295 [01:38<13:38,  3.10s/it]
2022-03-30 15:29:03,575 - INFO - tqdm - accuracy: 0.5830, batch_loss: 0.6462, loss: 0.6665 ||:  12%|#1        | 35/295 [01:51<14:09,  3.27s/it]
2022-03-30 15:29:15,924 - INFO - tqdm - accuracy: 0.5817, batch_loss: 0.6045, loss: 0.6662 ||:  13%|#3        | 39/295 [02:03<13:19,  3.12s/it]
2022-03-30 15:29:25,925 - INFO - tqdm - accuracy: 0.5893, batch_loss: 0.5922, loss: 0.6607 ||:  14%|#4        | 42/295 [02:13<13:22,  3.17s/it]
2022-03-30 15:29:38,954 - INFO - tqdm - accuracy: 0.5890, batch_loss: 0.5948, loss: 0.6606 ||:  16%|#5        | 46/295 [02:26<13:21,  3.22s/it]
2022-03-30 15:29:51,879 - INFO - tqdm - accuracy: 0.5906, batch_loss: 0.6074, loss: 0.6581 ||:  17%|#6        | 50/295 [02:39<13:11,  3.23s/it]
2022-03-30 15:30:02,198 - INFO - tqdm - accuracy: 0.5920, batch_loss: 0.6768, loss: 0.6571 ||:  18%|#7        | 53/295 [02:50<13:34,  3.37s/it]
2022-03-30 15:30:14,679 - INFO - tqdm - accuracy: 0.5987, batch_loss: 0.5091, loss: 0.6508 ||:  19%|#9        | 57/295 [03:02<12:33,  3.17s/it]
2022-03-30 15:30:24,902 - INFO - tqdm - accuracy: 0.6083, batch_loss: 0.4884, loss: 0.6460 ||:  20%|##        | 60/295 [03:12<12:56,  3.30s/it]
2022-03-30 15:30:37,483 - INFO - tqdm - accuracy: 0.6187, batch_loss: 0.5466, loss: 0.6396 ||:  22%|##1       | 64/295 [03:25<12:17,  3.19s/it]
2022-03-30 15:30:50,059 - INFO - tqdm - accuracy: 0.6227, batch_loss: 0.6127, loss: 0.6346 ||:  23%|##3       | 68/295 [03:37<11:49,  3.13s/it]
2022-03-30 15:31:02,915 - INFO - tqdm - accuracy: 0.6289, batch_loss: 0.6204, loss: 0.6294 ||:  24%|##4       | 72/295 [03:50<11:50,  3.19s/it]
2022-03-30 15:31:15,759 - INFO - tqdm - accuracy: 0.6353, batch_loss: 0.7532, loss: 0.6273 ||:  26%|##5       | 76/295 [04:03<11:40,  3.20s/it]
2022-03-30 15:31:29,063 - INFO - tqdm - accuracy: 0.6367, batch_loss: 0.5209, loss: 0.6266 ||:  27%|##7       | 80/295 [04:16<11:55,  3.33s/it]
2022-03-30 15:31:41,737 - INFO - tqdm - accuracy: 0.6417, batch_loss: 0.5715, loss: 0.6203 ||:  28%|##8       | 84/295 [04:29<11:26,  3.26s/it]
2022-03-30 15:31:53,883 - INFO - tqdm - accuracy: 0.6460, batch_loss: 0.4052, loss: 0.6134 ||:  30%|##9       | 88/295 [04:41<10:45,  3.12s/it]
2022-03-30 15:32:07,015 - INFO - tqdm - accuracy: 0.6491, batch_loss: 0.4584, loss: 0.6102 ||:  31%|###1      | 92/295 [04:54<10:51,  3.21s/it]
2022-03-30 15:32:20,337 - INFO - tqdm - accuracy: 0.6514, batch_loss: 0.4844, loss: 0.6089 ||:  33%|###2      | 96/295 [05:08<11:02,  3.33s/it]
2022-03-30 15:32:33,053 - INFO - tqdm - accuracy: 0.6559, batch_loss: 0.5054, loss: 0.6051 ||:  34%|###3      | 100/295 [05:20<10:38,  3.27s/it]
2022-03-30 15:32:46,290 - INFO - tqdm - accuracy: 0.6629, batch_loss: 0.3718, loss: 0.5985 ||:  35%|###5      | 104/295 [05:34<10:33,  3.32s/it]
2022-03-30 15:32:58,092 - INFO - tqdm - accuracy: 0.6667, batch_loss: 0.6249, loss: 0.5944 ||:  37%|###6      | 108/295 [05:46<09:22,  3.01s/it]
2022-03-30 15:33:10,966 - INFO - tqdm - accuracy: 0.6705, batch_loss: 0.5488, loss: 0.5903 ||:  38%|###7      | 112/295 [05:58<09:34,  3.14s/it]
2022-03-30 15:33:21,164 - INFO - tqdm - accuracy: 0.6723, batch_loss: 0.3748, loss: 0.5882 ||:  39%|###8      | 115/295 [06:09<10:01,  3.34s/it]
2022-03-30 15:33:31,423 - INFO - tqdm - accuracy: 0.6743, batch_loss: 0.4770, loss: 0.5871 ||:  40%|####      | 118/295 [06:19<09:56,  3.37s/it]
2022-03-30 15:33:43,574 - INFO - tqdm - accuracy: 0.6778, batch_loss: 0.4371, loss: 0.5846 ||:  41%|####1     | 122/295 [06:31<08:46,  3.05s/it]
2022-03-30 15:33:54,366 - INFO - tqdm - accuracy: 0.6823, batch_loss: 0.3653, loss: 0.5797 ||:  42%|####2     | 125/295 [06:42<09:38,  3.40s/it]
2022-03-30 15:34:04,555 - INFO - tqdm - accuracy: 0.6831, batch_loss: 0.4906, loss: 0.5784 ||:  43%|####3     | 128/295 [06:52<09:20,  3.35s/it]
2022-03-30 15:34:17,412 - INFO - tqdm - accuracy: 0.6858, batch_loss: 0.4836, loss: 0.5748 ||:  45%|####4     | 132/295 [07:05<08:47,  3.23s/it]
2022-03-30 15:34:30,592 - INFO - tqdm - accuracy: 0.6881, batch_loss: 0.4062, loss: 0.5719 ||:  46%|####6     | 136/295 [07:18<08:37,  3.26s/it]
2022-03-30 15:34:43,685 - INFO - tqdm - accuracy: 0.6910, batch_loss: 0.4609, loss: 0.5684 ||:  47%|####7     | 140/295 [07:31<08:30,  3.29s/it]
2022-03-30 15:34:54,164 - INFO - tqdm - accuracy: 0.6936, batch_loss: 0.3929, loss: 0.5653 ||:  48%|####8     | 143/295 [07:42<08:46,  3.46s/it]
2022-03-30 15:35:06,543 - INFO - tqdm - accuracy: 0.6962, batch_loss: 0.5009, loss: 0.5620 ||:  50%|####9     | 147/295 [07:54<07:41,  3.12s/it]
2022-03-30 15:35:18,570 - INFO - tqdm - accuracy: 0.7001, batch_loss: 0.3442, loss: 0.5570 ||:  51%|#####1    | 151/295 [08:06<07:14,  3.01s/it]
2022-03-30 15:35:29,092 - INFO - tqdm - accuracy: 0.7025, batch_loss: 0.3465, loss: 0.5543 ||:  52%|#####2    | 154/295 [08:17<07:52,  3.35s/it]
2022-03-30 15:35:39,180 - INFO - tqdm - accuracy: 0.7038, batch_loss: 0.5218, loss: 0.5521 ||:  53%|#####3    | 157/295 [08:27<07:46,  3.38s/it]
2022-03-30 15:35:52,162 - INFO - tqdm - accuracy: 0.7055, batch_loss: 0.3793, loss: 0.5504 ||:  55%|#####4    | 161/295 [08:40<07:18,  3.27s/it]
2022-03-30 15:36:02,373 - INFO - tqdm - accuracy: 0.7075, batch_loss: 0.4345, loss: 0.5483 ||:  56%|#####5    | 164/295 [08:50<07:17,  3.34s/it]
2022-03-30 15:36:15,525 - INFO - tqdm - accuracy: 0.7090, batch_loss: 0.3332, loss: 0.5472 ||:  57%|#####6    | 168/295 [09:03<07:03,  3.33s/it]
2022-03-30 15:36:26,987 - INFO - tqdm - accuracy: 0.7105, batch_loss: 0.4682, loss: 0.5458 ||:  58%|#####7    | 171/295 [09:14<07:22,  3.57s/it]
2022-03-30 15:36:37,266 - INFO - tqdm - accuracy: 0.7124, batch_loss: 0.3758, loss: 0.5431 ||:  59%|#####8    | 174/295 [09:25<06:56,  3.44s/it]
2022-03-30 15:36:50,604 - INFO - tqdm - accuracy: 0.7143, batch_loss: 0.5016, loss: 0.5407 ||:  60%|######    | 178/295 [09:38<06:33,  3.36s/it]
2022-03-30 15:37:03,352 - INFO - tqdm - accuracy: 0.7153, batch_loss: 0.5439, loss: 0.5396 ||:  62%|######1   | 182/295 [09:51<06:06,  3.24s/it]
2022-03-30 15:37:13,913 - INFO - tqdm - accuracy: 0.7162, batch_loss: 0.3596, loss: 0.5387 ||:  63%|######2   | 185/295 [10:01<06:13,  3.39s/it]
2022-03-30 15:37:24,742 - INFO - tqdm - accuracy: 0.7169, batch_loss: 0.4646, loss: 0.5387 ||:  64%|######3   | 188/295 [10:12<06:15,  3.51s/it]
2022-03-30 15:37:36,666 - INFO - tqdm - accuracy: 0.7185, batch_loss: 0.4945, loss: 0.5370 ||:  65%|######5   | 192/295 [10:24<05:21,  3.12s/it]
2022-03-30 15:37:48,775 - INFO - tqdm - accuracy: 0.7217, batch_loss: 0.4761, loss: 0.5339 ||:  66%|######6   | 196/295 [10:36<05:02,  3.05s/it]
2022-03-30 15:38:01,872 - INFO - tqdm - accuracy: 0.7225, batch_loss: 0.3915, loss: 0.5326 ||:  68%|######7   | 200/295 [10:49<05:05,  3.21s/it]
2022-03-30 15:38:12,689 - INFO - tqdm - accuracy: 0.7232, batch_loss: 0.3814, loss: 0.5314 ||:  69%|######8   | 203/295 [11:00<05:16,  3.44s/it]
2022-03-30 15:38:23,462 - INFO - tqdm - accuracy: 0.7231, batch_loss: 0.5956, loss: 0.5315 ||:  70%|######9   | 206/295 [11:11<05:20,  3.60s/it]
2022-03-30 15:38:33,778 - INFO - tqdm - accuracy: 0.7242, batch_loss: 0.3440, loss: 0.5299 ||:  71%|#######   | 209/295 [11:21<04:58,  3.47s/it]
2022-03-30 15:38:47,284 - INFO - tqdm - accuracy: 0.7266, batch_loss: 0.3542, loss: 0.5268 ||:  72%|#######2  | 213/295 [11:35<04:40,  3.42s/it]
2022-03-30 15:38:59,652 - INFO - tqdm - accuracy: 0.7281, batch_loss: 0.3225, loss: 0.5251 ||:  74%|#######3  | 217/295 [11:47<04:05,  3.15s/it]
2022-03-30 15:39:12,284 - INFO - tqdm - accuracy: 0.7292, batch_loss: 0.3771, loss: 0.5240 ||:  75%|#######4  | 221/295 [12:00<03:54,  3.17s/it]
2022-03-30 15:39:22,352 - INFO - tqdm - accuracy: 0.7310, batch_loss: 0.2529, loss: 0.5216 ||:  76%|#######5  | 224/295 [12:10<03:57,  3.35s/it]
2022-03-30 15:39:32,917 - INFO - tqdm - accuracy: 0.7317, batch_loss: 0.4926, loss: 0.5205 ||:  77%|#######6  | 227/295 [12:20<03:50,  3.40s/it]
2022-03-30 15:39:46,178 - INFO - tqdm - accuracy: 0.7332, batch_loss: 0.4341, loss: 0.5179 ||:  78%|#######8  | 231/295 [12:34<03:32,  3.32s/it]
2022-03-30 15:39:59,097 - INFO - tqdm - accuracy: 0.7337, batch_loss: 0.6474, loss: 0.5166 ||:  80%|#######9  | 235/295 [12:47<03:12,  3.21s/it]
2022-03-30 15:40:09,474 - INFO - tqdm - accuracy: 0.7338, batch_loss: 0.3911, loss: 0.5162 ||:  81%|########  | 238/295 [12:57<03:10,  3.34s/it]
2022-03-30 15:40:21,413 - INFO - tqdm - accuracy: 0.7345, batch_loss: 0.3546, loss: 0.5146 ||:  82%|########2 | 242/295 [13:09<02:43,  3.09s/it]
2022-03-30 15:40:32,082 - INFO - tqdm - accuracy: 0.7353, batch_loss: 0.4417, loss: 0.5137 ||:  83%|########3 | 245/295 [13:19<02:48,  3.37s/it]
2022-03-30 15:40:44,752 - INFO - tqdm - accuracy: 0.7364, batch_loss: 0.5472, loss: 0.5122 ||:  84%|########4 | 249/295 [13:32<02:32,  3.31s/it]
2022-03-30 15:40:58,604 - INFO - tqdm - accuracy: 0.7379, batch_loss: 0.3694, loss: 0.5107 ||:  86%|########5 | 253/295 [13:46<02:25,  3.45s/it]
2022-03-30 15:41:08,778 - INFO - tqdm - accuracy: 0.7380, batch_loss: 0.5181, loss: 0.5105 ||:  87%|########6 | 256/295 [13:56<02:13,  3.42s/it]
2022-03-30 15:41:19,052 - INFO - tqdm - accuracy: 0.7379, batch_loss: 0.4637, loss: 0.5099 ||:  88%|########7 | 259/295 [14:06<02:03,  3.42s/it]
2022-03-30 15:41:31,314 - INFO - tqdm - accuracy: 0.7396, batch_loss: 0.3689, loss: 0.5080 ||:  89%|########9 | 263/295 [14:19<01:41,  3.19s/it]
2022-03-30 15:41:41,425 - INFO - tqdm - accuracy: 0.7403, batch_loss: 0.3207, loss: 0.5071 ||:  90%|######### | 266/295 [14:29<01:36,  3.31s/it]
2022-03-30 15:41:53,915 - INFO - tqdm - accuracy: 0.7419, batch_loss: 0.5329, loss: 0.5047 ||:  92%|#########1| 270/295 [14:41<01:19,  3.17s/it]
2022-03-30 15:42:04,111 - INFO - tqdm - accuracy: 0.7428, batch_loss: 0.5657, loss: 0.5035 ||:  93%|#########2| 273/295 [14:52<01:12,  3.32s/it]
2022-03-30 15:42:14,206 - INFO - tqdm - accuracy: 0.7435, batch_loss: 0.3532, loss: 0.5017 ||:  94%|#########3| 276/295 [15:02<01:03,  3.32s/it]
2022-03-30 15:42:24,228 - INFO - tqdm - accuracy: 0.7441, batch_loss: 0.3482, loss: 0.5007 ||:  95%|#########4| 279/295 [15:12<00:53,  3.34s/it]
2022-03-30 15:42:34,522 - INFO - tqdm - accuracy: 0.7449, batch_loss: 0.3455, loss: 0.4999 ||:  96%|#########5| 282/295 [15:22<00:44,  3.44s/it]
2022-03-30 15:42:47,382 - INFO - tqdm - accuracy: 0.7464, batch_loss: 0.3991, loss: 0.4993 ||:  97%|#########6| 286/295 [15:35<00:29,  3.26s/it]
2022-03-30 15:42:57,817 - INFO - tqdm - accuracy: 0.7478, batch_loss: 0.3337, loss: 0.4975 ||:  98%|#########7| 289/295 [15:45<00:20,  3.40s/it]
2022-03-30 15:43:09,603 - INFO - tqdm - accuracy: 0.7494, batch_loss: 0.4014, loss: 0.4961 ||:  99%|#########9| 293/295 [15:57<00:06,  3.09s/it]
2022-03-30 15:43:12,383 - INFO - tqdm - accuracy: 0.7501, batch_loss: 0.2751, loss: 0.4953 ||: 100%|#########9| 294/295 [16:00<00:02,  3.00s/it]
2022-03-30 15:43:14,423 - INFO - tqdm - accuracy: 0.7504, batch_loss: 0.2302, loss: 0.4944 ||: 100%|##########| 295/295 [16:02<00:00,  2.71s/it]
2022-03-30 15:43:14,424 - INFO - tqdm - accuracy: 0.7504, batch_loss: 0.2302, loss: 0.4944 ||: 100%|##########| 295/295 [16:02<00:00,  3.26s/it]
2022-03-30 15:43:16,864 - INFO - allennlp.training.trainer - Validating
2022-03-30 15:43:16,866 - INFO - tqdm - 0%|          | 0/1635 [00:00<?, ?it/s]
2022-03-30 15:43:16,867 - INFO - allennlp.data.samplers.bucket_batch_sampler - No sorting keys given; trying to guess a good one
2022-03-30 15:43:16,867 - INFO - allennlp.data.samplers.bucket_batch_sampler - Using ['tokens'] as the sorting keys
2022-03-30 15:43:26,869 - INFO - tqdm - accuracy: 0.8209, batch_loss: 0.5951, loss: 0.4030 ||:   9%|9         | 148/1635 [00:10<01:38, 15.17it/s]
2022-03-30 15:43:36,981 - INFO - tqdm - accuracy: 0.8289, batch_loss: 0.1231, loss: 0.4106 ||:  19%|#8        | 304/1635 [00:20<01:19, 16.81it/s]
2022-03-30 15:43:47,060 - INFO - tqdm - accuracy: 0.8278, batch_loss: 0.4744, loss: 0.4112 ||:  28%|##7       | 453/1635 [00:30<01:20, 14.63it/s]
2022-03-30 15:43:57,210 - INFO - tqdm - accuracy: 0.8319, batch_loss: 0.0322, loss: 0.3960 ||:  36%|###6      | 595/1635 [00:40<01:21, 12.70it/s]
2022-03-30 15:44:07,302 - INFO - tqdm - accuracy: 0.8315, batch_loss: 1.1046, loss: 0.3869 ||:  46%|####5     | 745/1635 [00:50<01:02, 14.20it/s]
2022-03-30 15:44:17,387 - INFO - tqdm - accuracy: 0.8313, batch_loss: 0.0251, loss: 0.3844 ||:  54%|#####4    | 886/1635 [01:00<00:47, 15.75it/s]
2022-03-30 15:44:27,452 - INFO - tqdm - accuracy: 0.8322, batch_loss: 0.1012, loss: 0.3794 ||:  63%|######3   | 1037/1635 [01:10<00:44, 13.45it/s]
2022-03-30 15:44:37,578 - INFO - tqdm - accuracy: 0.8372, batch_loss: 0.1606, loss: 0.3668 ||:  72%|#######2  | 1179/1635 [01:20<00:43, 10.54it/s]
2022-03-30 15:44:47,686 - INFO - tqdm - accuracy: 0.8416, batch_loss: 2.4494, loss: 0.3648 ||:  81%|########1 | 1332/1635 [01:30<00:21, 13.90it/s]
2022-03-30 15:44:57,818 - INFO - tqdm - accuracy: 0.8379, batch_loss: 0.0274, loss: 0.3713 ||:  91%|######### | 1484/1635 [01:40<00:13, 10.85it/s]
2022-03-30 15:45:07,617 - INFO - tqdm - accuracy: 0.8373, batch_loss: 0.3240, loss: 0.3727 ||: 100%|#########9| 1629/1635 [01:50<00:00, 13.73it/s]
2022-03-30 15:45:07,718 - INFO - tqdm - accuracy: 0.8375, batch_loss: 0.2808, loss: 0.3725 ||: 100%|#########9| 1631/1635 [01:50<00:00, 14.98it/s]
2022-03-30 15:45:07,856 - INFO - tqdm - accuracy: 0.8377, batch_loss: 0.0306, loss: 0.3722 ||: 100%|#########9| 1633/1635 [01:50<00:00, 14.82it/s]
2022-03-30 15:45:07,958 - INFO - tqdm - accuracy: 0.8373, batch_loss: 1.2172, loss: 0.3728 ||: 100%|##########| 1635/1635 [01:51<00:00, 15.93it/s]
2022-03-30 15:45:07,959 - INFO - tqdm - accuracy: 0.8373, batch_loss: 1.2172, loss: 0.3728 ||: 100%|##########| 1635/1635 [01:51<00:00, 14.72it/s]
2022-03-30 15:45:07,959 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-03-30 15:45:07,960 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.750  |     0.837
2022-03-30 15:45:07,961 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  5756.214  |       N/A
2022-03-30 15:45:07,962 - INFO - allennlp.training.tensorboard_writer - loss               |     0.494  |     0.373
2022-03-30 15:45:07,962 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  7092.406  |       N/A
2022-03-30 15:45:12,630 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to './output_roberta-star/best.th'.
2022-03-30 15:45:13,951 - INFO - allennlp.training.trainer - Epoch duration: 0:18:01.866536
2022-03-30 15:45:13,951 - INFO - allennlp.training.trainer - Estimated training time remaining: 2:42:16
2022-03-30 15:45:13,951 - INFO - allennlp.training.trainer - Epoch 1/9
2022-03-30 15:45:13,951 - INFO - allennlp.training.trainer - Worker 0 memory usage: 6.9G
2022-03-30 15:45:13,952 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.3G
2022-03-30 15:45:13,953 - INFO - allennlp.training.trainer - Training
2022-03-30 15:45:13,953 - INFO - tqdm - 0%|          | 0/295 [00:00<?, ?it/s]
2022-03-30 15:45:27,389 - INFO - tqdm - accuracy: 0.8672, batch_loss: 0.3222, loss: 0.3249 ||:   1%|1         | 4/295 [00:13<16:28,  3.40s/it]
2022-03-30 15:45:37,643 - INFO - tqdm - accuracy: 0.8929, batch_loss: 0.2778, loss: 0.2916 ||:   2%|2         | 7/295 [00:23<15:58,  3.33s/it]
2022-03-30 15:45:50,492 - INFO - tqdm - accuracy: 0.8835, batch_loss: 0.4491, loss: 0.3117 ||:   4%|3         | 11/295 [00:36<15:22,  3.25s/it]
2022-03-30 15:46:02,748 - INFO - tqdm - accuracy: 0.8750, batch_loss: 0.2817, loss: 0.3244 ||:   5%|5         | 15/295 [00:48<14:28,  3.10s/it]
2022-03-30 15:46:15,144 - INFO - tqdm - accuracy: 0.8766, batch_loss: 0.1784, loss: 0.3198 ||:   6%|6         | 19/295 [01:01<14:15,  3.10s/it]
2022-03-30 15:46:25,355 - INFO - tqdm - accuracy: 0.8736, batch_loss: 0.4400, loss: 0.3185 ||:   7%|7         | 22/295 [01:11<15:01,  3.30s/it]
2022-03-30 15:46:35,380 - INFO - tqdm - accuracy: 0.8700, batch_loss: 0.2832, loss: 0.3224 ||:   8%|8         | 25/295 [01:21<14:36,  3.25s/it]
2022-03-30 15:46:48,310 - INFO - tqdm - accuracy: 0.8696, batch_loss: 0.3373, loss: 0.3151 ||:  10%|9         | 29/295 [01:34<14:59,  3.38s/it]
2022-03-30 15:46:59,197 - INFO - tqdm - accuracy: 0.8701, batch_loss: 0.1697, loss: 0.3112 ||:  11%|#         | 32/295 [01:45<15:37,  3.57s/it]
2022-03-30 15:47:11,819 - INFO - tqdm - accuracy: 0.8681, batch_loss: 0.3620, loss: 0.3155 ||:  12%|#2        | 36/295 [01:57<14:05,  3.26s/it]
2022-03-30 15:47:25,417 - INFO - tqdm - accuracy: 0.8656, batch_loss: 0.2246, loss: 0.3209 ||:  14%|#3        | 40/295 [02:11<14:28,  3.41s/it]
2022-03-30 15:47:38,498 - INFO - tqdm - accuracy: 0.8643, batch_loss: 0.6313, loss: 0.3273 ||:  15%|#4        | 44/295 [02:24<14:05,  3.37s/it]
2022-03-30 15:47:51,853 - INFO - tqdm - accuracy: 0.8633, batch_loss: 0.2790, loss: 0.3276 ||:  16%|#6        | 48/295 [02:37<13:44,  3.34s/it]
2022-03-30 15:48:02,933 - INFO - tqdm - accuracy: 0.8646, batch_loss: 0.4225, loss: 0.3254 ||:  17%|#7        | 51/295 [02:48<14:43,  3.62s/it]
2022-03-30 15:48:15,621 - INFO - tqdm - accuracy: 0.8642, batch_loss: 0.3021, loss: 0.3247 ||:  19%|#8        | 55/295 [03:01<12:58,  3.24s/it]
2022-03-30 15:48:28,801 - INFO - tqdm - accuracy: 0.8676, batch_loss: 0.3948, loss: 0.3198 ||:  20%|##        | 59/295 [03:14<13:00,  3.31s/it]
2022-03-30 15:48:41,628 - INFO - tqdm - accuracy: 0.8690, batch_loss: 0.3371, loss: 0.3197 ||:  21%|##1       | 63/295 [03:27<12:33,  3.25s/it]
2022-03-30 15:48:54,104 - INFO - tqdm - accuracy: 0.8685, batch_loss: 0.3102, loss: 0.3226 ||:  23%|##2       | 67/295 [03:40<12:18,  3.24s/it]
2022-03-30 15:49:07,099 - INFO - tqdm - accuracy: 0.8666, batch_loss: 0.2619, loss: 0.3228 ||:  24%|##4       | 71/295 [03:53<11:59,  3.21s/it]
2022-03-30 15:49:18,242 - INFO - tqdm - accuracy: 0.8674, batch_loss: 0.2912, loss: 0.3199 ||:  25%|##5       | 74/295 [04:04<12:59,  3.53s/it]
2022-03-30 15:49:28,451 - INFO - tqdm - accuracy: 0.8693, batch_loss: 0.3889, loss: 0.3178 ||:  26%|##6       | 77/295 [04:14<12:18,  3.39s/it]
2022-03-30 15:49:38,918 - INFO - tqdm - accuracy: 0.8688, batch_loss: 0.4103, loss: 0.3197 ||:  27%|##7       | 80/295 [04:24<12:28,  3.48s/it]
2022-03-30 15:49:51,125 - INFO - tqdm - accuracy: 0.8709, batch_loss: 0.2315, loss: 0.3150 ||:  28%|##8       | 84/295 [04:37<11:09,  3.17s/it]
2022-03-30 15:50:03,714 - INFO - tqdm - accuracy: 0.8697, batch_loss: 0.2046, loss: 0.3165 ||:  30%|##9       | 88/295 [04:49<10:54,  3.16s/it]
2022-03-30 15:50:13,943 - INFO - tqdm - accuracy: 0.8698, batch_loss: 0.3302, loss: 0.3159 ||:  31%|###       | 91/295 [04:59<11:07,  3.27s/it]
2022-03-30 15:50:23,974 - INFO - tqdm - accuracy: 0.8693, batch_loss: 0.1843, loss: 0.3164 ||:  32%|###1      | 94/295 [05:10<11:16,  3.36s/it]
2022-03-30 15:50:36,905 - INFO - tqdm - accuracy: 0.8709, batch_loss: 0.2397, loss: 0.3132 ||:  33%|###3      | 98/295 [05:22<10:33,  3.22s/it]
2022-03-30 15:50:48,782 - INFO - tqdm - accuracy: 0.8695, batch_loss: 0.3080, loss: 0.3133 ||:  35%|###4      | 102/295 [05:34<09:48,  3.05s/it]
2022-03-30 15:50:59,499 - INFO - tqdm - accuracy: 0.8699, batch_loss: 0.3752, loss: 0.3117 ||:  36%|###5      | 105/295 [05:45<10:50,  3.43s/it]
2022-03-30 15:51:11,190 - INFO - tqdm - accuracy: 0.8713, batch_loss: 0.1765, loss: 0.3098 ||:  37%|###6      | 109/295 [05:57<09:34,  3.09s/it]
2022-03-30 15:51:24,133 - INFO - tqdm - accuracy: 0.8697, batch_loss: 0.3295, loss: 0.3114 ||:  38%|###8      | 113/295 [06:10<09:38,  3.18s/it]
2022-03-30 15:51:37,337 - INFO - tqdm - accuracy: 0.8710, batch_loss: 0.2169, loss: 0.3101 ||:  40%|###9      | 117/295 [06:23<09:52,  3.33s/it]
2022-03-30 15:51:50,791 - INFO - tqdm - accuracy: 0.8724, batch_loss: 0.3692, loss: 0.3084 ||:  41%|####1     | 121/295 [06:36<09:45,  3.37s/it]
2022-03-30 15:52:03,017 - INFO - tqdm - accuracy: 0.8732, batch_loss: 0.1691, loss: 0.3057 ||:  42%|####2     | 125/295 [06:49<09:07,  3.22s/it]
2022-03-30 15:52:13,917 - INFO - tqdm - accuracy: 0.8730, batch_loss: 0.3537, loss: 0.3059 ||:  43%|####3     | 128/295 [06:59<09:47,  3.52s/it]
2022-03-30 15:52:24,247 - INFO - tqdm - accuracy: 0.8729, batch_loss: 0.2641, loss: 0.3054 ||:  44%|####4     | 131/295 [07:10<09:16,  3.39s/it]
2022-03-30 15:52:36,672 - INFO - tqdm - accuracy: 0.8750, batch_loss: 0.2900, loss: 0.3030 ||:  46%|####5     | 135/295 [07:22<08:17,  3.11s/it]
2022-03-30 15:52:47,483 - INFO - tqdm - accuracy: 0.8748, batch_loss: 0.1779, loss: 0.3033 ||:  47%|####6     | 138/295 [07:33<09:04,  3.47s/it]
2022-03-30 15:53:00,582 - INFO - tqdm - accuracy: 0.8761, batch_loss: 0.1004, loss: 0.3008 ||:  48%|####8     | 142/295 [07:46<08:35,  3.37s/it]
2022-03-30 15:53:11,070 - INFO - tqdm - accuracy: 0.8752, batch_loss: 0.4637, loss: 0.3034 ||:  49%|####9     | 145/295 [07:57<08:45,  3.51s/it]
2022-03-30 15:53:21,233 - INFO - tqdm - accuracy: 0.8758, batch_loss: 0.2387, loss: 0.3030 ||:  50%|#####     | 148/295 [08:07<08:22,  3.42s/it]
2022-03-30 15:53:31,377 - INFO - tqdm - accuracy: 0.8760, batch_loss: 0.2694, loss: 0.3025 ||:  51%|#####1    | 151/295 [08:17<08:09,  3.40s/it]
2022-03-30 15:53:43,841 - INFO - tqdm - accuracy: 0.8770, batch_loss: 0.2413, loss: 0.3003 ||:  53%|#####2    | 155/295 [08:29<07:29,  3.21s/it]
2022-03-30 15:53:57,158 - INFO - tqdm - accuracy: 0.8770, batch_loss: 0.2797, loss: 0.3008 ||:  54%|#####3    | 159/295 [08:43<07:25,  3.28s/it]
2022-03-30 15:54:09,825 - INFO - tqdm - accuracy: 0.8773, batch_loss: 0.1976, loss: 0.3004 ||:  55%|#####5    | 163/295 [08:55<06:56,  3.15s/it]
2022-03-30 15:54:22,228 - INFO - tqdm - accuracy: 0.8776, batch_loss: 0.2053, loss: 0.2987 ||:  57%|#####6    | 167/295 [09:08<06:39,  3.12s/it]
2022-03-30 15:54:34,869 - INFO - tqdm - accuracy: 0.8765, batch_loss: 0.3710, loss: 0.2998 ||:  58%|#####7    | 171/295 [09:20<06:27,  3.12s/it]
2022-03-30 15:54:47,770 - INFO - tqdm - accuracy: 0.8764, batch_loss: 0.3650, loss: 0.2987 ||:  59%|#####9    | 175/295 [09:33<06:29,  3.25s/it]
2022-03-30 15:55:00,915 - INFO - tqdm - accuracy: 0.8764, batch_loss: 0.3356, loss: 0.2984 ||:  61%|######    | 179/295 [09:46<06:26,  3.33s/it]
2022-03-30 15:55:11,888 - INFO - tqdm - accuracy: 0.8759, batch_loss: 0.3402, loss: 0.2992 ||:  62%|######1   | 182/295 [09:57<06:43,  3.57s/it]
2022-03-30 15:55:22,654 - INFO - tqdm - accuracy: 0.8757, batch_loss: 0.3835, loss: 0.2997 ||:  63%|######2   | 185/295 [10:08<06:34,  3.59s/it]
2022-03-30 15:55:35,183 - INFO - tqdm - accuracy: 0.8757, batch_loss: 0.3598, loss: 0.2999 ||:  64%|######4   | 189/295 [10:21<05:46,  3.27s/it]
2022-03-30 15:55:45,997 - INFO - tqdm - accuracy: 0.8761, batch_loss: 0.1895, loss: 0.2988 ||:  65%|######5   | 192/295 [10:32<05:56,  3.46s/it]
2022-03-30 15:55:59,269 - INFO - tqdm - accuracy: 0.8772, batch_loss: 0.1802, loss: 0.2967 ||:  66%|######6   | 196/295 [10:45<05:31,  3.35s/it]
2022-03-30 15:56:12,565 - INFO - tqdm - accuracy: 0.8770, batch_loss: 0.4361, loss: 0.2987 ||:  68%|######7   | 200/295 [10:58<05:17,  3.34s/it]
2022-03-30 15:56:22,775 - INFO - tqdm - accuracy: 0.8771, batch_loss: 0.3101, loss: 0.2989 ||:  69%|######8   | 203/295 [11:08<05:10,  3.37s/it]
2022-03-30 15:56:35,729 - INFO - tqdm - accuracy: 0.8772, batch_loss: 0.2610, loss: 0.2981 ||:  70%|#######   | 207/295 [11:21<04:46,  3.25s/it]
2022-03-30 15:56:46,645 - INFO - tqdm - accuracy: 0.8777, batch_loss: 0.2508, loss: 0.2977 ||:  71%|#######1  | 210/295 [11:32<04:52,  3.44s/it]
2022-03-30 15:56:59,330 - INFO - tqdm - accuracy: 0.8773, batch_loss: 0.3148, loss: 0.2975 ||:  73%|#######2  | 214/295 [11:45<04:21,  3.23s/it]
2022-03-30 15:57:12,218 - INFO - tqdm - accuracy: 0.8780, batch_loss: 0.1931, loss: 0.2963 ||:  74%|#######3  | 218/295 [11:58<04:06,  3.19s/it]
2022-03-30 15:57:22,697 - INFO - tqdm - accuracy: 0.8781, batch_loss: 0.2690, loss: 0.2963 ||:  75%|#######4  | 221/295 [12:08<04:08,  3.35s/it]
2022-03-30 15:57:35,964 - INFO - tqdm - accuracy: 0.8783, batch_loss: 0.1872, loss: 0.2959 ||:  76%|#######6  | 225/295 [12:22<03:59,  3.42s/it]
2022-03-30 15:57:49,415 - INFO - tqdm - accuracy: 0.8777, batch_loss: 0.4219, loss: 0.2972 ||:  78%|#######7  | 229/295 [12:35<03:46,  3.43s/it]
2022-03-30 15:58:01,918 - INFO - tqdm - accuracy: 0.8778, batch_loss: 0.4719, loss: 0.2971 ||:  79%|#######8  | 233/295 [12:47<03:21,  3.25s/it]
2022-03-30 15:58:14,803 - INFO - tqdm - accuracy: 0.8783, batch_loss: 0.2142, loss: 0.2968 ||:  80%|########  | 237/295 [13:00<03:05,  3.19s/it]
2022-03-30 15:58:25,284 - INFO - tqdm - accuracy: 0.8789, batch_loss: 0.2098, loss: 0.2955 ||:  81%|########1 | 240/295 [13:11<03:06,  3.40s/it]
2022-03-30 15:58:36,104 - INFO - tqdm - accuracy: 0.8786, batch_loss: 0.3968, loss: 0.2961 ||:  82%|########2 | 243/295 [13:22<03:02,  3.52s/it]
2022-03-30 15:58:48,779 - INFO - tqdm - accuracy: 0.8789, batch_loss: 0.2281, loss: 0.2954 ||:  84%|########3 | 247/295 [13:34<02:36,  3.27s/it]
2022-03-30 15:58:59,085 - INFO - tqdm - accuracy: 0.8796, batch_loss: 0.1728, loss: 0.2938 ||:  85%|########4 | 250/295 [13:45<02:32,  3.40s/it]
2022-03-30 15:59:12,717 - INFO - tqdm - accuracy: 0.8797, batch_loss: 0.4482, loss: 0.2937 ||:  86%|########6 | 254/295 [13:58<02:24,  3.53s/it]
2022-03-30 15:59:25,503 - INFO - tqdm - accuracy: 0.8798, batch_loss: 0.3915, loss: 0.2938 ||:  87%|########7 | 258/295 [14:11<02:01,  3.28s/it]
2022-03-30 15:59:36,152 - INFO - tqdm - accuracy: 0.8803, batch_loss: 0.2788, loss: 0.2931 ||:  88%|########8 | 261/295 [14:22<01:56,  3.44s/it]
2022-03-30 15:59:49,042 - INFO - tqdm - accuracy: 0.8795, batch_loss: 0.5833, loss: 0.2954 ||:  90%|########9 | 265/295 [14:35<01:39,  3.33s/it]
2022-03-30 16:00:01,779 - INFO - tqdm - accuracy: 0.8793, batch_loss: 0.2755, loss: 0.2959 ||:  91%|#########1| 269/295 [14:47<01:24,  3.24s/it]
2022-03-30 16:00:14,051 - INFO - tqdm - accuracy: 0.8793, batch_loss: 0.5436, loss: 0.2967 ||:  93%|#########2| 273/295 [15:00<01:07,  3.07s/it]
2022-03-30 16:00:26,876 - INFO - tqdm - accuracy: 0.8795, batch_loss: 0.3719, loss: 0.2963 ||:  94%|#########3| 277/295 [15:12<00:57,  3.18s/it]
2022-03-30 16:00:37,006 - INFO - tqdm - accuracy: 0.8799, batch_loss: 0.2215, loss: 0.2957 ||:  95%|#########4| 280/295 [15:23<00:49,  3.30s/it]
2022-03-30 16:00:47,316 - INFO - tqdm - accuracy: 0.8795, batch_loss: 0.2169, loss: 0.2960 ||:  96%|#########5| 283/295 [15:33<00:40,  3.39s/it]
2022-03-30 16:01:00,163 - INFO - tqdm - accuracy: 0.8793, batch_loss: 0.1689, loss: 0.2956 ||:  97%|#########7| 287/295 [15:46<00:26,  3.27s/it]
2022-03-30 16:01:10,586 - INFO - tqdm - accuracy: 0.8790, batch_loss: 0.3026, loss: 0.2958 ||:  98%|#########8| 290/295 [15:56<00:16,  3.38s/it]
2022-03-30 16:01:21,179 - INFO - tqdm - accuracy: 0.8793, batch_loss: 0.2479, loss: 0.2962 ||:  99%|#########9| 293/295 [16:07<00:06,  3.39s/it]
2022-03-30 16:01:24,620 - INFO - tqdm - accuracy: 0.8792, batch_loss: 0.1793, loss: 0.2958 ||: 100%|#########9| 294/295 [16:10<00:03,  3.41s/it]
2022-03-30 16:01:26,397 - INFO - tqdm - accuracy: 0.8792, batch_loss: 0.2464, loss: 0.2956 ||: 100%|##########| 295/295 [16:12<00:00,  2.92s/it]
2022-03-30 16:01:26,397 - INFO - tqdm - accuracy: 0.8792, batch_loss: 0.2464, loss: 0.2956 ||: 100%|##########| 295/295 [16:12<00:00,  3.30s/it]
2022-03-30 16:01:28,699 - INFO - allennlp.training.trainer - Validating
2022-03-30 16:01:28,701 - INFO - tqdm - 0%|          | 0/1635 [00:00<?, ?it/s]
2022-03-30 16:01:38,777 - INFO - tqdm - accuracy: 0.8907, batch_loss: 0.0173, loss: 0.3032 ||:   9%|9         | 151/1635 [00:10<01:37, 15.29it/s]
2022-03-30 16:01:48,793 - INFO - tqdm - accuracy: 0.8686, batch_loss: 1.0845, loss: 0.3399 ||:  18%|#7        | 293/1635 [00:20<01:25, 15.78it/s]
2022-03-30 16:01:58,813 - INFO - tqdm - accuracy: 0.8684, batch_loss: 0.4568, loss: 0.3529 ||:  27%|##6       | 437/1635 [00:30<01:20, 14.87it/s]
2022-03-30 16:02:08,878 - INFO - tqdm - accuracy: 0.8659, batch_loss: 0.0238, loss: 0.3496 ||:  35%|###5      | 578/1635 [00:40<01:13, 14.40it/s]
2022-03-30 16:02:18,968 - INFO - tqdm - accuracy: 0.8658, batch_loss: 0.1068, loss: 0.3519 ||:  44%|####3     | 719/1635 [00:50<01:21, 11.24it/s]
2022-03-30 16:02:28,997 - INFO - tqdm - accuracy: 0.8620, batch_loss: 0.0093, loss: 0.3593 ||:  53%|#####2    | 859/1635 [01:00<00:54, 14.27it/s]
2022-03-30 16:02:39,026 - INFO - tqdm - accuracy: 0.8564, batch_loss: 0.0566, loss: 0.3700 ||:  62%|######1   | 1006/1635 [01:10<00:44, 14.29it/s]
2022-03-30 16:02:49,084 - INFO - tqdm - accuracy: 0.8567, batch_loss: 0.2121, loss: 0.3680 ||:  71%|#######   | 1155/1635 [01:20<00:31, 15.03it/s]
2022-03-30 16:02:59,090 - INFO - tqdm - accuracy: 0.8606, batch_loss: 0.0485, loss: 0.3625 ||:  80%|#######9  | 1306/1635 [01:30<00:25, 12.79it/s]
2022-03-30 16:03:09,124 - INFO - tqdm - accuracy: 0.8592, batch_loss: 0.1101, loss: 0.3657 ||:  88%|########8 | 1445/1635 [01:40<00:13, 14.24it/s]
2022-03-30 16:03:19,126 - INFO - tqdm - accuracy: 0.8572, batch_loss: 0.0094, loss: 0.3690 ||:  97%|#########7| 1593/1635 [01:50<00:03, 13.12it/s]
2022-03-30 16:03:21,460 - INFO - tqdm - accuracy: 0.8572, batch_loss: 0.3455, loss: 0.3674 ||: 100%|#########9| 1628/1635 [01:52<00:00, 14.41it/s]
2022-03-30 16:03:21,600 - INFO - tqdm - accuracy: 0.8567, batch_loss: 2.2561, loss: 0.3685 ||: 100%|#########9| 1630/1635 [01:52<00:00, 14.38it/s]
2022-03-30 16:03:21,770 - INFO - tqdm - accuracy: 0.8570, batch_loss: 0.0268, loss: 0.3679 ||: 100%|#########9| 1633/1635 [01:53<00:00, 15.46it/s]
2022-03-30 16:03:21,869 - INFO - tqdm - accuracy: 0.8572, batch_loss: 0.0149, loss: 0.3677 ||: 100%|##########| 1635/1635 [01:53<00:00, 14.45it/s]
2022-03-30 16:03:21,869 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-03-30 16:03:21,870 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.879  |     0.857
2022-03-30 16:03:21,871 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9561.629  |       N/A
2022-03-30 16:03:21,871 - INFO - allennlp.training.tensorboard_writer - loss               |     0.296  |     0.368
2022-03-30 16:03:21,872 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  7092.406  |       N/A
2022-03-30 16:03:26,399 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to './output_roberta-star/best.th'.
2022-03-30 16:03:37,605 - INFO - allennlp.training.trainer - Epoch duration: 0:18:23.653804
2022-03-30 16:03:37,605 - INFO - allennlp.training.trainer - Estimated training time remaining: 2:25:42
2022-03-30 16:03:37,605 - INFO - allennlp.training.trainer - Epoch 2/9
2022-03-30 16:03:37,606 - INFO - allennlp.training.trainer - Worker 0 memory usage: 6.9G
2022-03-30 16:03:37,606 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.3G
2022-03-30 16:03:37,607 - INFO - allennlp.training.trainer - Training
2022-03-30 16:03:37,608 - INFO - tqdm - 0%|          | 0/295 [00:00<?, ?it/s]
2022-03-30 16:03:47,869 - INFO - tqdm - accuracy: 0.9583, batch_loss: 0.1810, loss: 0.1375 ||:   1%|1         | 3/295 [00:10<16:21,  3.36s/it]
2022-03-30 16:03:58,066 - INFO - tqdm - accuracy: 0.9375, batch_loss: 0.1275, loss: 0.1670 ||:   2%|2         | 6/295 [00:20<16:13,  3.37s/it]
2022-03-30 16:04:11,026 - INFO - tqdm - accuracy: 0.9437, batch_loss: 0.1439, loss: 0.1555 ||:   3%|3         | 10/295 [00:33<15:40,  3.30s/it]
2022-03-30 16:04:21,516 - INFO - tqdm - accuracy: 0.9519, batch_loss: 0.1673, loss: 0.1430 ||:   4%|4         | 13/295 [00:43<16:17,  3.47s/it]
2022-03-30 16:04:34,243 - INFO - tqdm - accuracy: 0.9485, batch_loss: 0.0472, loss: 0.1381 ||:   6%|5         | 17/295 [00:56<15:01,  3.24s/it]
2022-03-30 16:04:44,705 - INFO - tqdm - accuracy: 0.9500, batch_loss: 0.0820, loss: 0.1353 ||:   7%|6         | 20/295 [01:07<15:33,  3.39s/it]
2022-03-30 16:04:57,924 - INFO - tqdm - accuracy: 0.9466, batch_loss: 0.2512, loss: 0.1567 ||:   8%|8         | 24/295 [01:20<15:09,  3.36s/it]
2022-03-30 16:05:08,542 - INFO - tqdm - accuracy: 0.9468, batch_loss: 0.1302, loss: 0.1534 ||:   9%|9         | 27/295 [01:30<15:37,  3.50s/it]
2022-03-30 16:05:21,778 - INFO - tqdm - accuracy: 0.9466, batch_loss: 0.1057, loss: 0.1648 ||:  11%|#         | 31/295 [01:44<14:48,  3.37s/it]
2022-03-30 16:05:34,070 - INFO - tqdm - accuracy: 0.9437, batch_loss: 0.0590, loss: 0.1711 ||:  12%|#1        | 35/295 [01:56<13:37,  3.14s/it]
2022-03-30 16:05:46,653 - INFO - tqdm - accuracy: 0.9415, batch_loss: 0.1332, loss: 0.1752 ||:  13%|#3        | 39/295 [02:09<13:23,  3.14s/it]
2022-03-30 16:05:59,304 - INFO - tqdm - accuracy: 0.9419, batch_loss: 0.0930, loss: 0.1726 ||:  15%|#4        | 43/295 [02:21<13:06,  3.12s/it]
2022-03-30 16:06:12,753 - INFO - tqdm - accuracy: 0.9428, batch_loss: 0.0862, loss: 0.1702 ||:  16%|#5        | 47/295 [02:35<13:47,  3.34s/it]
2022-03-30 16:06:25,619 - INFO - tqdm - accuracy: 0.9393, batch_loss: 0.3020, loss: 0.1755 ||:  17%|#7        | 51/295 [02:48<13:09,  3.23s/it]
2022-03-30 16:06:36,092 - INFO - tqdm - accuracy: 0.9392, batch_loss: 0.1875, loss: 0.1742 ||:  18%|#8        | 54/295 [02:58<13:44,  3.42s/it]
2022-03-30 16:06:46,542 - INFO - tqdm - accuracy: 0.9397, batch_loss: 0.1401, loss: 0.1718 ||:  19%|#9        | 57/295 [03:08<13:41,  3.45s/it]
2022-03-30 16:06:56,907 - INFO - tqdm - accuracy: 0.9401, batch_loss: 0.0892, loss: 0.1691 ||:  20%|##        | 60/295 [03:19<13:31,  3.45s/it]
2022-03-30 16:07:09,571 - INFO - tqdm - accuracy: 0.9395, batch_loss: 0.0873, loss: 0.1699 ||:  22%|##1       | 64/295 [03:31<12:29,  3.24s/it]
2022-03-30 16:07:20,514 - INFO - tqdm - accuracy: 0.9422, batch_loss: 0.0420, loss: 0.1646 ||:  23%|##2       | 67/295 [03:42<13:41,  3.60s/it]
2022-03-30 16:07:33,726 - INFO - tqdm - accuracy: 0.9428, batch_loss: 0.1081, loss: 0.1629 ||:  24%|##4       | 71/295 [03:56<12:38,  3.39s/it]
2022-03-30 16:07:46,594 - INFO - tqdm - accuracy: 0.9442, batch_loss: 0.1603, loss: 0.1590 ||:  25%|##5       | 75/295 [04:08<11:47,  3.22s/it]
2022-03-30 16:07:59,733 - INFO - tqdm - accuracy: 0.9430, batch_loss: 0.3520, loss: 0.1616 ||:  27%|##6       | 79/295 [04:22<12:03,  3.35s/it]
2022-03-30 16:08:09,971 - INFO - tqdm - accuracy: 0.9447, batch_loss: 0.0306, loss: 0.1585 ||:  28%|##7       | 82/295 [04:32<11:55,  3.36s/it]
2022-03-30 16:08:20,039 - INFO - tqdm - accuracy: 0.9445, batch_loss: 0.2360, loss: 0.1587 ||:  29%|##8       | 85/295 [04:42<11:44,  3.36s/it]
2022-03-30 16:08:33,396 - INFO - tqdm - accuracy: 0.9449, batch_loss: 0.0835, loss: 0.1570 ||:  30%|###       | 89/295 [04:55<11:38,  3.39s/it]
2022-03-30 16:08:46,131 - INFO - tqdm - accuracy: 0.9425, batch_loss: 0.1274, loss: 0.1608 ||:  32%|###1      | 93/295 [05:08<10:52,  3.23s/it]
2022-03-30 16:08:58,058 - INFO - tqdm - accuracy: 0.9439, batch_loss: 0.0395, loss: 0.1585 ||:  33%|###2      | 97/295 [05:20<10:09,  3.08s/it]
2022-03-30 16:09:11,328 - INFO - tqdm - accuracy: 0.9443, batch_loss: 0.1204, loss: 0.1564 ||:  34%|###4      | 101/295 [05:33<10:36,  3.28s/it]
2022-03-30 16:09:23,963 - INFO - tqdm - accuracy: 0.9443, batch_loss: 0.1262, loss: 0.1554 ||:  36%|###5      | 105/295 [05:46<10:05,  3.19s/it]
2022-03-30 16:09:34,746 - INFO - tqdm - accuracy: 0.9453, batch_loss: 0.0356, loss: 0.1535 ||:  37%|###6      | 108/295 [05:57<10:52,  3.49s/it]
2022-03-30 16:09:48,348 - INFO - tqdm - accuracy: 0.9456, batch_loss: 0.0893, loss: 0.1535 ||:  38%|###7      | 112/295 [06:10<10:37,  3.48s/it]
2022-03-30 16:10:01,232 - INFO - tqdm - accuracy: 0.9466, batch_loss: 0.0894, loss: 0.1509 ||:  39%|###9      | 116/295 [06:23<09:44,  3.27s/it]
2022-03-30 16:10:11,796 - INFO - tqdm - accuracy: 0.9464, batch_loss: 0.3505, loss: 0.1533 ||:  40%|####      | 119/295 [06:34<10:03,  3.43s/it]
2022-03-30 16:10:21,931 - INFO - tqdm - accuracy: 0.9462, batch_loss: 0.0605, loss: 0.1536 ||:  41%|####1     | 122/295 [06:44<09:42,  3.37s/it]
2022-03-30 16:10:34,221 - INFO - tqdm - accuracy: 0.9449, batch_loss: 0.3278, loss: 0.1550 ||:  43%|####2     | 126/295 [06:56<09:05,  3.23s/it]
2022-03-30 16:10:44,522 - INFO - tqdm - accuracy: 0.9460, batch_loss: 0.0369, loss: 0.1536 ||:  44%|####3     | 129/295 [07:06<09:25,  3.41s/it]
2022-03-30 16:10:57,500 - INFO - tqdm - accuracy: 0.9452, batch_loss: 0.1341, loss: 0.1564 ||:  45%|####5     | 133/295 [07:19<08:47,  3.26s/it]
2022-03-30 16:11:07,554 - INFO - tqdm - accuracy: 0.9442, batch_loss: 0.1185, loss: 0.1589 ||:  46%|####6     | 136/295 [07:29<08:57,  3.38s/it]
2022-03-30 16:11:19,170 - INFO - tqdm - accuracy: 0.9429, batch_loss: 0.3274, loss: 0.1610 ||:  47%|####7     | 139/295 [07:41<09:40,  3.72s/it]
2022-03-30 16:11:29,358 - INFO - tqdm - accuracy: 0.9430, batch_loss: 0.2101, loss: 0.1604 ||:  48%|####8     | 142/295 [07:51<08:59,  3.53s/it]
2022-03-30 16:11:39,793 - INFO - tqdm - accuracy: 0.9435, batch_loss: 0.1146, loss: 0.1595 ||:  49%|####9     | 145/295 [08:02<08:42,  3.49s/it]
2022-03-30 16:11:50,201 - INFO - tqdm - accuracy: 0.9436, batch_loss: 0.1052, loss: 0.1594 ||:  50%|#####     | 148/295 [08:12<08:30,  3.47s/it]
2022-03-30 16:12:02,995 - INFO - tqdm - accuracy: 0.9435, batch_loss: 0.3081, loss: 0.1604 ||:  52%|#####1    | 152/295 [08:25<07:47,  3.27s/it]
2022-03-30 16:12:14,208 - INFO - tqdm - accuracy: 0.9427, batch_loss: 0.0527, loss: 0.1614 ||:  53%|#####2    | 155/295 [08:36<08:19,  3.56s/it]
2022-03-30 16:12:24,591 - INFO - tqdm - accuracy: 0.9422, batch_loss: 0.3451, loss: 0.1627 ||:  54%|#####3    | 158/295 [08:46<08:01,  3.52s/it]
2022-03-30 16:12:35,239 - INFO - tqdm - accuracy: 0.9421, batch_loss: 0.1383, loss: 0.1635 ||:  55%|#####4    | 161/295 [08:57<08:02,  3.60s/it]
2022-03-30 16:12:48,162 - INFO - tqdm - accuracy: 0.9413, batch_loss: 0.0971, loss: 0.1656 ||:  56%|#####5    | 165/295 [09:10<07:15,  3.35s/it]
2022-03-30 16:12:58,826 - INFO - tqdm - accuracy: 0.9416, batch_loss: 0.1222, loss: 0.1650 ||:  57%|#####6    | 168/295 [09:21<07:31,  3.56s/it]
2022-03-30 16:13:11,958 - INFO - tqdm - accuracy: 0.9422, batch_loss: 0.1158, loss: 0.1638 ||:  58%|#####8    | 172/295 [09:34<06:50,  3.34s/it]
2022-03-30 16:13:25,327 - INFO - tqdm - accuracy: 0.9412, batch_loss: 0.3680, loss: 0.1646 ||:  60%|#####9    | 176/295 [09:47<06:44,  3.40s/it]
2022-03-30 16:13:35,359 - INFO - tqdm - accuracy: 0.9410, batch_loss: 0.1572, loss: 0.1641 ||:  61%|######    | 179/295 [09:57<06:34,  3.40s/it]
2022-03-30 16:13:48,024 - INFO - tqdm - accuracy: 0.9406, batch_loss: 0.2319, loss: 0.1643 ||:  62%|######2   | 183/295 [10:10<05:58,  3.20s/it]
2022-03-30 16:13:58,835 - INFO - tqdm - accuracy: 0.9409, batch_loss: 0.1063, loss: 0.1636 ||:  63%|######3   | 186/295 [10:21<06:14,  3.43s/it]
2022-03-30 16:14:12,273 - INFO - tqdm - accuracy: 0.9403, batch_loss: 0.1240, loss: 0.1647 ||:  64%|######4   | 190/295 [10:34<05:54,  3.38s/it]
2022-03-30 16:14:22,809 - INFO - tqdm - accuracy: 0.9406, batch_loss: 0.0926, loss: 0.1643 ||:  65%|######5   | 193/295 [10:45<06:01,  3.54s/it]
2022-03-30 16:14:35,904 - INFO - tqdm - accuracy: 0.9405, batch_loss: 0.0850, loss: 0.1645 ||:  67%|######6   | 197/295 [10:58<05:29,  3.36s/it]
2022-03-30 16:14:48,890 - INFO - tqdm - accuracy: 0.9404, batch_loss: 0.2857, loss: 0.1652 ||:  68%|######8   | 201/295 [11:11<05:12,  3.33s/it]
2022-03-30 16:15:01,709 - INFO - tqdm - accuracy: 0.9405, batch_loss: 0.0360, loss: 0.1641 ||:  69%|######9   | 205/295 [11:24<04:54,  3.27s/it]
2022-03-30 16:15:14,989 - INFO - tqdm - accuracy: 0.9405, batch_loss: 0.1458, loss: 0.1632 ||:  71%|#######   | 209/295 [11:37<04:50,  3.38s/it]
2022-03-30 16:15:25,020 - INFO - tqdm - accuracy: 0.9404, batch_loss: 0.1165, loss: 0.1635 ||:  72%|#######1  | 212/295 [11:47<04:36,  3.33s/it]
2022-03-30 16:15:35,410 - INFO - tqdm - accuracy: 0.9403, batch_loss: 0.1991, loss: 0.1638 ||:  73%|#######2  | 215/295 [11:57<04:35,  3.44s/it]
2022-03-30 16:15:45,421 - INFO - tqdm - accuracy: 0.9402, batch_loss: 0.1326, loss: 0.1631 ||:  74%|#######3  | 218/295 [12:07<04:21,  3.40s/it]
2022-03-30 16:15:58,066 - INFO - tqdm - accuracy: 0.9396, batch_loss: 0.1139, loss: 0.1649 ||:  75%|#######5  | 222/295 [12:20<03:52,  3.18s/it]
2022-03-30 16:16:11,192 - INFO - tqdm - accuracy: 0.9398, batch_loss: 0.1421, loss: 0.1641 ||:  77%|#######6  | 226/295 [12:33<03:52,  3.37s/it]
2022-03-30 16:16:23,473 - INFO - tqdm - accuracy: 0.9403, batch_loss: 0.0950, loss: 0.1630 ||:  78%|#######7  | 230/295 [12:45<03:28,  3.20s/it]
2022-03-30 16:16:33,630 - INFO - tqdm - accuracy: 0.9410, batch_loss: 0.0769, loss: 0.1618 ||:  79%|#######8  | 233/295 [12:56<03:24,  3.31s/it]
2022-03-30 16:16:44,825 - INFO - tqdm - accuracy: 0.9413, batch_loss: 0.0628, loss: 0.1614 ||:  80%|########  | 236/295 [13:07<03:33,  3.61s/it]
2022-03-30 16:16:58,930 - INFO - tqdm - accuracy: 0.9414, batch_loss: 0.0738, loss: 0.1614 ||:  81%|########1 | 240/295 [13:21<03:20,  3.65s/it]
2022-03-30 16:17:12,480 - INFO - tqdm - accuracy: 0.9415, batch_loss: 0.1108, loss: 0.1610 ||:  83%|########2 | 244/295 [13:34<02:56,  3.46s/it]
2022-03-30 16:17:25,416 - INFO - tqdm - accuracy: 0.9414, batch_loss: 0.0477, loss: 0.1613 ||:  84%|########4 | 248/295 [13:47<02:32,  3.24s/it]
2022-03-30 16:17:36,017 - INFO - tqdm - accuracy: 0.9419, batch_loss: 0.0869, loss: 0.1602 ||:  85%|########5 | 251/295 [13:58<02:28,  3.39s/it]
2022-03-30 16:17:48,135 - INFO - tqdm - accuracy: 0.9420, batch_loss: 0.0311, loss: 0.1603 ||:  86%|########6 | 255/295 [14:10<02:06,  3.16s/it]
2022-03-30 16:18:01,927 - INFO - tqdm - accuracy: 0.9416, batch_loss: 0.2837, loss: 0.1618 ||:  88%|########7 | 259/295 [14:24<02:04,  3.47s/it]
2022-03-30 16:18:12,303 - INFO - tqdm - accuracy: 0.9411, batch_loss: 0.2991, loss: 0.1630 ||:  89%|########8 | 262/295 [14:34<01:52,  3.41s/it]
2022-03-30 16:18:24,519 - INFO - tqdm - accuracy: 0.9415, batch_loss: 0.1661, loss: 0.1623 ||:  90%|######### | 266/295 [14:46<01:30,  3.11s/it]
2022-03-30 16:18:37,290 - INFO - tqdm - accuracy: 0.9411, batch_loss: 0.1485, loss: 0.1633 ||:  92%|#########1| 270/295 [14:59<01:19,  3.17s/it]
2022-03-30 16:18:49,489 - INFO - tqdm - accuracy: 0.9417, batch_loss: 0.1241, loss: 0.1621 ||:  93%|#########2| 274/295 [15:11<01:05,  3.10s/it]
2022-03-30 16:19:01,683 - INFO - tqdm - accuracy: 0.9421, batch_loss: 0.0296, loss: 0.1613 ||:  94%|#########4| 278/295 [15:24<00:52,  3.07s/it]
2022-03-30 16:19:12,586 - INFO - tqdm - accuracy: 0.9421, batch_loss: 0.1004, loss: 0.1619 ||:  95%|#########5| 281/295 [15:34<00:48,  3.46s/it]
2022-03-30 16:19:25,329 - INFO - tqdm - accuracy: 0.9426, batch_loss: 0.0521, loss: 0.1606 ||:  97%|#########6| 285/295 [15:47<00:32,  3.25s/it]
2022-03-30 16:19:35,342 - INFO - tqdm - accuracy: 0.9428, batch_loss: 0.2451, loss: 0.1603 ||:  98%|#########7| 288/295 [15:57<00:23,  3.35s/it]
2022-03-30 16:19:45,919 - INFO - tqdm - accuracy: 0.9425, batch_loss: 0.2566, loss: 0.1606 ||:  99%|#########8| 291/295 [16:08<00:13,  3.44s/it]
2022-03-30 16:19:56,792 - INFO - tqdm - accuracy: 0.9426, batch_loss: 0.1221, loss: 0.1601 ||: 100%|#########9| 294/295 [16:19<00:03,  3.58s/it]
2022-03-30 16:19:59,080 - INFO - tqdm - accuracy: 0.9426, batch_loss: 0.2423, loss: 0.1604 ||: 100%|##########| 295/295 [16:21<00:00,  3.19s/it]
2022-03-30 16:19:59,080 - INFO - tqdm - accuracy: 0.9426, batch_loss: 0.2423, loss: 0.1604 ||: 100%|##########| 295/295 [16:21<00:00,  3.33s/it]
2022-03-30 16:20:01,425 - INFO - allennlp.training.trainer - Validating
2022-03-30 16:20:01,426 - INFO - tqdm - 0%|          | 0/1635 [00:00<?, ?it/s]
2022-03-30 16:20:11,512 - INFO - tqdm - accuracy: 0.8467, batch_loss: 0.0185, loss: 0.4777 ||:   9%|9         | 150/1635 [00:10<01:41, 14.61it/s]
2022-03-30 16:20:21,642 - INFO - tqdm - accuracy: 0.8658, batch_loss: 0.0085, loss: 0.4222 ||:  18%|#8        | 298/1635 [00:20<01:27, 15.36it/s]
2022-03-30 16:20:31,666 - INFO - tqdm - accuracy: 0.8547, batch_loss: 0.0039, loss: 0.4842 ||:  27%|##6       | 437/1635 [00:30<01:31, 13.15it/s]
2022-03-30 16:20:41,733 - INFO - tqdm - accuracy: 0.8574, batch_loss: 0.0029, loss: 0.4719 ||:  36%|###6      | 589/1635 [00:40<01:06, 15.68it/s]
2022-03-30 16:20:51,843 - INFO - tqdm - accuracy: 0.8547, batch_loss: 0.1580, loss: 0.4865 ||:  45%|####4     | 733/1635 [00:50<01:05, 13.79it/s]
2022-03-30 16:21:01,911 - INFO - tqdm - accuracy: 0.8588, batch_loss: 2.1954, loss: 0.4851 ||:  54%|#####3    | 878/1635 [01:00<00:56, 13.29it/s]
2022-03-30 16:21:12,005 - INFO - tqdm - accuracy: 0.8552, batch_loss: 0.5037, loss: 0.4951 ||:  63%|######2   | 1022/1635 [01:10<00:40, 15.06it/s]
2022-03-30 16:21:22,111 - INFO - tqdm - accuracy: 0.8573, batch_loss: 0.0048, loss: 0.4907 ||:  71%|#######1  | 1167/1635 [01:20<00:32, 14.59it/s]
2022-03-30 16:21:32,181 - INFO - tqdm - accuracy: 0.8583, batch_loss: 2.0131, loss: 0.4849 ||:  80%|########  | 1313/1635 [01:30<00:26, 12.05it/s]
2022-03-30 16:21:42,259 - INFO - tqdm - accuracy: 0.8588, batch_loss: 0.0035, loss: 0.4812 ||:  89%|########8 | 1452/1635 [01:40<00:15, 12.19it/s]
2022-03-30 16:21:52,282 - INFO - tqdm - accuracy: 0.8574, batch_loss: 0.0386, loss: 0.4860 ||:  98%|#########7| 1599/1635 [01:50<00:01, 18.75it/s]
2022-03-30 16:21:54,155 - INFO - tqdm - accuracy: 0.8572, batch_loss: 0.0036, loss: 0.4878 ||: 100%|#########9| 1628/1635 [01:52<00:00, 13.34it/s]
2022-03-30 16:21:54,314 - INFO - tqdm - accuracy: 0.8574, batch_loss: 0.0653, loss: 0.4873 ||: 100%|#########9| 1630/1635 [01:52<00:00, 13.12it/s]
2022-03-30 16:21:54,428 - INFO - tqdm - accuracy: 0.8569, batch_loss: 1.6306, loss: 0.4885 ||: 100%|#########9| 1632/1635 [01:53<00:00, 14.17it/s]
2022-03-30 16:21:54,646 - INFO - tqdm - accuracy: 0.8571, batch_loss: 0.0149, loss: 0.4879 ||: 100%|#########9| 1634/1635 [01:53<00:00, 12.20it/s]
2022-03-30 16:21:54,691 - INFO - tqdm - accuracy: 0.8572, batch_loss: 0.0073, loss: 0.4876 ||: 100%|##########| 1635/1635 [01:53<00:00, 14.44it/s]
2022-03-30 16:21:54,691 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-03-30 16:21:54,691 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.943  |     0.857
2022-03-30 16:21:54,692 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9561.629  |       N/A
2022-03-30 16:21:54,693 - INFO - allennlp.training.tensorboard_writer - loss               |     0.160  |     0.488
2022-03-30 16:21:54,694 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  7092.406  |       N/A
2022-03-30 16:21:59,221 - INFO - allennlp.training.trainer - Epoch duration: 0:18:21.615873
2022-03-30 16:21:59,222 - INFO - allennlp.training.trainer - Estimated training time remaining: 2:07:49
2022-03-30 16:21:59,222 - INFO - allennlp.training.trainer - Epoch 3/9
2022-03-30 16:21:59,222 - INFO - allennlp.training.trainer - Worker 0 memory usage: 6.9G
2022-03-30 16:21:59,222 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.3G
2022-03-30 16:21:59,224 - INFO - allennlp.training.trainer - Training
2022-03-30 16:21:59,224 - INFO - tqdm - 0%|          | 0/295 [00:00<?, ?it/s]
2022-03-30 16:22:12,784 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.2146, loss: 0.0974 ||:   1%|1         | 4/295 [00:13<16:52,  3.48s/it]
2022-03-30 16:22:26,335 - INFO - tqdm - accuracy: 0.9609, batch_loss: 0.0847, loss: 0.1139 ||:   3%|2         | 8/295 [00:27<16:07,  3.37s/it]
2022-03-30 16:22:36,866 - INFO - tqdm - accuracy: 0.9631, batch_loss: 0.0709, loss: 0.1041 ||:   4%|3         | 11/295 [00:37<16:50,  3.56s/it]
2022-03-30 16:22:46,983 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.1962, loss: 0.1004 ||:   5%|4         | 14/295 [00:47<16:25,  3.51s/it]
2022-03-30 16:22:57,562 - INFO - tqdm - accuracy: 0.9651, batch_loss: 0.0280, loss: 0.1065 ||:   6%|5         | 17/295 [00:58<16:08,  3.48s/it]
2022-03-30 16:23:07,778 - INFO - tqdm - accuracy: 0.9656, batch_loss: 0.0274, loss: 0.1024 ||:   7%|6         | 20/295 [01:08<15:57,  3.48s/it]
2022-03-30 16:23:19,813 - INFO - tqdm - accuracy: 0.9701, batch_loss: 0.0152, loss: 0.0906 ||:   8%|8         | 24/295 [01:20<14:00,  3.10s/it]
2022-03-30 16:23:31,118 - INFO - tqdm - accuracy: 0.9722, batch_loss: 0.0294, loss: 0.0871 ||:   9%|9         | 27/295 [01:31<15:40,  3.51s/it]
2022-03-30 16:23:41,479 - INFO - tqdm - accuracy: 0.9719, batch_loss: 0.0263, loss: 0.0864 ||:  10%|#         | 30/295 [01:42<15:24,  3.49s/it]
2022-03-30 16:23:54,864 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.0179, loss: 0.0912 ||:  12%|#1        | 34/295 [01:55<15:05,  3.47s/it]
2022-03-30 16:24:08,698 - INFO - tqdm - accuracy: 0.9671, batch_loss: 0.2169, loss: 0.0965 ||:  13%|#2        | 38/295 [02:09<15:09,  3.54s/it]
2022-03-30 16:24:18,930 - INFO - tqdm - accuracy: 0.9680, batch_loss: 0.0062, loss: 0.0959 ||:  14%|#3        | 41/295 [02:19<14:34,  3.44s/it]
2022-03-30 16:24:31,861 - INFO - tqdm - accuracy: 0.9681, batch_loss: 0.0345, loss: 0.0951 ||:  15%|#5        | 45/295 [02:32<14:02,  3.37s/it]
2022-03-30 16:24:41,978 - INFO - tqdm - accuracy: 0.9674, batch_loss: 0.0958, loss: 0.0983 ||:  16%|#6        | 48/295 [02:42<13:41,  3.32s/it]
2022-03-30 16:24:52,048 - INFO - tqdm - accuracy: 0.9669, batch_loss: 0.0866, loss: 0.0995 ||:  17%|#7        | 51/295 [02:52<13:24,  3.30s/it]
2022-03-30 16:25:05,026 - INFO - tqdm - accuracy: 0.9682, batch_loss: 0.0092, loss: 0.0984 ||:  19%|#8        | 55/295 [03:05<13:05,  3.27s/it]
2022-03-30 16:25:18,063 - INFO - tqdm - accuracy: 0.9698, batch_loss: 0.0110, loss: 0.0950 ||:  20%|##        | 59/295 [03:18<12:41,  3.23s/it]
2022-03-30 16:25:28,412 - INFO - tqdm - accuracy: 0.9698, batch_loss: 0.0621, loss: 0.0959 ||:  21%|##1       | 62/295 [03:29<12:53,  3.32s/it]
2022-03-30 16:25:38,526 - INFO - tqdm - accuracy: 0.9707, batch_loss: 0.0319, loss: 0.0942 ||:  22%|##2       | 65/295 [03:39<12:47,  3.34s/it]
2022-03-30 16:25:50,843 - INFO - tqdm - accuracy: 0.9710, batch_loss: 0.0554, loss: 0.0917 ||:  23%|##3       | 69/295 [03:51<11:38,  3.09s/it]
2022-03-30 16:26:04,414 - INFO - tqdm - accuracy: 0.9717, batch_loss: 0.0650, loss: 0.0912 ||:  25%|##4       | 73/295 [04:05<12:22,  3.34s/it]
2022-03-30 16:26:17,137 - INFO - tqdm - accuracy: 0.9712, batch_loss: 0.0809, loss: 0.0957 ||:  26%|##6       | 77/295 [04:17<11:32,  3.18s/it]
2022-03-30 16:26:30,019 - INFO - tqdm - accuracy: 0.9715, batch_loss: 0.0238, loss: 0.0943 ||:  27%|##7       | 81/295 [04:30<11:29,  3.22s/it]
2022-03-30 16:26:40,293 - INFO - tqdm - accuracy: 0.9721, batch_loss: 0.0128, loss: 0.0921 ||:  28%|##8       | 84/295 [04:41<11:55,  3.39s/it]
2022-03-30 16:26:51,588 - INFO - tqdm - accuracy: 0.9720, batch_loss: 0.0894, loss: 0.0923 ||:  29%|##9       | 87/295 [04:52<12:50,  3.71s/it]
2022-03-30 16:27:02,751 - INFO - tqdm - accuracy: 0.9722, batch_loss: 0.0942, loss: 0.0909 ||:  31%|###       | 90/295 [05:03<12:36,  3.69s/it]
2022-03-30 16:27:14,102 - INFO - tqdm - accuracy: 0.9724, batch_loss: 0.0748, loss: 0.0898 ||:  32%|###1      | 93/295 [05:14<12:51,  3.82s/it]
2022-03-30 16:27:24,870 - INFO - tqdm - accuracy: 0.9723, batch_loss: 0.1463, loss: 0.0897 ||:  33%|###2      | 96/295 [05:25<12:10,  3.67s/it]
2022-03-30 16:27:35,879 - INFO - tqdm - accuracy: 0.9722, batch_loss: 0.0110, loss: 0.0891 ||:  34%|###3      | 99/295 [05:36<12:09,  3.72s/it]
2022-03-30 16:27:46,351 - INFO - tqdm - accuracy: 0.9721, batch_loss: 0.0316, loss: 0.0886 ||:  35%|###4      | 102/295 [05:47<11:33,  3.59s/it]
2022-03-30 16:27:58,360 - INFO - tqdm - accuracy: 0.9711, batch_loss: 0.2323, loss: 0.0934 ||:  36%|###5      | 106/295 [05:59<10:00,  3.18s/it]
2022-03-30 16:28:08,460 - INFO - tqdm - accuracy: 0.9716, batch_loss: 0.0394, loss: 0.0924 ||:  37%|###6      | 109/295 [06:09<10:14,  3.30s/it]
2022-03-30 16:28:20,902 - INFO - tqdm - accuracy: 0.9721, batch_loss: 0.0234, loss: 0.0906 ||:  38%|###8      | 113/295 [06:21<09:34,  3.16s/it]
2022-03-30 16:28:31,235 - INFO - tqdm - accuracy: 0.9712, batch_loss: 0.0583, loss: 0.0920 ||:  39%|###9      | 116/295 [06:32<10:03,  3.37s/it]
2022-03-30 16:28:42,951 - INFO - tqdm - accuracy: 0.9711, batch_loss: 0.1996, loss: 0.0914 ||:  41%|####      | 120/295 [06:43<08:37,  2.95s/it]
2022-03-30 16:28:55,562 - INFO - tqdm - accuracy: 0.9710, batch_loss: 0.1780, loss: 0.0910 ||:  42%|####2     | 124/295 [06:56<08:55,  3.13s/it]
2022-03-30 16:29:08,565 - INFO - tqdm - accuracy: 0.9707, batch_loss: 0.1406, loss: 0.0923 ||:  43%|####3     | 128/295 [07:09<09:01,  3.24s/it]
2022-03-30 16:29:21,530 - INFO - tqdm - accuracy: 0.9714, batch_loss: 0.0606, loss: 0.0911 ||:  45%|####4     | 132/295 [07:22<09:11,  3.38s/it]
2022-03-30 16:29:31,633 - INFO - tqdm - accuracy: 0.9704, batch_loss: 0.1377, loss: 0.0919 ||:  46%|####5     | 135/295 [07:32<09:02,  3.39s/it]
2022-03-30 16:29:44,356 - INFO - tqdm - accuracy: 0.9705, batch_loss: 0.0255, loss: 0.0917 ||:  47%|####7     | 139/295 [07:45<08:25,  3.24s/it]
2022-03-30 16:29:57,366 - INFO - tqdm - accuracy: 0.9705, batch_loss: 0.0161, loss: 0.0911 ||:  48%|####8     | 143/295 [07:58<08:25,  3.32s/it]
2022-03-30 16:30:08,002 - INFO - tqdm - accuracy: 0.9698, batch_loss: 0.0134, loss: 0.0934 ||:  49%|####9     | 146/295 [08:08<08:46,  3.53s/it]
2022-03-30 16:30:21,588 - INFO - tqdm - accuracy: 0.9702, batch_loss: 0.0401, loss: 0.0918 ||:  51%|#####     | 150/295 [08:22<08:24,  3.48s/it]
2022-03-30 16:30:34,391 - INFO - tqdm - accuracy: 0.9700, batch_loss: 0.2305, loss: 0.0928 ||:  52%|#####2    | 154/295 [08:35<07:33,  3.22s/it]
2022-03-30 16:30:44,656 - INFO - tqdm - accuracy: 0.9697, batch_loss: 0.0391, loss: 0.0928 ||:  53%|#####3    | 157/295 [08:45<07:46,  3.38s/it]
2022-03-30 16:30:57,928 - INFO - tqdm - accuracy: 0.9701, batch_loss: 0.0700, loss: 0.0918 ||:  55%|#####4    | 161/295 [08:58<07:32,  3.37s/it]
2022-03-30 16:31:08,122 - INFO - tqdm - accuracy: 0.9703, batch_loss: 0.0811, loss: 0.0914 ||:  56%|#####5    | 164/295 [09:08<07:20,  3.36s/it]
2022-03-30 16:31:21,283 - INFO - tqdm - accuracy: 0.9708, batch_loss: 0.0808, loss: 0.0902 ||:  57%|#####6    | 168/295 [09:22<06:58,  3.30s/it]
2022-03-30 16:31:31,351 - INFO - tqdm - accuracy: 0.9709, batch_loss: 0.0105, loss: 0.0900 ||:  58%|#####7    | 171/295 [09:32<06:55,  3.35s/it]
2022-03-30 16:31:42,322 - INFO - tqdm - accuracy: 0.9713, batch_loss: 0.0208, loss: 0.0893 ||:  59%|#####8    | 174/295 [09:43<07:16,  3.61s/it]
2022-03-30 16:31:53,155 - INFO - tqdm - accuracy: 0.9712, batch_loss: 0.0559, loss: 0.0888 ||:  60%|######    | 177/295 [09:53<07:07,  3.62s/it]
2022-03-30 16:32:03,217 - INFO - tqdm - accuracy: 0.9712, batch_loss: 0.0839, loss: 0.0885 ||:  61%|######1   | 180/295 [10:03<06:41,  3.50s/it]
2022-03-30 16:32:16,263 - INFO - tqdm - accuracy: 0.9703, batch_loss: 0.0814, loss: 0.0911 ||:  62%|######2   | 184/295 [10:17<06:07,  3.31s/it]
2022-03-30 16:32:26,646 - INFO - tqdm - accuracy: 0.9708, batch_loss: 0.0264, loss: 0.0901 ||:  63%|######3   | 187/295 [10:27<05:58,  3.32s/it]
2022-03-30 16:32:36,818 - INFO - tqdm - accuracy: 0.9706, batch_loss: 0.0160, loss: 0.0902 ||:  64%|######4   | 190/295 [10:37<05:55,  3.38s/it]
2022-03-30 16:32:49,160 - INFO - tqdm - accuracy: 0.9704, batch_loss: 0.0129, loss: 0.0910 ||:  66%|######5   | 194/295 [10:49<05:12,  3.09s/it]
2022-03-30 16:33:01,757 - INFO - tqdm - accuracy: 0.9706, batch_loss: 0.0084, loss: 0.0901 ||:  67%|######7   | 198/295 [11:02<04:57,  3.07s/it]
2022-03-30 16:33:13,461 - INFO - tqdm - accuracy: 0.9709, batch_loss: 0.2235, loss: 0.0899 ||:  68%|######8   | 202/295 [11:14<04:31,  2.92s/it]
2022-03-30 16:33:25,715 - INFO - tqdm - accuracy: 0.9710, batch_loss: 0.0238, loss: 0.0897 ||:  70%|######9   | 206/295 [11:26<04:34,  3.08s/it]
2022-03-30 16:33:39,067 - INFO - tqdm - accuracy: 0.9710, batch_loss: 0.0431, loss: 0.0903 ||:  71%|#######1  | 210/295 [11:39<04:42,  3.32s/it]
2022-03-30 16:33:49,276 - INFO - tqdm - accuracy: 0.9708, batch_loss: 0.0251, loss: 0.0908 ||:  72%|#######2  | 213/295 [11:50<04:34,  3.34s/it]
2022-03-30 16:34:01,629 - INFO - tqdm - accuracy: 0.9708, batch_loss: 0.2351, loss: 0.0908 ||:  74%|#######3  | 217/295 [12:02<04:11,  3.22s/it]
2022-03-30 16:34:12,128 - INFO - tqdm - accuracy: 0.9705, batch_loss: 0.0502, loss: 0.0908 ||:  75%|#######4  | 220/295 [12:12<04:20,  3.47s/it]
2022-03-30 16:34:24,067 - INFO - tqdm - accuracy: 0.9704, batch_loss: 0.1299, loss: 0.0905 ||:  76%|#######5  | 224/295 [12:24<03:42,  3.14s/it]
2022-03-30 16:34:34,581 - INFO - tqdm - accuracy: 0.9703, batch_loss: 0.1419, loss: 0.0914 ||:  77%|#######6  | 227/295 [12:35<03:51,  3.41s/it]
2022-03-30 16:34:45,546 - INFO - tqdm - accuracy: 0.9704, batch_loss: 0.0645, loss: 0.0920 ||:  78%|#######7  | 230/295 [12:46<03:50,  3.55s/it]
2022-03-30 16:34:58,820 - INFO - tqdm - accuracy: 0.9706, batch_loss: 0.0313, loss: 0.0912 ||:  79%|#######9  | 234/295 [12:59<03:29,  3.43s/it]
2022-03-30 16:35:10,698 - INFO - tqdm - accuracy: 0.9703, batch_loss: 0.0129, loss: 0.0920 ||:  81%|########  | 238/295 [13:11<02:53,  3.04s/it]
2022-03-30 16:35:21,843 - INFO - tqdm - accuracy: 0.9703, batch_loss: 0.1698, loss: 0.0919 ||:  82%|########1 | 241/295 [13:22<03:03,  3.40s/it]
2022-03-30 16:35:31,886 - INFO - tqdm - accuracy: 0.9703, batch_loss: 0.1604, loss: 0.0917 ||:  83%|########2 | 244/295 [13:32<02:55,  3.44s/it]
2022-03-30 16:35:42,231 - INFO - tqdm - accuracy: 0.9701, batch_loss: 0.0499, loss: 0.0922 ||:  84%|########3 | 247/295 [13:43<02:46,  3.48s/it]
2022-03-30 16:35:52,622 - INFO - tqdm - accuracy: 0.9701, batch_loss: 0.0376, loss: 0.0918 ||:  85%|########4 | 250/295 [13:53<02:34,  3.43s/it]
2022-03-30 16:36:05,634 - INFO - tqdm - accuracy: 0.9695, batch_loss: 0.1149, loss: 0.0923 ||:  86%|########6 | 254/295 [14:06<02:14,  3.29s/it]
2022-03-30 16:36:16,201 - INFO - tqdm - accuracy: 0.9696, batch_loss: 0.2270, loss: 0.0925 ||:  87%|########7 | 257/295 [14:16<02:10,  3.44s/it]
2022-03-30 16:36:26,510 - INFO - tqdm - accuracy: 0.9698, batch_loss: 0.0272, loss: 0.0918 ||:  88%|########8 | 260/295 [14:27<01:59,  3.42s/it]
2022-03-30 16:36:38,663 - INFO - tqdm - accuracy: 0.9698, batch_loss: 0.0262, loss: 0.0919 ||:  89%|########9 | 264/295 [14:39<01:37,  3.14s/it]
2022-03-30 16:36:51,931 - INFO - tqdm - accuracy: 0.9697, batch_loss: 0.0352, loss: 0.0920 ||:  91%|######### | 268/295 [14:52<01:30,  3.34s/it]
2022-03-30 16:37:02,340 - INFO - tqdm - accuracy: 0.9698, batch_loss: 0.0248, loss: 0.0918 ||:  92%|#########1| 271/295 [15:03<01:21,  3.41s/it]
2022-03-30 16:37:14,991 - INFO - tqdm - accuracy: 0.9700, batch_loss: 0.0204, loss: 0.0913 ||:  93%|#########3| 275/295 [15:15<01:04,  3.22s/it]
2022-03-30 16:37:25,829 - INFO - tqdm - accuracy: 0.9698, batch_loss: 0.2522, loss: 0.0923 ||:  94%|#########4| 278/295 [15:26<00:59,  3.49s/it]
2022-03-30 16:37:36,131 - INFO - tqdm - accuracy: 0.9701, batch_loss: 0.0113, loss: 0.0915 ||:  95%|#########5| 281/295 [15:36<00:48,  3.46s/it]
2022-03-30 16:37:49,623 - INFO - tqdm - accuracy: 0.9697, batch_loss: 0.1754, loss: 0.0924 ||:  97%|#########6| 285/295 [15:50<00:34,  3.44s/it]
2022-03-30 16:38:02,212 - INFO - tqdm - accuracy: 0.9698, batch_loss: 0.0359, loss: 0.0920 ||:  98%|#########7| 289/295 [16:02<00:18,  3.16s/it]
2022-03-30 16:38:15,429 - INFO - tqdm - accuracy: 0.9699, batch_loss: 0.0141, loss: 0.0915 ||:  99%|#########9| 293/295 [16:16<00:06,  3.27s/it]
2022-03-30 16:38:18,626 - INFO - tqdm - accuracy: 0.9699, batch_loss: 0.0559, loss: 0.0914 ||: 100%|#########9| 294/295 [16:19<00:03,  3.25s/it]
2022-03-30 16:38:20,689 - INFO - tqdm - accuracy: 0.9700, batch_loss: 0.0167, loss: 0.0911 ||: 100%|##########| 295/295 [16:21<00:00,  2.89s/it]
2022-03-30 16:38:20,689 - INFO - tqdm - accuracy: 0.9700, batch_loss: 0.0167, loss: 0.0911 ||: 100%|##########| 295/295 [16:21<00:00,  3.33s/it]
2022-03-30 16:38:22,986 - INFO - allennlp.training.trainer - Validating
2022-03-30 16:38:22,988 - INFO - tqdm - 0%|          | 0/1635 [00:00<?, ?it/s]
2022-03-30 16:38:32,995 - INFO - tqdm - accuracy: 0.8514, batch_loss: 0.0007, loss: 0.5235 ||:   8%|8         | 138/1635 [00:10<01:24, 17.62it/s]
2022-03-30 16:38:43,123 - INFO - tqdm - accuracy: 0.8460, batch_loss: 0.2967, loss: 0.5484 ||:  18%|#7        | 289/1635 [00:20<01:20, 16.81it/s]
2022-03-30 16:38:53,262 - INFO - tqdm - accuracy: 0.8418, batch_loss: 0.0288, loss: 0.5689 ||:  26%|##6       | 433/1635 [00:30<01:18, 15.28it/s]
2022-03-30 16:39:03,413 - INFO - tqdm - accuracy: 0.8405, batch_loss: 0.0013, loss: 0.5818 ||:  35%|###5      | 580/1635 [00:40<01:18, 13.44it/s]
2022-03-30 16:39:13,544 - INFO - tqdm - accuracy: 0.8457, batch_loss: 3.5805, loss: 0.5598 ||:  44%|####3     | 716/1635 [00:50<01:09, 13.17it/s]
2022-03-30 16:39:23,614 - INFO - tqdm - accuracy: 0.8469, batch_loss: 2.4919, loss: 0.5634 ||:  53%|#####2    | 859/1635 [01:00<00:56, 13.78it/s]
2022-03-30 16:39:33,690 - INFO - tqdm - accuracy: 0.8512, batch_loss: 0.0012, loss: 0.5511 ||:  62%|######2   | 1018/1635 [01:10<00:34, 17.89it/s]
2022-03-30 16:39:43,726 - INFO - tqdm - accuracy: 0.8485, batch_loss: 0.0045, loss: 0.5558 ||:  71%|#######1  | 1168/1635 [01:20<00:32, 14.43it/s]
2022-03-30 16:39:53,825 - INFO - tqdm - accuracy: 0.8497, batch_loss: 0.6245, loss: 0.5562 ||:  80%|########  | 1314/1635 [01:30<00:22, 14.01it/s]
2022-03-30 16:40:03,898 - INFO - tqdm - accuracy: 0.8506, batch_loss: 1.8852, loss: 0.5523 ||:  90%|########9 | 1466/1635 [01:40<00:11, 14.61it/s]
2022-03-30 16:40:14,037 - INFO - tqdm - accuracy: 0.8512, batch_loss: 3.7063, loss: 0.5500 ||:  98%|#########8| 1606/1635 [01:51<00:02, 14.15it/s]
2022-03-30 16:40:15,484 - INFO - tqdm - accuracy: 0.8504, batch_loss: 0.0016, loss: 0.5511 ||: 100%|#########9| 1628/1635 [01:52<00:00, 14.39it/s]
2022-03-30 16:40:15,594 - INFO - tqdm - accuracy: 0.8503, batch_loss: 1.2235, loss: 0.5512 ||: 100%|#########9| 1630/1635 [01:52<00:00, 15.30it/s]
2022-03-30 16:40:15,716 - INFO - tqdm - accuracy: 0.8502, batch_loss: 0.8597, loss: 0.5511 ||: 100%|#########9| 1632/1635 [01:52<00:00, 15.61it/s]
2022-03-30 16:40:15,882 - INFO - tqdm - accuracy: 0.8504, batch_loss: 0.0019, loss: 0.5506 ||: 100%|#########9| 1634/1635 [01:52<00:00, 14.36it/s]
2022-03-30 16:40:16,026 - INFO - tqdm - accuracy: 0.8505, batch_loss: 0.0054, loss: 0.5503 ||: 100%|##########| 1635/1635 [01:53<00:00, 14.46it/s]
2022-03-30 16:40:16,026 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-03-30 16:40:16,027 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.970  |     0.850
2022-03-30 16:40:16,027 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9572.260  |       N/A
2022-03-30 16:40:16,029 - INFO - allennlp.training.tensorboard_writer - loss               |     0.091  |     0.550
2022-03-30 16:40:16,029 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  7092.406  |       N/A
2022-03-30 16:40:20,802 - INFO - allennlp.training.trainer - Epoch duration: 0:18:21.579700
2022-03-30 16:40:20,802 - INFO - allennlp.training.trainer - Estimated training time remaining: 1:49:43
2022-03-30 16:40:20,802 - INFO - allennlp.training.trainer - Epoch 4/9
2022-03-30 16:40:20,802 - INFO - allennlp.training.trainer - Worker 0 memory usage: 6.9G
2022-03-30 16:40:20,803 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.3G
2022-03-30 16:40:20,804 - INFO - allennlp.training.trainer - Training
2022-03-30 16:40:20,804 - INFO - tqdm - 0%|          | 0/295 [00:00<?, ?it/s]
2022-03-30 16:40:34,264 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0456, loss: 0.0382 ||:   1%|1         | 4/295 [00:13<16:25,  3.39s/it]
2022-03-30 16:40:46,290 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0585, loss: 0.0362 ||:   3%|2         | 8/295 [00:25<14:48,  3.10s/it]
2022-03-30 16:40:58,798 - INFO - tqdm - accuracy: 0.9818, batch_loss: 0.0130, loss: 0.0378 ||:   4%|4         | 12/295 [00:37<14:24,  3.06s/it]
2022-03-30 16:41:08,929 - INFO - tqdm - accuracy: 0.9854, batch_loss: 0.0080, loss: 0.0324 ||:   5%|5         | 15/295 [00:48<15:17,  3.28s/it]
2022-03-30 16:41:21,624 - INFO - tqdm - accuracy: 0.9819, batch_loss: 0.0688, loss: 0.0365 ||:   6%|6         | 19/295 [01:00<14:26,  3.14s/it]
2022-03-30 16:41:35,113 - INFO - tqdm - accuracy: 0.9837, batch_loss: 0.0361, loss: 0.0354 ||:   8%|7         | 23/295 [01:14<15:05,  3.33s/it]
2022-03-30 16:41:48,499 - INFO - tqdm - accuracy: 0.9815, batch_loss: 0.0895, loss: 0.0380 ||:   9%|9         | 27/295 [01:27<15:05,  3.38s/it]
2022-03-30 16:41:59,289 - INFO - tqdm - accuracy: 0.9812, batch_loss: 0.0331, loss: 0.0394 ||:  10%|#         | 30/295 [01:38<15:18,  3.47s/it]
2022-03-30 16:42:09,607 - INFO - tqdm - accuracy: 0.9830, batch_loss: 0.0076, loss: 0.0380 ||:  11%|#1        | 33/295 [01:48<15:11,  3.48s/it]
2022-03-30 16:42:20,482 - INFO - tqdm - accuracy: 0.9826, batch_loss: 0.0970, loss: 0.0377 ||:  12%|#2        | 36/295 [01:59<15:19,  3.55s/it]
2022-03-30 16:42:30,796 - INFO - tqdm - accuracy: 0.9824, batch_loss: 0.0033, loss: 0.0373 ||:  13%|#3        | 39/295 [02:09<14:56,  3.50s/it]
2022-03-30 16:42:43,867 - INFO - tqdm - accuracy: 0.9818, batch_loss: 0.2309, loss: 0.0394 ||:  15%|#4        | 43/295 [02:23<14:06,  3.36s/it]
2022-03-30 16:42:56,715 - INFO - tqdm - accuracy: 0.9820, batch_loss: 0.0663, loss: 0.0395 ||:  16%|#5        | 47/295 [02:35<13:13,  3.20s/it]
2022-03-30 16:43:07,723 - INFO - tqdm - accuracy: 0.9831, batch_loss: 0.0295, loss: 0.0382 ||:  17%|#6        | 50/295 [02:46<14:18,  3.51s/it]
2022-03-30 16:43:21,224 - INFO - tqdm - accuracy: 0.9832, batch_loss: 0.0536, loss: 0.0381 ||:  18%|#8        | 54/295 [03:00<14:00,  3.49s/it]
2022-03-30 16:43:34,310 - INFO - tqdm - accuracy: 0.9817, batch_loss: 0.0550, loss: 0.0420 ||:  20%|#9        | 58/295 [03:13<13:38,  3.45s/it]
2022-03-30 16:43:46,542 - INFO - tqdm - accuracy: 0.9829, batch_loss: 0.0185, loss: 0.0401 ||:  21%|##1       | 62/295 [03:25<12:12,  3.14s/it]
2022-03-30 16:43:56,799 - INFO - tqdm - accuracy: 0.9817, batch_loss: 0.0066, loss: 0.0413 ||:  22%|##2       | 65/295 [03:35<13:04,  3.41s/it]
2022-03-30 16:44:07,574 - INFO - tqdm - accuracy: 0.9811, batch_loss: 0.0217, loss: 0.0446 ||:  23%|##3       | 68/295 [03:46<13:30,  3.57s/it]
2022-03-30 16:44:20,291 - INFO - tqdm - accuracy: 0.9809, batch_loss: 0.0037, loss: 0.0483 ||:  24%|##4       | 72/295 [03:59<12:01,  3.23s/it]
2022-03-30 16:44:32,947 - INFO - tqdm - accuracy: 0.9811, batch_loss: 0.0072, loss: 0.0479 ||:  26%|##5       | 76/295 [04:12<11:26,  3.14s/it]
2022-03-30 16:44:43,083 - INFO - tqdm - accuracy: 0.9818, batch_loss: 0.0063, loss: 0.0467 ||:  27%|##6       | 79/295 [04:22<11:57,  3.32s/it]
2022-03-30 16:44:53,202 - INFO - tqdm - accuracy: 0.9825, batch_loss: 0.0153, loss: 0.0453 ||:  28%|##7       | 82/295 [04:32<11:49,  3.33s/it]
2022-03-30 16:45:06,807 - INFO - tqdm - accuracy: 0.9818, batch_loss: 0.0753, loss: 0.0478 ||:  29%|##9       | 86/295 [04:46<12:00,  3.45s/it]
2022-03-30 16:45:19,603 - INFO - tqdm - accuracy: 0.9812, batch_loss: 0.0210, loss: 0.0494 ||:  31%|###       | 90/295 [04:58<11:15,  3.30s/it]
2022-03-30 16:45:30,316 - INFO - tqdm - accuracy: 0.9805, batch_loss: 0.2070, loss: 0.0513 ||:  32%|###1      | 93/295 [05:09<11:40,  3.47s/it]
2022-03-30 16:45:43,142 - INFO - tqdm - accuracy: 0.9803, batch_loss: 0.0897, loss: 0.0522 ||:  33%|###2      | 97/295 [05:22<10:41,  3.24s/it]
2022-03-30 16:45:53,885 - INFO - tqdm - accuracy: 0.9791, batch_loss: 0.2082, loss: 0.0551 ||:  34%|###3      | 100/295 [05:33<11:25,  3.52s/it]
2022-03-30 16:46:06,955 - INFO - tqdm - accuracy: 0.9793, batch_loss: 0.0097, loss: 0.0548 ||:  35%|###5      | 104/295 [05:46<10:39,  3.35s/it]
2022-03-30 16:46:19,987 - INFO - tqdm - accuracy: 0.9800, batch_loss: 0.0093, loss: 0.0535 ||:  37%|###6      | 108/295 [05:59<10:11,  3.27s/it]
2022-03-30 16:46:33,095 - INFO - tqdm - accuracy: 0.9802, batch_loss: 0.0452, loss: 0.0538 ||:  38%|###7      | 112/295 [06:12<09:56,  3.26s/it]
2022-03-30 16:46:43,357 - INFO - tqdm - accuracy: 0.9802, batch_loss: 0.0592, loss: 0.0536 ||:  39%|###8      | 115/295 [06:22<09:57,  3.32s/it]
2022-03-30 16:46:55,700 - INFO - tqdm - accuracy: 0.9803, batch_loss: 0.0108, loss: 0.0533 ||:  40%|####      | 119/295 [06:34<09:21,  3.19s/it]
2022-03-30 16:47:06,475 - INFO - tqdm - accuracy: 0.9805, batch_loss: 0.0382, loss: 0.0537 ||:  41%|####1     | 122/295 [06:45<09:42,  3.37s/it]
2022-03-30 16:47:18,989 - INFO - tqdm - accuracy: 0.9806, batch_loss: 0.0071, loss: 0.0532 ||:  43%|####2     | 126/295 [06:58<09:14,  3.28s/it]
2022-03-30 16:47:29,067 - INFO - tqdm - accuracy: 0.9811, batch_loss: 0.0215, loss: 0.0525 ||:  44%|####3     | 129/295 [07:08<09:11,  3.32s/it]
2022-03-30 16:47:39,864 - INFO - tqdm - accuracy: 0.9813, batch_loss: 0.0229, loss: 0.0519 ||:  45%|####4     | 132/295 [07:19<09:35,  3.53s/it]
2022-03-30 16:47:51,013 - INFO - tqdm - accuracy: 0.9812, batch_loss: 0.0161, loss: 0.0525 ||:  46%|####5     | 135/295 [07:30<09:49,  3.68s/it]
2022-03-30 16:48:01,843 - INFO - tqdm - accuracy: 0.9814, batch_loss: 0.0089, loss: 0.0517 ||:  47%|####6     | 138/295 [07:41<09:39,  3.69s/it]
2022-03-30 16:48:13,966 - INFO - tqdm - accuracy: 0.9815, batch_loss: 0.0267, loss: 0.0514 ||:  48%|####8     | 142/295 [07:53<08:02,  3.15s/it]
2022-03-30 16:48:24,390 - INFO - tqdm - accuracy: 0.9819, batch_loss: 0.0092, loss: 0.0506 ||:  49%|####9     | 145/295 [08:03<08:17,  3.31s/it]
2022-03-30 16:48:36,990 - INFO - tqdm - accuracy: 0.9815, batch_loss: 0.0022, loss: 0.0528 ||:  51%|#####     | 149/295 [08:16<07:33,  3.11s/it]
2022-03-30 16:48:47,435 - INFO - tqdm - accuracy: 0.9815, batch_loss: 0.0099, loss: 0.0546 ||:  52%|#####1    | 152/295 [08:26<07:59,  3.35s/it]
2022-03-30 16:48:57,642 - INFO - tqdm - accuracy: 0.9816, batch_loss: 0.0041, loss: 0.0541 ||:  53%|#####2    | 155/295 [08:36<07:52,  3.38s/it]
2022-03-30 16:49:10,455 - INFO - tqdm - accuracy: 0.9817, batch_loss: 0.1986, loss: 0.0544 ||:  54%|#####3    | 159/295 [08:49<07:21,  3.24s/it]
2022-03-30 16:49:23,450 - INFO - tqdm - accuracy: 0.9816, batch_loss: 0.0381, loss: 0.0558 ||:  55%|#####5    | 163/295 [09:02<07:06,  3.23s/it]
2022-03-30 16:49:36,503 - INFO - tqdm - accuracy: 0.9813, batch_loss: 0.0048, loss: 0.0583 ||:  57%|#####6    | 167/295 [09:15<07:01,  3.29s/it]
2022-03-30 16:49:49,498 - INFO - tqdm - accuracy: 0.9810, batch_loss: 0.1426, loss: 0.0583 ||:  58%|#####7    | 171/295 [09:28<06:54,  3.35s/it]
2022-03-30 16:49:59,659 - INFO - tqdm - accuracy: 0.9808, batch_loss: 0.0408, loss: 0.0582 ||:  59%|#####8    | 174/295 [09:38<06:56,  3.45s/it]
2022-03-30 16:50:12,866 - INFO - tqdm - accuracy: 0.9809, batch_loss: 0.0247, loss: 0.0591 ||:  60%|######    | 178/295 [09:52<06:33,  3.36s/it]
2022-03-30 16:50:26,165 - INFO - tqdm - accuracy: 0.9808, batch_loss: 0.0759, loss: 0.0594 ||:  62%|######1   | 182/295 [10:05<06:17,  3.34s/it]
2022-03-30 16:50:37,857 - INFO - tqdm - accuracy: 0.9809, batch_loss: 0.0415, loss: 0.0593 ||:  63%|######2   | 185/295 [10:17<06:51,  3.74s/it]
2022-03-30 16:50:51,415 - INFO - tqdm - accuracy: 0.9807, batch_loss: 0.0077, loss: 0.0602 ||:  64%|######4   | 189/295 [10:30<06:17,  3.56s/it]
2022-03-30 16:51:01,455 - INFO - tqdm - accuracy: 0.9810, batch_loss: 0.0214, loss: 0.0596 ||:  65%|######5   | 192/295 [10:40<05:51,  3.41s/it]
2022-03-30 16:51:11,608 - INFO - tqdm - accuracy: 0.9812, batch_loss: 0.0165, loss: 0.0589 ||:  66%|######6   | 195/295 [10:50<05:38,  3.39s/it]
2022-03-30 16:51:24,710 - INFO - tqdm - accuracy: 0.9812, batch_loss: 0.0066, loss: 0.0591 ||:  67%|######7   | 199/295 [11:03<05:14,  3.28s/it]
2022-03-30 16:51:35,193 - INFO - tqdm - accuracy: 0.9813, batch_loss: 0.0128, loss: 0.0592 ||:  68%|######8   | 202/295 [11:14<05:17,  3.41s/it]
2022-03-30 16:51:47,637 - INFO - tqdm - accuracy: 0.9812, batch_loss: 0.2344, loss: 0.0596 ||:  70%|######9   | 206/295 [11:26<04:42,  3.17s/it]
2022-03-30 16:51:57,822 - INFO - tqdm - accuracy: 0.9813, batch_loss: 0.0109, loss: 0.0593 ||:  71%|#######   | 209/295 [11:37<04:43,  3.30s/it]
2022-03-30 16:52:10,153 - INFO - tqdm - accuracy: 0.9814, batch_loss: 0.0417, loss: 0.0588 ||:  72%|#######2  | 213/295 [11:49<04:13,  3.10s/it]
2022-03-30 16:52:23,131 - INFO - tqdm - accuracy: 0.9811, batch_loss: 0.1760, loss: 0.0590 ||:  74%|#######3  | 217/295 [12:02<04:15,  3.28s/it]
2022-03-30 16:52:33,700 - INFO - tqdm - accuracy: 0.9807, batch_loss: 0.1107, loss: 0.0601 ||:  75%|#######4  | 220/295 [12:12<04:18,  3.45s/it]
2022-03-30 16:52:43,939 - INFO - tqdm - accuracy: 0.9808, batch_loss: 0.0052, loss: 0.0599 ||:  76%|#######5  | 223/295 [12:23<04:04,  3.39s/it]
2022-03-30 16:52:54,463 - INFO - tqdm - accuracy: 0.9811, batch_loss: 0.0128, loss: 0.0594 ||:  77%|#######6  | 226/295 [12:33<03:56,  3.43s/it]
2022-03-30 16:53:04,504 - INFO - tqdm - accuracy: 0.9813, batch_loss: 0.0140, loss: 0.0589 ||:  78%|#######7  | 229/295 [12:43<03:42,  3.37s/it]
2022-03-30 16:53:14,555 - INFO - tqdm - accuracy: 0.9813, batch_loss: 0.0887, loss: 0.0587 ||:  79%|#######8  | 232/295 [12:53<03:29,  3.32s/it]
2022-03-30 16:53:26,832 - INFO - tqdm - accuracy: 0.9816, batch_loss: 0.0061, loss: 0.0578 ||:  80%|########  | 236/295 [13:06<03:06,  3.17s/it]
2022-03-30 16:53:40,290 - INFO - tqdm - accuracy: 0.9816, batch_loss: 0.0126, loss: 0.0578 ||:  81%|########1 | 240/295 [13:19<03:03,  3.33s/it]
2022-03-30 16:53:52,951 - INFO - tqdm - accuracy: 0.9816, batch_loss: 0.0318, loss: 0.0584 ||:  83%|########2 | 244/295 [13:32<02:46,  3.27s/it]
2022-03-30 16:54:05,238 - INFO - tqdm - accuracy: 0.9817, batch_loss: 0.0350, loss: 0.0583 ||:  84%|########4 | 248/295 [13:44<02:27,  3.14s/it]
2022-03-30 16:54:15,587 - INFO - tqdm - accuracy: 0.9818, batch_loss: 0.1452, loss: 0.0583 ||:  85%|########5 | 251/295 [13:54<02:27,  3.34s/it]
2022-03-30 16:54:26,032 - INFO - tqdm - accuracy: 0.9819, batch_loss: 0.0361, loss: 0.0580 ||:  86%|########6 | 254/295 [14:05<02:22,  3.47s/it]
2022-03-30 16:54:36,776 - INFO - tqdm - accuracy: 0.9819, batch_loss: 0.0155, loss: 0.0582 ||:  87%|########7 | 257/295 [14:15<02:14,  3.55s/it]
2022-03-30 16:54:49,875 - INFO - tqdm - accuracy: 0.9813, batch_loss: 0.0272, loss: 0.0594 ||:  88%|########8 | 261/295 [14:29<01:53,  3.35s/it]
2022-03-30 16:55:00,161 - INFO - tqdm - accuracy: 0.9815, batch_loss: 0.0340, loss: 0.0591 ||:  89%|########9 | 264/295 [14:39<01:47,  3.47s/it]
2022-03-30 16:55:10,853 - INFO - tqdm - accuracy: 0.9815, batch_loss: 0.0134, loss: 0.0597 ||:  91%|######### | 267/295 [14:50<01:37,  3.50s/it]
2022-03-30 16:55:23,196 - INFO - tqdm - accuracy: 0.9818, batch_loss: 0.0026, loss: 0.0590 ||:  92%|#########1| 271/295 [15:02<01:16,  3.20s/it]
2022-03-30 16:55:35,884 - INFO - tqdm - accuracy: 0.9816, batch_loss: 0.2696, loss: 0.0597 ||:  93%|#########3| 275/295 [15:15<01:02,  3.14s/it]
2022-03-30 16:55:46,295 - INFO - tqdm - accuracy: 0.9816, batch_loss: 0.0179, loss: 0.0605 ||:  94%|#########4| 278/295 [15:25<00:57,  3.37s/it]
2022-03-30 16:55:57,217 - INFO - tqdm - accuracy: 0.9815, batch_loss: 0.0376, loss: 0.0603 ||:  95%|#########5| 281/295 [15:36<00:49,  3.50s/it]
2022-03-30 16:56:10,010 - INFO - tqdm - accuracy: 0.9817, batch_loss: 0.0294, loss: 0.0598 ||:  97%|#########6| 285/295 [15:49<00:33,  3.31s/it]
2022-03-30 16:56:20,793 - INFO - tqdm - accuracy: 0.9816, batch_loss: 0.0245, loss: 0.0605 ||:  98%|#########7| 288/295 [15:59<00:24,  3.49s/it]
2022-03-30 16:56:31,708 - INFO - tqdm - accuracy: 0.9816, batch_loss: 0.0165, loss: 0.0604 ||:  99%|#########8| 291/295 [16:10<00:14,  3.61s/it]
2022-03-30 16:56:41,632 - INFO - tqdm - accuracy: 0.9816, batch_loss: 0.0291, loss: 0.0602 ||: 100%|#########9| 294/295 [16:20<00:03,  3.45s/it]
2022-03-30 16:56:43,930 - INFO - tqdm - accuracy: 0.9815, batch_loss: 0.0953, loss: 0.0603 ||: 100%|##########| 295/295 [16:23<00:00,  3.10s/it]
2022-03-30 16:56:43,931 - INFO - tqdm - accuracy: 0.9815, batch_loss: 0.0953, loss: 0.0603 ||: 100%|##########| 295/295 [16:23<00:00,  3.33s/it]
2022-03-30 16:56:46,361 - INFO - allennlp.training.trainer - Validating
2022-03-30 16:56:46,362 - INFO - tqdm - 0%|          | 0/1635 [00:00<?, ?it/s]
2022-03-30 16:56:56,386 - INFO - tqdm - accuracy: 0.8523, batch_loss: 0.0026, loss: 0.6124 ||:   9%|9         | 149/1635 [00:10<01:57, 12.64it/s]
2022-03-30 16:57:06,508 - INFO - tqdm - accuracy: 0.8495, batch_loss: 0.0021, loss: 0.6340 ||:  18%|#8        | 299/1635 [00:20<01:54, 11.69it/s]
2022-03-30 16:57:16,515 - INFO - tqdm - accuracy: 0.8567, batch_loss: 0.0010, loss: 0.5903 ||:  27%|##7       | 443/1635 [00:30<01:27, 13.69it/s]
2022-03-30 16:57:26,647 - INFO - tqdm - accuracy: 0.8466, batch_loss: 2.9333, loss: 0.6265 ||:  36%|###6      | 590/1635 [00:40<01:20, 12.94it/s]
2022-03-30 16:57:36,705 - INFO - tqdm - accuracy: 0.8516, batch_loss: 0.0003, loss: 0.6018 ||:  45%|####4     | 728/1635 [00:50<01:12, 12.48it/s]
2022-03-30 16:57:46,710 - INFO - tqdm - accuracy: 0.8536, batch_loss: 0.0062, loss: 0.6135 ||:  53%|#####3    | 871/1635 [01:00<00:49, 15.30it/s]
2022-03-30 16:57:56,825 - INFO - tqdm - accuracy: 0.8536, batch_loss: 1.2226, loss: 0.6033 ||:  62%|######2   | 1018/1635 [01:10<00:43, 14.17it/s]
2022-03-30 16:58:06,828 - INFO - tqdm - accuracy: 0.8520, batch_loss: 0.3060, loss: 0.6019 ||:  70%|#######   | 1149/1635 [01:20<00:38, 12.50it/s]
2022-03-30 16:58:16,846 - INFO - tqdm - accuracy: 0.8531, batch_loss: 0.0189, loss: 0.5845 ||:  80%|#######9  | 1300/1635 [01:30<00:20, 16.68it/s]
2022-03-30 16:58:26,863 - INFO - tqdm - accuracy: 0.8529, batch_loss: 0.0008, loss: 0.5896 ||:  89%|########8 | 1448/1635 [01:40<00:13, 13.41it/s]
2022-03-30 16:58:36,932 - INFO - tqdm - accuracy: 0.8530, batch_loss: 0.0015, loss: 0.5891 ||:  98%|#########7| 1599/1635 [01:50<00:02, 13.82it/s]
2022-03-30 16:58:39,082 - INFO - tqdm - accuracy: 0.8535, batch_loss: 0.0022, loss: 0.5868 ||: 100%|#########9| 1628/1635 [01:52<00:00, 13.78it/s]
2022-03-30 16:58:39,296 - INFO - tqdm - accuracy: 0.8534, batch_loss: 0.0011, loss: 0.5884 ||: 100%|#########9| 1630/1635 [01:52<00:00, 12.07it/s]
2022-03-30 16:58:39,436 - INFO - tqdm - accuracy: 0.8532, batch_loss: 0.0047, loss: 0.5889 ||: 100%|#########9| 1632/1635 [01:53<00:00, 12.66it/s]
2022-03-30 16:58:39,590 - INFO - tqdm - accuracy: 0.8526, batch_loss: 0.8067, loss: 0.5910 ||: 100%|##########| 1635/1635 [01:53<00:00, 14.68it/s]
2022-03-30 16:58:39,591 - INFO - tqdm - accuracy: 0.8526, batch_loss: 0.8067, loss: 0.5910 ||: 100%|##########| 1635/1635 [01:53<00:00, 14.44it/s]
2022-03-30 16:58:39,591 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-03-30 16:58:39,591 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.982  |     0.853
2022-03-30 16:58:39,593 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9572.260  |       N/A
2022-03-30 16:58:39,593 - INFO - allennlp.training.tensorboard_writer - loss               |     0.060  |     0.591
2022-03-30 16:58:39,594 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  7092.406  |       N/A
2022-03-30 16:58:44,386 - INFO - allennlp.training.trainer - Epoch duration: 0:18:23.583680
2022-03-30 16:58:44,386 - INFO - allennlp.training.trainer - Estimated training time remaining: 1:31:32
2022-03-30 16:58:44,386 - INFO - allennlp.training.trainer - Epoch 5/9
2022-03-30 16:58:44,386 - INFO - allennlp.training.trainer - Worker 0 memory usage: 6.9G
2022-03-30 16:58:44,387 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.3G
2022-03-30 16:58:44,388 - INFO - allennlp.training.trainer - Training
2022-03-30 16:58:44,388 - INFO - tqdm - 0%|          | 0/295 [00:00<?, ?it/s]
2022-03-30 16:58:57,317 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0093, loss: 0.0307 ||:   1%|1         | 4/295 [00:12<16:11,  3.34s/it]
2022-03-30 16:59:08,483 - INFO - tqdm - accuracy: 0.9866, batch_loss: 0.0120, loss: 0.0357 ||:   2%|2         | 7/295 [00:24<17:55,  3.74s/it]
2022-03-30 16:59:21,536 - INFO - tqdm - accuracy: 0.9858, batch_loss: 0.0151, loss: 0.0399 ||:   4%|3         | 11/295 [00:37<16:04,  3.40s/it]
2022-03-30 16:59:34,389 - INFO - tqdm - accuracy: 0.9875, batch_loss: 0.0144, loss: 0.0432 ||:   5%|5         | 15/295 [00:50<15:08,  3.24s/it]
2022-03-30 16:59:46,907 - INFO - tqdm - accuracy: 0.9885, batch_loss: 0.0077, loss: 0.0378 ||:   6%|6         | 19/295 [01:02<14:36,  3.18s/it]
2022-03-30 16:59:57,234 - INFO - tqdm - accuracy: 0.9858, batch_loss: 0.0407, loss: 0.0379 ||:   7%|7         | 22/295 [01:12<15:18,  3.36s/it]
2022-03-30 17:00:10,160 - INFO - tqdm - accuracy: 0.9856, batch_loss: 0.0048, loss: 0.0402 ||:   9%|8         | 26/295 [01:25<14:41,  3.28s/it]
2022-03-30 17:00:22,659 - INFO - tqdm - accuracy: 0.9864, batch_loss: 0.1319, loss: 0.0408 ||:  10%|#         | 30/295 [01:38<14:06,  3.19s/it]
2022-03-30 17:00:35,541 - INFO - tqdm - accuracy: 0.9871, batch_loss: 0.0280, loss: 0.0385 ||:  12%|#1        | 34/295 [01:51<13:54,  3.20s/it]
2022-03-30 17:00:48,374 - INFO - tqdm - accuracy: 0.9868, batch_loss: 0.0666, loss: 0.0371 ||:  13%|#2        | 38/295 [02:03<13:35,  3.17s/it]
2022-03-30 17:00:58,817 - INFO - tqdm - accuracy: 0.9855, batch_loss: 0.2317, loss: 0.0414 ||:  14%|#3        | 41/295 [02:14<14:05,  3.33s/it]
2022-03-30 17:01:11,668 - INFO - tqdm - accuracy: 0.9861, batch_loss: 0.0047, loss: 0.0400 ||:  15%|#5        | 45/295 [02:27<13:37,  3.27s/it]
2022-03-30 17:01:24,663 - INFO - tqdm - accuracy: 0.9866, batch_loss: 0.0076, loss: 0.0386 ||:  17%|#6        | 49/295 [02:40<13:17,  3.24s/it]
2022-03-30 17:01:36,271 - INFO - tqdm - accuracy: 0.9862, batch_loss: 0.0287, loss: 0.0397 ||:  18%|#7        | 52/295 [02:51<14:33,  3.60s/it]
2022-03-30 17:01:49,532 - INFO - tqdm - accuracy: 0.9866, batch_loss: 0.1344, loss: 0.0398 ||:  19%|#8        | 56/295 [03:05<13:39,  3.43s/it]
2022-03-30 17:01:59,808 - INFO - tqdm - accuracy: 0.9868, batch_loss: 0.1198, loss: 0.0401 ||:  20%|##        | 59/295 [03:15<13:28,  3.43s/it]
2022-03-30 17:02:10,244 - INFO - tqdm - accuracy: 0.9864, batch_loss: 0.0623, loss: 0.0400 ||:  21%|##1       | 62/295 [03:25<13:30,  3.48s/it]
2022-03-30 17:02:20,611 - INFO - tqdm - accuracy: 0.9870, batch_loss: 0.0106, loss: 0.0386 ||:  22%|##2       | 65/295 [03:36<13:17,  3.47s/it]
2022-03-30 17:02:30,614 - INFO - tqdm - accuracy: 0.9871, batch_loss: 0.0050, loss: 0.0397 ||:  23%|##3       | 68/295 [03:46<12:42,  3.36s/it]
2022-03-30 17:02:43,583 - INFO - tqdm - accuracy: 0.9874, batch_loss: 0.0101, loss: 0.0405 ||:  24%|##4       | 72/295 [03:59<12:13,  3.29s/it]
2022-03-30 17:02:54,555 - INFO - tqdm - accuracy: 0.9871, batch_loss: 0.0633, loss: 0.0407 ||:  25%|##5       | 75/295 [04:10<13:00,  3.55s/it]
2022-03-30 17:03:04,796 - INFO - tqdm - accuracy: 0.9868, batch_loss: 0.0101, loss: 0.0420 ||:  26%|##6       | 78/295 [04:20<12:35,  3.48s/it]
2022-03-30 17:03:18,614 - INFO - tqdm - accuracy: 0.9874, batch_loss: 0.0202, loss: 0.0406 ||:  28%|##7       | 82/295 [04:34<12:40,  3.57s/it]
2022-03-30 17:03:29,417 - INFO - tqdm - accuracy: 0.9879, batch_loss: 0.0202, loss: 0.0397 ||:  29%|##8       | 85/295 [04:45<12:26,  3.56s/it]
2022-03-30 17:03:42,699 - INFO - tqdm - accuracy: 0.9881, batch_loss: 0.0069, loss: 0.0412 ||:  30%|###       | 89/295 [04:58<11:44,  3.42s/it]
2022-03-30 17:03:56,385 - INFO - tqdm - accuracy: 0.9886, batch_loss: 0.0061, loss: 0.0399 ||:  32%|###1      | 93/295 [05:11<11:51,  3.52s/it]
2022-03-30 17:04:06,398 - INFO - tqdm - accuracy: 0.9886, batch_loss: 0.0434, loss: 0.0395 ||:  33%|###2      | 96/295 [05:22<11:18,  3.41s/it]
2022-03-30 17:04:16,658 - INFO - tqdm - accuracy: 0.9889, batch_loss: 0.0055, loss: 0.0385 ||:  34%|###3      | 99/295 [05:32<11:21,  3.48s/it]
2022-03-30 17:04:27,550 - INFO - tqdm - accuracy: 0.9893, batch_loss: 0.0142, loss: 0.0376 ||:  35%|###4      | 102/295 [05:43<11:20,  3.52s/it]
2022-03-30 17:04:37,818 - INFO - tqdm - accuracy: 0.9893, batch_loss: 0.0016, loss: 0.0382 ||:  36%|###5      | 105/295 [05:53<10:56,  3.46s/it]
2022-03-30 17:04:48,444 - INFO - tqdm - accuracy: 0.9890, batch_loss: 0.0449, loss: 0.0390 ||:  37%|###6      | 108/295 [06:04<10:46,  3.46s/it]
2022-03-30 17:04:59,054 - INFO - tqdm - accuracy: 0.9893, batch_loss: 0.0020, loss: 0.0380 ||:  38%|###7      | 111/295 [06:14<10:52,  3.54s/it]
2022-03-30 17:05:11,869 - INFO - tqdm - accuracy: 0.9894, batch_loss: 0.1147, loss: 0.0379 ||:  39%|###8      | 115/295 [06:27<09:51,  3.28s/it]
2022-03-30 17:05:24,457 - INFO - tqdm - accuracy: 0.9895, batch_loss: 0.0369, loss: 0.0372 ||:  40%|####      | 119/295 [06:40<09:29,  3.23s/it]
2022-03-30 17:05:37,229 - INFO - tqdm - accuracy: 0.9893, batch_loss: 0.0316, loss: 0.0368 ||:  42%|####1     | 123/295 [06:52<09:10,  3.20s/it]
2022-03-30 17:05:47,734 - INFO - tqdm - accuracy: 0.9893, batch_loss: 0.0278, loss: 0.0362 ||:  43%|####2     | 126/295 [07:03<09:33,  3.39s/it]
2022-03-30 17:05:59,678 - INFO - tqdm - accuracy: 0.9894, batch_loss: 0.0392, loss: 0.0356 ||:  44%|####4     | 130/295 [07:15<08:34,  3.12s/it]
2022-03-30 17:06:10,898 - INFO - tqdm - accuracy: 0.9892, batch_loss: 0.0378, loss: 0.0357 ||:  45%|####5     | 133/295 [07:26<09:24,  3.48s/it]
2022-03-30 17:06:23,528 - INFO - tqdm - accuracy: 0.9890, batch_loss: 0.0070, loss: 0.0358 ||:  46%|####6     | 137/295 [07:39<08:27,  3.21s/it]
2022-03-30 17:06:33,856 - INFO - tqdm - accuracy: 0.9893, batch_loss: 0.0128, loss: 0.0352 ||:  47%|####7     | 140/295 [07:49<08:30,  3.29s/it]
2022-03-30 17:06:46,350 - INFO - tqdm - accuracy: 0.9891, batch_loss: 0.1226, loss: 0.0355 ||:  49%|####8     | 144/295 [08:01<08:05,  3.21s/it]
2022-03-30 17:06:57,890 - INFO - tqdm - accuracy: 0.9892, batch_loss: 0.0273, loss: 0.0348 ||:  50%|#####     | 148/295 [08:13<07:17,  2.98s/it]
2022-03-30 17:07:08,809 - INFO - tqdm - accuracy: 0.9894, batch_loss: 0.0016, loss: 0.0342 ||:  51%|#####1    | 151/295 [08:24<08:21,  3.48s/it]
2022-03-30 17:07:22,115 - INFO - tqdm - accuracy: 0.9895, batch_loss: 0.0038, loss: 0.0336 ||:  53%|#####2    | 155/295 [08:37<07:54,  3.39s/it]
2022-03-30 17:07:34,862 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.0013, loss: 0.0337 ||:  54%|#####3    | 159/295 [08:50<07:14,  3.20s/it]
2022-03-30 17:07:47,508 - INFO - tqdm - accuracy: 0.9895, batch_loss: 0.0799, loss: 0.0345 ||:  55%|#####5    | 163/295 [09:03<06:59,  3.18s/it]
2022-03-30 17:08:01,058 - INFO - tqdm - accuracy: 0.9888, batch_loss: 0.0513, loss: 0.0355 ||:  57%|#####6    | 167/295 [09:16<07:11,  3.37s/it]
2022-03-30 17:08:14,331 - INFO - tqdm - accuracy: 0.9889, batch_loss: 0.0099, loss: 0.0351 ||:  58%|#####7    | 171/295 [09:29<07:08,  3.45s/it]
2022-03-30 17:08:27,635 - INFO - tqdm - accuracy: 0.9889, batch_loss: 0.0573, loss: 0.0347 ||:  59%|#####9    | 175/295 [09:43<06:54,  3.46s/it]
2022-03-30 17:08:40,337 - INFO - tqdm - accuracy: 0.9887, batch_loss: 0.0177, loss: 0.0351 ||:  61%|######    | 179/295 [09:55<06:22,  3.30s/it]
2022-03-30 17:08:50,569 - INFO - tqdm - accuracy: 0.9887, batch_loss: 0.0020, loss: 0.0351 ||:  62%|######1   | 182/295 [10:06<06:18,  3.35s/it]
2022-03-30 17:09:03,152 - INFO - tqdm - accuracy: 0.9882, batch_loss: 0.0976, loss: 0.0363 ||:  63%|######3   | 186/295 [10:18<05:47,  3.19s/it]
2022-03-30 17:09:16,483 - INFO - tqdm - accuracy: 0.9883, batch_loss: 0.0237, loss: 0.0360 ||:  64%|######4   | 190/295 [10:32<05:51,  3.35s/it]
2022-03-30 17:09:27,090 - INFO - tqdm - accuracy: 0.9882, batch_loss: 0.0052, loss: 0.0366 ||:  65%|######5   | 193/295 [10:42<05:57,  3.51s/it]
2022-03-30 17:09:39,662 - INFO - tqdm - accuracy: 0.9879, batch_loss: 0.0038, loss: 0.0376 ||:  67%|######6   | 197/295 [10:55<05:18,  3.25s/it]
2022-03-30 17:09:52,132 - INFO - tqdm - accuracy: 0.9876, batch_loss: 0.0696, loss: 0.0385 ||:  68%|######8   | 201/295 [11:07<04:52,  3.11s/it]
2022-03-30 17:10:02,983 - INFO - tqdm - accuracy: 0.9874, batch_loss: 0.0068, loss: 0.0385 ||:  69%|######9   | 204/295 [11:18<05:10,  3.41s/it]
2022-03-30 17:10:16,111 - INFO - tqdm - accuracy: 0.9874, batch_loss: 0.1390, loss: 0.0386 ||:  71%|#######   | 208/295 [11:31<04:50,  3.34s/it]
2022-03-30 17:10:27,532 - INFO - tqdm - accuracy: 0.9876, batch_loss: 0.0181, loss: 0.0382 ||:  72%|#######1  | 211/295 [11:43<05:11,  3.71s/it]
2022-03-30 17:10:38,828 - INFO - tqdm - accuracy: 0.9877, batch_loss: 0.0028, loss: 0.0379 ||:  73%|#######2  | 214/295 [11:54<05:02,  3.74s/it]
2022-03-30 17:10:48,836 - INFO - tqdm - accuracy: 0.9879, batch_loss: 0.0332, loss: 0.0376 ||:  74%|#######3  | 217/295 [12:04<04:35,  3.53s/it]
2022-03-30 17:11:02,408 - INFO - tqdm - accuracy: 0.9880, batch_loss: 0.0371, loss: 0.0374 ||:  75%|#######4  | 221/295 [12:18<04:17,  3.48s/it]
2022-03-30 17:11:15,960 - INFO - tqdm - accuracy: 0.9878, batch_loss: 0.0089, loss: 0.0375 ||:  76%|#######6  | 225/295 [12:31<04:03,  3.48s/it]
2022-03-30 17:11:29,247 - INFO - tqdm - accuracy: 0.9877, batch_loss: 0.0336, loss: 0.0375 ||:  78%|#######7  | 229/295 [12:44<03:42,  3.37s/it]
2022-03-30 17:11:39,392 - INFO - tqdm - accuracy: 0.9879, batch_loss: 0.0093, loss: 0.0371 ||:  79%|#######8  | 232/295 [12:55<03:36,  3.44s/it]
2022-03-30 17:11:52,034 - INFO - tqdm - accuracy: 0.9876, batch_loss: 0.2128, loss: 0.0379 ||:  80%|########  | 236/295 [13:07<03:09,  3.21s/it]
2022-03-30 17:12:03,786 - INFO - tqdm - accuracy: 0.9876, batch_loss: 0.0282, loss: 0.0378 ||:  81%|########1 | 239/295 [13:19<03:24,  3.65s/it]
2022-03-30 17:12:14,254 - INFO - tqdm - accuracy: 0.9876, batch_loss: 0.0030, loss: 0.0375 ||:  82%|########2 | 242/295 [13:29<03:05,  3.50s/it]
2022-03-30 17:12:24,572 - INFO - tqdm - accuracy: 0.9874, batch_loss: 0.1040, loss: 0.0381 ||:  83%|########3 | 245/295 [13:40<02:48,  3.37s/it]
2022-03-30 17:12:37,901 - INFO - tqdm - accuracy: 0.9876, batch_loss: 0.0089, loss: 0.0377 ||:  84%|########4 | 249/295 [13:53<02:34,  3.35s/it]
2022-03-30 17:12:48,061 - INFO - tqdm - accuracy: 0.9875, batch_loss: 0.0309, loss: 0.0380 ||:  85%|########5 | 252/295 [14:03<02:25,  3.38s/it]
2022-03-30 17:13:00,870 - INFO - tqdm - accuracy: 0.9872, batch_loss: 0.0890, loss: 0.0380 ||:  87%|########6 | 256/295 [14:16<02:05,  3.23s/it]
2022-03-30 17:13:13,395 - INFO - tqdm - accuracy: 0.9873, batch_loss: 0.0014, loss: 0.0378 ||:  88%|########8 | 260/295 [14:29<01:48,  3.10s/it]
2022-03-30 17:13:23,424 - INFO - tqdm - accuracy: 0.9874, batch_loss: 0.0028, loss: 0.0375 ||:  89%|########9 | 263/295 [14:39<01:44,  3.26s/it]
2022-03-30 17:13:35,864 - INFO - tqdm - accuracy: 0.9875, batch_loss: 0.1110, loss: 0.0375 ||:  91%|######### | 267/295 [14:51<01:28,  3.15s/it]
2022-03-30 17:13:46,030 - INFO - tqdm - accuracy: 0.9874, batch_loss: 0.0191, loss: 0.0377 ||:  92%|#########1| 270/295 [15:01<01:23,  3.34s/it]
2022-03-30 17:13:56,311 - INFO - tqdm - accuracy: 0.9873, batch_loss: 0.0179, loss: 0.0378 ||:  93%|#########2| 273/295 [15:11<01:15,  3.45s/it]
2022-03-30 17:14:09,705 - INFO - tqdm - accuracy: 0.9873, batch_loss: 0.0390, loss: 0.0377 ||:  94%|#########3| 277/295 [15:25<01:02,  3.46s/it]
2022-03-30 17:14:20,769 - INFO - tqdm - accuracy: 0.9873, batch_loss: 0.0614, loss: 0.0376 ||:  95%|#########4| 280/295 [15:36<00:54,  3.61s/it]
2022-03-30 17:14:33,683 - INFO - tqdm - accuracy: 0.9872, batch_loss: 0.1850, loss: 0.0380 ||:  96%|#########6| 284/295 [15:49<00:36,  3.31s/it]
2022-03-30 17:14:45,733 - INFO - tqdm - accuracy: 0.9872, batch_loss: 0.0371, loss: 0.0380 ||:  98%|#########7| 288/295 [16:01<00:21,  3.10s/it]
2022-03-30 17:14:55,901 - INFO - tqdm - accuracy: 0.9871, batch_loss: 0.0087, loss: 0.0381 ||:  99%|#########8| 291/295 [16:11<00:12,  3.24s/it]
2022-03-30 17:15:05,777 - INFO - tqdm - accuracy: 0.9871, batch_loss: 0.0394, loss: 0.0379 ||: 100%|#########9| 294/295 [16:21<00:03,  3.28s/it]
2022-03-30 17:15:08,045 - INFO - tqdm - accuracy: 0.9872, batch_loss: 0.0053, loss: 0.0378 ||: 100%|##########| 295/295 [16:23<00:00,  2.98s/it]
2022-03-30 17:15:08,045 - INFO - tqdm - accuracy: 0.9872, batch_loss: 0.0053, loss: 0.0378 ||: 100%|##########| 295/295 [16:23<00:00,  3.33s/it]
2022-03-30 17:15:10,384 - INFO - allennlp.training.trainer - Validating
2022-03-30 17:15:10,386 - INFO - tqdm - 0%|          | 0/1635 [00:00<?, ?it/s]
2022-03-30 17:15:20,463 - INFO - tqdm - accuracy: 0.8456, batch_loss: 3.2975, loss: 0.6964 ||:   9%|9         | 149/1635 [00:10<01:39, 14.93it/s]
2022-03-30 17:15:30,472 - INFO - tqdm - accuracy: 0.8633, batch_loss: 0.0289, loss: 0.5947 ||:  18%|#8        | 300/1635 [00:20<01:21, 16.34it/s]
2022-03-30 17:15:40,568 - INFO - tqdm - accuracy: 0.8624, batch_loss: 0.2239, loss: 0.6053 ||:  27%|##7       | 447/1635 [00:30<01:28, 13.38it/s]
2022-03-30 17:15:50,650 - INFO - tqdm - accuracy: 0.8638, batch_loss: 0.0037, loss: 0.5997 ||:  36%|###6      | 591/1635 [00:40<01:13, 14.19it/s]
2022-03-30 17:16:00,759 - INFO - tqdm - accuracy: 0.8529, batch_loss: 0.0002, loss: 0.6681 ||:  45%|####4     | 734/1635 [00:50<01:02, 14.36it/s]
2022-03-30 17:16:10,883 - INFO - tqdm - accuracy: 0.8563, batch_loss: 0.0009, loss: 0.6563 ||:  54%|#####4    | 887/1635 [01:00<00:52, 14.26it/s]
2022-03-30 17:16:21,006 - INFO - tqdm - accuracy: 0.8541, batch_loss: 6.7957, loss: 0.6694 ||:  63%|######2   | 1025/1635 [01:10<00:41, 14.79it/s]
2022-03-30 17:16:31,131 - INFO - tqdm - accuracy: 0.8576, batch_loss: 0.9467, loss: 0.6562 ||:  72%|#######1  | 1173/1635 [01:20<00:30, 15.16it/s]
2022-03-30 17:16:41,241 - INFO - tqdm - accuracy: 0.8550, batch_loss: 3.1851, loss: 0.6812 ||:  81%|########  | 1317/1635 [01:30<00:20, 15.41it/s]
2022-03-30 17:16:51,247 - INFO - tqdm - accuracy: 0.8576, batch_loss: 0.0009, loss: 0.6650 ||:  89%|########8 | 1454/1635 [01:40<00:13, 12.97it/s]
2022-03-30 17:17:01,368 - INFO - tqdm - accuracy: 0.8587, batch_loss: 2.4838, loss: 0.6596 ||:  98%|#########8| 1603/1635 [01:50<00:02, 13.02it/s]
2022-03-30 17:17:03,007 - INFO - tqdm - accuracy: 0.8589, batch_loss: 3.7144, loss: 0.6576 ||: 100%|#########9| 1627/1635 [01:52<00:00, 15.09it/s]
2022-03-30 17:17:03,109 - INFO - tqdm - accuracy: 0.8588, batch_loss: 0.0522, loss: 0.6588 ||: 100%|#########9| 1629/1635 [01:52<00:00, 16.10it/s]
2022-03-30 17:17:03,313 - INFO - tqdm - accuracy: 0.8587, batch_loss: 0.0002, loss: 0.6588 ||: 100%|#########9| 1631/1635 [01:52<00:00, 13.64it/s]
2022-03-30 17:17:03,429 - INFO - tqdm - accuracy: 0.8588, batch_loss: 0.0014, loss: 0.6581 ||: 100%|#########9| 1633/1635 [01:53<00:00, 14.52it/s]
2022-03-30 17:17:03,692 - INFO - tqdm - accuracy: 0.8590, batch_loss: 0.0002, loss: 0.6574 ||: 100%|##########| 1635/1635 [01:53<00:00, 11.48it/s]
2022-03-30 17:17:03,693 - INFO - tqdm - accuracy: 0.8590, batch_loss: 0.0002, loss: 0.6574 ||: 100%|##########| 1635/1635 [01:53<00:00, 14.43it/s]
2022-03-30 17:17:03,693 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-03-30 17:17:03,694 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.987  |     0.859
2022-03-30 17:17:03,695 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9572.260  |       N/A
2022-03-30 17:17:03,696 - INFO - allennlp.training.tensorboard_writer - loss               |     0.038  |     0.657
2022-03-30 17:17:03,696 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  7092.406  |       N/A
2022-03-30 17:17:08,596 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to './output_roberta-star/best.th'.
2022-03-30 17:17:20,177 - INFO - allennlp.training.trainer - Epoch duration: 0:18:35.791221
2022-03-30 17:17:20,178 - INFO - allennlp.training.trainer - Estimated training time remaining: 1:13:25
2022-03-30 17:17:20,178 - INFO - allennlp.training.trainer - Epoch 6/9
2022-03-30 17:17:20,178 - INFO - allennlp.training.trainer - Worker 0 memory usage: 6.9G
2022-03-30 17:17:20,178 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.3G
2022-03-30 17:17:20,180 - INFO - allennlp.training.trainer - Training
2022-03-30 17:17:20,180 - INFO - tqdm - 0%|          | 0/295 [00:00<?, ?it/s]
2022-03-30 17:17:32,593 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0448, loss: 0.0371 ||:   1%|1         | 4/295 [00:12<15:17,  3.15s/it]
2022-03-30 17:17:43,120 - INFO - tqdm - accuracy: 0.9866, batch_loss: 0.0076, loss: 0.0324 ||:   2%|2         | 7/295 [00:22<16:30,  3.44s/it]
2022-03-30 17:17:56,140 - INFO - tqdm - accuracy: 0.9858, batch_loss: 0.0230, loss: 0.0301 ||:   4%|3         | 11/295 [00:35<15:51,  3.35s/it]
2022-03-30 17:18:06,304 - INFO - tqdm - accuracy: 0.9866, batch_loss: 0.0030, loss: 0.0282 ||:   5%|4         | 14/295 [00:46<15:47,  3.37s/it]
2022-03-30 17:18:20,116 - INFO - tqdm - accuracy: 0.9878, batch_loss: 0.0048, loss: 0.0279 ||:   6%|6         | 18/295 [00:59<16:25,  3.56s/it]
2022-03-30 17:18:32,489 - INFO - tqdm - accuracy: 0.9901, batch_loss: 0.0325, loss: 0.0252 ||:   7%|7         | 22/295 [01:12<14:35,  3.21s/it]
2022-03-30 17:18:42,817 - INFO - tqdm - accuracy: 0.9912, batch_loss: 0.0023, loss: 0.0233 ||:   8%|8         | 25/295 [01:22<15:23,  3.42s/it]
2022-03-30 17:18:52,954 - INFO - tqdm - accuracy: 0.9911, batch_loss: 0.0051, loss: 0.0236 ||:   9%|9         | 28/295 [01:32<14:59,  3.37s/it]
2022-03-30 17:19:03,368 - INFO - tqdm - accuracy: 0.9919, batch_loss: 0.0013, loss: 0.0218 ||:  11%|#         | 31/295 [01:43<15:03,  3.42s/it]
2022-03-30 17:19:13,441 - INFO - tqdm - accuracy: 0.9926, batch_loss: 0.0015, loss: 0.0204 ||:  12%|#1        | 34/295 [01:53<14:43,  3.38s/it]
2022-03-30 17:19:26,882 - INFO - tqdm - accuracy: 0.9926, batch_loss: 0.0110, loss: 0.0195 ||:  13%|#2        | 38/295 [02:06<14:45,  3.45s/it]
2022-03-30 17:19:39,937 - INFO - tqdm - accuracy: 0.9926, batch_loss: 0.0014, loss: 0.0196 ||:  14%|#4        | 42/295 [02:19<13:52,  3.29s/it]
2022-03-30 17:19:49,947 - INFO - tqdm - accuracy: 0.9917, batch_loss: 0.0022, loss: 0.0232 ||:  15%|#5        | 45/295 [02:29<13:48,  3.31s/it]
2022-03-30 17:20:00,516 - INFO - tqdm - accuracy: 0.9922, batch_loss: 0.0023, loss: 0.0223 ||:  16%|#6        | 48/295 [02:40<14:13,  3.45s/it]
2022-03-30 17:20:13,037 - INFO - tqdm - accuracy: 0.9922, batch_loss: 0.0013, loss: 0.0262 ||:  18%|#7        | 52/295 [02:52<12:50,  3.17s/it]
2022-03-30 17:20:26,623 - INFO - tqdm - accuracy: 0.9927, batch_loss: 0.0031, loss: 0.0249 ||:  19%|#8        | 56/295 [03:06<13:28,  3.38s/it]
2022-03-30 17:20:39,495 - INFO - tqdm - accuracy: 0.9932, batch_loss: 0.0070, loss: 0.0235 ||:  20%|##        | 60/295 [03:19<12:43,  3.25s/it]
2022-03-30 17:20:50,526 - INFO - tqdm - accuracy: 0.9921, batch_loss: 0.0403, loss: 0.0251 ||:  21%|##1       | 63/295 [03:30<13:25,  3.47s/it]
2022-03-30 17:21:03,634 - INFO - tqdm - accuracy: 0.9916, batch_loss: 0.0013, loss: 0.0253 ||:  23%|##2       | 67/295 [03:43<12:47,  3.36s/it]
2022-03-30 17:21:13,865 - INFO - tqdm - accuracy: 0.9920, batch_loss: 0.0036, loss: 0.0244 ||:  24%|##3       | 70/295 [03:53<12:48,  3.42s/it]
2022-03-30 17:21:24,550 - INFO - tqdm - accuracy: 0.9923, batch_loss: 0.0027, loss: 0.0236 ||:  25%|##4       | 73/295 [04:04<13:06,  3.54s/it]
2022-03-30 17:21:34,925 - INFO - tqdm - accuracy: 0.9922, batch_loss: 0.0017, loss: 0.0233 ||:  26%|##5       | 76/295 [04:14<12:37,  3.46s/it]
2022-03-30 17:21:48,084 - INFO - tqdm - accuracy: 0.9926, batch_loss: 0.0011, loss: 0.0223 ||:  27%|##7       | 80/295 [04:27<12:01,  3.35s/it]
2022-03-30 17:21:58,231 - INFO - tqdm - accuracy: 0.9921, batch_loss: 0.0215, loss: 0.0230 ||:  28%|##8       | 83/295 [04:38<11:49,  3.35s/it]
2022-03-30 17:22:08,271 - INFO - tqdm - accuracy: 0.9916, batch_loss: 0.0137, loss: 0.0242 ||:  29%|##9       | 86/295 [04:48<11:42,  3.36s/it]
2022-03-30 17:22:18,310 - INFO - tqdm - accuracy: 0.9916, batch_loss: 0.0014, loss: 0.0248 ||:  30%|###       | 89/295 [04:58<11:38,  3.39s/it]
2022-03-30 17:22:29,734 - INFO - tqdm - accuracy: 0.9918, batch_loss: 0.0090, loss: 0.0242 ||:  31%|###1      | 92/295 [05:09<12:08,  3.59s/it]
2022-03-30 17:22:42,183 - INFO - tqdm - accuracy: 0.9919, batch_loss: 0.1392, loss: 0.0250 ||:  33%|###2      | 96/295 [05:22<10:36,  3.20s/it]
2022-03-30 17:22:52,253 - INFO - tqdm - accuracy: 0.9921, batch_loss: 0.0091, loss: 0.0244 ||:  34%|###3      | 99/295 [05:32<10:51,  3.32s/it]
2022-03-30 17:23:04,404 - INFO - tqdm - accuracy: 0.9921, batch_loss: 0.0017, loss: 0.0242 ||:  35%|###4      | 103/295 [05:44<09:54,  3.10s/it]
2022-03-30 17:23:14,688 - INFO - tqdm - accuracy: 0.9915, batch_loss: 0.0154, loss: 0.0270 ||:  36%|###5      | 106/295 [05:54<10:25,  3.31s/it]
2022-03-30 17:23:27,341 - INFO - tqdm - accuracy: 0.9912, batch_loss: 0.0378, loss: 0.0272 ||:  37%|###7      | 110/295 [06:07<09:50,  3.19s/it]
2022-03-30 17:23:40,759 - INFO - tqdm - accuracy: 0.9915, batch_loss: 0.0048, loss: 0.0266 ||:  39%|###8      | 114/295 [06:20<10:12,  3.38s/it]
2022-03-30 17:23:54,213 - INFO - tqdm - accuracy: 0.9918, batch_loss: 0.0114, loss: 0.0262 ||:  40%|####      | 118/295 [06:34<10:01,  3.40s/it]
2022-03-30 17:24:04,379 - INFO - tqdm - accuracy: 0.9917, batch_loss: 0.0089, loss: 0.0260 ||:  41%|####1     | 121/295 [06:44<09:48,  3.38s/it]
2022-03-30 17:24:15,108 - INFO - tqdm - accuracy: 0.9917, batch_loss: 0.0017, loss: 0.0263 ||:  42%|####2     | 124/295 [06:54<09:59,  3.51s/it]
2022-03-30 17:24:26,140 - INFO - tqdm - accuracy: 0.9916, batch_loss: 0.0124, loss: 0.0261 ||:  43%|####3     | 127/295 [07:05<10:02,  3.59s/it]
2022-03-30 17:24:38,701 - INFO - tqdm - accuracy: 0.9914, batch_loss: 0.0899, loss: 0.0262 ||:  44%|####4     | 131/295 [07:18<08:55,  3.27s/it]
2022-03-30 17:24:51,367 - INFO - tqdm - accuracy: 0.9910, batch_loss: 0.0038, loss: 0.0273 ||:  46%|####5     | 135/295 [07:31<08:29,  3.18s/it]
2022-03-30 17:25:03,162 - INFO - tqdm - accuracy: 0.9909, batch_loss: 0.0119, loss: 0.0273 ||:  47%|####6     | 138/295 [07:42<09:30,  3.64s/it]
2022-03-30 17:25:13,351 - INFO - tqdm - accuracy: 0.9911, batch_loss: 0.0013, loss: 0.0269 ||:  48%|####7     | 141/295 [07:53<08:49,  3.44s/it]
2022-03-30 17:25:25,797 - INFO - tqdm - accuracy: 0.9914, batch_loss: 0.0027, loss: 0.0264 ||:  49%|####9     | 145/295 [08:05<07:56,  3.18s/it]
2022-03-30 17:25:38,694 - INFO - tqdm - accuracy: 0.9914, batch_loss: 0.0041, loss: 0.0262 ||:  51%|#####     | 149/295 [08:18<07:52,  3.24s/it]
2022-03-30 17:25:52,196 - INFO - tqdm - accuracy: 0.9912, batch_loss: 0.0323, loss: 0.0261 ||:  52%|#####1    | 153/295 [08:32<08:10,  3.45s/it]
2022-03-30 17:26:02,764 - INFO - tqdm - accuracy: 0.9912, batch_loss: 0.0018, loss: 0.0259 ||:  53%|#####2    | 156/295 [08:42<08:16,  3.57s/it]
2022-03-30 17:26:15,980 - INFO - tqdm - accuracy: 0.9914, batch_loss: 0.0152, loss: 0.0255 ||:  54%|#####4    | 160/295 [08:55<07:34,  3.37s/it]
2022-03-30 17:26:27,420 - INFO - tqdm - accuracy: 0.9914, batch_loss: 0.0213, loss: 0.0267 ||:  55%|#####5    | 163/295 [09:07<07:56,  3.61s/it]
2022-03-30 17:26:40,701 - INFO - tqdm - accuracy: 0.9912, batch_loss: 0.0713, loss: 0.0267 ||:  57%|#####6    | 167/295 [09:20<07:20,  3.44s/it]
2022-03-30 17:26:53,323 - INFO - tqdm - accuracy: 0.9909, batch_loss: 0.2225, loss: 0.0275 ||:  58%|#####7    | 171/295 [09:33<06:39,  3.22s/it]
2022-03-30 17:27:03,895 - INFO - tqdm - accuracy: 0.9907, batch_loss: 0.2895, loss: 0.0288 ||:  59%|#####8    | 174/295 [09:43<06:50,  3.39s/it]
2022-03-30 17:27:17,309 - INFO - tqdm - accuracy: 0.9907, batch_loss: 0.0010, loss: 0.0285 ||:  60%|######    | 178/295 [09:57<06:33,  3.36s/it]
2022-03-30 17:27:29,610 - INFO - tqdm - accuracy: 0.9907, batch_loss: 0.0126, loss: 0.0293 ||:  62%|######1   | 182/295 [10:09<06:01,  3.20s/it]
2022-03-30 17:27:40,089 - INFO - tqdm - accuracy: 0.9907, batch_loss: 0.0929, loss: 0.0294 ||:  63%|######2   | 185/295 [10:19<06:16,  3.42s/it]
2022-03-30 17:27:53,106 - INFO - tqdm - accuracy: 0.9909, batch_loss: 0.0049, loss: 0.0289 ||:  64%|######4   | 189/295 [10:32<05:49,  3.30s/it]
2022-03-30 17:28:05,691 - INFO - tqdm - accuracy: 0.9909, batch_loss: 0.0028, loss: 0.0289 ||:  65%|######5   | 193/295 [10:45<05:24,  3.18s/it]
2022-03-30 17:28:16,054 - INFO - tqdm - accuracy: 0.9911, batch_loss: 0.0074, loss: 0.0286 ||:  66%|######6   | 196/295 [10:55<05:37,  3.41s/it]
2022-03-30 17:28:28,798 - INFO - tqdm - accuracy: 0.9909, batch_loss: 0.1128, loss: 0.0289 ||:  68%|######7   | 200/295 [11:08<05:10,  3.27s/it]
2022-03-30 17:28:41,424 - INFO - tqdm - accuracy: 0.9910, batch_loss: 0.0028, loss: 0.0294 ||:  69%|######9   | 204/295 [11:21<04:52,  3.21s/it]
2022-03-30 17:28:54,188 - INFO - tqdm - accuracy: 0.9910, batch_loss: 0.0550, loss: 0.0292 ||:  71%|#######   | 208/295 [11:34<04:35,  3.17s/it]
2022-03-30 17:29:04,485 - INFO - tqdm - accuracy: 0.9910, batch_loss: 0.0027, loss: 0.0290 ||:  72%|#######1  | 211/295 [11:44<04:42,  3.36s/it]
2022-03-30 17:29:14,724 - INFO - tqdm - accuracy: 0.9909, batch_loss: 0.0050, loss: 0.0290 ||:  73%|#######2  | 214/295 [11:54<04:29,  3.33s/it]
2022-03-30 17:29:25,251 - INFO - tqdm - accuracy: 0.9908, batch_loss: 0.0178, loss: 0.0291 ||:  74%|#######3  | 217/295 [12:05<04:29,  3.46s/it]
2022-03-30 17:29:35,408 - INFO - tqdm - accuracy: 0.9906, batch_loss: 0.0340, loss: 0.0291 ||:  75%|#######4  | 220/295 [12:15<04:14,  3.39s/it]
2022-03-30 17:29:46,307 - INFO - tqdm - accuracy: 0.9906, batch_loss: 0.0026, loss: 0.0291 ||:  76%|#######5  | 223/295 [12:26<04:10,  3.48s/it]
2022-03-30 17:29:56,680 - INFO - tqdm - accuracy: 0.9907, batch_loss: 0.0225, loss: 0.0289 ||:  77%|#######6  | 226/295 [12:36<03:57,  3.44s/it]
2022-03-30 17:30:09,870 - INFO - tqdm - accuracy: 0.9906, batch_loss: 0.0401, loss: 0.0294 ||:  78%|#######7  | 230/295 [12:49<03:37,  3.34s/it]
2022-03-30 17:30:20,510 - INFO - tqdm - accuracy: 0.9907, batch_loss: 0.0063, loss: 0.0291 ||:  79%|#######8  | 233/295 [13:00<03:37,  3.51s/it]
2022-03-30 17:30:33,293 - INFO - tqdm - accuracy: 0.9909, batch_loss: 0.0061, loss: 0.0288 ||:  80%|########  | 237/295 [13:13<03:09,  3.27s/it]
2022-03-30 17:30:43,610 - INFO - tqdm - accuracy: 0.9908, batch_loss: 0.0946, loss: 0.0291 ||:  81%|########1 | 240/295 [13:23<03:08,  3.43s/it]
2022-03-30 17:30:53,659 - INFO - tqdm - accuracy: 0.9909, batch_loss: 0.0017, loss: 0.0288 ||:  82%|########2 | 243/295 [13:33<02:55,  3.38s/it]
2022-03-30 17:31:05,561 - INFO - tqdm - accuracy: 0.9909, batch_loss: 0.0018, loss: 0.0290 ||:  84%|########3 | 247/295 [13:45<02:31,  3.15s/it]
2022-03-30 17:31:15,720 - INFO - tqdm - accuracy: 0.9910, batch_loss: 0.0058, loss: 0.0287 ||:  85%|########4 | 250/295 [13:55<02:26,  3.26s/it]
2022-03-30 17:31:28,409 - INFO - tqdm - accuracy: 0.9909, batch_loss: 0.2453, loss: 0.0296 ||:  86%|########6 | 254/295 [14:08<02:08,  3.12s/it]
2022-03-30 17:31:41,313 - INFO - tqdm - accuracy: 0.9909, batch_loss: 0.0038, loss: 0.0293 ||:  87%|########7 | 258/295 [14:21<01:58,  3.21s/it]
2022-03-30 17:31:51,401 - INFO - tqdm - accuracy: 0.9910, batch_loss: 0.0223, loss: 0.0291 ||:  88%|########8 | 261/295 [14:31<01:52,  3.31s/it]
2022-03-30 17:32:01,545 - INFO - tqdm - accuracy: 0.9911, batch_loss: 0.0145, loss: 0.0288 ||:  89%|########9 | 264/295 [14:41<01:45,  3.40s/it]
2022-03-30 17:32:12,119 - INFO - tqdm - accuracy: 0.9911, batch_loss: 0.0018, loss: 0.0287 ||:  91%|######### | 267/295 [14:51<01:37,  3.50s/it]
2022-03-30 17:32:25,163 - INFO - tqdm - accuracy: 0.9910, batch_loss: 0.0294, loss: 0.0287 ||:  92%|#########1| 271/295 [15:04<01:19,  3.33s/it]
2022-03-30 17:32:37,571 - INFO - tqdm - accuracy: 0.9911, batch_loss: 0.0054, loss: 0.0283 ||:  93%|#########3| 275/295 [15:17<01:03,  3.17s/it]
2022-03-30 17:32:48,056 - INFO - tqdm - accuracy: 0.9910, batch_loss: 0.0178, loss: 0.0287 ||:  94%|#########4| 278/295 [15:27<00:58,  3.42s/it]
2022-03-30 17:32:58,447 - INFO - tqdm - accuracy: 0.9910, batch_loss: 0.0034, loss: 0.0287 ||:  95%|#########5| 281/295 [15:38<00:49,  3.51s/it]
2022-03-30 17:33:10,872 - INFO - tqdm - accuracy: 0.9911, batch_loss: 0.0222, loss: 0.0284 ||:  97%|#########6| 285/295 [15:50<00:32,  3.21s/it]
2022-03-30 17:33:21,147 - INFO - tqdm - accuracy: 0.9912, batch_loss: 0.0283, loss: 0.0282 ||:  98%|#########7| 288/295 [16:00<00:23,  3.37s/it]
2022-03-30 17:33:31,296 - INFO - tqdm - accuracy: 0.9911, batch_loss: 0.0008, loss: 0.0288 ||:  99%|#########8| 291/295 [16:11<00:13,  3.40s/it]
2022-03-30 17:33:41,715 - INFO - tqdm - accuracy: 0.9909, batch_loss: 0.1650, loss: 0.0292 ||: 100%|#########9| 294/295 [16:21<00:03,  3.38s/it]
2022-03-30 17:33:44,042 - INFO - tqdm - accuracy: 0.9909, batch_loss: 0.0023, loss: 0.0291 ||: 100%|##########| 295/295 [16:23<00:00,  3.06s/it]
2022-03-30 17:33:44,043 - INFO - tqdm - accuracy: 0.9909, batch_loss: 0.0023, loss: 0.0291 ||: 100%|##########| 295/295 [16:23<00:00,  3.34s/it]
2022-03-30 17:33:46,439 - INFO - allennlp.training.trainer - Validating
2022-03-30 17:33:46,441 - INFO - tqdm - 0%|          | 0/1635 [00:00<?, ?it/s]
2022-03-30 17:33:56,540 - INFO - tqdm - accuracy: 0.8289, batch_loss: 0.0007, loss: 0.7828 ||:   9%|9         | 149/1635 [00:10<01:21, 18.34it/s]
2022-03-30 17:34:06,557 - INFO - tqdm - accuracy: 0.8471, batch_loss: 0.0398, loss: 0.6986 ||:  18%|#7        | 291/1635 [00:20<01:35, 14.02it/s]
2022-03-30 17:34:16,655 - INFO - tqdm - accuracy: 0.8410, batch_loss: 3.4128, loss: 0.7587 ||:  27%|##6       | 434/1635 [00:30<01:28, 13.57it/s]
2022-03-30 17:34:26,813 - INFO - tqdm - accuracy: 0.8467, batch_loss: 0.0009, loss: 0.7615 ||:  35%|###5      | 574/1635 [00:40<01:27, 12.16it/s]
2022-03-30 17:34:37,037 - INFO - tqdm - accuracy: 0.8502, batch_loss: 0.7041, loss: 0.7507 ||:  44%|####4     | 721/1635 [00:50<01:18, 11.67it/s]
2022-03-30 17:34:47,083 - INFO - tqdm - accuracy: 0.8495, batch_loss: 5.8162, loss: 0.7512 ||:  54%|#####3    | 877/1635 [01:00<00:51, 14.61it/s]
2022-03-30 17:34:57,160 - INFO - tqdm - accuracy: 0.8511, batch_loss: 0.0002, loss: 0.7559 ||:  63%|######2   | 1024/1635 [01:10<00:35, 17.09it/s]
2022-03-30 17:35:07,302 - INFO - tqdm - accuracy: 0.8485, batch_loss: 3.5123, loss: 0.7639 ||:  72%|#######1  | 1172/1635 [01:20<00:51,  9.03it/s]
2022-03-30 17:35:17,398 - INFO - tqdm - accuracy: 0.8524, batch_loss: 0.0021, loss: 0.7558 ||:  80%|########  | 1311/1635 [01:30<00:20, 15.47it/s]
2022-03-30 17:35:27,485 - INFO - tqdm - accuracy: 0.8521, batch_loss: 0.0004, loss: 0.7545 ||:  90%|########9 | 1464/1635 [01:41<00:10, 16.90it/s]
2022-03-30 17:35:37,568 - INFO - tqdm - accuracy: 0.8548, batch_loss: 0.0005, loss: 0.7370 ||:  98%|#########8| 1605/1635 [01:51<00:02, 11.25it/s]
2022-03-30 17:35:39,154 - INFO - tqdm - accuracy: 0.8553, batch_loss: 0.0004, loss: 0.7344 ||: 100%|#########9| 1628/1635 [01:52<00:00, 14.59it/s]
2022-03-30 17:35:39,301 - INFO - tqdm - accuracy: 0.8552, batch_loss: 0.0010, loss: 0.7351 ||: 100%|#########9| 1630/1635 [01:52<00:00, 14.28it/s]
2022-03-30 17:35:39,445 - INFO - tqdm - accuracy: 0.8551, batch_loss: 0.0004, loss: 0.7367 ||: 100%|#########9| 1632/1635 [01:53<00:00, 14.18it/s]
2022-03-30 17:35:39,555 - INFO - tqdm - accuracy: 0.8550, batch_loss: 3.7909, loss: 0.7381 ||: 100%|#########9| 1634/1635 [01:53<00:00, 15.16it/s]
2022-03-30 17:35:39,601 - INFO - tqdm - accuracy: 0.8550, batch_loss: 0.0005, loss: 0.7377 ||: 100%|##########| 1635/1635 [01:53<00:00, 14.45it/s]
2022-03-30 17:35:39,601 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-03-30 17:35:39,602 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.991  |     0.855
2022-03-30 17:35:39,603 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9572.260  |       N/A
2022-03-30 17:35:39,603 - INFO - allennlp.training.tensorboard_writer - loss               |     0.029  |     0.738
2022-03-30 17:35:39,604 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  7092.406  |       N/A
2022-03-30 17:35:44,563 - INFO - allennlp.training.trainer - Epoch duration: 0:18:24.385639
2022-03-30 17:35:44,564 - INFO - allennlp.training.trainer - Estimated training time remaining: 0:55:05
2022-03-30 17:35:44,564 - INFO - allennlp.training.trainer - Epoch 7/9
2022-03-30 17:35:44,564 - INFO - allennlp.training.trainer - Worker 0 memory usage: 6.9G
2022-03-30 17:35:44,564 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.3G
2022-03-30 17:35:44,565 - INFO - allennlp.training.trainer - Training
2022-03-30 17:35:44,566 - INFO - tqdm - 0%|          | 0/295 [00:00<?, ?it/s]
2022-03-30 17:35:55,352 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0018, loss: 0.0043 ||:   1%|1         | 3/295 [00:10<17:52,  3.67s/it]
2022-03-30 17:36:08,085 - INFO - tqdm - accuracy: 0.9911, batch_loss: 0.0026, loss: 0.0189 ||:   2%|2         | 7/295 [00:23<15:35,  3.25s/it]
2022-03-30 17:36:18,497 - INFO - tqdm - accuracy: 0.9906, batch_loss: 0.0118, loss: 0.0188 ||:   3%|3         | 10/295 [00:33<16:01,  3.37s/it]
2022-03-30 17:36:29,606 - INFO - tqdm - accuracy: 0.9904, batch_loss: 0.0010, loss: 0.0177 ||:   4%|4         | 13/295 [00:45<16:41,  3.55s/it]
2022-03-30 17:36:41,951 - INFO - tqdm - accuracy: 0.9926, batch_loss: 0.0035, loss: 0.0152 ||:   6%|5         | 17/295 [00:57<15:09,  3.27s/it]
2022-03-30 17:36:55,682 - INFO - tqdm - accuracy: 0.9925, batch_loss: 0.0035, loss: 0.0176 ||:   7%|7         | 21/295 [01:11<15:46,  3.45s/it]
2022-03-30 17:37:06,455 - INFO - tqdm - accuracy: 0.9935, batch_loss: 0.0242, loss: 0.0171 ||:   8%|8         | 24/295 [01:21<16:00,  3.54s/it]
2022-03-30 17:37:16,719 - INFO - tqdm - accuracy: 0.9942, batch_loss: 0.0186, loss: 0.0161 ||:   9%|9         | 27/295 [01:32<15:46,  3.53s/it]
2022-03-30 17:37:26,965 - INFO - tqdm - accuracy: 0.9948, batch_loss: 0.0027, loss: 0.0148 ||:  10%|#         | 30/295 [01:42<15:20,  3.47s/it]
2022-03-30 17:37:36,967 - INFO - tqdm - accuracy: 0.9953, batch_loss: 0.0032, loss: 0.0143 ||:  11%|#1        | 33/295 [01:52<14:33,  3.33s/it]
2022-03-30 17:37:47,913 - INFO - tqdm - accuracy: 0.9957, batch_loss: 0.0031, loss: 0.0140 ||:  12%|#2        | 36/295 [02:03<15:12,  3.52s/it]
2022-03-30 17:37:58,048 - INFO - tqdm - accuracy: 0.9960, batch_loss: 0.0142, loss: 0.0137 ||:  13%|#3        | 39/295 [02:13<14:41,  3.44s/it]
2022-03-30 17:38:11,159 - INFO - tqdm - accuracy: 0.9964, batch_loss: 0.0010, loss: 0.0127 ||:  15%|#4        | 43/295 [02:26<14:03,  3.35s/it]
2022-03-30 17:38:21,363 - INFO - tqdm - accuracy: 0.9966, batch_loss: 0.0017, loss: 0.0120 ||:  16%|#5        | 46/295 [02:36<13:51,  3.34s/it]
2022-03-30 17:38:31,393 - INFO - tqdm - accuracy: 0.9968, batch_loss: 0.0031, loss: 0.0114 ||:  17%|#6        | 49/295 [02:46<13:38,  3.33s/it]
2022-03-30 17:38:41,595 - INFO - tqdm - accuracy: 0.9958, batch_loss: 0.0016, loss: 0.0125 ||:  18%|#7        | 52/295 [02:57<13:26,  3.32s/it]
2022-03-30 17:38:51,623 - INFO - tqdm - accuracy: 0.9955, batch_loss: 0.0036, loss: 0.0168 ||:  19%|#8        | 55/295 [03:07<13:18,  3.33s/it]
2022-03-30 17:39:04,248 - INFO - tqdm - accuracy: 0.9947, batch_loss: 0.0021, loss: 0.0179 ||:  20%|##        | 59/295 [03:19<12:46,  3.25s/it]
2022-03-30 17:39:14,514 - INFO - tqdm - accuracy: 0.9939, batch_loss: 0.0023, loss: 0.0192 ||:  21%|##1       | 62/295 [03:29<13:02,  3.36s/it]
2022-03-30 17:39:26,965 - INFO - tqdm - accuracy: 0.9943, batch_loss: 0.0012, loss: 0.0181 ||:  22%|##2       | 66/295 [03:42<12:08,  3.18s/it]
2022-03-30 17:39:37,124 - INFO - tqdm - accuracy: 0.9937, batch_loss: 0.0015, loss: 0.0182 ||:  23%|##3       | 69/295 [03:52<12:41,  3.37s/it]
2022-03-30 17:39:47,848 - INFO - tqdm - accuracy: 0.9935, batch_loss: 0.0011, loss: 0.0182 ||:  24%|##4       | 72/295 [04:03<12:43,  3.42s/it]
2022-03-30 17:39:58,484 - INFO - tqdm - accuracy: 0.9933, batch_loss: 0.0017, loss: 0.0179 ||:  25%|##5       | 75/295 [04:13<12:52,  3.51s/it]
2022-03-30 17:40:11,272 - INFO - tqdm - accuracy: 0.9933, batch_loss: 0.0153, loss: 0.0185 ||:  27%|##6       | 79/295 [04:26<12:04,  3.36s/it]
2022-03-30 17:40:24,220 - INFO - tqdm - accuracy: 0.9936, batch_loss: 0.0053, loss: 0.0178 ||:  28%|##8       | 83/295 [04:39<11:25,  3.23s/it]
2022-03-30 17:40:37,503 - INFO - tqdm - accuracy: 0.9939, batch_loss: 0.0021, loss: 0.0173 ||:  29%|##9       | 87/295 [04:52<11:37,  3.35s/it]
2022-03-30 17:40:50,144 - INFO - tqdm - accuracy: 0.9938, batch_loss: 0.0011, loss: 0.0180 ||:  31%|###       | 91/295 [05:05<10:43,  3.15s/it]
2022-03-30 17:41:00,254 - INFO - tqdm - accuracy: 0.9933, batch_loss: 0.0532, loss: 0.0186 ||:  32%|###1      | 94/295 [05:15<10:46,  3.21s/it]
2022-03-30 17:41:12,887 - INFO - tqdm - accuracy: 0.9930, batch_loss: 0.0390, loss: 0.0192 ||:  33%|###3      | 98/295 [05:28<10:31,  3.21s/it]
2022-03-30 17:41:23,295 - INFO - tqdm - accuracy: 0.9932, batch_loss: 0.0119, loss: 0.0188 ||:  34%|###4      | 101/295 [05:38<10:40,  3.30s/it]
2022-03-30 17:41:33,790 - INFO - tqdm - accuracy: 0.9934, batch_loss: 0.0010, loss: 0.0185 ||:  35%|###5      | 104/295 [05:49<11:02,  3.47s/it]
2022-03-30 17:41:45,029 - INFO - tqdm - accuracy: 0.9930, batch_loss: 0.0308, loss: 0.0196 ||:  36%|###6      | 107/295 [06:00<11:22,  3.63s/it]
2022-03-30 17:41:58,014 - INFO - tqdm - accuracy: 0.9932, batch_loss: 0.0087, loss: 0.0194 ||:  38%|###7      | 111/295 [06:13<10:26,  3.40s/it]
2022-03-30 17:42:10,707 - INFO - tqdm - accuracy: 0.9935, batch_loss: 0.0016, loss: 0.0188 ||:  39%|###8      | 115/295 [06:26<09:55,  3.31s/it]
2022-03-30 17:42:23,561 - INFO - tqdm - accuracy: 0.9934, batch_loss: 0.0008, loss: 0.0186 ||:  40%|####      | 119/295 [06:38<09:12,  3.14s/it]
2022-03-30 17:42:37,261 - INFO - tqdm - accuracy: 0.9931, batch_loss: 0.0067, loss: 0.0192 ||:  42%|####1     | 123/295 [06:52<09:51,  3.44s/it]
2022-03-30 17:42:49,847 - INFO - tqdm - accuracy: 0.9926, batch_loss: 0.0016, loss: 0.0203 ||:  43%|####3     | 127/295 [07:05<09:05,  3.25s/it]
2022-03-30 17:43:02,809 - INFO - tqdm - accuracy: 0.9926, batch_loss: 0.0851, loss: 0.0205 ||:  44%|####4     | 131/295 [07:18<08:54,  3.26s/it]
2022-03-30 17:43:13,057 - INFO - tqdm - accuracy: 0.9923, batch_loss: 0.0064, loss: 0.0206 ||:  45%|####5     | 134/295 [07:28<09:00,  3.36s/it]
2022-03-30 17:43:24,028 - INFO - tqdm - accuracy: 0.9922, batch_loss: 0.0371, loss: 0.0205 ||:  46%|####6     | 137/295 [07:39<09:25,  3.58s/it]
2022-03-30 17:43:34,161 - INFO - tqdm - accuracy: 0.9924, batch_loss: 0.0029, loss: 0.0201 ||:  47%|####7     | 140/295 [07:49<08:56,  3.46s/it]
2022-03-30 17:43:44,865 - INFO - tqdm - accuracy: 0.9926, batch_loss: 0.0029, loss: 0.0199 ||:  48%|####8     | 143/295 [08:00<09:09,  3.61s/it]
2022-03-30 17:43:57,487 - INFO - tqdm - accuracy: 0.9926, batch_loss: 0.0032, loss: 0.0196 ||:  50%|####9     | 147/295 [08:12<07:58,  3.24s/it]
2022-03-30 17:44:10,320 - INFO - tqdm - accuracy: 0.9921, batch_loss: 0.0009, loss: 0.0209 ||:  51%|#####1    | 151/295 [08:25<07:49,  3.26s/it]
2022-03-30 17:44:21,461 - INFO - tqdm - accuracy: 0.9921, batch_loss: 0.0014, loss: 0.0209 ||:  52%|#####2    | 154/295 [08:36<08:28,  3.60s/it]
2022-03-30 17:44:34,718 - INFO - tqdm - accuracy: 0.9921, batch_loss: 0.0102, loss: 0.0208 ||:  54%|#####3    | 158/295 [08:50<07:41,  3.37s/it]
2022-03-30 17:44:47,232 - INFO - tqdm - accuracy: 0.9921, batch_loss: 0.0061, loss: 0.0207 ||:  55%|#####4    | 162/295 [09:02<07:04,  3.19s/it]
2022-03-30 17:44:59,588 - INFO - tqdm - accuracy: 0.9919, batch_loss: 0.2742, loss: 0.0219 ||:  56%|#####6    | 166/295 [09:15<06:38,  3.09s/it]
2022-03-30 17:45:13,076 - INFO - tqdm - accuracy: 0.9919, batch_loss: 0.0036, loss: 0.0219 ||:  58%|#####7    | 170/295 [09:28<06:55,  3.33s/it]
2022-03-30 17:45:26,161 - INFO - tqdm - accuracy: 0.9921, batch_loss: 0.0020, loss: 0.0214 ||:  59%|#####8    | 174/295 [09:41<06:28,  3.21s/it]
2022-03-30 17:45:36,560 - INFO - tqdm - accuracy: 0.9921, batch_loss: 0.0160, loss: 0.0215 ||:  60%|######    | 177/295 [09:51<06:41,  3.41s/it]
2022-03-30 17:45:47,422 - INFO - tqdm - accuracy: 0.9922, batch_loss: 0.0136, loss: 0.0214 ||:  61%|######1   | 180/295 [10:02<06:56,  3.62s/it]
2022-03-30 17:45:57,835 - INFO - tqdm - accuracy: 0.9921, batch_loss: 0.0038, loss: 0.0212 ||:  62%|######2   | 183/295 [10:13<06:39,  3.57s/it]
2022-03-30 17:46:07,853 - INFO - tqdm - accuracy: 0.9921, batch_loss: 0.0006, loss: 0.0212 ||:  63%|######3   | 186/295 [10:23<06:13,  3.43s/it]
2022-03-30 17:46:20,997 - INFO - tqdm - accuracy: 0.9913, batch_loss: 0.0028, loss: 0.0235 ||:  64%|######4   | 190/295 [10:36<05:54,  3.37s/it]
2022-03-30 17:46:31,116 - INFO - tqdm - accuracy: 0.9911, batch_loss: 0.0012, loss: 0.0245 ||:  65%|######5   | 193/295 [10:46<05:36,  3.30s/it]
2022-03-30 17:46:41,223 - INFO - tqdm - accuracy: 0.9911, batch_loss: 0.0158, loss: 0.0247 ||:  66%|######6   | 196/295 [10:56<05:28,  3.31s/it]
2022-03-30 17:46:53,150 - INFO - tqdm - accuracy: 0.9912, batch_loss: 0.0111, loss: 0.0245 ||:  67%|######7   | 199/295 [11:08<05:55,  3.70s/it]
2022-03-30 17:47:05,551 - INFO - tqdm - accuracy: 0.9914, batch_loss: 0.0011, loss: 0.0241 ||:  69%|######8   | 203/295 [11:20<05:04,  3.31s/it]
2022-03-30 17:47:16,543 - INFO - tqdm - accuracy: 0.9915, batch_loss: 0.0024, loss: 0.0238 ||:  70%|######9   | 206/295 [11:31<05:11,  3.50s/it]
2022-03-30 17:47:29,155 - INFO - tqdm - accuracy: 0.9917, batch_loss: 0.0040, loss: 0.0235 ||:  71%|#######1  | 210/295 [11:44<04:31,  3.20s/it]
2022-03-30 17:47:39,233 - INFO - tqdm - accuracy: 0.9918, batch_loss: 0.0103, loss: 0.0233 ||:  72%|#######2  | 213/295 [11:54<04:34,  3.34s/it]
2022-03-30 17:47:49,569 - INFO - tqdm - accuracy: 0.9919, batch_loss: 0.0104, loss: 0.0231 ||:  73%|#######3  | 216/295 [12:05<04:25,  3.36s/it]
2022-03-30 17:47:59,705 - INFO - tqdm - accuracy: 0.9920, batch_loss: 0.0091, loss: 0.0229 ||:  74%|#######4  | 219/295 [12:15<04:19,  3.41s/it]
2022-03-30 17:48:13,189 - INFO - tqdm - accuracy: 0.9922, batch_loss: 0.0017, loss: 0.0226 ||:  76%|#######5  | 223/295 [12:28<04:04,  3.40s/it]
2022-03-30 17:48:25,684 - INFO - tqdm - accuracy: 0.9922, batch_loss: 0.0364, loss: 0.0224 ||:  77%|#######6  | 227/295 [12:41<03:38,  3.21s/it]
2022-03-30 17:48:38,442 - INFO - tqdm - accuracy: 0.9922, batch_loss: 0.0144, loss: 0.0228 ||:  78%|#######8  | 231/295 [12:53<03:23,  3.18s/it]
2022-03-30 17:48:51,341 - INFO - tqdm - accuracy: 0.9922, batch_loss: 0.0022, loss: 0.0226 ||:  80%|#######9  | 235/295 [13:06<03:22,  3.37s/it]
2022-03-30 17:49:04,329 - INFO - tqdm - accuracy: 0.9923, batch_loss: 0.0009, loss: 0.0223 ||:  81%|########1 | 239/295 [13:19<03:07,  3.35s/it]
2022-03-30 17:49:16,669 - INFO - tqdm - accuracy: 0.9922, batch_loss: 0.0042, loss: 0.0231 ||:  82%|########2 | 243/295 [13:32<02:42,  3.13s/it]
2022-03-30 17:49:27,125 - INFO - tqdm - accuracy: 0.9923, batch_loss: 0.0007, loss: 0.0229 ||:  83%|########3 | 246/295 [13:42<02:45,  3.37s/it]
2022-03-30 17:49:39,408 - INFO - tqdm - accuracy: 0.9922, batch_loss: 0.0512, loss: 0.0228 ||:  85%|########4 | 250/295 [13:54<02:20,  3.13s/it]
2022-03-30 17:49:51,441 - INFO - tqdm - accuracy: 0.9924, batch_loss: 0.0028, loss: 0.0225 ||:  86%|########6 | 254/295 [14:06<02:02,  2.98s/it]
2022-03-30 17:50:01,470 - INFO - tqdm - accuracy: 0.9923, batch_loss: 0.0511, loss: 0.0225 ||:  87%|########7 | 257/295 [14:16<02:02,  3.23s/it]
2022-03-30 17:50:11,685 - INFO - tqdm - accuracy: 0.9924, batch_loss: 0.0091, loss: 0.0223 ||:  88%|########8 | 260/295 [14:27<01:58,  3.39s/it]
2022-03-30 17:50:21,896 - INFO - tqdm - accuracy: 0.9922, batch_loss: 0.0500, loss: 0.0232 ||:  89%|########9 | 263/295 [14:37<01:48,  3.40s/it]
2022-03-30 17:50:34,779 - INFO - tqdm - accuracy: 0.9922, batch_loss: 0.0070, loss: 0.0231 ||:  91%|######### | 267/295 [14:50<01:34,  3.36s/it]
2022-03-30 17:50:44,809 - INFO - tqdm - accuracy: 0.9921, batch_loss: 0.0409, loss: 0.0230 ||:  92%|#########1| 270/295 [15:00<01:23,  3.35s/it]
2022-03-30 17:50:55,445 - INFO - tqdm - accuracy: 0.9922, batch_loss: 0.0018, loss: 0.0228 ||:  93%|#########2| 273/295 [15:10<01:17,  3.51s/it]
2022-03-30 17:51:08,499 - INFO - tqdm - accuracy: 0.9922, batch_loss: 0.0057, loss: 0.0227 ||:  94%|#########3| 277/295 [15:23<00:59,  3.31s/it]
2022-03-30 17:51:18,595 - INFO - tqdm - accuracy: 0.9923, batch_loss: 0.0178, loss: 0.0226 ||:  95%|#########4| 280/295 [15:34<00:49,  3.32s/it]
2022-03-30 17:51:31,188 - INFO - tqdm - accuracy: 0.9922, batch_loss: 0.0376, loss: 0.0227 ||:  96%|#########6| 284/295 [15:46<00:34,  3.15s/it]
2022-03-30 17:51:43,738 - INFO - tqdm - accuracy: 0.9922, batch_loss: 0.0011, loss: 0.0226 ||:  98%|#########7| 288/295 [15:59<00:22,  3.18s/it]
2022-03-30 17:51:54,768 - INFO - tqdm - accuracy: 0.9923, batch_loss: 0.0012, loss: 0.0224 ||:  99%|#########8| 291/295 [16:10<00:14,  3.58s/it]
2022-03-30 17:52:04,343 - INFO - tqdm - accuracy: 0.9921, batch_loss: 0.0023, loss: 0.0224 ||: 100%|#########9| 294/295 [16:19<00:03,  3.30s/it]
2022-03-30 17:52:06,832 - INFO - tqdm - accuracy: 0.9922, batch_loss: 0.0345, loss: 0.0225 ||: 100%|##########| 295/295 [16:22<00:00,  3.05s/it]
2022-03-30 17:52:06,832 - INFO - tqdm - accuracy: 0.9922, batch_loss: 0.0345, loss: 0.0225 ||: 100%|##########| 295/295 [16:22<00:00,  3.33s/it]
2022-03-30 17:52:09,256 - INFO - allennlp.training.trainer - Validating
2022-03-30 17:52:09,258 - INFO - tqdm - 0%|          | 0/1635 [00:00<?, ?it/s]
2022-03-30 17:52:19,328 - INFO - tqdm - accuracy: 0.8697, batch_loss: 0.0003, loss: 0.7273 ||:   9%|8         | 142/1635 [00:10<01:41, 14.67it/s]
2022-03-30 17:52:29,357 - INFO - tqdm - accuracy: 0.8724, batch_loss: 0.0001, loss: 0.7042 ||:  18%|#7        | 290/1635 [00:20<01:33, 14.35it/s]
2022-03-30 17:52:39,398 - INFO - tqdm - accuracy: 0.8696, batch_loss: 0.0012, loss: 0.7285 ||:  27%|##6       | 437/1635 [00:30<01:27, 13.75it/s]
2022-03-30 17:52:49,501 - INFO - tqdm - accuracy: 0.8646, batch_loss: 1.0721, loss: 0.7534 ||:  35%|###5      | 576/1635 [00:40<01:15, 14.10it/s]
2022-03-30 17:52:59,530 - INFO - tqdm - accuracy: 0.8568, batch_loss: 0.0001, loss: 0.7754 ||:  44%|####3     | 716/1635 [00:50<01:05, 14.08it/s]
2022-03-30 17:53:09,591 - INFO - tqdm - accuracy: 0.8590, batch_loss: 0.0235, loss: 0.7628 ||:  53%|#####2    | 862/1635 [01:00<00:47, 16.12it/s]
2022-03-30 17:53:19,687 - INFO - tqdm - accuracy: 0.8590, batch_loss: 0.0001, loss: 0.7666 ||:  62%|######1   | 1007/1635 [01:10<00:41, 15.00it/s]
2022-03-30 17:53:29,895 - INFO - tqdm - accuracy: 0.8602, batch_loss: 0.0003, loss: 0.7668 ||:  71%|#######   | 1155/1635 [01:20<00:45, 10.47it/s]
2022-03-30 17:53:39,974 - INFO - tqdm - accuracy: 0.8613, batch_loss: 0.5378, loss: 0.7627 ||:  79%|#######9  | 1298/1635 [01:30<00:29, 11.40it/s]
2022-03-30 17:53:50,001 - INFO - tqdm - accuracy: 0.8619, batch_loss: 0.0008, loss: 0.7526 ||:  89%|########8 | 1448/1635 [01:40<00:12, 14.79it/s]
2022-03-30 17:54:00,156 - INFO - tqdm - accuracy: 0.8583, batch_loss: 0.0001, loss: 0.7784 ||:  98%|#########7| 1602/1635 [01:50<00:02, 13.22it/s]
2022-03-30 17:54:01,949 - INFO - tqdm - accuracy: 0.8596, batch_loss: 0.0001, loss: 0.7722 ||: 100%|#########9| 1628/1635 [01:52<00:00, 15.52it/s]
2022-03-30 17:54:02,087 - INFO - tqdm - accuracy: 0.8595, batch_loss: 0.0007, loss: 0.7736 ||: 100%|#########9| 1630/1635 [01:52<00:00, 15.21it/s]
2022-03-30 17:54:02,344 - INFO - tqdm - accuracy: 0.8588, batch_loss: 4.8295, loss: 0.7758 ||: 100%|#########9| 1632/1635 [01:53<00:00, 11.86it/s]
2022-03-30 17:54:02,477 - INFO - tqdm - accuracy: 0.8584, batch_loss: 0.9252, loss: 0.7773 ||: 100%|##########| 1635/1635 [01:53<00:00, 14.53it/s]
2022-03-30 17:54:02,478 - INFO - tqdm - accuracy: 0.8584, batch_loss: 0.9252, loss: 0.7773 ||: 100%|##########| 1635/1635 [01:53<00:00, 14.44it/s]
2022-03-30 17:54:02,478 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-03-30 17:54:02,478 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.992  |     0.858
2022-03-30 17:54:02,479 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9572.260  |       N/A
2022-03-30 17:54:02,480 - INFO - allennlp.training.tensorboard_writer - loss               |     0.022  |     0.777
2022-03-30 17:54:02,481 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  7092.406  |       N/A
2022-03-30 17:54:07,308 - INFO - allennlp.training.trainer - Epoch duration: 0:18:22.743762
2022-03-30 17:54:07,308 - INFO - allennlp.training.trainer - Estimated training time remaining: 0:36:43
2022-03-30 17:54:07,308 - INFO - allennlp.training.trainer - Epoch 8/9
2022-03-30 17:54:07,308 - INFO - allennlp.training.trainer - Worker 0 memory usage: 6.9G
2022-03-30 17:54:07,308 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.3G
2022-03-30 17:54:07,310 - INFO - allennlp.training.trainer - Training
2022-03-30 17:54:07,310 - INFO - tqdm - 0%|          | 0/295 [00:00<?, ?it/s]
2022-03-30 17:54:18,428 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.1274, loss: 0.0439 ||:   1%|1         | 3/295 [00:11<17:53,  3.68s/it]
2022-03-30 17:54:31,514 - INFO - tqdm - accuracy: 0.9955, batch_loss: 0.0016, loss: 0.0209 ||:   2%|2         | 7/295 [00:24<16:08,  3.36s/it]
2022-03-30 17:54:41,704 - INFO - tqdm - accuracy: 0.9906, batch_loss: 0.0094, loss: 0.0251 ||:   3%|3         | 10/295 [00:34<16:15,  3.42s/it]
2022-03-30 17:54:54,818 - INFO - tqdm - accuracy: 0.9911, batch_loss: 0.0007, loss: 0.0206 ||:   5%|4         | 14/295 [00:47<15:30,  3.31s/it]
2022-03-30 17:55:05,244 - INFO - tqdm - accuracy: 0.9908, batch_loss: 0.0005, loss: 0.0190 ||:   6%|5         | 17/295 [00:57<15:46,  3.41s/it]
2022-03-30 17:55:18,465 - INFO - tqdm - accuracy: 0.9926, batch_loss: 0.0065, loss: 0.0160 ||:   7%|7         | 21/295 [01:11<15:34,  3.41s/it]
2022-03-30 17:55:32,394 - INFO - tqdm - accuracy: 0.9938, batch_loss: 0.0014, loss: 0.0143 ||:   8%|8         | 25/295 [01:25<15:52,  3.53s/it]
2022-03-30 17:55:46,578 - INFO - tqdm - accuracy: 0.9935, batch_loss: 0.0013, loss: 0.0201 ||:  10%|9         | 29/295 [01:39<16:15,  3.67s/it]
2022-03-30 17:56:00,166 - INFO - tqdm - accuracy: 0.9924, batch_loss: 0.2554, loss: 0.0282 ||:  11%|#1        | 33/295 [01:52<15:29,  3.55s/it]
2022-03-30 17:56:11,055 - INFO - tqdm - accuracy: 0.9931, batch_loss: 0.0011, loss: 0.0260 ||:  12%|#2        | 36/295 [02:03<15:43,  3.64s/it]
2022-03-30 17:56:21,515 - INFO - tqdm - accuracy: 0.9936, batch_loss: 0.0011, loss: 0.0244 ||:  13%|#3        | 39/295 [02:14<15:15,  3.58s/it]
2022-03-30 17:56:33,873 - INFO - tqdm - accuracy: 0.9927, batch_loss: 0.0055, loss: 0.0244 ||:  15%|#4        | 43/295 [02:26<13:11,  3.14s/it]
2022-03-30 17:56:44,885 - INFO - tqdm - accuracy: 0.9925, batch_loss: 0.0037, loss: 0.0239 ||:  16%|#5        | 46/295 [02:37<14:25,  3.47s/it]
2022-03-30 17:56:55,173 - INFO - tqdm - accuracy: 0.9930, batch_loss: 0.0013, loss: 0.0225 ||:  17%|#6        | 49/295 [02:47<14:10,  3.46s/it]
2022-03-30 17:57:08,278 - INFO - tqdm - accuracy: 0.9935, batch_loss: 0.0104, loss: 0.0213 ||:  18%|#7        | 53/295 [03:00<13:31,  3.35s/it]
2022-03-30 17:57:21,617 - INFO - tqdm - accuracy: 0.9940, batch_loss: 0.0023, loss: 0.0199 ||:  19%|#9        | 57/295 [03:14<13:28,  3.40s/it]
2022-03-30 17:57:34,745 - INFO - tqdm - accuracy: 0.9939, batch_loss: 0.0067, loss: 0.0212 ||:  21%|##        | 61/295 [03:27<12:50,  3.29s/it]
2022-03-30 17:57:44,968 - INFO - tqdm - accuracy: 0.9941, batch_loss: 0.0052, loss: 0.0204 ||:  22%|##1       | 64/295 [03:37<12:40,  3.29s/it]
2022-03-30 17:57:55,578 - INFO - tqdm - accuracy: 0.9944, batch_loss: 0.0054, loss: 0.0196 ||:  23%|##2       | 67/295 [03:48<13:22,  3.52s/it]
2022-03-30 17:58:08,455 - INFO - tqdm - accuracy: 0.9947, batch_loss: 0.0012, loss: 0.0187 ||:  24%|##4       | 71/295 [04:01<12:08,  3.25s/it]
2022-03-30 17:58:18,620 - INFO - tqdm - accuracy: 0.9949, batch_loss: 0.0090, loss: 0.0184 ||:  25%|##5       | 74/295 [04:11<12:19,  3.35s/it]
2022-03-30 17:58:30,499 - INFO - tqdm - accuracy: 0.9948, batch_loss: 0.0015, loss: 0.0184 ||:  26%|##6       | 78/295 [04:23<11:10,  3.09s/it]
2022-03-30 17:58:40,580 - INFO - tqdm - accuracy: 0.9946, batch_loss: 0.0052, loss: 0.0181 ||:  27%|##7       | 81/295 [04:33<11:38,  3.26s/it]
2022-03-30 17:58:53,873 - INFO - tqdm - accuracy: 0.9938, batch_loss: 0.0697, loss: 0.0189 ||:  29%|##8       | 85/295 [04:46<11:36,  3.32s/it]
2022-03-30 17:59:04,457 - INFO - tqdm - accuracy: 0.9940, batch_loss: 0.0085, loss: 0.0184 ||:  30%|##9       | 88/295 [04:57<11:48,  3.42s/it]
2022-03-30 17:59:14,848 - INFO - tqdm - accuracy: 0.9938, batch_loss: 0.0033, loss: 0.0185 ||:  31%|###       | 91/295 [05:07<11:54,  3.50s/it]
2022-03-30 17:59:25,306 - INFO - tqdm - accuracy: 0.9940, batch_loss: 0.0009, loss: 0.0180 ||:  32%|###1      | 94/295 [05:17<11:45,  3.51s/it]
2022-03-30 17:59:37,626 - INFO - tqdm - accuracy: 0.9943, batch_loss: 0.0051, loss: 0.0174 ||:  33%|###3      | 98/295 [05:30<10:22,  3.16s/it]
2022-03-30 17:59:50,002 - INFO - tqdm - accuracy: 0.9945, batch_loss: 0.0007, loss: 0.0167 ||:  35%|###4      | 102/295 [05:42<09:55,  3.08s/it]
2022-03-30 18:00:03,274 - INFO - tqdm - accuracy: 0.9947, batch_loss: 0.0022, loss: 0.0162 ||:  36%|###5      | 106/295 [05:55<10:13,  3.25s/it]
2022-03-30 18:00:13,573 - INFO - tqdm - accuracy: 0.9946, batch_loss: 0.0012, loss: 0.0160 ||:  37%|###6      | 109/295 [06:06<10:24,  3.36s/it]
2022-03-30 18:00:26,184 - INFO - tqdm - accuracy: 0.9945, batch_loss: 0.0096, loss: 0.0159 ||:  38%|###8      | 113/295 [06:18<09:40,  3.19s/it]
2022-03-30 18:00:37,357 - INFO - tqdm - accuracy: 0.9946, batch_loss: 0.0126, loss: 0.0156 ||:  39%|###9      | 116/295 [06:30<10:48,  3.63s/it]
2022-03-30 18:00:50,695 - INFO - tqdm - accuracy: 0.9945, batch_loss: 0.0028, loss: 0.0158 ||:  41%|####      | 120/295 [06:43<09:56,  3.41s/it]
2022-03-30 18:01:01,095 - INFO - tqdm - accuracy: 0.9947, batch_loss: 0.0171, loss: 0.0156 ||:  42%|####1     | 123/295 [06:53<09:57,  3.48s/it]
2022-03-30 18:01:13,583 - INFO - tqdm - accuracy: 0.9946, batch_loss: 0.0009, loss: 0.0154 ||:  43%|####3     | 127/295 [07:06<08:43,  3.12s/it]
2022-03-30 18:01:26,527 - INFO - tqdm - accuracy: 0.9948, batch_loss: 0.0039, loss: 0.0150 ||:  44%|####4     | 131/295 [07:19<08:43,  3.19s/it]
2022-03-30 18:01:37,625 - INFO - tqdm - accuracy: 0.9949, batch_loss: 0.0011, loss: 0.0148 ||:  45%|####5     | 134/295 [07:30<09:38,  3.59s/it]
2022-03-30 18:01:51,164 - INFO - tqdm - accuracy: 0.9950, batch_loss: 0.0007, loss: 0.0144 ||:  47%|####6     | 138/295 [07:43<09:10,  3.51s/it]
2022-03-30 18:02:01,300 - INFO - tqdm - accuracy: 0.9951, batch_loss: 0.0006, loss: 0.0141 ||:  48%|####7     | 141/295 [07:53<08:40,  3.38s/it]
2022-03-30 18:02:13,590 - INFO - tqdm - accuracy: 0.9953, batch_loss: 0.0009, loss: 0.0139 ||:  49%|####9     | 145/295 [08:06<08:02,  3.21s/it]
2022-03-30 18:02:26,300 - INFO - tqdm - accuracy: 0.9952, batch_loss: 0.0023, loss: 0.0137 ||:  51%|#####     | 149/295 [08:18<07:40,  3.16s/it]
2022-03-30 18:02:40,214 - INFO - tqdm - accuracy: 0.9953, batch_loss: 0.0007, loss: 0.0134 ||:  52%|#####1    | 153/295 [08:32<08:22,  3.54s/it]
2022-03-30 18:02:53,215 - INFO - tqdm - accuracy: 0.9954, batch_loss: 0.0007, loss: 0.0131 ||:  53%|#####3    | 157/295 [08:45<07:41,  3.34s/it]
2022-03-30 18:03:03,306 - INFO - tqdm - accuracy: 0.9955, batch_loss: 0.0011, loss: 0.0129 ||:  54%|#####4    | 160/295 [08:55<07:37,  3.39s/it]
2022-03-30 18:03:15,904 - INFO - tqdm - accuracy: 0.9956, batch_loss: 0.0006, loss: 0.0126 ||:  56%|#####5    | 164/295 [09:08<06:59,  3.21s/it]
2022-03-30 18:03:26,100 - INFO - tqdm - accuracy: 0.9955, batch_loss: 0.0005, loss: 0.0136 ||:  57%|#####6    | 167/295 [09:18<07:00,  3.29s/it]
2022-03-30 18:03:38,608 - INFO - tqdm - accuracy: 0.9956, batch_loss: 0.0010, loss: 0.0133 ||:  58%|#####7    | 171/295 [09:31<06:32,  3.17s/it]
2022-03-30 18:03:51,256 - INFO - tqdm - accuracy: 0.9957, batch_loss: 0.0007, loss: 0.0130 ||:  59%|#####9    | 175/295 [09:43<06:27,  3.23s/it]
2022-03-30 18:04:03,930 - INFO - tqdm - accuracy: 0.9958, batch_loss: 0.0007, loss: 0.0128 ||:  61%|######    | 179/295 [09:56<06:09,  3.18s/it]
2022-03-30 18:04:16,409 - INFO - tqdm - accuracy: 0.9959, batch_loss: 0.0032, loss: 0.0127 ||:  62%|######2   | 183/295 [10:09<05:42,  3.06s/it]
2022-03-30 18:04:26,775 - INFO - tqdm - accuracy: 0.9958, batch_loss: 0.0004, loss: 0.0126 ||:  63%|######3   | 186/295 [10:19<05:56,  3.27s/it]
2022-03-30 18:04:37,642 - INFO - tqdm - accuracy: 0.9959, batch_loss: 0.0088, loss: 0.0125 ||:  64%|######4   | 189/295 [10:30<06:17,  3.56s/it]
2022-03-30 18:04:48,071 - INFO - tqdm - accuracy: 0.9958, batch_loss: 0.0340, loss: 0.0129 ||:  65%|######5   | 192/295 [10:40<05:59,  3.49s/it]
2022-03-30 18:04:58,954 - INFO - tqdm - accuracy: 0.9958, batch_loss: 0.0007, loss: 0.0127 ||:  66%|######6   | 195/295 [10:51<05:52,  3.52s/it]
2022-03-30 18:05:11,800 - INFO - tqdm - accuracy: 0.9956, batch_loss: 0.0003, loss: 0.0131 ||:  67%|######7   | 199/295 [11:04<05:20,  3.34s/it]
2022-03-30 18:05:23,824 - INFO - tqdm - accuracy: 0.9957, batch_loss: 0.0005, loss: 0.0129 ||:  69%|######8   | 203/295 [11:16<04:43,  3.08s/it]
2022-03-30 18:05:33,849 - INFO - tqdm - accuracy: 0.9956, batch_loss: 0.0061, loss: 0.0128 ||:  70%|######9   | 206/295 [11:26<04:53,  3.30s/it]
2022-03-30 18:05:46,902 - INFO - tqdm - accuracy: 0.9957, batch_loss: 0.0006, loss: 0.0126 ||:  71%|#######1  | 210/295 [11:39<04:46,  3.37s/it]
2022-03-30 18:06:00,328 - INFO - tqdm - accuracy: 0.9958, batch_loss: 0.0014, loss: 0.0124 ||:  73%|#######2  | 214/295 [11:53<04:43,  3.49s/it]
2022-03-30 18:06:13,017 - INFO - tqdm - accuracy: 0.9958, batch_loss: 0.0004, loss: 0.0122 ||:  74%|#######3  | 218/295 [12:05<04:14,  3.30s/it]
2022-03-30 18:06:25,636 - INFO - tqdm - accuracy: 0.9959, batch_loss: 0.0006, loss: 0.0121 ||:  75%|#######5  | 222/295 [12:18<03:51,  3.18s/it]
2022-03-30 18:06:38,403 - INFO - tqdm - accuracy: 0.9959, batch_loss: 0.0005, loss: 0.0124 ||:  77%|#######6  | 226/295 [12:31<03:43,  3.24s/it]
2022-03-30 18:06:49,349 - INFO - tqdm - accuracy: 0.9959, batch_loss: 0.0008, loss: 0.0122 ||:  78%|#######7  | 229/295 [12:42<03:46,  3.43s/it]
2022-03-30 18:07:01,998 - INFO - tqdm - accuracy: 0.9960, batch_loss: 0.0006, loss: 0.0121 ||:  79%|#######8  | 233/295 [12:54<03:14,  3.14s/it]
2022-03-30 18:07:12,696 - INFO - tqdm - accuracy: 0.9959, batch_loss: 0.0026, loss: 0.0121 ||:  80%|########  | 236/295 [13:05<03:21,  3.42s/it]
2022-03-30 18:07:25,297 - INFO - tqdm - accuracy: 0.9960, batch_loss: 0.0008, loss: 0.0119 ||:  81%|########1 | 240/295 [13:17<02:54,  3.18s/it]
2022-03-30 18:07:35,380 - INFO - tqdm - accuracy: 0.9959, batch_loss: 0.0005, loss: 0.0124 ||:  82%|########2 | 243/295 [13:28<02:52,  3.31s/it]
2022-03-30 18:07:46,096 - INFO - tqdm - accuracy: 0.9959, batch_loss: 0.0010, loss: 0.0123 ||:  83%|########3 | 246/295 [13:38<02:49,  3.46s/it]
2022-03-30 18:07:57,371 - INFO - tqdm - accuracy: 0.9960, batch_loss: 0.0016, loss: 0.0121 ||:  84%|########4 | 249/295 [13:50<02:49,  3.67s/it]
2022-03-30 18:08:10,163 - INFO - tqdm - accuracy: 0.9958, batch_loss: 0.0004, loss: 0.0126 ||:  86%|########5 | 253/295 [14:02<02:20,  3.34s/it]
2022-03-30 18:08:20,425 - INFO - tqdm - accuracy: 0.9958, batch_loss: 0.0017, loss: 0.0125 ||:  87%|########6 | 256/295 [14:13<02:13,  3.42s/it]
2022-03-30 18:08:33,644 - INFO - tqdm - accuracy: 0.9958, batch_loss: 0.0009, loss: 0.0131 ||:  88%|########8 | 260/295 [14:26<01:56,  3.32s/it]
2022-03-30 18:08:43,705 - INFO - tqdm - accuracy: 0.9958, batch_loss: 0.0003, loss: 0.0130 ||:  89%|########9 | 263/295 [14:36<01:47,  3.36s/it]
2022-03-30 18:08:55,927 - INFO - tqdm - accuracy: 0.9957, batch_loss: 0.0246, loss: 0.0137 ||:  91%|######### | 267/295 [14:48<01:26,  3.10s/it]
2022-03-30 18:09:07,019 - INFO - tqdm - accuracy: 0.9957, batch_loss: 0.0006, loss: 0.0135 ||:  92%|#########1| 270/295 [14:59<01:26,  3.45s/it]
2022-03-30 18:09:17,437 - INFO - tqdm - accuracy: 0.9958, batch_loss: 0.0065, loss: 0.0134 ||:  93%|#########2| 273/295 [15:10<01:16,  3.50s/it]
2022-03-30 18:09:27,666 - INFO - tqdm - accuracy: 0.9958, batch_loss: 0.0007, loss: 0.0133 ||:  94%|#########3| 276/295 [15:20<01:04,  3.37s/it]
2022-03-30 18:09:40,575 - INFO - tqdm - accuracy: 0.9959, batch_loss: 0.0007, loss: 0.0131 ||:  95%|#########4| 280/295 [15:33<00:48,  3.26s/it]
2022-03-30 18:09:50,666 - INFO - tqdm - accuracy: 0.9959, batch_loss: 0.0011, loss: 0.0130 ||:  96%|#########5| 283/295 [15:43<00:40,  3.41s/it]
2022-03-30 18:10:00,765 - INFO - tqdm - accuracy: 0.9960, batch_loss: 0.0007, loss: 0.0129 ||:  97%|#########6| 286/295 [15:53<00:29,  3.30s/it]
2022-03-30 18:10:11,505 - INFO - tqdm - accuracy: 0.9959, batch_loss: 0.0267, loss: 0.0130 ||:  98%|#########7| 289/295 [16:04<00:21,  3.50s/it]
2022-03-30 18:10:24,145 - INFO - tqdm - accuracy: 0.9959, batch_loss: 0.0005, loss: 0.0128 ||:  99%|#########9| 293/295 [16:16<00:06,  3.26s/it]
2022-03-30 18:10:27,634 - INFO - tqdm - accuracy: 0.9959, batch_loss: 0.1722, loss: 0.0134 ||: 100%|#########9| 294/295 [16:20<00:03,  3.33s/it]
2022-03-30 18:10:29,722 - INFO - tqdm - accuracy: 0.9959, batch_loss: 0.0029, loss: 0.0133 ||: 100%|##########| 295/295 [16:22<00:00,  2.96s/it]
2022-03-30 18:10:29,722 - INFO - tqdm - accuracy: 0.9959, batch_loss: 0.0029, loss: 0.0133 ||: 100%|##########| 295/295 [16:22<00:00,  3.33s/it]
2022-03-30 18:10:32,076 - INFO - allennlp.training.trainer - Validating
2022-03-30 18:10:32,077 - INFO - tqdm - 0%|          | 0/1635 [00:00<?, ?it/s]
2022-03-30 18:10:42,139 - INFO - tqdm - accuracy: 0.8500, batch_loss: 0.0006, loss: 0.9430 ||:   9%|9         | 150/1635 [00:10<01:45, 14.01it/s]
2022-03-30 18:10:52,319 - INFO - tqdm - accuracy: 0.8433, batch_loss: 4.3368, loss: 1.0122 ||:  18%|#8        | 300/1635 [00:20<01:38, 13.55it/s]
2022-03-30 18:11:02,371 - INFO - tqdm - accuracy: 0.8419, batch_loss: 0.0055, loss: 0.9787 ||:  27%|##7       | 449/1635 [00:30<01:08, 17.36it/s]
2022-03-30 18:11:12,549 - INFO - tqdm - accuracy: 0.8459, batch_loss: 0.0001, loss: 0.9382 ||:  36%|###5      | 584/1635 [00:40<01:39, 10.59it/s]
2022-03-30 18:11:22,626 - INFO - tqdm - accuracy: 0.8460, batch_loss: 4.1829, loss: 0.9133 ||:  45%|####4     | 734/1635 [00:50<01:07, 13.42it/s]
2022-03-30 18:11:32,629 - INFO - tqdm - accuracy: 0.8497, batch_loss: 0.0002, loss: 0.9033 ||:  54%|#####3    | 878/1635 [01:00<00:48, 15.51it/s]
2022-03-30 18:11:42,703 - INFO - tqdm - accuracy: 0.8579, batch_loss: 0.0003, loss: 0.8552 ||:  63%|######2   | 1024/1635 [01:10<00:40, 14.99it/s]
2022-03-30 18:11:52,877 - INFO - tqdm - accuracy: 0.8589, batch_loss: 0.0001, loss: 0.8374 ||:  71%|#######1  | 1169/1635 [01:20<00:38, 12.18it/s]
2022-03-30 18:12:02,884 - INFO - tqdm - accuracy: 0.8564, batch_loss: 1.0229, loss: 0.8556 ||:  80%|########  | 1313/1635 [01:30<00:18, 17.41it/s]
2022-03-30 18:12:12,989 - INFO - tqdm - accuracy: 0.8553, batch_loss: 2.7239, loss: 0.8615 ||:  89%|########8 | 1451/1635 [01:40<00:11, 15.44it/s]
2022-03-30 18:12:23,130 - INFO - tqdm - accuracy: 0.8568, batch_loss: 0.0021, loss: 0.8517 ||:  98%|#########8| 1603/1635 [01:51<00:01, 16.35it/s]
2022-03-30 18:12:24,864 - INFO - tqdm - accuracy: 0.8566, batch_loss: 0.0004, loss: 0.8504 ||: 100%|#########9| 1628/1635 [01:52<00:00, 14.04it/s]
2022-03-30 18:12:25,031 - INFO - tqdm - accuracy: 0.8567, batch_loss: 0.0085, loss: 0.8493 ||: 100%|#########9| 1630/1635 [01:52<00:00, 13.42it/s]
2022-03-30 18:12:25,142 - INFO - tqdm - accuracy: 0.8566, batch_loss: 1.2947, loss: 0.8491 ||: 100%|#########9| 1632/1635 [01:53<00:00, 14.45it/s]
2022-03-30 18:12:25,336 - INFO - tqdm - accuracy: 0.8565, batch_loss: 0.0003, loss: 0.8487 ||: 100%|#########9| 1634/1635 [01:53<00:00, 12.97it/s]
2022-03-30 18:12:25,421 - INFO - tqdm - accuracy: 0.8563, batch_loss: 1.1460, loss: 0.8489 ||: 100%|##########| 1635/1635 [01:53<00:00, 14.43it/s]
2022-03-30 18:12:25,422 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-03-30 18:12:25,422 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.996  |     0.856
2022-03-30 18:12:25,423 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9572.260  |       N/A
2022-03-30 18:12:25,424 - INFO - allennlp.training.tensorboard_writer - loss               |     0.013  |     0.849
2022-03-30 18:12:25,425 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  7092.406  |       N/A
2022-03-30 18:12:30,378 - INFO - allennlp.training.trainer - Epoch duration: 0:18:23.069845
2022-03-30 18:12:30,378 - INFO - allennlp.training.trainer - Estimated training time remaining: 0:18:22
2022-03-30 18:12:30,378 - INFO - allennlp.training.trainer - Epoch 9/9
2022-03-30 18:12:30,378 - INFO - allennlp.training.trainer - Worker 0 memory usage: 6.9G
2022-03-30 18:12:30,379 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.3G
2022-03-30 18:12:30,380 - INFO - allennlp.training.trainer - Training
2022-03-30 18:12:30,380 - INFO - tqdm - 0%|          | 0/295 [00:00<?, ?it/s]
2022-03-30 18:12:42,774 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0070, loss: 0.0031 ||:   1%|1         | 4/295 [00:12<14:54,  3.08s/it]
2022-03-30 18:12:55,362 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0138, loss: 0.0046 ||:   3%|2         | 8/295 [00:24<14:45,  3.08s/it]
2022-03-30 18:13:08,080 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0007, loss: 0.0036 ||:   4%|4         | 12/295 [00:37<14:48,  3.14s/it]
2022-03-30 18:13:18,942 - INFO - tqdm - accuracy: 0.9979, batch_loss: 0.0009, loss: 0.0053 ||:   5%|5         | 15/295 [00:48<16:01,  3.43s/it]
2022-03-30 18:13:29,265 - INFO - tqdm - accuracy: 0.9965, batch_loss: 0.0300, loss: 0.0062 ||:   6%|6         | 18/295 [00:58<15:49,  3.43s/it]
2022-03-30 18:13:42,054 - INFO - tqdm - accuracy: 0.9972, batch_loss: 0.0005, loss: 0.0058 ||:   7%|7         | 22/295 [01:11<14:55,  3.28s/it]
2022-03-30 18:13:54,475 - INFO - tqdm - accuracy: 0.9976, batch_loss: 0.0013, loss: 0.0052 ||:   9%|8         | 26/295 [01:24<14:01,  3.13s/it]
2022-03-30 18:14:04,548 - INFO - tqdm - accuracy: 0.9978, batch_loss: 0.0004, loss: 0.0048 ||:  10%|9         | 29/295 [01:34<14:27,  3.26s/it]
2022-03-30 18:14:15,303 - INFO - tqdm - accuracy: 0.9980, batch_loss: 0.0009, loss: 0.0045 ||:  11%|#         | 32/295 [01:44<15:18,  3.49s/it]
2022-03-30 18:14:28,359 - INFO - tqdm - accuracy: 0.9974, batch_loss: 0.0026, loss: 0.0083 ||:  12%|#2        | 36/295 [01:57<14:13,  3.30s/it]
2022-03-30 18:14:38,492 - INFO - tqdm - accuracy: 0.9976, batch_loss: 0.0020, loss: 0.0077 ||:  13%|#3        | 39/295 [02:08<14:00,  3.28s/it]
2022-03-30 18:14:49,261 - INFO - tqdm - accuracy: 0.9978, batch_loss: 0.0007, loss: 0.0072 ||:  14%|#4        | 42/295 [02:18<14:43,  3.49s/it]
2022-03-30 18:15:00,047 - INFO - tqdm - accuracy: 0.9979, batch_loss: 0.0013, loss: 0.0070 ||:  15%|#5        | 45/295 [02:29<15:00,  3.60s/it]
2022-03-30 18:15:10,359 - INFO - tqdm - accuracy: 0.9980, batch_loss: 0.0002, loss: 0.0067 ||:  16%|#6        | 48/295 [02:39<14:15,  3.46s/it]
2022-03-30 18:15:23,824 - INFO - tqdm - accuracy: 0.9982, batch_loss: 0.0026, loss: 0.0063 ||:  18%|#7        | 52/295 [02:53<13:48,  3.41s/it]
2022-03-30 18:15:33,992 - INFO - tqdm - accuracy: 0.9983, batch_loss: 0.0007, loss: 0.0061 ||:  19%|#8        | 55/295 [03:03<13:35,  3.40s/it]
2022-03-30 18:15:44,067 - INFO - tqdm - accuracy: 0.9984, batch_loss: 0.0006, loss: 0.0058 ||:  20%|#9        | 58/295 [03:13<13:11,  3.34s/it]
2022-03-30 18:15:55,117 - INFO - tqdm - accuracy: 0.9974, batch_loss: 0.0026, loss: 0.0075 ||:  21%|##        | 61/295 [03:24<14:07,  3.62s/it]
2022-03-30 18:16:06,347 - INFO - tqdm - accuracy: 0.9976, batch_loss: 0.0007, loss: 0.0074 ||:  22%|##1       | 64/295 [03:35<14:06,  3.66s/it]
2022-03-30 18:16:19,381 - INFO - tqdm - accuracy: 0.9977, batch_loss: 0.0041, loss: 0.0072 ||:  23%|##3       | 68/295 [03:48<12:51,  3.40s/it]
2022-03-30 18:16:29,625 - INFO - tqdm - accuracy: 0.9978, batch_loss: 0.0012, loss: 0.0072 ||:  24%|##4       | 71/295 [03:59<12:27,  3.34s/it]
2022-03-30 18:16:42,546 - INFO - tqdm - accuracy: 0.9975, batch_loss: 0.0311, loss: 0.0073 ||:  25%|##5       | 75/295 [04:12<11:47,  3.21s/it]
2022-03-30 18:16:52,795 - INFO - tqdm - accuracy: 0.9972, batch_loss: 0.0544, loss: 0.0077 ||:  26%|##6       | 78/295 [04:22<12:04,  3.34s/it]
2022-03-30 18:17:05,602 - INFO - tqdm - accuracy: 0.9973, batch_loss: 0.0006, loss: 0.0073 ||:  28%|##7       | 82/295 [04:35<11:39,  3.28s/it]
2022-03-30 18:17:19,220 - INFO - tqdm - accuracy: 0.9967, batch_loss: 0.0006, loss: 0.0080 ||:  29%|##9       | 86/295 [04:48<11:54,  3.42s/it]
2022-03-30 18:17:32,609 - INFO - tqdm - accuracy: 0.9965, batch_loss: 0.0120, loss: 0.0082 ||:  31%|###       | 90/295 [05:02<11:41,  3.42s/it]
2022-03-30 18:17:45,937 - INFO - tqdm - accuracy: 0.9967, batch_loss: 0.0017, loss: 0.0079 ||:  32%|###1      | 94/295 [05:15<11:22,  3.40s/it]
2022-03-30 18:17:59,301 - INFO - tqdm - accuracy: 0.9968, batch_loss: 0.0012, loss: 0.0076 ||:  33%|###3      | 98/295 [05:28<11:02,  3.36s/it]
2022-03-30 18:18:09,418 - INFO - tqdm - accuracy: 0.9969, batch_loss: 0.0016, loss: 0.0075 ||:  34%|###4      | 101/295 [05:39<11:16,  3.49s/it]
2022-03-30 18:18:22,693 - INFO - tqdm - accuracy: 0.9970, batch_loss: 0.0022, loss: 0.0073 ||:  36%|###5      | 105/295 [05:52<10:43,  3.39s/it]
2022-03-30 18:18:33,464 - INFO - tqdm - accuracy: 0.9971, batch_loss: 0.0011, loss: 0.0071 ||:  37%|###6      | 108/295 [06:03<11:19,  3.63s/it]
2022-03-30 18:18:46,239 - INFO - tqdm - accuracy: 0.9967, batch_loss: 0.0100, loss: 0.0093 ||:  38%|###7      | 112/295 [06:15<10:00,  3.28s/it]
2022-03-30 18:18:58,977 - INFO - tqdm - accuracy: 0.9968, batch_loss: 0.0011, loss: 0.0093 ||:  39%|###9      | 116/295 [06:28<09:31,  3.19s/it]
2022-03-30 18:19:09,341 - INFO - tqdm - accuracy: 0.9968, batch_loss: 0.0006, loss: 0.0091 ||:  40%|####      | 119/295 [06:38<09:38,  3.29s/it]
2022-03-30 18:19:22,786 - INFO - tqdm - accuracy: 0.9964, batch_loss: 0.0048, loss: 0.0105 ||:  42%|####1     | 123/295 [06:52<09:38,  3.37s/it]
2022-03-30 18:19:35,176 - INFO - tqdm - accuracy: 0.9966, batch_loss: 0.0004, loss: 0.0103 ||:  43%|####3     | 127/295 [07:04<08:58,  3.20s/it]
2022-03-30 18:19:45,395 - INFO - tqdm - accuracy: 0.9964, batch_loss: 0.0680, loss: 0.0106 ||:  44%|####4     | 130/295 [07:15<09:24,  3.42s/it]
2022-03-30 18:19:55,682 - INFO - tqdm - accuracy: 0.9962, batch_loss: 0.0011, loss: 0.0110 ||:  45%|####5     | 133/295 [07:25<09:08,  3.39s/it]
2022-03-30 18:20:09,199 - INFO - tqdm - accuracy: 0.9963, batch_loss: 0.0004, loss: 0.0107 ||:  46%|####6     | 137/295 [07:38<09:00,  3.42s/it]
2022-03-30 18:20:22,376 - INFO - tqdm - accuracy: 0.9962, batch_loss: 0.0010, loss: 0.0107 ||:  48%|####7     | 141/295 [07:51<08:32,  3.33s/it]
2022-03-30 18:20:35,509 - INFO - tqdm - accuracy: 0.9957, batch_loss: 0.0175, loss: 0.0115 ||:  49%|####9     | 145/295 [08:05<08:24,  3.37s/it]
2022-03-30 18:20:47,385 - INFO - tqdm - accuracy: 0.9958, batch_loss: 0.0211, loss: 0.0114 ||:  51%|#####     | 149/295 [08:17<07:29,  3.08s/it]
2022-03-30 18:21:00,135 - INFO - tqdm - accuracy: 0.9959, batch_loss: 0.0173, loss: 0.0113 ||:  52%|#####1    | 153/295 [08:29<07:24,  3.13s/it]
2022-03-30 18:21:10,259 - INFO - tqdm - accuracy: 0.9958, batch_loss: 0.0007, loss: 0.0118 ||:  53%|#####2    | 156/295 [08:39<07:41,  3.32s/it]
2022-03-30 18:21:20,521 - INFO - tqdm - accuracy: 0.9959, batch_loss: 0.0032, loss: 0.0116 ||:  54%|#####3    | 159/295 [08:50<07:47,  3.44s/it]
2022-03-30 18:21:33,706 - INFO - tqdm - accuracy: 0.9960, batch_loss: 0.0006, loss: 0.0114 ||:  55%|#####5    | 163/295 [09:03<07:18,  3.32s/it]
2022-03-30 18:21:46,006 - INFO - tqdm - accuracy: 0.9961, batch_loss: 0.0005, loss: 0.0112 ||:  57%|#####6    | 167/295 [09:15<06:45,  3.17s/it]
2022-03-30 18:21:58,890 - INFO - tqdm - accuracy: 0.9962, batch_loss: 0.0007, loss: 0.0110 ||:  58%|#####7    | 171/295 [09:28<06:26,  3.12s/it]
2022-03-30 18:22:09,820 - INFO - tqdm - accuracy: 0.9962, batch_loss: 0.0068, loss: 0.0108 ||:  59%|#####8    | 174/295 [09:39<06:54,  3.43s/it]
2022-03-30 18:22:21,283 - INFO - tqdm - accuracy: 0.9963, batch_loss: 0.0003, loss: 0.0106 ||:  60%|######    | 178/295 [09:50<05:46,  2.97s/it]
2022-03-30 18:22:31,781 - INFO - tqdm - accuracy: 0.9964, batch_loss: 0.0015, loss: 0.0104 ||:  61%|######1   | 181/295 [10:01<06:12,  3.27s/it]
2022-03-30 18:22:45,347 - INFO - tqdm - accuracy: 0.9965, batch_loss: 0.0022, loss: 0.0103 ||:  63%|######2   | 185/295 [10:14<06:12,  3.39s/it]
2022-03-30 18:22:55,602 - INFO - tqdm - accuracy: 0.9965, batch_loss: 0.0017, loss: 0.0101 ||:  64%|######3   | 188/295 [10:25<06:09,  3.46s/it]
2022-03-30 18:23:08,829 - INFO - tqdm - accuracy: 0.9964, batch_loss: 0.0031, loss: 0.0102 ||:  65%|######5   | 192/295 [10:38<05:48,  3.38s/it]
2022-03-30 18:23:19,272 - INFO - tqdm - accuracy: 0.9965, batch_loss: 0.0005, loss: 0.0100 ||:  66%|######6   | 195/295 [10:48<05:52,  3.52s/it]
2022-03-30 18:23:31,884 - INFO - tqdm - accuracy: 0.9965, batch_loss: 0.0004, loss: 0.0099 ||:  67%|######7   | 199/295 [11:01<05:08,  3.21s/it]
2022-03-30 18:23:45,189 - INFO - tqdm - accuracy: 0.9966, batch_loss: 0.0008, loss: 0.0097 ||:  69%|######8   | 203/295 [11:14<05:02,  3.29s/it]
2022-03-30 18:23:58,007 - INFO - tqdm - accuracy: 0.9967, batch_loss: 0.0003, loss: 0.0096 ||:  70%|#######   | 207/295 [11:27<04:43,  3.22s/it]
2022-03-30 18:24:08,050 - INFO - tqdm - accuracy: 0.9964, batch_loss: 0.0014, loss: 0.0098 ||:  71%|#######1  | 210/295 [11:37<04:35,  3.24s/it]
2022-03-30 18:24:19,087 - INFO - tqdm - accuracy: 0.9965, batch_loss: 0.0007, loss: 0.0097 ||:  72%|#######2  | 213/295 [11:48<04:46,  3.50s/it]
2022-03-30 18:24:29,359 - INFO - tqdm - accuracy: 0.9965, batch_loss: 0.0004, loss: 0.0096 ||:  73%|#######3  | 216/295 [11:58<04:35,  3.49s/it]
2022-03-30 18:24:41,427 - INFO - tqdm - accuracy: 0.9963, batch_loss: 0.0322, loss: 0.0100 ||:  75%|#######4  | 220/295 [12:11<03:52,  3.10s/it]
2022-03-30 18:24:54,009 - INFO - tqdm - accuracy: 0.9964, batch_loss: 0.0011, loss: 0.0099 ||:  76%|#######5  | 224/295 [12:23<03:48,  3.21s/it]
2022-03-30 18:25:06,818 - INFO - tqdm - accuracy: 0.9964, batch_loss: 0.0201, loss: 0.0099 ||:  77%|#######7  | 228/295 [12:36<03:32,  3.16s/it]
2022-03-30 18:25:19,733 - INFO - tqdm - accuracy: 0.9965, batch_loss: 0.0005, loss: 0.0099 ||:  79%|#######8  | 232/295 [12:49<03:21,  3.20s/it]
2022-03-30 18:25:31,840 - INFO - tqdm - accuracy: 0.9964, batch_loss: 0.0046, loss: 0.0102 ||:  80%|########  | 236/295 [13:01<02:56,  2.99s/it]
2022-03-30 18:25:45,210 - INFO - tqdm - accuracy: 0.9965, batch_loss: 0.0061, loss: 0.0101 ||:  81%|########1 | 240/295 [13:14<02:58,  3.25s/it]
2022-03-30 18:25:56,016 - INFO - tqdm - accuracy: 0.9961, batch_loss: 0.0021, loss: 0.0112 ||:  82%|########2 | 243/295 [13:25<02:59,  3.44s/it]
2022-03-30 18:26:09,634 - INFO - tqdm - accuracy: 0.9962, batch_loss: 0.0009, loss: 0.0111 ||:  84%|########3 | 247/295 [13:39<02:46,  3.47s/it]
2022-03-30 18:26:20,912 - INFO - tqdm - accuracy: 0.9962, batch_loss: 0.0004, loss: 0.0110 ||:  85%|########4 | 250/295 [13:50<02:43,  3.64s/it]
2022-03-30 18:26:33,715 - INFO - tqdm - accuracy: 0.9961, batch_loss: 0.0273, loss: 0.0112 ||:  86%|########6 | 254/295 [14:03<02:15,  3.31s/it]
2022-03-30 18:26:44,003 - INFO - tqdm - accuracy: 0.9961, batch_loss: 0.0141, loss: 0.0111 ||:  87%|########7 | 257/295 [14:13<02:12,  3.48s/it]
2022-03-30 18:26:54,965 - INFO - tqdm - accuracy: 0.9962, batch_loss: 0.0007, loss: 0.0111 ||:  88%|########8 | 260/295 [14:24<02:03,  3.54s/it]
2022-03-30 18:27:06,133 - INFO - tqdm - accuracy: 0.9962, batch_loss: 0.0007, loss: 0.0110 ||:  89%|########9 | 263/295 [14:35<01:59,  3.73s/it]
2022-03-30 18:27:16,306 - INFO - tqdm - accuracy: 0.9962, batch_loss: 0.0004, loss: 0.0108 ||:  90%|######### | 266/295 [14:45<01:39,  3.45s/it]
2022-03-30 18:27:29,876 - INFO - tqdm - accuracy: 0.9962, batch_loss: 0.0006, loss: 0.0108 ||:  92%|#########1| 270/295 [14:59<01:27,  3.49s/it]
2022-03-30 18:27:40,192 - INFO - tqdm - accuracy: 0.9962, batch_loss: 0.0004, loss: 0.0107 ||:  93%|#########2| 273/295 [15:09<01:15,  3.42s/it]
2022-03-30 18:27:50,316 - INFO - tqdm - accuracy: 0.9963, batch_loss: 0.0009, loss: 0.0106 ||:  94%|#########3| 276/295 [15:19<01:04,  3.41s/it]
2022-03-30 18:28:03,936 - INFO - tqdm - accuracy: 0.9962, batch_loss: 0.0003, loss: 0.0105 ||:  95%|#########4| 280/295 [15:33<00:52,  3.53s/it]
2022-03-30 18:28:14,502 - INFO - tqdm - accuracy: 0.9961, batch_loss: 0.0595, loss: 0.0106 ||:  96%|#########5| 283/295 [15:44<00:42,  3.58s/it]
2022-03-30 18:28:24,685 - INFO - tqdm - accuracy: 0.9962, batch_loss: 0.0124, loss: 0.0106 ||:  97%|#########6| 286/295 [15:54<00:31,  3.46s/it]
2022-03-30 18:28:36,640 - INFO - tqdm - accuracy: 0.9962, batch_loss: 0.0004, loss: 0.0105 ||:  98%|#########8| 290/295 [16:06<00:15,  3.11s/it]
2022-03-30 18:28:47,826 - INFO - tqdm - accuracy: 0.9963, batch_loss: 0.0011, loss: 0.0104 ||:  99%|#########9| 293/295 [16:17<00:07,  3.52s/it]
2022-03-30 18:28:51,623 - INFO - tqdm - accuracy: 0.9962, batch_loss: 0.1636, loss: 0.0109 ||: 100%|#########9| 294/295 [16:21<00:03,  3.61s/it]
2022-03-30 18:28:53,605 - INFO - tqdm - accuracy: 0.9961, batch_loss: 0.1317, loss: 0.0113 ||: 100%|##########| 295/295 [16:23<00:00,  3.12s/it]
2022-03-30 18:28:53,606 - INFO - tqdm - accuracy: 0.9961, batch_loss: 0.1317, loss: 0.0113 ||: 100%|##########| 295/295 [16:23<00:00,  3.33s/it]
2022-03-30 18:28:56,076 - INFO - allennlp.training.trainer - Validating
2022-03-30 18:28:56,077 - INFO - tqdm - 0%|          | 0/1635 [00:00<?, ?it/s]
2022-03-30 18:29:06,130 - INFO - tqdm - accuracy: 0.8878, batch_loss: 0.0001, loss: 0.6578 ||:   9%|8         | 147/1635 [00:10<01:55, 12.85it/s]
2022-03-30 18:29:16,279 - INFO - tqdm - accuracy: 0.8711, batch_loss: 0.0001, loss: 0.8334 ||:  18%|#7        | 287/1635 [00:20<02:02, 11.04it/s]
2022-03-30 18:29:26,312 - INFO - tqdm - accuracy: 0.8703, batch_loss: 0.0003, loss: 0.8216 ||:  26%|##6       | 428/1635 [00:30<01:19, 15.12it/s]
2022-03-30 18:29:36,326 - INFO - tqdm - accuracy: 0.8617, batch_loss: 0.0001, loss: 0.8641 ||:  36%|###5      | 582/1635 [00:40<01:15, 13.93it/s]
2022-03-30 18:29:46,378 - INFO - tqdm - accuracy: 0.8575, batch_loss: 0.0002, loss: 0.8807 ||:  44%|####3     | 716/1635 [00:50<01:05, 14.05it/s]
2022-03-30 18:29:56,506 - INFO - tqdm - accuracy: 0.8576, batch_loss: 0.0001, loss: 0.8818 ||:  53%|#####2    | 864/1635 [01:00<00:51, 15.05it/s]
2022-03-30 18:30:06,548 - INFO - tqdm - accuracy: 0.8562, batch_loss: 3.3884, loss: 0.8925 ||:  62%|######1   | 1012/1635 [01:10<00:41, 15.01it/s]
2022-03-30 18:30:16,595 - INFO - tqdm - accuracy: 0.8542, batch_loss: 0.0001, loss: 0.8987 ||:  71%|#######   | 1156/1635 [01:20<00:34, 13.79it/s]
2022-03-30 18:30:26,622 - INFO - tqdm - accuracy: 0.8564, batch_loss: 0.0001, loss: 0.8820 ||:  80%|#######9  | 1302/1635 [01:30<00:23, 14.40it/s]
2022-03-30 18:30:36,666 - INFO - tqdm - accuracy: 0.8558, batch_loss: 0.0008, loss: 0.8803 ||:  88%|########8 | 1446/1635 [01:40<00:11, 16.04it/s]
2022-03-30 18:30:46,737 - INFO - tqdm - accuracy: 0.8596, batch_loss: 0.0004, loss: 0.8604 ||:  98%|#########7| 1596/1635 [01:50<00:02, 14.07it/s]
2022-03-30 18:30:48,874 - INFO - tqdm - accuracy: 0.8585, batch_loss: 0.0001, loss: 0.8703 ||: 100%|#########9| 1629/1635 [01:52<00:00, 18.88it/s]
2022-03-30 18:30:49,011 - INFO - tqdm - accuracy: 0.8584, batch_loss: 0.0003, loss: 0.8696 ||: 100%|#########9| 1631/1635 [01:52<00:00, 17.59it/s]
2022-03-30 18:30:49,157 - INFO - tqdm - accuracy: 0.8586, batch_loss: 0.0007, loss: 0.8680 ||: 100%|#########9| 1634/1635 [01:53<00:00, 18.51it/s]
2022-03-30 18:30:49,226 - INFO - tqdm - accuracy: 0.8587, batch_loss: 0.0002, loss: 0.8675 ||: 100%|##########| 1635/1635 [01:53<00:00, 14.45it/s]
2022-03-30 18:30:49,226 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-03-30 18:30:49,227 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.996  |     0.859
2022-03-30 18:30:49,228 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9572.260  |       N/A
2022-03-30 18:30:49,229 - INFO - allennlp.training.tensorboard_writer - loss               |     0.011  |     0.867
2022-03-30 18:30:49,229 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  7092.406  |       N/A
2022-03-30 18:30:54,018 - INFO - allennlp.training.trainer - Epoch duration: 0:18:23.639289
2022-03-30 18:30:54,018 - INFO - allennlp.training.checkpointer - loading best weights
2022-03-30 18:30:54,793 - INFO - allennlp.common.util - Metrics: {
  "best_epoch": 5,
  "peak_worker_0_memory_MB": 7092.40625,
  "peak_gpu_0_memory_MB": 9572.26025390625,
  "training_duration": "3:03:37.145542",
  "training_start_epoch": 0,
  "training_epochs": 9,
  "epoch": 9,
  "training_accuracy": 0.9960751034263287,
  "training_loss": 0.011288434436974673,
  "training_worker_0_memory_MB": 7092.40625,
  "training_gpu_0_memory_MB": 9572.26025390625,
  "validation_accuracy": 0.8587155963302753,
  "validation_loss": 0.8674658431906638,
  "best_validation_accuracy": 0.8590214067278288,
  "best_validation_loss": 0.657437597946287
}
2022-03-30 18:30:54,793 - INFO - allennlp.models.archival - archiving weights and vocabulary to ./output_roberta-star/model.tar.gz
