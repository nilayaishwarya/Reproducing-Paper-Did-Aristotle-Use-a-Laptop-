2022-04-10 14:51:19,277 - INFO - allennlp.common.params - random_seed = 42
2022-04-10 14:51:19,277 - INFO - allennlp.common.params - numpy_seed = 42
2022-04-10 14:51:19,277 - INFO - allennlp.common.params - pytorch_seed = 42
2022-04-10 14:51:19,943 - INFO - allennlp.common.checks - Pytorch version: 1.7.1
2022-04-10 14:51:19,943 - INFO - allennlp.common.params - type = default
2022-04-10 14:51:19,943 - INFO - allennlp.common.params - dataset_reader.type = strategy_qa_reader
2022-04-10 14:51:19,944 - INFO - allennlp.common.params - dataset_reader.lazy = False
2022-04-10 14:51:19,944 - INFO - allennlp.common.params - dataset_reader.cache_directory = None
2022-04-10 14:51:19,944 - INFO - allennlp.common.params - dataset_reader.max_instances = None
2022-04-10 14:51:19,944 - INFO - allennlp.common.params - dataset_reader.manual_distributed_sharding = False
2022-04-10 14:51:19,944 - INFO - allennlp.common.params - dataset_reader.manual_multi_process_sharding = False
2022-04-10 14:51:19,944 - INFO - allennlp.common.params - dataset_reader.tokenizer_wrapper.type = default
2022-04-10 14:51:19,944 - INFO - allennlp.common.params - dataset_reader.tokenizer_wrapper.pretrained_model = roberta-large
2022-04-10 14:51:19,945 - INFO - allennlp.common.params - dataset_reader.tokenizer_wrapper.call_kwargs.return_offsets_mapping = True
2022-04-10 14:51:19,945 - INFO - allennlp.common.params - dataset_reader.tokenizer_wrapper.call_kwargs.return_special_tokens_mask = True
2022-04-10 14:51:19,945 - INFO - allennlp.common.params - dataset_reader.tokenizer_wrapper.call_kwargs.truncation = only_first
2022-04-10 14:51:21,130 - INFO - allennlp.common.params - dataset_reader.save_tokenizer = True
2022-04-10 14:51:21,130 - INFO - allennlp.common.params - dataset_reader.is_training = True
2022-04-10 14:51:21,130 - INFO - allennlp.common.params - dataset_reader.pickle.action = None
2022-04-10 14:51:21,131 - INFO - allennlp.common.params - dataset_reader.pickle.file_name = None
2022-04-10 14:51:21,131 - INFO - allennlp.common.params - dataset_reader.pickle.path = ../pickle
2022-04-10 14:51:21,131 - INFO - allennlp.common.params - dataset_reader.pickle.save_even_when_max_instances = False
2022-04-10 14:51:21,131 - INFO - allennlp.common.params - dataset_reader.paragraphs_source = IR-ORA-D
2022-04-10 14:51:21,131 - INFO - allennlp.common.params - dataset_reader.answer_last_decomposition_step = False
2022-04-10 14:51:21,131 - INFO - allennlp.common.params - dataset_reader.skip_if_context_missing = True
2022-04-10 14:51:21,131 - INFO - allennlp.common.params - dataset_reader.paragraphs_limit = 10
2022-04-10 14:51:21,131 - INFO - allennlp.common.params - dataset_reader.generated_decompositions_paths = None
2022-04-10 14:51:21,131 - INFO - allennlp.common.params - dataset_reader.save_elasticsearch_cache = True
2022-04-10 14:51:21,581 - INFO - filelock - Lock 140686297383056 acquired on cache.lock
2022-04-10 14:51:22,264 - INFO - filelock - Lock 140686297383056 released on cache.lock
2022-04-10 14:51:22,264 - INFO - allennlp.common.params - train_data_path = data/strategyqa/train.json
2022-04-10 14:51:22,264 - INFO - allennlp.common.params - vocabulary = <allennlp.common.lazy.Lazy object at 0x7ff414ca3110>
2022-04-10 14:51:22,265 - INFO - allennlp.common.params - datasets_for_vocab_creation = None
2022-04-10 14:51:22,265 - INFO - allennlp.common.params - validation_dataset_reader.type = strategy_qa_reader
2022-04-10 14:51:22,265 - INFO - allennlp.common.params - validation_dataset_reader.lazy = False
2022-04-10 14:51:22,265 - INFO - allennlp.common.params - validation_dataset_reader.cache_directory = None
2022-04-10 14:51:22,265 - INFO - allennlp.common.params - validation_dataset_reader.max_instances = None
2022-04-10 14:51:22,265 - INFO - allennlp.common.params - validation_dataset_reader.manual_distributed_sharding = False
2022-04-10 14:51:22,265 - INFO - allennlp.common.params - validation_dataset_reader.manual_multi_process_sharding = False
2022-04-10 14:51:22,266 - INFO - allennlp.common.params - validation_dataset_reader.tokenizer_wrapper.type = default
2022-04-10 14:51:22,266 - INFO - allennlp.common.params - validation_dataset_reader.tokenizer_wrapper.pretrained_model = roberta-large
2022-04-10 14:51:22,266 - INFO - allennlp.common.params - validation_dataset_reader.tokenizer_wrapper.call_kwargs.return_offsets_mapping = True
2022-04-10 14:51:22,266 - INFO - allennlp.common.params - validation_dataset_reader.tokenizer_wrapper.call_kwargs.return_special_tokens_mask = True
2022-04-10 14:51:22,266 - INFO - allennlp.common.params - validation_dataset_reader.tokenizer_wrapper.call_kwargs.truncation = only_first
2022-04-10 14:51:23,431 - INFO - allennlp.common.params - validation_dataset_reader.save_tokenizer = False
2022-04-10 14:51:23,431 - INFO - allennlp.common.params - validation_dataset_reader.is_training = False
2022-04-10 14:51:23,432 - INFO - allennlp.common.params - validation_dataset_reader.pickle.action = None
2022-04-10 14:51:23,432 - INFO - allennlp.common.params - validation_dataset_reader.pickle.file_name = None
2022-04-10 14:51:23,432 - INFO - allennlp.common.params - validation_dataset_reader.pickle.path = ../pickle
2022-04-10 14:51:23,432 - INFO - allennlp.common.params - validation_dataset_reader.pickle.save_even_when_max_instances = False
2022-04-10 14:51:23,432 - INFO - allennlp.common.params - validation_dataset_reader.paragraphs_source = IR-ORA-D
2022-04-10 14:51:23,432 - INFO - allennlp.common.params - validation_dataset_reader.answer_last_decomposition_step = False
2022-04-10 14:51:23,432 - INFO - allennlp.common.params - validation_dataset_reader.skip_if_context_missing = True
2022-04-10 14:51:23,432 - INFO - allennlp.common.params - validation_dataset_reader.paragraphs_limit = 10
2022-04-10 14:51:23,432 - INFO - allennlp.common.params - validation_dataset_reader.generated_decompositions_paths = None
2022-04-10 14:51:23,432 - INFO - allennlp.common.params - validation_dataset_reader.save_elasticsearch_cache = True
2022-04-10 14:51:23,432 - INFO - filelock - Lock 140686295877840 acquired on cache.lock
2022-04-10 14:51:24,266 - INFO - filelock - Lock 140686295877840 released on cache.lock
2022-04-10 14:51:24,267 - INFO - allennlp.common.params - validation_data_path = data/strategyqa/dev.json
2022-04-10 14:51:24,267 - INFO - allennlp.common.params - validation_data_loader = None
2022-04-10 14:51:24,267 - INFO - allennlp.common.params - test_data_path = None
2022-04-10 14:51:24,267 - INFO - allennlp.common.params - evaluate_on_test = True
2022-04-10 14:51:24,267 - INFO - allennlp.common.params - batch_weight_key = 
2022-04-10 14:51:24,267 - INFO - allennlp.training.util - Reading training data from data/strategyqa/train.json
2022-04-10 14:51:24,268 - INFO - tqdm - reading instances: 0it [00:00, ?it/s]
2022-04-10 14:51:24,268 - INFO - src.data.dataset_readers.base.base_dataset_reader - Reading the dataset from scratch
2022-04-10 14:51:24,268 - INFO - src.data.dataset_readers.strategy_qa_reader - Reading the dataset:
2022-04-10 14:51:24,268 - INFO - src.data.dataset_readers.strategy_qa_reader - Reading file at data/strategyqa/train.json
2022-04-10 14:51:31,515 - INFO - filelock - Lock 140685470034192 acquired on cache.lock
2022-04-10 14:51:32,297 - INFO - filelock - Lock 140685470034192 released on cache.lock
2022-04-10 14:51:32,334 - INFO - allennlp.training.util - Reading validation data from data/strategyqa/dev.json
2022-04-10 14:51:32,335 - INFO - tqdm - reading instances: 0it [00:00, ?it/s]
2022-04-10 14:51:32,335 - INFO - src.data.dataset_readers.base.base_dataset_reader - Reading the dataset from scratch
2022-04-10 14:51:32,335 - INFO - src.data.dataset_readers.strategy_qa_reader - Reading the dataset:
2022-04-10 14:51:32,335 - INFO - src.data.dataset_readers.strategy_qa_reader - Reading file at data/strategyqa/dev.json
2022-04-10 14:51:32,770 - INFO - src.data.dataset_readers.utils.elasticsearch_utils - Empty results from Elasticsearch for: actions commercial pilots take concerning engine noise arriving departing Orange County, California?
2022-04-10 14:51:33,089 - INFO - filelock - Lock 140685477447504 acquired on cache.lock
2022-04-10 14:51:33,848 - INFO - filelock - Lock 140685477447504 released on cache.lock
2022-04-10 14:51:33,883 - INFO - allennlp.common.params - type = from_instances
2022-04-10 14:51:33,883 - INFO - allennlp.common.params - min_count = None
2022-04-10 14:51:33,883 - INFO - allennlp.common.params - max_vocab_size = None
2022-04-10 14:51:33,883 - INFO - allennlp.common.params - non_padded_namespaces = ('*tags', '*labels')
2022-04-10 14:51:33,884 - INFO - allennlp.common.params - pretrained_files = None
2022-04-10 14:51:33,884 - INFO - allennlp.common.params - only_include_pretrained_words = False
2022-04-10 14:51:33,884 - INFO - allennlp.common.params - tokens_to_add = None
2022-04-10 14:51:33,884 - INFO - allennlp.common.params - min_pretrained_embeddings = None
2022-04-10 14:51:33,884 - INFO - allennlp.common.params - padding_token = @@PADDING@@
2022-04-10 14:51:33,884 - INFO - allennlp.common.params - oov_token = @@UNKNOWN@@
2022-04-10 14:51:33,884 - INFO - allennlp.data.vocabulary - Fitting token dictionary from dataset.
2022-04-10 14:51:33,884 - INFO - tqdm - building vocab: 0it [00:00, ?it/s]
2022-04-10 14:51:33,890 - INFO - allennlp.common.params - model.type = hf_classifier
2022-04-10 14:51:33,890 - INFO - allennlp.common.params - model.regularizer = None
2022-04-10 14:51:33,891 - INFO - allennlp.common.params - model.pretrained_model = roberta-large
2022-04-10 14:51:33,891 - INFO - allennlp.common.params - model.tokenizer_wrapper.type = default
2022-04-10 14:51:33,891 - INFO - allennlp.common.params - model.tokenizer_wrapper.pretrained_model = roberta-large
2022-04-10 14:51:33,891 - INFO - allennlp.common.params - model.tokenizer_wrapper.call_kwargs.return_offsets_mapping = True
2022-04-10 14:51:33,891 - INFO - allennlp.common.params - model.tokenizer_wrapper.call_kwargs.return_special_tokens_mask = True
2022-04-10 14:51:33,891 - INFO - allennlp.common.params - model.tokenizer_wrapper.call_kwargs.truncation = only_first
2022-04-10 14:51:35,280 - INFO - allennlp.common.params - model.num_labels = 2
2022-04-10 14:51:35,280 - INFO - allennlp.common.params - model.label_namespace = labels
2022-04-10 14:51:35,281 - INFO - allennlp.common.params - model.transformer_weights_path = None
2022-04-10 14:51:35,281 - INFO - allennlp.common.params - model.initializer.regexes.0.1.type = pretrained
2022-04-10 14:51:35,281 - INFO - allennlp.common.params - model.initializer.regexes.0.1.weights_file_path = output_roberta-star/model.tar.gz
2022-04-10 14:51:35,282 - INFO - allennlp.models.archival - extracting archive file output_roberta-star/model.tar.gz to temp dir /tmp/tmp9dmvmka3
2022-04-10 14:51:46,977 - INFO - allennlp.models.archival - removing temporary unarchived model dir at /tmp/tmp9dmvmka3
2022-04-10 14:51:47,148 - INFO - allennlp.common.params - model.initializer.prevent_regexes = None
2022-04-10 14:52:01,981 - INFO - allennlp.nn.initializers - Initializing parameters
2022-04-10 14:52:01,981 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.embeddings.word_embeddings.weight using .* initializer
2022-04-10 14:52:01,990 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.embeddings.position_embeddings.weight using .* initializer
2022-04-10 14:52:01,991 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.embeddings.token_type_embeddings.weight using .* initializer
2022-04-10 14:52:01,991 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.embeddings.LayerNorm.weight using .* initializer
2022-04-10 14:52:01,991 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.embeddings.LayerNorm.bias using .* initializer
2022-04-10 14:52:01,991 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.self.query.weight using .* initializer
2022-04-10 14:52:01,992 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.self.query.bias using .* initializer
2022-04-10 14:52:01,992 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.self.key.weight using .* initializer
2022-04-10 14:52:01,992 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.self.key.bias using .* initializer
2022-04-10 14:52:01,992 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.self.value.weight using .* initializer
2022-04-10 14:52:01,993 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.self.value.bias using .* initializer
2022-04-10 14:52:01,993 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.output.dense.weight using .* initializer
2022-04-10 14:52:01,993 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.output.dense.bias using .* initializer
2022-04-10 14:52:01,993 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.output.LayerNorm.weight using .* initializer
2022-04-10 14:52:01,994 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.output.LayerNorm.bias using .* initializer
2022-04-10 14:52:01,994 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.intermediate.dense.weight using .* initializer
2022-04-10 14:52:01,995 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.intermediate.dense.bias using .* initializer
2022-04-10 14:52:01,995 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.output.dense.weight using .* initializer
2022-04-10 14:52:01,996 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.output.dense.bias using .* initializer
2022-04-10 14:52:01,996 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.output.LayerNorm.weight using .* initializer
2022-04-10 14:52:01,996 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.output.LayerNorm.bias using .* initializer
2022-04-10 14:52:01,996 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.self.query.weight using .* initializer
2022-04-10 14:52:01,997 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.self.query.bias using .* initializer
2022-04-10 14:52:01,997 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.self.key.weight using .* initializer
2022-04-10 14:52:01,997 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.self.key.bias using .* initializer
2022-04-10 14:52:01,997 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.self.value.weight using .* initializer
2022-04-10 14:52:01,998 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.self.value.bias using .* initializer
2022-04-10 14:52:01,998 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.output.dense.weight using .* initializer
2022-04-10 14:52:01,998 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.output.dense.bias using .* initializer
2022-04-10 14:52:01,998 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.output.LayerNorm.weight using .* initializer
2022-04-10 14:52:01,999 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.output.LayerNorm.bias using .* initializer
2022-04-10 14:52:01,999 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.intermediate.dense.weight using .* initializer
2022-04-10 14:52:02,000 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.intermediate.dense.bias using .* initializer
2022-04-10 14:52:02,000 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.output.dense.weight using .* initializer
2022-04-10 14:52:02,001 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.output.dense.bias using .* initializer
2022-04-10 14:52:02,001 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.output.LayerNorm.weight using .* initializer
2022-04-10 14:52:02,001 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.output.LayerNorm.bias using .* initializer
2022-04-10 14:52:02,001 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.self.query.weight using .* initializer
2022-04-10 14:52:02,001 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.self.query.bias using .* initializer
2022-04-10 14:52:02,002 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.self.key.weight using .* initializer
2022-04-10 14:52:02,002 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.self.key.bias using .* initializer
2022-04-10 14:52:02,002 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.self.value.weight using .* initializer
2022-04-10 14:52:02,002 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.self.value.bias using .* initializer
2022-04-10 14:52:02,003 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.output.dense.weight using .* initializer
2022-04-10 14:52:02,003 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.output.dense.bias using .* initializer
2022-04-10 14:52:02,003 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.output.LayerNorm.weight using .* initializer
2022-04-10 14:52:02,003 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.output.LayerNorm.bias using .* initializer
2022-04-10 14:52:02,003 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.intermediate.dense.weight using .* initializer
2022-04-10 14:52:02,004 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.intermediate.dense.bias using .* initializer
2022-04-10 14:52:02,004 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.output.dense.weight using .* initializer
2022-04-10 14:52:02,005 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.output.dense.bias using .* initializer
2022-04-10 14:52:02,005 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.output.LayerNorm.weight using .* initializer
2022-04-10 14:52:02,005 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.output.LayerNorm.bias using .* initializer
2022-04-10 14:52:02,006 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.self.query.weight using .* initializer
2022-04-10 14:52:02,006 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.self.query.bias using .* initializer
2022-04-10 14:52:02,006 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.self.key.weight using .* initializer
2022-04-10 14:52:02,006 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.self.key.bias using .* initializer
2022-04-10 14:52:02,007 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.self.value.weight using .* initializer
2022-04-10 14:52:02,007 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.self.value.bias using .* initializer
2022-04-10 14:52:02,007 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.output.dense.weight using .* initializer
2022-04-10 14:52:02,007 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.output.dense.bias using .* initializer
2022-04-10 14:52:02,008 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.output.LayerNorm.weight using .* initializer
2022-04-10 14:52:02,008 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.output.LayerNorm.bias using .* initializer
2022-04-10 14:52:02,008 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.intermediate.dense.weight using .* initializer
2022-04-10 14:52:02,009 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.intermediate.dense.bias using .* initializer
2022-04-10 14:52:02,009 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.output.dense.weight using .* initializer
2022-04-10 14:52:02,010 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.output.dense.bias using .* initializer
2022-04-10 14:52:02,010 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.output.LayerNorm.weight using .* initializer
2022-04-10 14:52:02,010 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.output.LayerNorm.bias using .* initializer
2022-04-10 14:52:02,010 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.self.query.weight using .* initializer
2022-04-10 14:52:02,011 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.self.query.bias using .* initializer
2022-04-10 14:52:02,011 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.self.key.weight using .* initializer
2022-04-10 14:52:02,011 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.self.key.bias using .* initializer
2022-04-10 14:52:02,011 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.self.value.weight using .* initializer
2022-04-10 14:52:02,012 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.self.value.bias using .* initializer
2022-04-10 14:52:02,012 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.output.dense.weight using .* initializer
2022-04-10 14:52:02,012 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.output.dense.bias using .* initializer
2022-04-10 14:52:02,012 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.output.LayerNorm.weight using .* initializer
2022-04-10 14:52:02,012 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.output.LayerNorm.bias using .* initializer
2022-04-10 14:52:02,012 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.intermediate.dense.weight using .* initializer
2022-04-10 14:52:02,013 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.intermediate.dense.bias using .* initializer
2022-04-10 14:52:02,014 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.output.dense.weight using .* initializer
2022-04-10 14:52:02,015 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.output.dense.bias using .* initializer
2022-04-10 14:52:02,015 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.output.LayerNorm.weight using .* initializer
2022-04-10 14:52:02,015 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.output.LayerNorm.bias using .* initializer
2022-04-10 14:52:02,015 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.self.query.weight using .* initializer
2022-04-10 14:52:02,015 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.self.query.bias using .* initializer
2022-04-10 14:52:02,015 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.self.key.weight using .* initializer
2022-04-10 14:52:02,016 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.self.key.bias using .* initializer
2022-04-10 14:52:02,016 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.self.value.weight using .* initializer
2022-04-10 14:52:02,016 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.self.value.bias using .* initializer
2022-04-10 14:52:02,016 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.output.dense.weight using .* initializer
2022-04-10 14:52:02,017 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.output.dense.bias using .* initializer
2022-04-10 14:52:02,017 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.output.LayerNorm.weight using .* initializer
2022-04-10 14:52:02,017 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.output.LayerNorm.bias using .* initializer
2022-04-10 14:52:02,017 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.intermediate.dense.weight using .* initializer
2022-04-10 14:52:02,018 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.intermediate.dense.bias using .* initializer
2022-04-10 14:52:02,018 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.output.dense.weight using .* initializer
2022-04-10 14:52:02,019 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.output.dense.bias using .* initializer
2022-04-10 14:52:02,019 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.output.LayerNorm.weight using .* initializer
2022-04-10 14:52:02,019 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.output.LayerNorm.bias using .* initializer
2022-04-10 14:52:02,020 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.self.query.weight using .* initializer
2022-04-10 14:52:02,020 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.self.query.bias using .* initializer
2022-04-10 14:52:02,020 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.self.key.weight using .* initializer
2022-04-10 14:52:02,020 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.self.key.bias using .* initializer
2022-04-10 14:52:02,020 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.self.value.weight using .* initializer
2022-04-10 14:52:02,021 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.self.value.bias using .* initializer
2022-04-10 14:52:02,021 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.output.dense.weight using .* initializer
2022-04-10 14:52:02,021 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.output.dense.bias using .* initializer
2022-04-10 14:52:02,021 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.output.LayerNorm.weight using .* initializer
2022-04-10 14:52:02,022 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.output.LayerNorm.bias using .* initializer
2022-04-10 14:52:02,022 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.intermediate.dense.weight using .* initializer
2022-04-10 14:52:02,023 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.intermediate.dense.bias using .* initializer
2022-04-10 14:52:02,023 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.output.dense.weight using .* initializer
2022-04-10 14:52:02,024 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.output.dense.bias using .* initializer
2022-04-10 14:52:02,024 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.output.LayerNorm.weight using .* initializer
2022-04-10 14:52:02,024 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.output.LayerNorm.bias using .* initializer
2022-04-10 14:52:02,024 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.self.query.weight using .* initializer
2022-04-10 14:52:02,024 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.self.query.bias using .* initializer
2022-04-10 14:52:02,024 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.self.key.weight using .* initializer
2022-04-10 14:52:02,025 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.self.key.bias using .* initializer
2022-04-10 14:52:02,025 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.self.value.weight using .* initializer
2022-04-10 14:52:02,025 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.self.value.bias using .* initializer
2022-04-10 14:52:02,025 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.output.dense.weight using .* initializer
2022-04-10 14:52:02,026 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.output.dense.bias using .* initializer
2022-04-10 14:52:02,026 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.output.LayerNorm.weight using .* initializer
2022-04-10 14:52:02,026 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.output.LayerNorm.bias using .* initializer
2022-04-10 14:52:02,026 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.intermediate.dense.weight using .* initializer
2022-04-10 14:52:02,027 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.intermediate.dense.bias using .* initializer
2022-04-10 14:52:02,027 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.output.dense.weight using .* initializer
2022-04-10 14:52:02,028 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.output.dense.bias using .* initializer
2022-04-10 14:52:02,028 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.output.LayerNorm.weight using .* initializer
2022-04-10 14:52:02,029 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.output.LayerNorm.bias using .* initializer
2022-04-10 14:52:02,029 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.self.query.weight using .* initializer
2022-04-10 14:52:02,029 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.self.query.bias using .* initializer
2022-04-10 14:52:02,029 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.self.key.weight using .* initializer
2022-04-10 14:52:02,029 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.self.key.bias using .* initializer
2022-04-10 14:52:02,030 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.self.value.weight using .* initializer
2022-04-10 14:52:02,030 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.self.value.bias using .* initializer
2022-04-10 14:52:02,030 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.output.dense.weight using .* initializer
2022-04-10 14:52:02,030 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.output.dense.bias using .* initializer
2022-04-10 14:52:02,030 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.output.LayerNorm.weight using .* initializer
2022-04-10 14:52:02,031 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.output.LayerNorm.bias using .* initializer
2022-04-10 14:52:02,031 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.intermediate.dense.weight using .* initializer
2022-04-10 14:52:02,032 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.intermediate.dense.bias using .* initializer
2022-04-10 14:52:02,032 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.output.dense.weight using .* initializer
2022-04-10 14:52:02,033 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.output.dense.bias using .* initializer
2022-04-10 14:52:02,033 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.output.LayerNorm.weight using .* initializer
2022-04-10 14:52:02,033 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.output.LayerNorm.bias using .* initializer
2022-04-10 14:52:02,033 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.self.query.weight using .* initializer
2022-04-10 14:52:02,033 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.self.query.bias using .* initializer
2022-04-10 14:52:02,033 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.self.key.weight using .* initializer
2022-04-10 14:52:02,034 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.self.key.bias using .* initializer
2022-04-10 14:52:02,034 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.self.value.weight using .* initializer
2022-04-10 14:52:02,034 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.self.value.bias using .* initializer
2022-04-10 14:52:02,034 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.output.dense.weight using .* initializer
2022-04-10 14:52:02,035 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.output.dense.bias using .* initializer
2022-04-10 14:52:02,035 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.output.LayerNorm.weight using .* initializer
2022-04-10 14:52:02,035 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.output.LayerNorm.bias using .* initializer
2022-04-10 14:52:02,035 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.intermediate.dense.weight using .* initializer
2022-04-10 14:52:02,036 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.intermediate.dense.bias using .* initializer
2022-04-10 14:52:02,036 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.output.dense.weight using .* initializer
2022-04-10 14:52:02,037 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.output.dense.bias using .* initializer
2022-04-10 14:52:02,037 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.output.LayerNorm.weight using .* initializer
2022-04-10 14:52:02,037 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.output.LayerNorm.bias using .* initializer
2022-04-10 14:52:02,038 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.self.query.weight using .* initializer
2022-04-10 14:52:02,038 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.self.query.bias using .* initializer
2022-04-10 14:52:02,038 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.self.key.weight using .* initializer
2022-04-10 14:52:02,038 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.self.key.bias using .* initializer
2022-04-10 14:52:02,039 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.self.value.weight using .* initializer
2022-04-10 14:52:02,039 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.self.value.bias using .* initializer
2022-04-10 14:52:02,039 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.output.dense.weight using .* initializer
2022-04-10 14:52:02,039 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.output.dense.bias using .* initializer
2022-04-10 14:52:02,039 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.output.LayerNorm.weight using .* initializer
2022-04-10 14:52:02,040 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.output.LayerNorm.bias using .* initializer
2022-04-10 14:52:02,040 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.intermediate.dense.weight using .* initializer
2022-04-10 14:52:02,041 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.intermediate.dense.bias using .* initializer
2022-04-10 14:52:02,041 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.output.dense.weight using .* initializer
2022-04-10 14:52:02,042 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.output.dense.bias using .* initializer
2022-04-10 14:52:02,042 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.output.LayerNorm.weight using .* initializer
2022-04-10 14:52:02,042 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.output.LayerNorm.bias using .* initializer
2022-04-10 14:52:02,042 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.self.query.weight using .* initializer
2022-04-10 14:52:02,042 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.self.query.bias using .* initializer
2022-04-10 14:52:02,043 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.self.key.weight using .* initializer
2022-04-10 14:52:02,043 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.self.key.bias using .* initializer
2022-04-10 14:52:02,043 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.self.value.weight using .* initializer
2022-04-10 14:52:02,043 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.self.value.bias using .* initializer
2022-04-10 14:52:02,043 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.output.dense.weight using .* initializer
2022-04-10 14:52:02,044 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.output.dense.bias using .* initializer
2022-04-10 14:52:02,044 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.output.LayerNorm.weight using .* initializer
2022-04-10 14:52:02,044 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.output.LayerNorm.bias using .* initializer
2022-04-10 14:52:02,044 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.intermediate.dense.weight using .* initializer
2022-04-10 14:52:02,045 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.intermediate.dense.bias using .* initializer
2022-04-10 14:52:02,045 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.output.dense.weight using .* initializer
2022-04-10 14:52:02,046 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.output.dense.bias using .* initializer
2022-04-10 14:52:02,046 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.output.LayerNorm.weight using .* initializer
2022-04-10 14:52:02,046 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.output.LayerNorm.bias using .* initializer
2022-04-10 14:52:02,047 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.self.query.weight using .* initializer
2022-04-10 14:52:02,047 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.self.query.bias using .* initializer
2022-04-10 14:52:02,047 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.self.key.weight using .* initializer
2022-04-10 14:52:02,047 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.self.key.bias using .* initializer
2022-04-10 14:52:02,048 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.self.value.weight using .* initializer
2022-04-10 14:52:02,048 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.self.value.bias using .* initializer
2022-04-10 14:52:02,048 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.output.dense.weight using .* initializer
2022-04-10 14:52:02,048 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.output.dense.bias using .* initializer
2022-04-10 14:52:02,049 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.output.LayerNorm.weight using .* initializer
2022-04-10 14:52:02,049 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.output.LayerNorm.bias using .* initializer
2022-04-10 14:52:02,049 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.intermediate.dense.weight using .* initializer
2022-04-10 14:52:02,050 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.intermediate.dense.bias using .* initializer
2022-04-10 14:52:02,050 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.output.dense.weight using .* initializer
2022-04-10 14:52:02,051 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.output.dense.bias using .* initializer
2022-04-10 14:52:02,051 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.output.LayerNorm.weight using .* initializer
2022-04-10 14:52:02,051 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.output.LayerNorm.bias using .* initializer
2022-04-10 14:52:02,051 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.self.query.weight using .* initializer
2022-04-10 14:52:02,052 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.self.query.bias using .* initializer
2022-04-10 14:52:02,052 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.self.key.weight using .* initializer
2022-04-10 14:52:02,052 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.self.key.bias using .* initializer
2022-04-10 14:52:02,052 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.self.value.weight using .* initializer
2022-04-10 14:52:02,052 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.self.value.bias using .* initializer
2022-04-10 14:52:02,053 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.output.dense.weight using .* initializer
2022-04-10 14:52:02,053 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.output.dense.bias using .* initializer
2022-04-10 14:52:02,053 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.output.LayerNorm.weight using .* initializer
2022-04-10 14:52:02,053 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.output.LayerNorm.bias using .* initializer
2022-04-10 14:52:02,053 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.intermediate.dense.weight using .* initializer
2022-04-10 14:52:02,054 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.intermediate.dense.bias using .* initializer
2022-04-10 14:52:02,054 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.output.dense.weight using .* initializer
2022-04-10 14:52:02,055 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.output.dense.bias using .* initializer
2022-04-10 14:52:02,055 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.output.LayerNorm.weight using .* initializer
2022-04-10 14:52:02,056 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.output.LayerNorm.bias using .* initializer
2022-04-10 14:52:02,056 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.self.query.weight using .* initializer
2022-04-10 14:52:02,056 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.self.query.bias using .* initializer
2022-04-10 14:52:02,056 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.self.key.weight using .* initializer
2022-04-10 14:52:02,057 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.self.key.bias using .* initializer
2022-04-10 14:52:02,057 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.self.value.weight using .* initializer
2022-04-10 14:52:02,057 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.self.value.bias using .* initializer
2022-04-10 14:52:02,057 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.output.dense.weight using .* initializer
2022-04-10 14:52:02,058 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.output.dense.bias using .* initializer
2022-04-10 14:52:02,058 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.output.LayerNorm.weight using .* initializer
2022-04-10 14:52:02,058 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.output.LayerNorm.bias using .* initializer
2022-04-10 14:52:02,058 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.intermediate.dense.weight using .* initializer
2022-04-10 14:52:02,059 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.intermediate.dense.bias using .* initializer
2022-04-10 14:52:02,059 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.output.dense.weight using .* initializer
2022-04-10 14:52:02,060 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.output.dense.bias using .* initializer
2022-04-10 14:52:02,060 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.output.LayerNorm.weight using .* initializer
2022-04-10 14:52:02,060 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.output.LayerNorm.bias using .* initializer
2022-04-10 14:52:02,060 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.self.query.weight using .* initializer
2022-04-10 14:52:02,061 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.self.query.bias using .* initializer
2022-04-10 14:52:02,061 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.self.key.weight using .* initializer
2022-04-10 14:52:02,061 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.self.key.bias using .* initializer
2022-04-10 14:52:02,061 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.self.value.weight using .* initializer
2022-04-10 14:52:02,062 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.self.value.bias using .* initializer
2022-04-10 14:52:02,062 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.output.dense.weight using .* initializer
2022-04-10 14:52:02,062 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.output.dense.bias using .* initializer
2022-04-10 14:52:02,062 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.output.LayerNorm.weight using .* initializer
2022-04-10 14:52:02,063 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.output.LayerNorm.bias using .* initializer
2022-04-10 14:52:02,063 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.intermediate.dense.weight using .* initializer
2022-04-10 14:52:02,064 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.intermediate.dense.bias using .* initializer
2022-04-10 14:52:02,064 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.output.dense.weight using .* initializer
2022-04-10 14:52:02,065 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.output.dense.bias using .* initializer
2022-04-10 14:52:02,065 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.output.LayerNorm.weight using .* initializer
2022-04-10 14:52:02,065 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.output.LayerNorm.bias using .* initializer
2022-04-10 14:52:02,065 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.self.query.weight using .* initializer
2022-04-10 14:52:02,065 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.self.query.bias using .* initializer
2022-04-10 14:52:02,066 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.self.key.weight using .* initializer
2022-04-10 14:52:02,066 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.self.key.bias using .* initializer
2022-04-10 14:52:02,066 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.self.value.weight using .* initializer
2022-04-10 14:52:02,066 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.self.value.bias using .* initializer
2022-04-10 14:52:02,067 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.output.dense.weight using .* initializer
2022-04-10 14:52:02,067 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.output.dense.bias using .* initializer
2022-04-10 14:52:02,067 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.output.LayerNorm.weight using .* initializer
2022-04-10 14:52:02,067 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.output.LayerNorm.bias using .* initializer
2022-04-10 14:52:02,067 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.intermediate.dense.weight using .* initializer
2022-04-10 14:52:02,068 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.intermediate.dense.bias using .* initializer
2022-04-10 14:52:02,069 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.output.dense.weight using .* initializer
2022-04-10 14:52:02,070 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.output.dense.bias using .* initializer
2022-04-10 14:52:02,070 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.output.LayerNorm.weight using .* initializer
2022-04-10 14:52:02,070 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.output.LayerNorm.bias using .* initializer
2022-04-10 14:52:02,070 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.self.query.weight using .* initializer
2022-04-10 14:52:02,070 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.self.query.bias using .* initializer
2022-04-10 14:52:02,070 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.self.key.weight using .* initializer
2022-04-10 14:52:02,071 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.self.key.bias using .* initializer
2022-04-10 14:52:02,071 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.self.value.weight using .* initializer
2022-04-10 14:52:02,071 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.self.value.bias using .* initializer
2022-04-10 14:52:02,071 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.output.dense.weight using .* initializer
2022-04-10 14:52:02,072 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.output.dense.bias using .* initializer
2022-04-10 14:52:02,072 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.output.LayerNorm.weight using .* initializer
2022-04-10 14:52:02,072 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.output.LayerNorm.bias using .* initializer
2022-04-10 14:52:02,072 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.intermediate.dense.weight using .* initializer
2022-04-10 14:52:02,073 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.intermediate.dense.bias using .* initializer
2022-04-10 14:52:02,073 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.output.dense.weight using .* initializer
2022-04-10 14:52:02,074 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.output.dense.bias using .* initializer
2022-04-10 14:52:02,074 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.output.LayerNorm.weight using .* initializer
2022-04-10 14:52:02,075 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.output.LayerNorm.bias using .* initializer
2022-04-10 14:52:02,075 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.self.query.weight using .* initializer
2022-04-10 14:52:02,075 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.self.query.bias using .* initializer
2022-04-10 14:52:02,075 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.self.key.weight using .* initializer
2022-04-10 14:52:02,075 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.self.key.bias using .* initializer
2022-04-10 14:52:02,076 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.self.value.weight using .* initializer
2022-04-10 14:52:02,076 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.self.value.bias using .* initializer
2022-04-10 14:52:02,076 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.output.dense.weight using .* initializer
2022-04-10 14:52:02,076 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.output.dense.bias using .* initializer
2022-04-10 14:52:02,077 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.output.LayerNorm.weight using .* initializer
2022-04-10 14:52:02,077 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.output.LayerNorm.bias using .* initializer
2022-04-10 14:52:02,077 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.intermediate.dense.weight using .* initializer
2022-04-10 14:52:02,078 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.intermediate.dense.bias using .* initializer
2022-04-10 14:52:02,078 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.output.dense.weight using .* initializer
2022-04-10 14:52:02,079 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.output.dense.bias using .* initializer
2022-04-10 14:52:02,079 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.output.LayerNorm.weight using .* initializer
2022-04-10 14:52:02,079 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.output.LayerNorm.bias using .* initializer
2022-04-10 14:52:02,079 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.self.query.weight using .* initializer
2022-04-10 14:52:02,080 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.self.query.bias using .* initializer
2022-04-10 14:52:02,080 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.self.key.weight using .* initializer
2022-04-10 14:52:02,080 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.self.key.bias using .* initializer
2022-04-10 14:52:02,080 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.self.value.weight using .* initializer
2022-04-10 14:52:02,081 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.self.value.bias using .* initializer
2022-04-10 14:52:02,081 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.output.dense.weight using .* initializer
2022-04-10 14:52:02,081 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.output.dense.bias using .* initializer
2022-04-10 14:52:02,081 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.output.LayerNorm.weight using .* initializer
2022-04-10 14:52:02,081 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.output.LayerNorm.bias using .* initializer
2022-04-10 14:52:02,082 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.intermediate.dense.weight using .* initializer
2022-04-10 14:52:02,083 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.intermediate.dense.bias using .* initializer
2022-04-10 14:52:02,083 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.output.dense.weight using .* initializer
2022-04-10 14:52:02,084 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.output.dense.bias using .* initializer
2022-04-10 14:52:02,084 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.output.LayerNorm.weight using .* initializer
2022-04-10 14:52:02,084 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.output.LayerNorm.bias using .* initializer
2022-04-10 14:52:02,084 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.self.query.weight using .* initializer
2022-04-10 14:52:02,084 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.self.query.bias using .* initializer
2022-04-10 14:52:02,085 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.self.key.weight using .* initializer
2022-04-10 14:52:02,085 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.self.key.bias using .* initializer
2022-04-10 14:52:02,085 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.self.value.weight using .* initializer
2022-04-10 14:52:02,085 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.self.value.bias using .* initializer
2022-04-10 14:52:02,086 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.output.dense.weight using .* initializer
2022-04-10 14:52:02,086 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.output.dense.bias using .* initializer
2022-04-10 14:52:02,086 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.output.LayerNorm.weight using .* initializer
2022-04-10 14:52:02,086 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.output.LayerNorm.bias using .* initializer
2022-04-10 14:52:02,086 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.intermediate.dense.weight using .* initializer
2022-04-10 14:52:02,087 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.intermediate.dense.bias using .* initializer
2022-04-10 14:52:02,087 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.output.dense.weight using .* initializer
2022-04-10 14:52:02,088 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.output.dense.bias using .* initializer
2022-04-10 14:52:02,089 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.output.LayerNorm.weight using .* initializer
2022-04-10 14:52:02,089 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.output.LayerNorm.bias using .* initializer
2022-04-10 14:52:02,089 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.self.query.weight using .* initializer
2022-04-10 14:52:02,089 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.self.query.bias using .* initializer
2022-04-10 14:52:02,089 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.self.key.weight using .* initializer
2022-04-10 14:52:02,090 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.self.key.bias using .* initializer
2022-04-10 14:52:02,090 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.self.value.weight using .* initializer
2022-04-10 14:52:02,090 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.self.value.bias using .* initializer
2022-04-10 14:52:02,090 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.output.dense.weight using .* initializer
2022-04-10 14:52:02,091 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.output.dense.bias using .* initializer
2022-04-10 14:52:02,091 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.output.LayerNorm.weight using .* initializer
2022-04-10 14:52:02,091 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.output.LayerNorm.bias using .* initializer
2022-04-10 14:52:02,091 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.intermediate.dense.weight using .* initializer
2022-04-10 14:52:02,092 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.intermediate.dense.bias using .* initializer
2022-04-10 14:52:02,092 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.output.dense.weight using .* initializer
2022-04-10 14:52:02,093 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.output.dense.bias using .* initializer
2022-04-10 14:52:02,093 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.output.LayerNorm.weight using .* initializer
2022-04-10 14:52:02,093 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.output.LayerNorm.bias using .* initializer
2022-04-10 14:52:02,094 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.self.query.weight using .* initializer
2022-04-10 14:52:02,094 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.self.query.bias using .* initializer
2022-04-10 14:52:02,094 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.self.key.weight using .* initializer
2022-04-10 14:52:02,094 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.self.key.bias using .* initializer
2022-04-10 14:52:02,095 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.self.value.weight using .* initializer
2022-04-10 14:52:02,095 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.self.value.bias using .* initializer
2022-04-10 14:52:02,095 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.output.dense.weight using .* initializer
2022-04-10 14:52:02,095 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.output.dense.bias using .* initializer
2022-04-10 14:52:02,096 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.output.LayerNorm.weight using .* initializer
2022-04-10 14:52:02,096 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.output.LayerNorm.bias using .* initializer
2022-04-10 14:52:02,096 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.intermediate.dense.weight using .* initializer
2022-04-10 14:52:02,097 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.intermediate.dense.bias using .* initializer
2022-04-10 14:52:02,097 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.output.dense.weight using .* initializer
2022-04-10 14:52:02,098 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.output.dense.bias using .* initializer
2022-04-10 14:52:02,098 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.output.LayerNorm.weight using .* initializer
2022-04-10 14:52:02,098 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.output.LayerNorm.bias using .* initializer
2022-04-10 14:52:02,098 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.self.query.weight using .* initializer
2022-04-10 14:52:02,099 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.self.query.bias using .* initializer
2022-04-10 14:52:02,099 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.self.key.weight using .* initializer
2022-04-10 14:52:02,099 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.self.key.bias using .* initializer
2022-04-10 14:52:02,099 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.self.value.weight using .* initializer
2022-04-10 14:52:02,100 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.self.value.bias using .* initializer
2022-04-10 14:52:02,100 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.output.dense.weight using .* initializer
2022-04-10 14:52:02,100 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.output.dense.bias using .* initializer
2022-04-10 14:52:02,100 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.output.LayerNorm.weight using .* initializer
2022-04-10 14:52:02,100 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.output.LayerNorm.bias using .* initializer
2022-04-10 14:52:02,101 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.intermediate.dense.weight using .* initializer
2022-04-10 14:52:02,102 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.intermediate.dense.bias using .* initializer
2022-04-10 14:52:02,102 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.output.dense.weight using .* initializer
2022-04-10 14:52:02,103 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.output.dense.bias using .* initializer
2022-04-10 14:52:02,103 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.output.LayerNorm.weight using .* initializer
2022-04-10 14:52:02,103 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.output.LayerNorm.bias using .* initializer
2022-04-10 14:52:02,103 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.pooler.dense.weight using .* initializer
2022-04-10 14:52:02,103 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.pooler.dense.bias using .* initializer
2022-04-10 14:52:02,104 - INFO - allennlp.nn.initializers - Initializing _classifier.classifier.dense.weight using .* initializer
2022-04-10 14:52:02,104 - INFO - allennlp.nn.initializers - Initializing _classifier.classifier.dense.bias using .* initializer
2022-04-10 14:52:02,104 - INFO - allennlp.nn.initializers - Initializing _classifier.classifier.out_proj.weight using .* initializer
2022-04-10 14:52:02,104 - INFO - allennlp.nn.initializers - Initializing _classifier.classifier.out_proj.bias using .* initializer
2022-04-10 14:52:02,104 - INFO - allennlp.nn.initializers - Done initializing parameters; the following parameters are using their default initialization from their code
2022-04-10 14:52:02,660 - INFO - filelock - Lock 140685470032336 acquired on ./output_ir-ora-d_hyper4/vocabulary/.lock
2022-04-10 14:52:02,660 - INFO - filelock - Lock 140685470032336 released on ./output_ir-ora-d_hyper4/vocabulary/.lock
2022-04-10 14:52:02,660 - INFO - allennlp.common.params - data_loader.type = pytorch_dataloader
2022-04-10 14:52:02,661 - INFO - allennlp.common.params - data_loader.batch_size = 1
2022-04-10 14:52:02,661 - INFO - allennlp.common.params - data_loader.shuffle = False
2022-04-10 14:52:02,661 - INFO - allennlp.common.params - data_loader.sampler = None
2022-04-10 14:52:02,661 - INFO - allennlp.common.params - data_loader.num_workers = 0
2022-04-10 14:52:02,662 - INFO - allennlp.common.params - data_loader.pin_memory = False
2022-04-10 14:52:02,662 - INFO - allennlp.common.params - data_loader.drop_last = False
2022-04-10 14:52:02,662 - INFO - allennlp.common.params - data_loader.timeout = 0
2022-04-10 14:52:02,662 - INFO - allennlp.common.params - data_loader.worker_init_fn = None
2022-04-10 14:52:02,662 - INFO - allennlp.common.params - data_loader.multiprocessing_context = None
2022-04-10 14:52:02,662 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None
2022-04-10 14:52:02,662 - INFO - allennlp.common.params - data_loader.batch_sampler.type = bucket
2022-04-10 14:52:02,662 - INFO - allennlp.common.params - data_loader.batch_sampler.batch_size = 2
2022-04-10 14:52:02,662 - INFO - allennlp.common.params - data_loader.batch_sampler.sorting_keys = None
2022-04-10 14:52:02,663 - INFO - allennlp.common.params - data_loader.batch_sampler.padding_noise = 0.1
2022-04-10 14:52:02,663 - INFO - allennlp.common.params - data_loader.batch_sampler.drop_last = False
2022-04-10 14:52:02,663 - INFO - allennlp.common.params - data_loader.type = pytorch_dataloader
2022-04-10 14:52:02,663 - INFO - allennlp.common.params - data_loader.batch_size = 1
2022-04-10 14:52:02,664 - INFO - allennlp.common.params - data_loader.shuffle = False
2022-04-10 14:52:02,664 - INFO - allennlp.common.params - data_loader.sampler = None
2022-04-10 14:52:02,664 - INFO - allennlp.common.params - data_loader.num_workers = 0
2022-04-10 14:52:02,664 - INFO - allennlp.common.params - data_loader.pin_memory = False
2022-04-10 14:52:02,664 - INFO - allennlp.common.params - data_loader.drop_last = False
2022-04-10 14:52:02,664 - INFO - allennlp.common.params - data_loader.timeout = 0
2022-04-10 14:52:02,664 - INFO - allennlp.common.params - data_loader.worker_init_fn = None
2022-04-10 14:52:02,664 - INFO - allennlp.common.params - data_loader.multiprocessing_context = None
2022-04-10 14:52:02,664 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None
2022-04-10 14:52:02,664 - INFO - allennlp.common.params - data_loader.batch_sampler.type = bucket
2022-04-10 14:52:02,665 - INFO - allennlp.common.params - data_loader.batch_sampler.batch_size = 2
2022-04-10 14:52:02,665 - INFO - allennlp.common.params - data_loader.batch_sampler.sorting_keys = None
2022-04-10 14:52:02,665 - INFO - allennlp.common.params - data_loader.batch_sampler.padding_noise = 0.1
2022-04-10 14:52:02,665 - INFO - allennlp.common.params - data_loader.batch_sampler.drop_last = False
2022-04-10 14:52:02,665 - INFO - allennlp.common.params - trainer.type = gradient_descent
2022-04-10 14:52:02,666 - INFO - allennlp.common.params - trainer.patience = None
2022-04-10 14:52:02,666 - INFO - allennlp.common.params - trainer.validation_metric = +accuracy
2022-04-10 14:52:02,666 - INFO - allennlp.common.params - trainer.num_epochs = 15
2022-04-10 14:52:02,666 - INFO - allennlp.common.params - trainer.cuda_device = 0
2022-04-10 14:52:02,666 - INFO - allennlp.common.params - trainer.grad_norm = None
2022-04-10 14:52:02,666 - INFO - allennlp.common.params - trainer.grad_clipping = None
2022-04-10 14:52:02,666 - INFO - allennlp.common.params - trainer.distributed = False
2022-04-10 14:52:02,666 - INFO - allennlp.common.params - trainer.world_size = 1
2022-04-10 14:52:02,667 - INFO - allennlp.common.params - trainer.num_gradient_accumulation_steps = 16
2022-04-10 14:52:02,667 - INFO - allennlp.common.params - trainer.use_amp = False
2022-04-10 14:52:02,667 - INFO - allennlp.common.params - trainer.no_grad = None
2022-04-10 14:52:02,667 - INFO - allennlp.common.params - trainer.momentum_scheduler = None
2022-04-10 14:52:02,667 - INFO - allennlp.common.params - trainer.tensorboard_writer = <allennlp.common.lazy.Lazy object at 0x7ff40a84e410>
2022-04-10 14:52:02,667 - INFO - allennlp.common.params - trainer.moving_average = None
2022-04-10 14:52:02,667 - INFO - allennlp.common.params - trainer.checkpointer = <allennlp.common.lazy.Lazy object at 0x7ff40a84e490>
2022-04-10 14:52:02,667 - INFO - allennlp.common.params - trainer.batch_callbacks = None
2022-04-10 14:52:02,668 - INFO - allennlp.common.params - trainer.epoch_callbacks = None
2022-04-10 14:52:02,668 - INFO - allennlp.common.params - trainer.end_callbacks = None
2022-04-10 14:52:02,668 - INFO - allennlp.common.params - trainer.trainer_callbacks = None
2022-04-10 14:52:05,480 - INFO - allennlp.common.params - trainer.optimizer.type = huggingface_adamw
2022-04-10 14:52:05,481 - INFO - allennlp.common.params - trainer.optimizer.parameter_groups = None
2022-04-10 14:52:05,481 - INFO - allennlp.common.params - trainer.optimizer.lr = 5e-06
2022-04-10 14:52:05,481 - INFO - allennlp.common.params - trainer.optimizer.betas = [0.9, 0.98]
2022-04-10 14:52:05,481 - INFO - allennlp.common.params - trainer.optimizer.eps = 1e-06
2022-04-10 14:52:05,481 - INFO - allennlp.common.params - trainer.optimizer.weight_decay = 0.1
2022-04-10 14:52:05,481 - INFO - allennlp.common.params - trainer.optimizer.correct_bias = True
2022-04-10 14:52:05,482 - INFO - allennlp.training.optimizers - Number of trainable parameters: 356411394
2022-04-10 14:52:05,483 - INFO - allennlp.common.util - The following parameters are Frozen (without gradient):
2022-04-10 14:52:05,486 - INFO - allennlp.common.util - The following parameters are Tunable (with gradient):
2022-04-10 14:52:05,486 - INFO - allennlp.common.util - _classifier.roberta.embeddings.word_embeddings.weight
2022-04-10 14:52:05,486 - INFO - allennlp.common.util - _classifier.roberta.embeddings.position_embeddings.weight
2022-04-10 14:52:05,486 - INFO - allennlp.common.util - _classifier.roberta.embeddings.token_type_embeddings.weight
2022-04-10 14:52:05,486 - INFO - allennlp.common.util - _classifier.roberta.embeddings.LayerNorm.weight
2022-04-10 14:52:05,486 - INFO - allennlp.common.util - _classifier.roberta.embeddings.LayerNorm.bias
2022-04-10 14:52:05,486 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.self.query.weight
2022-04-10 14:52:05,487 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.self.query.bias
2022-04-10 14:52:05,487 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.self.key.weight
2022-04-10 14:52:05,487 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.self.key.bias
2022-04-10 14:52:05,487 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.self.value.weight
2022-04-10 14:52:05,487 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.self.value.bias
2022-04-10 14:52:05,487 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.output.dense.weight
2022-04-10 14:52:05,487 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.output.dense.bias
2022-04-10 14:52:05,487 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.output.LayerNorm.weight
2022-04-10 14:52:05,487 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.output.LayerNorm.bias
2022-04-10 14:52:05,487 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.intermediate.dense.weight
2022-04-10 14:52:05,487 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.intermediate.dense.bias
2022-04-10 14:52:05,488 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.output.dense.weight
2022-04-10 14:52:05,488 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.output.dense.bias
2022-04-10 14:52:05,488 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.output.LayerNorm.weight
2022-04-10 14:52:05,488 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.output.LayerNorm.bias
2022-04-10 14:52:05,488 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.self.query.weight
2022-04-10 14:52:05,488 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.self.query.bias
2022-04-10 14:52:05,488 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.self.key.weight
2022-04-10 14:52:05,488 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.self.key.bias
2022-04-10 14:52:05,488 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.self.value.weight
2022-04-10 14:52:05,488 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.self.value.bias
2022-04-10 14:52:05,488 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.output.dense.weight
2022-04-10 14:52:05,488 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.output.dense.bias
2022-04-10 14:52:05,488 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.output.LayerNorm.weight
2022-04-10 14:52:05,488 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.output.LayerNorm.bias
2022-04-10 14:52:05,489 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.intermediate.dense.weight
2022-04-10 14:52:05,489 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.intermediate.dense.bias
2022-04-10 14:52:05,489 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.output.dense.weight
2022-04-10 14:52:05,489 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.output.dense.bias
2022-04-10 14:52:05,489 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.output.LayerNorm.weight
2022-04-10 14:52:05,489 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.output.LayerNorm.bias
2022-04-10 14:52:05,489 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.self.query.weight
2022-04-10 14:52:05,489 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.self.query.bias
2022-04-10 14:52:05,489 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.self.key.weight
2022-04-10 14:52:05,489 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.self.key.bias
2022-04-10 14:52:05,489 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.self.value.weight
2022-04-10 14:52:05,489 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.self.value.bias
2022-04-10 14:52:05,489 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.output.dense.weight
2022-04-10 14:52:05,490 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.output.dense.bias
2022-04-10 14:52:05,490 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.output.LayerNorm.weight
2022-04-10 14:52:05,490 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.output.LayerNorm.bias
2022-04-10 14:52:05,490 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.intermediate.dense.weight
2022-04-10 14:52:05,490 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.intermediate.dense.bias
2022-04-10 14:52:05,490 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.output.dense.weight
2022-04-10 14:52:05,490 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.output.dense.bias
2022-04-10 14:52:05,490 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.output.LayerNorm.weight
2022-04-10 14:52:05,490 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.output.LayerNorm.bias
2022-04-10 14:52:05,490 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.self.query.weight
2022-04-10 14:52:05,490 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.self.query.bias
2022-04-10 14:52:05,490 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.self.key.weight
2022-04-10 14:52:05,490 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.self.key.bias
2022-04-10 14:52:05,491 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.self.value.weight
2022-04-10 14:52:05,491 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.self.value.bias
2022-04-10 14:52:05,491 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.output.dense.weight
2022-04-10 14:52:05,491 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.output.dense.bias
2022-04-10 14:52:05,491 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.output.LayerNorm.weight
2022-04-10 14:52:05,491 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.output.LayerNorm.bias
2022-04-10 14:52:05,491 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.intermediate.dense.weight
2022-04-10 14:52:05,491 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.intermediate.dense.bias
2022-04-10 14:52:05,491 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.output.dense.weight
2022-04-10 14:52:05,491 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.output.dense.bias
2022-04-10 14:52:05,491 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.output.LayerNorm.weight
2022-04-10 14:52:05,491 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.output.LayerNorm.bias
2022-04-10 14:52:05,491 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.self.query.weight
2022-04-10 14:52:05,492 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.self.query.bias
2022-04-10 14:52:05,492 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.self.key.weight
2022-04-10 14:52:05,492 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.self.key.bias
2022-04-10 14:52:05,492 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.self.value.weight
2022-04-10 14:52:05,492 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.self.value.bias
2022-04-10 14:52:05,492 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.output.dense.weight
2022-04-10 14:52:05,492 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.output.dense.bias
2022-04-10 14:52:05,492 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.output.LayerNorm.weight
2022-04-10 14:52:05,492 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.output.LayerNorm.bias
2022-04-10 14:52:05,492 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.intermediate.dense.weight
2022-04-10 14:52:05,492 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.intermediate.dense.bias
2022-04-10 14:52:05,492 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.output.dense.weight
2022-04-10 14:52:05,492 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.output.dense.bias
2022-04-10 14:52:05,493 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.output.LayerNorm.weight
2022-04-10 14:52:05,493 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.output.LayerNorm.bias
2022-04-10 14:52:05,493 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.self.query.weight
2022-04-10 14:52:05,493 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.self.query.bias
2022-04-10 14:52:05,493 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.self.key.weight
2022-04-10 14:52:05,493 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.self.key.bias
2022-04-10 14:52:05,493 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.self.value.weight
2022-04-10 14:52:05,493 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.self.value.bias
2022-04-10 14:52:05,493 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.output.dense.weight
2022-04-10 14:52:05,493 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.output.dense.bias
2022-04-10 14:52:05,493 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.output.LayerNorm.weight
2022-04-10 14:52:05,493 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.output.LayerNorm.bias
2022-04-10 14:52:05,493 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.intermediate.dense.weight
2022-04-10 14:52:05,494 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.intermediate.dense.bias
2022-04-10 14:52:05,494 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.output.dense.weight
2022-04-10 14:52:05,494 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.output.dense.bias
2022-04-10 14:52:05,494 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.output.LayerNorm.weight
2022-04-10 14:52:05,494 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.output.LayerNorm.bias
2022-04-10 14:52:05,494 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.self.query.weight
2022-04-10 14:52:05,494 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.self.query.bias
2022-04-10 14:52:05,494 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.self.key.weight
2022-04-10 14:52:05,494 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.self.key.bias
2022-04-10 14:52:05,494 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.self.value.weight
2022-04-10 14:52:05,494 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.self.value.bias
2022-04-10 14:52:05,494 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.output.dense.weight
2022-04-10 14:52:05,495 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.output.dense.bias
2022-04-10 14:52:05,495 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.output.LayerNorm.weight
2022-04-10 14:52:05,495 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.output.LayerNorm.bias
2022-04-10 14:52:05,495 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.intermediate.dense.weight
2022-04-10 14:52:05,495 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.intermediate.dense.bias
2022-04-10 14:52:05,495 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.output.dense.weight
2022-04-10 14:52:05,495 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.output.dense.bias
2022-04-10 14:52:05,495 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.output.LayerNorm.weight
2022-04-10 14:52:05,495 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.output.LayerNorm.bias
2022-04-10 14:52:05,495 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.self.query.weight
2022-04-10 14:52:05,495 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.self.query.bias
2022-04-10 14:52:05,495 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.self.key.weight
2022-04-10 14:52:05,496 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.self.key.bias
2022-04-10 14:52:05,496 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.self.value.weight
2022-04-10 14:52:05,496 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.self.value.bias
2022-04-10 14:52:05,496 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.output.dense.weight
2022-04-10 14:52:05,496 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.output.dense.bias
2022-04-10 14:52:05,496 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.output.LayerNorm.weight
2022-04-10 14:52:05,496 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.output.LayerNorm.bias
2022-04-10 14:52:05,496 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.intermediate.dense.weight
2022-04-10 14:52:05,496 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.intermediate.dense.bias
2022-04-10 14:52:05,496 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.output.dense.weight
2022-04-10 14:52:05,496 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.output.dense.bias
2022-04-10 14:52:05,496 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.output.LayerNorm.weight
2022-04-10 14:52:05,497 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.output.LayerNorm.bias
2022-04-10 14:52:05,497 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.self.query.weight
2022-04-10 14:52:05,497 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.self.query.bias
2022-04-10 14:52:05,497 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.self.key.weight
2022-04-10 14:52:05,497 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.self.key.bias
2022-04-10 14:52:05,497 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.self.value.weight
2022-04-10 14:52:05,497 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.self.value.bias
2022-04-10 14:52:05,497 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.output.dense.weight
2022-04-10 14:52:05,497 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.output.dense.bias
2022-04-10 14:52:05,497 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.output.LayerNorm.weight
2022-04-10 14:52:05,497 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.output.LayerNorm.bias
2022-04-10 14:52:05,498 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.intermediate.dense.weight
2022-04-10 14:52:05,498 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.intermediate.dense.bias
2022-04-10 14:52:05,498 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.output.dense.weight
2022-04-10 14:52:05,498 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.output.dense.bias
2022-04-10 14:52:05,498 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.output.LayerNorm.weight
2022-04-10 14:52:05,498 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.output.LayerNorm.bias
2022-04-10 14:52:05,498 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.self.query.weight
2022-04-10 14:52:05,498 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.self.query.bias
2022-04-10 14:52:05,498 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.self.key.weight
2022-04-10 14:52:05,498 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.self.key.bias
2022-04-10 14:52:05,498 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.self.value.weight
2022-04-10 14:52:05,498 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.self.value.bias
2022-04-10 14:52:05,498 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.output.dense.weight
2022-04-10 14:52:05,499 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.output.dense.bias
2022-04-10 14:52:05,499 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.output.LayerNorm.weight
2022-04-10 14:52:05,499 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.output.LayerNorm.bias
2022-04-10 14:52:05,499 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.intermediate.dense.weight
2022-04-10 14:52:05,499 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.intermediate.dense.bias
2022-04-10 14:52:05,499 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.output.dense.weight
2022-04-10 14:52:05,499 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.output.dense.bias
2022-04-10 14:52:05,499 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.output.LayerNorm.weight
2022-04-10 14:52:05,499 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.output.LayerNorm.bias
2022-04-10 14:52:05,499 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.self.query.weight
2022-04-10 14:52:05,499 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.self.query.bias
2022-04-10 14:52:05,499 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.self.key.weight
2022-04-10 14:52:05,499 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.self.key.bias
2022-04-10 14:52:05,500 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.self.value.weight
2022-04-10 14:52:05,500 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.self.value.bias
2022-04-10 14:52:05,500 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.output.dense.weight
2022-04-10 14:52:05,500 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.output.dense.bias
2022-04-10 14:52:05,500 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.output.LayerNorm.weight
2022-04-10 14:52:05,500 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.output.LayerNorm.bias
2022-04-10 14:52:05,500 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.intermediate.dense.weight
2022-04-10 14:52:05,500 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.intermediate.dense.bias
2022-04-10 14:52:05,500 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.output.dense.weight
2022-04-10 14:52:05,500 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.output.dense.bias
2022-04-10 14:52:05,500 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.output.LayerNorm.weight
2022-04-10 14:52:05,500 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.output.LayerNorm.bias
2022-04-10 14:52:05,500 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.self.query.weight
2022-04-10 14:52:05,501 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.self.query.bias
2022-04-10 14:52:05,501 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.self.key.weight
2022-04-10 14:52:05,501 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.self.key.bias
2022-04-10 14:52:05,501 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.self.value.weight
2022-04-10 14:52:05,501 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.self.value.bias
2022-04-10 14:52:05,501 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.output.dense.weight
2022-04-10 14:52:05,501 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.output.dense.bias
2022-04-10 14:52:05,501 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.output.LayerNorm.weight
2022-04-10 14:52:05,501 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.output.LayerNorm.bias
2022-04-10 14:52:05,501 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.intermediate.dense.weight
2022-04-10 14:52:05,501 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.intermediate.dense.bias
2022-04-10 14:52:05,502 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.output.dense.weight
2022-04-10 14:52:05,502 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.output.dense.bias
2022-04-10 14:52:05,502 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.output.LayerNorm.weight
2022-04-10 14:52:05,502 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.output.LayerNorm.bias
2022-04-10 14:52:05,502 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.self.query.weight
2022-04-10 14:52:05,502 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.self.query.bias
2022-04-10 14:52:05,502 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.self.key.weight
2022-04-10 14:52:05,502 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.self.key.bias
2022-04-10 14:52:05,502 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.self.value.weight
2022-04-10 14:52:05,502 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.self.value.bias
2022-04-10 14:52:05,502 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.output.dense.weight
2022-04-10 14:52:05,502 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.output.dense.bias
2022-04-10 14:52:05,503 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.output.LayerNorm.weight
2022-04-10 14:52:05,503 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.output.LayerNorm.bias
2022-04-10 14:52:05,503 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.intermediate.dense.weight
2022-04-10 14:52:05,503 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.intermediate.dense.bias
2022-04-10 14:52:05,503 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.output.dense.weight
2022-04-10 14:52:05,503 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.output.dense.bias
2022-04-10 14:52:05,503 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.output.LayerNorm.weight
2022-04-10 14:52:05,503 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.output.LayerNorm.bias
2022-04-10 14:52:05,503 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.self.query.weight
2022-04-10 14:52:05,503 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.self.query.bias
2022-04-10 14:52:05,503 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.self.key.weight
2022-04-10 14:52:05,504 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.self.key.bias
2022-04-10 14:52:05,504 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.self.value.weight
2022-04-10 14:52:05,504 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.self.value.bias
2022-04-10 14:52:05,504 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.output.dense.weight
2022-04-10 14:52:05,504 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.output.dense.bias
2022-04-10 14:52:05,504 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.output.LayerNorm.weight
2022-04-10 14:52:05,504 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.output.LayerNorm.bias
2022-04-10 14:52:05,504 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.intermediate.dense.weight
2022-04-10 14:52:05,504 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.intermediate.dense.bias
2022-04-10 14:52:05,504 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.output.dense.weight
2022-04-10 14:52:05,504 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.output.dense.bias
2022-04-10 14:52:05,504 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.output.LayerNorm.weight
2022-04-10 14:52:05,505 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.output.LayerNorm.bias
2022-04-10 14:52:05,505 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.self.query.weight
2022-04-10 14:52:05,505 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.self.query.bias
2022-04-10 14:52:05,505 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.self.key.weight
2022-04-10 14:52:05,505 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.self.key.bias
2022-04-10 14:52:05,505 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.self.value.weight
2022-04-10 14:52:05,505 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.self.value.bias
2022-04-10 14:52:05,505 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.output.dense.weight
2022-04-10 14:52:05,505 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.output.dense.bias
2022-04-10 14:52:05,505 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.output.LayerNorm.weight
2022-04-10 14:52:05,505 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.output.LayerNorm.bias
2022-04-10 14:52:05,505 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.intermediate.dense.weight
2022-04-10 14:52:05,506 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.intermediate.dense.bias
2022-04-10 14:52:05,506 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.output.dense.weight
2022-04-10 14:52:05,506 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.output.dense.bias
2022-04-10 14:52:05,506 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.output.LayerNorm.weight
2022-04-10 14:52:05,506 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.output.LayerNorm.bias
2022-04-10 14:52:05,506 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.self.query.weight
2022-04-10 14:52:05,506 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.self.query.bias
2022-04-10 14:52:05,506 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.self.key.weight
2022-04-10 14:52:05,506 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.self.key.bias
2022-04-10 14:52:05,506 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.self.value.weight
2022-04-10 14:52:05,506 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.self.value.bias
2022-04-10 14:52:05,507 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.output.dense.weight
2022-04-10 14:52:05,507 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.output.dense.bias
2022-04-10 14:52:05,507 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.output.LayerNorm.weight
2022-04-10 14:52:05,507 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.output.LayerNorm.bias
2022-04-10 14:52:05,507 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.intermediate.dense.weight
2022-04-10 14:52:05,507 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.intermediate.dense.bias
2022-04-10 14:52:05,507 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.output.dense.weight
2022-04-10 14:52:05,507 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.output.dense.bias
2022-04-10 14:52:05,507 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.output.LayerNorm.weight
2022-04-10 14:52:05,507 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.output.LayerNorm.bias
2022-04-10 14:52:05,508 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.self.query.weight
2022-04-10 14:52:05,508 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.self.query.bias
2022-04-10 14:52:05,508 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.self.key.weight
2022-04-10 14:52:05,508 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.self.key.bias
2022-04-10 14:52:05,508 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.self.value.weight
2022-04-10 14:52:05,508 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.self.value.bias
2022-04-10 14:52:05,508 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.output.dense.weight
2022-04-10 14:52:05,508 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.output.dense.bias
2022-04-10 14:52:05,508 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.output.LayerNorm.weight
2022-04-10 14:52:05,508 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.output.LayerNorm.bias
2022-04-10 14:52:05,508 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.intermediate.dense.weight
2022-04-10 14:52:05,509 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.intermediate.dense.bias
2022-04-10 14:52:05,509 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.output.dense.weight
2022-04-10 14:52:05,509 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.output.dense.bias
2022-04-10 14:52:05,509 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.output.LayerNorm.weight
2022-04-10 14:52:05,509 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.output.LayerNorm.bias
2022-04-10 14:52:05,509 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.self.query.weight
2022-04-10 14:52:05,509 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.self.query.bias
2022-04-10 14:52:05,509 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.self.key.weight
2022-04-10 14:52:05,509 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.self.key.bias
2022-04-10 14:52:05,509 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.self.value.weight
2022-04-10 14:52:05,509 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.self.value.bias
2022-04-10 14:52:05,510 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.output.dense.weight
2022-04-10 14:52:05,510 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.output.dense.bias
2022-04-10 14:52:05,510 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.output.LayerNorm.weight
2022-04-10 14:52:05,510 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.output.LayerNorm.bias
2022-04-10 14:52:05,510 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.intermediate.dense.weight
2022-04-10 14:52:05,510 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.intermediate.dense.bias
2022-04-10 14:52:05,510 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.output.dense.weight
2022-04-10 14:52:05,510 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.output.dense.bias
2022-04-10 14:52:05,510 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.output.LayerNorm.weight
2022-04-10 14:52:05,510 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.output.LayerNorm.bias
2022-04-10 14:52:05,510 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.self.query.weight
2022-04-10 14:52:05,511 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.self.query.bias
2022-04-10 14:52:05,511 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.self.key.weight
2022-04-10 14:52:05,511 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.self.key.bias
2022-04-10 14:52:05,511 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.self.value.weight
2022-04-10 14:52:05,511 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.self.value.bias
2022-04-10 14:52:05,511 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.output.dense.weight
2022-04-10 14:52:05,511 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.output.dense.bias
2022-04-10 14:52:05,511 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.output.LayerNorm.weight
2022-04-10 14:52:05,511 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.output.LayerNorm.bias
2022-04-10 14:52:05,511 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.intermediate.dense.weight
2022-04-10 14:52:05,511 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.intermediate.dense.bias
2022-04-10 14:52:05,512 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.output.dense.weight
2022-04-10 14:52:05,512 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.output.dense.bias
2022-04-10 14:52:05,512 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.output.LayerNorm.weight
2022-04-10 14:52:05,512 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.output.LayerNorm.bias
2022-04-10 14:52:05,512 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.self.query.weight
2022-04-10 14:52:05,512 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.self.query.bias
2022-04-10 14:52:05,512 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.self.key.weight
2022-04-10 14:52:05,512 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.self.key.bias
2022-04-10 14:52:05,512 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.self.value.weight
2022-04-10 14:52:05,512 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.self.value.bias
2022-04-10 14:52:05,512 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.output.dense.weight
2022-04-10 14:52:05,513 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.output.dense.bias
2022-04-10 14:52:05,513 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.output.LayerNorm.weight
2022-04-10 14:52:05,513 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.output.LayerNorm.bias
2022-04-10 14:52:05,513 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.intermediate.dense.weight
2022-04-10 14:52:05,513 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.intermediate.dense.bias
2022-04-10 14:52:05,513 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.output.dense.weight
2022-04-10 14:52:05,513 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.output.dense.bias
2022-04-10 14:52:05,513 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.output.LayerNorm.weight
2022-04-10 14:52:05,513 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.output.LayerNorm.bias
2022-04-10 14:52:05,513 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.self.query.weight
2022-04-10 14:52:05,513 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.self.query.bias
2022-04-10 14:52:05,513 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.self.key.weight
2022-04-10 14:52:05,514 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.self.key.bias
2022-04-10 14:52:05,514 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.self.value.weight
2022-04-10 14:52:05,514 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.self.value.bias
2022-04-10 14:52:05,514 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.output.dense.weight
2022-04-10 14:52:05,514 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.output.dense.bias
2022-04-10 14:52:05,514 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.output.LayerNorm.weight
2022-04-10 14:52:05,514 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.output.LayerNorm.bias
2022-04-10 14:52:05,514 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.intermediate.dense.weight
2022-04-10 14:52:05,514 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.intermediate.dense.bias
2022-04-10 14:52:05,514 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.output.dense.weight
2022-04-10 14:52:05,514 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.output.dense.bias
2022-04-10 14:52:05,515 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.output.LayerNorm.weight
2022-04-10 14:52:05,515 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.output.LayerNorm.bias
2022-04-10 14:52:05,515 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.self.query.weight
2022-04-10 14:52:05,515 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.self.query.bias
2022-04-10 14:52:05,515 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.self.key.weight
2022-04-10 14:52:05,515 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.self.key.bias
2022-04-10 14:52:05,515 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.self.value.weight
2022-04-10 14:52:05,515 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.self.value.bias
2022-04-10 14:52:05,515 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.output.dense.weight
2022-04-10 14:52:05,515 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.output.dense.bias
2022-04-10 14:52:05,516 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.output.LayerNorm.weight
2022-04-10 14:52:05,516 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.output.LayerNorm.bias
2022-04-10 14:52:05,516 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.intermediate.dense.weight
2022-04-10 14:52:05,516 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.intermediate.dense.bias
2022-04-10 14:52:05,516 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.output.dense.weight
2022-04-10 14:52:05,516 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.output.dense.bias
2022-04-10 14:52:05,516 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.output.LayerNorm.weight
2022-04-10 14:52:05,516 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.output.LayerNorm.bias
2022-04-10 14:52:05,516 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.self.query.weight
2022-04-10 14:52:05,516 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.self.query.bias
2022-04-10 14:52:05,516 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.self.key.weight
2022-04-10 14:52:05,517 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.self.key.bias
2022-04-10 14:52:05,517 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.self.value.weight
2022-04-10 14:52:05,517 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.self.value.bias
2022-04-10 14:52:05,517 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.output.dense.weight
2022-04-10 14:52:05,517 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.output.dense.bias
2022-04-10 14:52:05,517 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.output.LayerNorm.weight
2022-04-10 14:52:05,517 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.output.LayerNorm.bias
2022-04-10 14:52:05,517 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.intermediate.dense.weight
2022-04-10 14:52:05,517 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.intermediate.dense.bias
2022-04-10 14:52:05,517 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.output.dense.weight
2022-04-10 14:52:05,517 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.output.dense.bias
2022-04-10 14:52:05,518 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.output.LayerNorm.weight
2022-04-10 14:52:05,518 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.output.LayerNorm.bias
2022-04-10 14:52:05,518 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.self.query.weight
2022-04-10 14:52:05,518 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.self.query.bias
2022-04-10 14:52:05,518 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.self.key.weight
2022-04-10 14:52:05,518 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.self.key.bias
2022-04-10 14:52:05,518 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.self.value.weight
2022-04-10 14:52:05,518 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.self.value.bias
2022-04-10 14:52:05,518 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.output.dense.weight
2022-04-10 14:52:05,518 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.output.dense.bias
2022-04-10 14:52:05,519 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.output.LayerNorm.weight
2022-04-10 14:52:05,519 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.output.LayerNorm.bias
2022-04-10 14:52:05,519 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.intermediate.dense.weight
2022-04-10 14:52:05,519 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.intermediate.dense.bias
2022-04-10 14:52:05,519 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.output.dense.weight
2022-04-10 14:52:05,519 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.output.dense.bias
2022-04-10 14:52:05,519 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.output.LayerNorm.weight
2022-04-10 14:52:05,519 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.output.LayerNorm.bias
2022-04-10 14:52:05,519 - INFO - allennlp.common.util - _classifier.roberta.pooler.dense.weight
2022-04-10 14:52:05,519 - INFO - allennlp.common.util - _classifier.roberta.pooler.dense.bias
2022-04-10 14:52:05,519 - INFO - allennlp.common.util - _classifier.classifier.dense.weight
2022-04-10 14:52:05,520 - INFO - allennlp.common.util - _classifier.classifier.dense.bias
2022-04-10 14:52:05,520 - INFO - allennlp.common.util - _classifier.classifier.out_proj.weight
2022-04-10 14:52:05,520 - INFO - allennlp.common.util - _classifier.classifier.out_proj.bias
2022-04-10 14:52:05,520 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.type = slanted_triangular
2022-04-10 14:52:05,520 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.cut_frac = 0.06
2022-04-10 14:52:05,520 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.ratio = 32
2022-04-10 14:52:05,521 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.last_epoch = -1
2022-04-10 14:52:05,521 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.gradual_unfreezing = False
2022-04-10 14:52:05,521 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.discriminative_fine_tuning = False
2022-04-10 14:52:05,521 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.decay_factor = 0.38
2022-04-10 14:52:05,521 - INFO - allennlp.common.params - type = default
2022-04-10 14:52:05,521 - INFO - allennlp.common.params - keep_serialized_model_every_num_seconds = None
2022-04-10 14:52:05,521 - INFO - allennlp.common.params - num_serialized_models_to_keep = 2
2022-04-10 14:52:05,521 - INFO - allennlp.common.params - model_save_interval = None
2022-04-10 14:52:05,522 - INFO - allennlp.common.params - summary_interval = 100
2022-04-10 14:52:05,522 - INFO - allennlp.common.params - histogram_interval = None
2022-04-10 14:52:05,522 - INFO - allennlp.common.params - batch_size_interval = None
2022-04-10 14:52:05,522 - INFO - allennlp.common.params - should_log_parameter_statistics = True
2022-04-10 14:52:05,522 - INFO - allennlp.common.params - should_log_learning_rate = False
2022-04-10 14:52:05,522 - INFO - allennlp.common.params - get_batch_num_total = None
2022-04-10 14:52:05,524 - WARNING - allennlp.training.trainer - You provided a validation dataset but patience was set to None, meaning that early stopping is disabled
2022-04-10 14:52:05,617 - INFO - allennlp.training.trainer - Beginning training.
2022-04-10 14:52:05,617 - INFO - allennlp.training.trainer - Epoch 0/14
2022-04-10 14:52:05,617 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-10 14:52:05,618 - INFO - allennlp.training.trainer - GPU 0 memory usage: 1.3G
2022-04-10 14:52:05,619 - INFO - allennlp.training.trainer - Training
2022-04-10 14:52:05,619 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-10 14:52:05,620 - INFO - allennlp.data.samplers.bucket_batch_sampler - No sorting keys given; trying to guess a good one
2022-04-10 14:52:05,620 - INFO - allennlp.data.samplers.bucket_batch_sampler - Using ['tokens'] as the sorting keys
2022-04-10 14:52:16,082 - INFO - tqdm - accuracy: 0.4688, batch_loss: 1.5490, loss: 1.5490 ||:   2%|1         | 1/65 [00:10<11:09, 10.46s/it]
2022-04-10 14:52:26,658 - INFO - tqdm - accuracy: 0.5156, batch_loss: 1.5604, loss: 1.5547 ||:   3%|3         | 2/65 [00:21<11:03, 10.53s/it]
2022-04-10 14:52:37,383 - INFO - tqdm - accuracy: 0.5208, batch_loss: 2.0270, loss: 1.7121 ||:   5%|4         | 3/65 [00:31<10:58, 10.62s/it]
2022-04-10 14:52:48,282 - INFO - tqdm - accuracy: 0.5469, batch_loss: 1.3607, loss: 1.6243 ||:   6%|6         | 4/65 [00:42<10:54, 10.73s/it]
2022-04-10 14:52:59,414 - INFO - tqdm - accuracy: 0.5312, batch_loss: 1.2561, loss: 1.5506 ||:   8%|7         | 5/65 [00:53<10:52, 10.87s/it]
2022-04-10 14:53:10,840 - INFO - tqdm - accuracy: 0.5208, batch_loss: 1.7565, loss: 1.5849 ||:   9%|9         | 6/65 [01:05<10:52, 11.06s/it]
2022-04-10 14:53:22,527 - INFO - tqdm - accuracy: 0.5223, batch_loss: 1.5226, loss: 1.5760 ||:  11%|#         | 7/65 [01:16<10:53, 11.27s/it]
2022-04-10 14:53:34,365 - INFO - tqdm - accuracy: 0.5117, batch_loss: 1.6108, loss: 1.5804 ||:  12%|#2        | 8/65 [01:28<10:52, 11.45s/it]
2022-04-10 14:53:45,945 - INFO - tqdm - accuracy: 0.5035, batch_loss: 1.8277, loss: 1.6079 ||:  14%|#3        | 9/65 [01:40<10:43, 11.49s/it]
2022-04-10 14:53:57,237 - INFO - tqdm - accuracy: 0.4938, batch_loss: 1.6599, loss: 1.6131 ||:  15%|#5        | 10/65 [01:51<10:28, 11.43s/it]
2022-04-10 14:54:08,600 - INFO - tqdm - accuracy: 0.4943, batch_loss: 1.5749, loss: 1.6096 ||:  17%|#6        | 11/65 [02:02<10:16, 11.41s/it]
2022-04-10 14:54:20,034 - INFO - tqdm - accuracy: 0.5026, batch_loss: 1.1128, loss: 1.5682 ||:  18%|#8        | 12/65 [02:14<10:05, 11.42s/it]
2022-04-10 14:54:31,592 - INFO - tqdm - accuracy: 0.5120, batch_loss: 1.0572, loss: 1.5289 ||:  20%|##        | 13/65 [02:25<09:55, 11.46s/it]
2022-04-10 14:54:43,090 - INFO - tqdm - accuracy: 0.5112, batch_loss: 1.6546, loss: 1.5379 ||:  22%|##1       | 14/65 [02:37<09:45, 11.47s/it]
2022-04-10 14:54:54,650 - INFO - tqdm - accuracy: 0.5146, batch_loss: 1.2909, loss: 1.5214 ||:  23%|##3       | 15/65 [02:49<09:34, 11.50s/it]
2022-04-10 14:55:06,181 - INFO - tqdm - accuracy: 0.5098, batch_loss: 1.2701, loss: 1.5057 ||:  25%|##4       | 16/65 [03:00<09:23, 11.51s/it]
2022-04-10 14:55:17,694 - INFO - tqdm - accuracy: 0.5147, batch_loss: 0.7093, loss: 1.4589 ||:  26%|##6       | 17/65 [03:12<09:12, 11.51s/it]
2022-04-10 14:55:29,202 - INFO - tqdm - accuracy: 0.5191, batch_loss: 0.9785, loss: 1.4322 ||:  28%|##7       | 18/65 [03:23<09:00, 11.51s/it]
2022-04-10 14:55:40,718 - INFO - tqdm - accuracy: 0.5197, batch_loss: 1.0442, loss: 1.4117 ||:  29%|##9       | 19/65 [03:35<08:49, 11.51s/it]
2022-04-10 14:55:51,986 - INFO - tqdm - accuracy: 0.5203, batch_loss: 0.8600, loss: 1.3842 ||:  31%|###       | 20/65 [03:46<08:34, 11.44s/it]
2022-04-10 14:56:03,503 - INFO - tqdm - accuracy: 0.5119, batch_loss: 1.4654, loss: 1.3880 ||:  32%|###2      | 21/65 [03:57<08:24, 11.46s/it]
2022-04-10 14:56:14,923 - INFO - tqdm - accuracy: 0.5128, batch_loss: 1.0263, loss: 1.3716 ||:  34%|###3      | 22/65 [04:09<08:12, 11.45s/it]
2022-04-10 14:56:26,461 - INFO - tqdm - accuracy: 0.5149, batch_loss: 1.0153, loss: 1.3561 ||:  35%|###5      | 23/65 [04:20<08:01, 11.48s/it]
2022-04-10 14:56:38,012 - INFO - tqdm - accuracy: 0.5143, batch_loss: 0.9639, loss: 1.3398 ||:  37%|###6      | 24/65 [04:32<07:51, 11.50s/it]
2022-04-10 14:56:49,577 - INFO - tqdm - accuracy: 0.5112, batch_loss: 1.1255, loss: 1.3312 ||:  38%|###8      | 25/65 [04:43<07:40, 11.52s/it]
2022-04-10 14:57:01,146 - INFO - tqdm - accuracy: 0.5060, batch_loss: 0.9676, loss: 1.3172 ||:  40%|####      | 26/65 [04:55<07:29, 11.53s/it]
2022-04-10 14:57:12,723 - INFO - tqdm - accuracy: 0.5081, batch_loss: 0.7173, loss: 1.2950 ||:  42%|####1     | 27/65 [05:07<07:18, 11.55s/it]
2022-04-10 14:57:24,309 - INFO - tqdm - accuracy: 0.5100, batch_loss: 0.7910, loss: 1.2770 ||:  43%|####3     | 28/65 [05:18<07:07, 11.56s/it]
2022-04-10 14:57:35,919 - INFO - tqdm - accuracy: 0.5086, batch_loss: 0.9139, loss: 1.2645 ||:  45%|####4     | 29/65 [05:30<06:56, 11.57s/it]
2022-04-10 14:57:47,491 - INFO - tqdm - accuracy: 0.5073, batch_loss: 0.9719, loss: 1.2547 ||:  46%|####6     | 30/65 [05:41<06:45, 11.57s/it]
2022-04-10 14:57:59,056 - INFO - tqdm - accuracy: 0.5121, batch_loss: 0.6213, loss: 1.2343 ||:  48%|####7     | 31/65 [05:53<06:33, 11.57s/it]
2022-04-10 14:58:10,669 - INFO - tqdm - accuracy: 0.5078, batch_loss: 0.9852, loss: 1.2265 ||:  49%|####9     | 32/65 [06:05<06:22, 11.58s/it]
2022-04-10 14:58:22,215 - INFO - tqdm - accuracy: 0.5133, batch_loss: 0.6219, loss: 1.2082 ||:  51%|#####     | 33/65 [06:16<06:10, 11.57s/it]
2022-04-10 14:58:33,760 - INFO - tqdm - accuracy: 0.5129, batch_loss: 0.7652, loss: 1.1951 ||:  52%|#####2    | 34/65 [06:28<05:58, 11.56s/it]
2022-04-10 14:58:45,299 - INFO - tqdm - accuracy: 0.5107, batch_loss: 0.7912, loss: 1.1836 ||:  54%|#####3    | 35/65 [06:39<05:46, 11.56s/it]
2022-04-10 14:58:56,831 - INFO - tqdm - accuracy: 0.5130, batch_loss: 0.6849, loss: 1.1698 ||:  55%|#####5    | 36/65 [06:51<05:34, 11.55s/it]
2022-04-10 14:59:08,363 - INFO - tqdm - accuracy: 0.5118, batch_loss: 0.7782, loss: 1.1592 ||:  57%|#####6    | 37/65 [07:02<05:23, 11.54s/it]
2022-04-10 14:59:19,885 - INFO - tqdm - accuracy: 0.5107, batch_loss: 0.6867, loss: 1.1467 ||:  58%|#####8    | 38/65 [07:14<05:11, 11.54s/it]
2022-04-10 14:59:31,379 - INFO - tqdm - accuracy: 0.5096, batch_loss: 0.7291, loss: 1.1360 ||:  60%|######    | 39/65 [07:25<04:59, 11.52s/it]
2022-04-10 14:59:42,897 - INFO - tqdm - accuracy: 0.5094, batch_loss: 0.6768, loss: 1.1246 ||:  62%|######1   | 40/65 [07:37<04:48, 11.52s/it]
2022-04-10 14:59:54,385 - INFO - tqdm - accuracy: 0.5091, batch_loss: 0.6776, loss: 1.1136 ||:  63%|######3   | 41/65 [07:48<04:36, 11.51s/it]
2022-04-10 15:00:05,889 - INFO - tqdm - accuracy: 0.5134, batch_loss: 0.6626, loss: 1.1029 ||:  65%|######4   | 42/65 [08:00<04:24, 11.51s/it]
2022-04-10 15:00:17,380 - INFO - tqdm - accuracy: 0.5124, batch_loss: 0.7992, loss: 1.0958 ||:  66%|######6   | 43/65 [08:11<04:13, 11.50s/it]
2022-04-10 15:00:28,897 - INFO - tqdm - accuracy: 0.5135, batch_loss: 0.6553, loss: 1.0858 ||:  68%|######7   | 44/65 [08:23<04:01, 11.51s/it]
2022-04-10 15:00:40,414 - INFO - tqdm - accuracy: 0.5146, batch_loss: 0.6786, loss: 1.0768 ||:  69%|######9   | 45/65 [08:34<03:50, 11.51s/it]
2022-04-10 15:00:51,950 - INFO - tqdm - accuracy: 0.5149, batch_loss: 0.6880, loss: 1.0683 ||:  71%|#######   | 46/65 [08:46<03:38, 11.52s/it]
2022-04-10 15:01:03,480 - INFO - tqdm - accuracy: 0.5166, batch_loss: 0.6713, loss: 1.0599 ||:  72%|#######2  | 47/65 [08:57<03:27, 11.52s/it]
2022-04-10 15:01:15,014 - INFO - tqdm - accuracy: 0.5169, batch_loss: 0.7398, loss: 1.0532 ||:  74%|#######3  | 48/65 [09:09<03:15, 11.53s/it]
2022-04-10 15:01:26,583 - INFO - tqdm - accuracy: 0.5185, batch_loss: 0.6860, loss: 1.0457 ||:  75%|#######5  | 49/65 [09:20<03:04, 11.54s/it]
2022-04-10 15:01:38,132 - INFO - tqdm - accuracy: 0.5200, batch_loss: 0.6636, loss: 1.0381 ||:  77%|#######6  | 50/65 [09:32<02:53, 11.54s/it]
2022-04-10 15:01:49,681 - INFO - tqdm - accuracy: 0.5202, batch_loss: 0.7364, loss: 1.0322 ||:  78%|#######8  | 51/65 [09:44<02:41, 11.54s/it]
2022-04-10 15:02:01,248 - INFO - tqdm - accuracy: 0.5240, batch_loss: 0.6110, loss: 1.0241 ||:  80%|########  | 52/65 [09:55<02:30, 11.55s/it]
2022-04-10 15:02:12,789 - INFO - tqdm - accuracy: 0.5236, batch_loss: 0.7093, loss: 1.0181 ||:  82%|########1 | 53/65 [10:07<02:18, 11.55s/it]
2022-04-10 15:02:23,995 - INFO - tqdm - accuracy: 0.5292, batch_loss: 0.5602, loss: 1.0096 ||:  83%|########3 | 54/65 [10:18<02:05, 11.45s/it]
2022-04-10 15:02:35,524 - INFO - tqdm - accuracy: 0.5270, batch_loss: 0.7567, loss: 1.0050 ||:  85%|########4 | 55/65 [10:29<01:54, 11.47s/it]
2022-04-10 15:02:47,067 - INFO - tqdm - accuracy: 0.5276, batch_loss: 0.7024, loss: 0.9996 ||:  86%|########6 | 56/65 [10:41<01:43, 11.49s/it]
2022-04-10 15:02:58,593 - INFO - tqdm - accuracy: 0.5239, batch_loss: 0.8033, loss: 0.9962 ||:  88%|########7 | 57/65 [10:52<01:32, 11.50s/it]
2022-04-10 15:03:10,116 - INFO - tqdm - accuracy: 0.5240, batch_loss: 0.6906, loss: 0.9909 ||:  89%|########9 | 58/65 [11:04<01:20, 11.51s/it]
2022-04-10 15:03:21,632 - INFO - tqdm - accuracy: 0.5241, batch_loss: 0.7226, loss: 0.9864 ||:  91%|######### | 59/65 [11:16<01:09, 11.51s/it]
2022-04-10 15:03:33,177 - INFO - tqdm - accuracy: 0.5242, batch_loss: 0.6652, loss: 0.9810 ||:  92%|#########2| 60/65 [11:27<00:57, 11.52s/it]
2022-04-10 15:03:44,710 - INFO - tqdm - accuracy: 0.5238, batch_loss: 0.6761, loss: 0.9760 ||:  94%|#########3| 61/65 [11:39<00:46, 11.52s/it]
2022-04-10 15:03:56,211 - INFO - tqdm - accuracy: 0.5265, batch_loss: 0.6753, loss: 0.9712 ||:  95%|#########5| 62/65 [11:50<00:34, 11.52s/it]
2022-04-10 15:04:07,731 - INFO - tqdm - accuracy: 0.5261, batch_loss: 0.6965, loss: 0.9668 ||:  97%|#########6| 63/65 [12:02<00:23, 11.52s/it]
2022-04-10 15:04:19,254 - INFO - tqdm - accuracy: 0.5256, batch_loss: 0.7212, loss: 0.9630 ||:  98%|#########8| 64/65 [12:13<00:11, 11.52s/it]
2022-04-10 15:04:24,368 - INFO - tqdm - accuracy: 0.5260, batch_loss: 0.7253, loss: 0.9593 ||: 100%|##########| 65/65 [12:18<00:00,  9.60s/it]
2022-04-10 15:04:24,368 - INFO - tqdm - accuracy: 0.5260, batch_loss: 0.7253, loss: 0.9593 ||: 100%|##########| 65/65 [12:18<00:00, 11.37s/it]
2022-04-10 15:04:27,283 - INFO - allennlp.training.trainer - Validating
2022-04-10 15:04:27,285 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-10 15:04:27,285 - INFO - allennlp.data.samplers.bucket_batch_sampler - No sorting keys given; trying to guess a good one
2022-04-10 15:04:27,285 - INFO - allennlp.data.samplers.bucket_batch_sampler - Using ['tokens'] as the sorting keys
2022-04-10 15:04:37,416 - INFO - tqdm - accuracy: 0.4875, batch_loss: 0.6155, loss: 0.7018 ||:  35%|###4      | 40/115 [00:10<00:19,  3.90it/s]
2022-04-10 15:04:47,566 - INFO - tqdm - accuracy: 0.4562, batch_loss: 0.6334, loss: 0.6970 ||:  70%|######9   | 80/115 [00:20<00:08,  3.95it/s]
2022-04-10 15:04:56,527 - INFO - tqdm - accuracy: 0.4716, batch_loss: 0.8056, loss: 0.6966 ||: 100%|##########| 115/115 [00:29<00:00,  3.83it/s]
2022-04-10 15:04:56,527 - INFO - tqdm - accuracy: 0.4716, batch_loss: 0.8056, loss: 0.6966 ||: 100%|##########| 115/115 [00:29<00:00,  3.93it/s]
2022-04-10 15:04:56,527 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-10 15:04:56,529 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.526  |     0.472
2022-04-10 15:04:56,529 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  1359.606  |       N/A
2022-04-10 15:04:56,530 - INFO - allennlp.training.tensorboard_writer - loss               |     0.959  |     0.697
2022-04-10 15:04:56,530 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5804.914  |       N/A
2022-04-10 15:05:02,482 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to './output_ir-ora-d_hyper4/best.th'.
2022-04-10 15:05:03,731 - INFO - allennlp.training.trainer - Epoch duration: 0:12:58.113455
2022-04-10 15:05:03,731 - INFO - allennlp.training.trainer - Estimated training time remaining: 3:01:33
2022-04-10 15:05:03,731 - INFO - allennlp.training.trainer - Epoch 1/14
2022-04-10 15:05:03,731 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-10 15:05:03,732 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-10 15:05:03,733 - INFO - allennlp.training.trainer - Training
2022-04-10 15:05:03,733 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-10 15:05:15,079 - INFO - tqdm - accuracy: 0.4375, batch_loss: 0.7155, loss: 0.7155 ||:   2%|1         | 1/65 [00:11<12:06, 11.35s/it]
2022-04-10 15:05:26,662 - INFO - tqdm - accuracy: 0.5000, batch_loss: 0.6825, loss: 0.6990 ||:   3%|3         | 2/65 [00:22<12:03, 11.48s/it]
2022-04-10 15:05:38,510 - INFO - tqdm - accuracy: 0.4479, batch_loss: 0.7322, loss: 0.7101 ||:   5%|4         | 3/65 [00:34<12:02, 11.65s/it]
2022-04-10 15:05:50,150 - INFO - tqdm - accuracy: 0.4844, batch_loss: 0.6777, loss: 0.7020 ||:   6%|6         | 4/65 [00:46<11:50, 11.65s/it]
2022-04-10 15:06:01,624 - INFO - tqdm - accuracy: 0.5188, batch_loss: 0.6815, loss: 0.6979 ||:   8%|7         | 5/65 [00:57<11:35, 11.58s/it]
2022-04-10 15:06:13,133 - INFO - tqdm - accuracy: 0.5312, batch_loss: 0.6752, loss: 0.6941 ||:   9%|9         | 6/65 [01:09<11:21, 11.56s/it]
2022-04-10 15:06:24,661 - INFO - tqdm - accuracy: 0.5268, batch_loss: 0.6953, loss: 0.6943 ||:  11%|#         | 7/65 [01:20<11:09, 11.55s/it]
2022-04-10 15:06:36,198 - INFO - tqdm - accuracy: 0.5117, batch_loss: 0.6976, loss: 0.6947 ||:  12%|#2        | 8/65 [01:32<10:58, 11.55s/it]
2022-04-10 15:06:47,778 - INFO - tqdm - accuracy: 0.5312, batch_loss: 0.6707, loss: 0.6920 ||:  14%|#3        | 9/65 [01:44<10:47, 11.56s/it]
2022-04-10 15:06:59,339 - INFO - tqdm - accuracy: 0.5437, batch_loss: 0.6599, loss: 0.6888 ||:  15%|#5        | 10/65 [01:55<10:35, 11.56s/it]
2022-04-10 15:07:10,905 - INFO - tqdm - accuracy: 0.5398, batch_loss: 0.7029, loss: 0.6901 ||:  17%|#6        | 11/65 [02:07<10:24, 11.56s/it]
2022-04-10 15:07:22,490 - INFO - tqdm - accuracy: 0.5417, batch_loss: 0.6719, loss: 0.6886 ||:  18%|#8        | 12/65 [02:18<10:13, 11.57s/it]
2022-04-10 15:07:34,065 - INFO - tqdm - accuracy: 0.5361, batch_loss: 0.7091, loss: 0.6902 ||:  20%|##        | 13/65 [02:30<10:01, 11.57s/it]
2022-04-10 15:07:45,513 - INFO - tqdm - accuracy: 0.5357, batch_loss: 0.7098, loss: 0.6916 ||:  22%|##1       | 14/65 [02:41<09:48, 11.53s/it]
2022-04-10 15:07:57,080 - INFO - tqdm - accuracy: 0.5312, batch_loss: 0.7014, loss: 0.6922 ||:  23%|##3       | 15/65 [02:53<09:37, 11.54s/it]
2022-04-10 15:08:08,646 - INFO - tqdm - accuracy: 0.5332, batch_loss: 0.6912, loss: 0.6922 ||:  25%|##4       | 16/65 [03:04<09:25, 11.55s/it]
2022-04-10 15:08:20,196 - INFO - tqdm - accuracy: 0.5331, batch_loss: 0.7242, loss: 0.6940 ||:  26%|##6       | 17/65 [03:16<09:14, 11.55s/it]
2022-04-10 15:08:31,759 - INFO - tqdm - accuracy: 0.5382, batch_loss: 0.6273, loss: 0.6903 ||:  28%|##7       | 18/65 [03:28<09:03, 11.55s/it]
2022-04-10 15:08:43,309 - INFO - tqdm - accuracy: 0.5362, batch_loss: 0.6954, loss: 0.6906 ||:  29%|##9       | 19/65 [03:39<08:51, 11.55s/it]
2022-04-10 15:08:54,847 - INFO - tqdm - accuracy: 0.5422, batch_loss: 0.6660, loss: 0.6894 ||:  31%|###       | 20/65 [03:51<08:39, 11.55s/it]
2022-04-10 15:09:06,397 - INFO - tqdm - accuracy: 0.5461, batch_loss: 0.6357, loss: 0.6868 ||:  32%|###2      | 21/65 [04:02<08:28, 11.55s/it]
2022-04-10 15:09:17,792 - INFO - tqdm - accuracy: 0.5455, batch_loss: 0.6967, loss: 0.6873 ||:  34%|###3      | 22/65 [04:14<08:14, 11.50s/it]
2022-04-10 15:09:29,335 - INFO - tqdm - accuracy: 0.5489, batch_loss: 0.6658, loss: 0.6863 ||:  35%|###5      | 23/65 [04:25<08:03, 11.51s/it]
2022-04-10 15:09:40,873 - INFO - tqdm - accuracy: 0.5456, batch_loss: 0.7216, loss: 0.6878 ||:  37%|###6      | 24/65 [04:37<07:52, 11.52s/it]
2022-04-10 15:09:52,404 - INFO - tqdm - accuracy: 0.5513, batch_loss: 0.6176, loss: 0.6850 ||:  38%|###8      | 25/65 [04:48<07:40, 11.52s/it]
2022-04-10 15:10:03,945 - INFO - tqdm - accuracy: 0.5493, batch_loss: 0.6940, loss: 0.6853 ||:  40%|####      | 26/65 [05:00<07:29, 11.53s/it]
2022-04-10 15:10:15,477 - INFO - tqdm - accuracy: 0.5475, batch_loss: 0.6703, loss: 0.6848 ||:  42%|####1     | 27/65 [05:11<07:18, 11.53s/it]
2022-04-10 15:10:27,012 - INFO - tqdm - accuracy: 0.5502, batch_loss: 0.6678, loss: 0.6842 ||:  43%|####3     | 28/65 [05:23<07:06, 11.53s/it]
2022-04-10 15:10:38,524 - INFO - tqdm - accuracy: 0.5496, batch_loss: 0.6884, loss: 0.6843 ||:  45%|####4     | 29/65 [05:34<06:54, 11.53s/it]
2022-04-10 15:10:50,039 - INFO - tqdm - accuracy: 0.5469, batch_loss: 0.6984, loss: 0.6848 ||:  46%|####6     | 30/65 [05:46<06:43, 11.52s/it]
2022-04-10 15:11:01,571 - INFO - tqdm - accuracy: 0.5433, batch_loss: 0.6934, loss: 0.6851 ||:  48%|####7     | 31/65 [05:57<06:31, 11.53s/it]
2022-04-10 15:11:13,095 - INFO - tqdm - accuracy: 0.5449, batch_loss: 0.6471, loss: 0.6839 ||:  49%|####9     | 32/65 [06:09<06:20, 11.53s/it]
2022-04-10 15:11:24,622 - INFO - tqdm - accuracy: 0.5417, batch_loss: 0.6980, loss: 0.6843 ||:  51%|#####     | 33/65 [06:20<06:08, 11.53s/it]
2022-04-10 15:11:36,149 - INFO - tqdm - accuracy: 0.5441, batch_loss: 0.6771, loss: 0.6841 ||:  52%|#####2    | 34/65 [06:32<05:57, 11.53s/it]
2022-04-10 15:11:48,050 - INFO - tqdm - accuracy: 0.5420, batch_loss: 0.6860, loss: 0.6842 ||:  54%|#####3    | 35/65 [06:44<05:49, 11.64s/it]
2022-04-10 15:11:59,551 - INFO - tqdm - accuracy: 0.5460, batch_loss: 0.6531, loss: 0.6833 ||:  55%|#####5    | 36/65 [06:55<05:36, 11.60s/it]
2022-04-10 15:12:11,134 - INFO - tqdm - accuracy: 0.5481, batch_loss: 0.6460, loss: 0.6823 ||:  57%|#####6    | 37/65 [07:07<05:24, 11.59s/it]
2022-04-10 15:12:22,721 - INFO - tqdm - accuracy: 0.5510, batch_loss: 0.6671, loss: 0.6819 ||:  58%|#####8    | 38/65 [07:18<05:12, 11.59s/it]
2022-04-10 15:12:34,330 - INFO - tqdm - accuracy: 0.5505, batch_loss: 0.7083, loss: 0.6826 ||:  60%|######    | 39/65 [07:30<05:01, 11.60s/it]
2022-04-10 15:12:45,942 - INFO - tqdm - accuracy: 0.5516, batch_loss: 0.6631, loss: 0.6821 ||:  62%|######1   | 40/65 [07:42<04:50, 11.60s/it]
2022-04-10 15:12:57,530 - INFO - tqdm - accuracy: 0.5511, batch_loss: 0.6839, loss: 0.6821 ||:  63%|######3   | 41/65 [07:53<04:38, 11.60s/it]
2022-04-10 15:13:09,081 - INFO - tqdm - accuracy: 0.5565, batch_loss: 0.6282, loss: 0.6808 ||:  65%|######4   | 42/65 [08:05<04:26, 11.58s/it]
2022-04-10 15:13:20,641 - INFO - tqdm - accuracy: 0.5545, batch_loss: 0.6952, loss: 0.6812 ||:  66%|######6   | 43/65 [08:16<04:14, 11.58s/it]
2022-04-10 15:13:32,189 - INFO - tqdm - accuracy: 0.5554, batch_loss: 0.6568, loss: 0.6806 ||:  68%|######7   | 44/65 [08:28<04:02, 11.57s/it]
2022-04-10 15:13:43,713 - INFO - tqdm - accuracy: 0.5556, batch_loss: 0.6365, loss: 0.6796 ||:  69%|######9   | 45/65 [08:39<03:51, 11.55s/it]
2022-04-10 15:13:55,243 - INFO - tqdm - accuracy: 0.5564, batch_loss: 0.6613, loss: 0.6792 ||:  71%|#######   | 46/65 [08:51<03:39, 11.55s/it]
2022-04-10 15:14:06,770 - INFO - tqdm - accuracy: 0.5572, batch_loss: 0.7047, loss: 0.6798 ||:  72%|#######2  | 47/65 [09:03<03:27, 11.54s/it]
2022-04-10 15:14:18,291 - INFO - tqdm - accuracy: 0.5560, batch_loss: 0.7274, loss: 0.6808 ||:  74%|#######3  | 48/65 [09:14<03:16, 11.53s/it]
2022-04-10 15:14:29,798 - INFO - tqdm - accuracy: 0.5536, batch_loss: 0.7740, loss: 0.6827 ||:  75%|#######5  | 49/65 [09:26<03:04, 11.53s/it]
2022-04-10 15:14:41,324 - INFO - tqdm - accuracy: 0.5556, batch_loss: 0.6754, loss: 0.6825 ||:  77%|#######6  | 50/65 [09:37<02:52, 11.53s/it]
2022-04-10 15:14:52,845 - INFO - tqdm - accuracy: 0.5570, batch_loss: 0.6697, loss: 0.6823 ||:  78%|#######8  | 51/65 [09:49<02:41, 11.52s/it]
2022-04-10 15:15:04,353 - INFO - tqdm - accuracy: 0.5577, batch_loss: 0.6862, loss: 0.6824 ||:  80%|########  | 52/65 [10:00<02:29, 11.52s/it]
2022-04-10 15:15:15,866 - INFO - tqdm - accuracy: 0.5601, batch_loss: 0.6112, loss: 0.6810 ||:  82%|########1 | 53/65 [10:12<02:18, 11.52s/it]
2022-04-10 15:15:27,379 - INFO - tqdm - accuracy: 0.5608, batch_loss: 0.7126, loss: 0.6816 ||:  83%|########3 | 54/65 [10:23<02:06, 11.52s/it]
2022-04-10 15:15:38,903 - INFO - tqdm - accuracy: 0.5636, batch_loss: 0.5801, loss: 0.6797 ||:  85%|########4 | 55/65 [10:35<01:55, 11.52s/it]
2022-04-10 15:15:50,422 - INFO - tqdm - accuracy: 0.5636, batch_loss: 0.6643, loss: 0.6795 ||:  86%|########6 | 56/65 [10:46<01:43, 11.52s/it]
2022-04-10 15:16:01,724 - INFO - tqdm - accuracy: 0.5652, batch_loss: 0.5976, loss: 0.6780 ||:  88%|########7 | 57/65 [10:57<01:31, 11.45s/it]
2022-04-10 15:16:13,249 - INFO - tqdm - accuracy: 0.5641, batch_loss: 0.6990, loss: 0.6784 ||:  89%|########9 | 58/65 [11:09<01:20, 11.48s/it]
2022-04-10 15:16:24,788 - INFO - tqdm - accuracy: 0.5620, batch_loss: 0.6736, loss: 0.6783 ||:  91%|######### | 59/65 [11:21<01:08, 11.49s/it]
2022-04-10 15:16:36,314 - INFO - tqdm - accuracy: 0.5620, batch_loss: 0.6761, loss: 0.6783 ||:  92%|#########2| 60/65 [11:32<00:57, 11.50s/it]
2022-04-10 15:16:47,824 - INFO - tqdm - accuracy: 0.5645, batch_loss: 0.5865, loss: 0.6768 ||:  94%|#########3| 61/65 [11:44<00:46, 11.51s/it]
2022-04-10 15:16:59,357 - INFO - tqdm - accuracy: 0.5665, batch_loss: 0.5651, loss: 0.6750 ||:  95%|#########5| 62/65 [11:55<00:34, 11.51s/it]
2022-04-10 15:17:10,873 - INFO - tqdm - accuracy: 0.5665, batch_loss: 0.6450, loss: 0.6745 ||:  97%|#########6| 63/65 [12:07<00:23, 11.51s/it]
2022-04-10 15:17:22,070 - INFO - tqdm - accuracy: 0.5667, batch_loss: 0.7077, loss: 0.6750 ||:  98%|#########8| 64/65 [12:18<00:11, 11.42s/it]
2022-04-10 15:17:27,186 - INFO - tqdm - accuracy: 0.5677, batch_loss: 0.6610, loss: 0.6748 ||: 100%|##########| 65/65 [12:23<00:00,  9.53s/it]
2022-04-10 15:17:27,186 - INFO - tqdm - accuracy: 0.5677, batch_loss: 0.6610, loss: 0.6748 ||: 100%|##########| 65/65 [12:23<00:00, 11.44s/it]
2022-04-10 15:17:30,096 - INFO - allennlp.training.trainer - Validating
2022-04-10 15:17:30,098 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-10 15:17:40,142 - INFO - tqdm - accuracy: 0.6500, batch_loss: 0.7142, loss: 0.7012 ||:  35%|###4      | 40/115 [00:10<00:19,  3.93it/s]
2022-04-10 15:17:50,378 - INFO - tqdm - accuracy: 0.6062, batch_loss: 0.9283, loss: 0.7006 ||:  70%|######9   | 80/115 [00:20<00:09,  3.87it/s]
2022-04-10 15:17:59,372 - INFO - tqdm - accuracy: 0.5983, batch_loss: 0.7197, loss: 0.7150 ||: 100%|##########| 115/115 [00:29<00:00,  3.83it/s]
2022-04-10 15:17:59,372 - INFO - tqdm - accuracy: 0.5983, batch_loss: 0.7197, loss: 0.7150 ||: 100%|##########| 115/115 [00:29<00:00,  3.93it/s]
2022-04-10 15:17:59,372 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-10 15:17:59,373 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.568  |     0.598
2022-04-10 15:17:59,374 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-10 15:17:59,375 - INFO - allennlp.training.tensorboard_writer - loss               |     0.675  |     0.715
2022-04-10 15:17:59,375 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5804.914  |       N/A
2022-04-10 15:18:05,166 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to './output_ir-ora-d_hyper4/best.th'.
2022-04-10 15:18:17,574 - INFO - allennlp.training.trainer - Epoch duration: 0:13:13.843154
2022-04-10 15:18:17,575 - INFO - allennlp.training.trainer - Estimated training time remaining: 2:50:17
2022-04-10 15:18:17,575 - INFO - allennlp.training.trainer - Epoch 2/14
2022-04-10 15:18:17,575 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-10 15:18:17,575 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-10 15:18:17,576 - INFO - allennlp.training.trainer - Training
2022-04-10 15:18:17,577 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-10 15:18:28,744 - INFO - tqdm - accuracy: 0.5312, batch_loss: 0.7263, loss: 0.7263 ||:   2%|1         | 1/65 [00:11<11:54, 11.17s/it]
2022-04-10 15:18:40,240 - INFO - tqdm - accuracy: 0.6562, batch_loss: 0.5418, loss: 0.6341 ||:   3%|3         | 2/65 [00:22<11:55, 11.36s/it]
2022-04-10 15:18:52,086 - INFO - tqdm - accuracy: 0.6354, batch_loss: 0.6121, loss: 0.6267 ||:   5%|4         | 3/65 [00:34<11:58, 11.58s/it]
2022-04-10 15:19:04,018 - INFO - tqdm - accuracy: 0.6172, batch_loss: 0.5852, loss: 0.6163 ||:   6%|6         | 4/65 [00:46<11:54, 11.72s/it]
2022-04-10 15:19:15,548 - INFO - tqdm - accuracy: 0.6312, batch_loss: 0.5258, loss: 0.5982 ||:   8%|7         | 5/65 [00:57<11:39, 11.65s/it]
2022-04-10 15:19:26,796 - INFO - tqdm - accuracy: 0.6406, batch_loss: 0.5391, loss: 0.5884 ||:   9%|9         | 6/65 [01:09<11:19, 11.51s/it]
2022-04-10 15:19:38,206 - INFO - tqdm - accuracy: 0.6205, batch_loss: 0.6941, loss: 0.6035 ||:  11%|#         | 7/65 [01:20<11:05, 11.48s/it]
2022-04-10 15:19:49,777 - INFO - tqdm - accuracy: 0.6250, batch_loss: 0.6435, loss: 0.6085 ||:  12%|#2        | 8/65 [01:32<10:56, 11.51s/it]
2022-04-10 15:20:01,497 - INFO - tqdm - accuracy: 0.6389, batch_loss: 0.5572, loss: 0.6028 ||:  14%|#3        | 9/65 [01:43<10:48, 11.58s/it]
2022-04-10 15:20:13,097 - INFO - tqdm - accuracy: 0.6406, batch_loss: 0.6008, loss: 0.6026 ||:  15%|#5        | 10/65 [01:55<10:37, 11.58s/it]
2022-04-10 15:20:24,628 - INFO - tqdm - accuracy: 0.6335, batch_loss: 0.6541, loss: 0.6073 ||:  17%|#6        | 11/65 [02:07<10:24, 11.57s/it]
2022-04-10 15:20:36,144 - INFO - tqdm - accuracy: 0.6406, batch_loss: 0.5879, loss: 0.6057 ||:  18%|#8        | 12/65 [02:18<10:12, 11.55s/it]
2022-04-10 15:20:47,321 - INFO - tqdm - accuracy: 0.6241, batch_loss: 0.8106, loss: 0.6214 ||:  20%|##        | 13/65 [02:29<09:54, 11.44s/it]
2022-04-10 15:20:58,868 - INFO - tqdm - accuracy: 0.6264, batch_loss: 0.6402, loss: 0.6228 ||:  22%|##1       | 14/65 [02:41<09:45, 11.47s/it]
2022-04-10 15:21:10,408 - INFO - tqdm - accuracy: 0.6242, batch_loss: 0.7969, loss: 0.6344 ||:  23%|##3       | 15/65 [02:52<09:34, 11.49s/it]
2022-04-10 15:21:21,947 - INFO - tqdm - accuracy: 0.6321, batch_loss: 0.5466, loss: 0.6289 ||:  25%|##4       | 16/65 [03:04<09:23, 11.51s/it]
2022-04-10 15:21:33,521 - INFO - tqdm - accuracy: 0.6243, batch_loss: 0.7174, loss: 0.6341 ||:  26%|##6       | 17/65 [03:15<09:13, 11.53s/it]
2022-04-10 15:21:45,130 - INFO - tqdm - accuracy: 0.6191, batch_loss: 0.7695, loss: 0.6416 ||:  28%|##7       | 18/65 [03:27<09:02, 11.55s/it]
2022-04-10 15:21:56,721 - INFO - tqdm - accuracy: 0.6227, batch_loss: 0.6284, loss: 0.6409 ||:  29%|##9       | 19/65 [03:39<08:51, 11.56s/it]
2022-04-10 15:22:08,305 - INFO - tqdm - accuracy: 0.6291, batch_loss: 0.6203, loss: 0.6399 ||:  31%|###       | 20/65 [03:50<08:40, 11.57s/it]
2022-04-10 15:22:19,883 - INFO - tqdm - accuracy: 0.6259, batch_loss: 0.6466, loss: 0.6402 ||:  32%|###2      | 21/65 [04:02<08:29, 11.57s/it]
2022-04-10 15:22:31,443 - INFO - tqdm - accuracy: 0.6273, batch_loss: 0.5809, loss: 0.6375 ||:  34%|###3      | 22/65 [04:13<08:17, 11.57s/it]
2022-04-10 15:22:42,970 - INFO - tqdm - accuracy: 0.6313, batch_loss: 0.5703, loss: 0.6346 ||:  35%|###5      | 23/65 [04:25<08:05, 11.56s/it]
2022-04-10 15:22:54,387 - INFO - tqdm - accuracy: 0.6336, batch_loss: 0.6383, loss: 0.6347 ||:  37%|###6      | 24/65 [04:36<07:52, 11.51s/it]
2022-04-10 15:23:05,928 - INFO - tqdm - accuracy: 0.6358, batch_loss: 0.6086, loss: 0.6337 ||:  38%|###8      | 25/65 [04:48<07:40, 11.52s/it]
2022-04-10 15:23:17,467 - INFO - tqdm - accuracy: 0.6366, batch_loss: 0.6141, loss: 0.6329 ||:  40%|####      | 26/65 [04:59<07:29, 11.53s/it]
2022-04-10 15:23:28,980 - INFO - tqdm - accuracy: 0.6373, batch_loss: 0.5792, loss: 0.6310 ||:  42%|####1     | 27/65 [05:11<07:17, 11.52s/it]
2022-04-10 15:23:40,491 - INFO - tqdm - accuracy: 0.6425, batch_loss: 0.5122, loss: 0.6267 ||:  43%|####3     | 28/65 [05:22<07:06, 11.52s/it]
2022-04-10 15:23:51,975 - INFO - tqdm - accuracy: 0.6397, batch_loss: 0.6204, loss: 0.6265 ||:  45%|####4     | 29/65 [05:34<06:54, 11.51s/it]
2022-04-10 15:24:03,498 - INFO - tqdm - accuracy: 0.6382, batch_loss: 0.5780, loss: 0.6249 ||:  46%|####6     | 30/65 [05:45<06:42, 11.51s/it]
2022-04-10 15:24:15,050 - INFO - tqdm - accuracy: 0.6377, batch_loss: 0.5774, loss: 0.6233 ||:  48%|####7     | 31/65 [05:57<06:31, 11.52s/it]
2022-04-10 15:24:26,598 - INFO - tqdm - accuracy: 0.6422, batch_loss: 0.5562, loss: 0.6212 ||:  49%|####9     | 32/65 [06:09<06:20, 11.53s/it]
2022-04-10 15:24:38,184 - INFO - tqdm - accuracy: 0.6398, batch_loss: 0.7152, loss: 0.6241 ||:  51%|#####     | 33/65 [06:20<06:09, 11.55s/it]
2022-04-10 15:24:49,769 - INFO - tqdm - accuracy: 0.6394, batch_loss: 0.6511, loss: 0.6249 ||:  52%|#####2    | 34/65 [06:32<05:58, 11.56s/it]
2022-04-10 15:25:01,396 - INFO - tqdm - accuracy: 0.6408, batch_loss: 0.5263, loss: 0.6221 ||:  54%|#####3    | 35/65 [06:43<05:47, 11.58s/it]
2022-04-10 15:25:12,991 - INFO - tqdm - accuracy: 0.6421, batch_loss: 0.5708, loss: 0.6206 ||:  55%|#####5    | 36/65 [06:55<05:35, 11.58s/it]
2022-04-10 15:25:24,577 - INFO - tqdm - accuracy: 0.6424, batch_loss: 0.6176, loss: 0.6206 ||:  57%|#####6    | 37/65 [07:07<05:24, 11.58s/it]
2022-04-10 15:25:36,164 - INFO - tqdm - accuracy: 0.6477, batch_loss: 0.4417, loss: 0.6159 ||:  58%|#####8    | 38/65 [07:18<05:12, 11.59s/it]
2022-04-10 15:25:47,739 - INFO - tqdm - accuracy: 0.6472, batch_loss: 0.5416, loss: 0.6140 ||:  60%|######    | 39/65 [07:30<05:01, 11.58s/it]
2022-04-10 15:25:59,310 - INFO - tqdm - accuracy: 0.6466, batch_loss: 0.6457, loss: 0.6147 ||:  62%|######1   | 40/65 [07:41<04:49, 11.58s/it]
2022-04-10 15:26:10,872 - INFO - tqdm - accuracy: 0.6461, batch_loss: 0.5564, loss: 0.6133 ||:  63%|######3   | 41/65 [07:53<04:37, 11.57s/it]
2022-04-10 15:26:22,321 - INFO - tqdm - accuracy: 0.6478, batch_loss: 0.5682, loss: 0.6123 ||:  65%|######4   | 42/65 [08:04<04:25, 11.54s/it]
2022-04-10 15:26:33,880 - INFO - tqdm - accuracy: 0.6451, batch_loss: 0.6260, loss: 0.6126 ||:  66%|######6   | 43/65 [08:16<04:13, 11.54s/it]
2022-04-10 15:26:45,425 - INFO - tqdm - accuracy: 0.6468, batch_loss: 0.5652, loss: 0.6115 ||:  68%|######7   | 44/65 [08:27<04:02, 11.54s/it]
2022-04-10 15:26:56,983 - INFO - tqdm - accuracy: 0.6484, batch_loss: 0.5937, loss: 0.6111 ||:  69%|######9   | 45/65 [08:39<03:50, 11.55s/it]
2022-04-10 15:27:08,529 - INFO - tqdm - accuracy: 0.6506, batch_loss: 0.5476, loss: 0.6097 ||:  71%|#######   | 46/65 [08:50<03:39, 11.55s/it]
2022-04-10 15:27:20,061 - INFO - tqdm - accuracy: 0.6494, batch_loss: 0.7068, loss: 0.6118 ||:  72%|#######2  | 47/65 [09:02<03:27, 11.54s/it]
2022-04-10 15:27:31,609 - INFO - tqdm - accuracy: 0.6489, batch_loss: 0.6928, loss: 0.6135 ||:  74%|#######3  | 48/65 [09:14<03:16, 11.54s/it]
2022-04-10 15:27:43,161 - INFO - tqdm - accuracy: 0.6477, batch_loss: 0.5785, loss: 0.6128 ||:  75%|#######5  | 49/65 [09:25<03:04, 11.55s/it]
2022-04-10 15:27:54,711 - INFO - tqdm - accuracy: 0.6460, batch_loss: 0.6260, loss: 0.6130 ||:  77%|#######6  | 50/65 [09:37<02:53, 11.55s/it]
2022-04-10 15:28:06,251 - INFO - tqdm - accuracy: 0.6456, batch_loss: 0.6373, loss: 0.6135 ||:  78%|#######8  | 51/65 [09:48<02:41, 11.55s/it]
2022-04-10 15:28:17,782 - INFO - tqdm - accuracy: 0.6452, batch_loss: 0.5930, loss: 0.6131 ||:  80%|########  | 52/65 [10:00<02:30, 11.54s/it]
2022-04-10 15:28:29,303 - INFO - tqdm - accuracy: 0.6472, batch_loss: 0.5206, loss: 0.6114 ||:  82%|########1 | 53/65 [10:11<02:18, 11.54s/it]
2022-04-10 15:28:40,857 - INFO - tqdm - accuracy: 0.6479, batch_loss: 0.6084, loss: 0.6113 ||:  83%|########3 | 54/65 [10:23<02:06, 11.54s/it]
2022-04-10 15:28:52,377 - INFO - tqdm - accuracy: 0.6481, batch_loss: 0.6378, loss: 0.6118 ||:  85%|########4 | 55/65 [10:34<01:55, 11.53s/it]
2022-04-10 15:29:03,903 - INFO - tqdm - accuracy: 0.6482, batch_loss: 0.6070, loss: 0.6117 ||:  86%|########6 | 56/65 [10:46<01:43, 11.53s/it]
2022-04-10 15:29:15,422 - INFO - tqdm - accuracy: 0.6467, batch_loss: 0.7480, loss: 0.6141 ||:  88%|########7 | 57/65 [10:57<01:32, 11.53s/it]
2022-04-10 15:29:26,700 - INFO - tqdm - accuracy: 0.6469, batch_loss: 0.5203, loss: 0.6125 ||:  89%|########9 | 58/65 [11:09<01:20, 11.45s/it]
2022-04-10 15:29:38,231 - INFO - tqdm - accuracy: 0.6497, batch_loss: 0.4502, loss: 0.6097 ||:  91%|######### | 59/65 [11:20<01:08, 11.48s/it]
2022-04-10 15:29:49,753 - INFO - tqdm - accuracy: 0.6488, batch_loss: 0.6785, loss: 0.6109 ||:  92%|#########2| 60/65 [11:32<00:57, 11.49s/it]
2022-04-10 15:30:01,283 - INFO - tqdm - accuracy: 0.6520, batch_loss: 0.4965, loss: 0.6090 ||:  94%|#########3| 61/65 [11:43<00:46, 11.50s/it]
2022-04-10 15:30:12,823 - INFO - tqdm - accuracy: 0.6510, batch_loss: 0.7326, loss: 0.6110 ||:  95%|#########5| 62/65 [11:55<00:34, 11.51s/it]
2022-04-10 15:30:24,340 - INFO - tqdm - accuracy: 0.6501, batch_loss: 0.5809, loss: 0.6105 ||:  97%|#########6| 63/65 [12:06<00:23, 11.51s/it]
2022-04-10 15:30:35,892 - INFO - tqdm - accuracy: 0.6483, batch_loss: 0.7067, loss: 0.6120 ||:  98%|#########8| 64/65 [12:18<00:11, 11.53s/it]
2022-04-10 15:30:41,015 - INFO - tqdm - accuracy: 0.6487, batch_loss: 0.7496, loss: 0.6141 ||: 100%|##########| 65/65 [12:23<00:00,  9.60s/it]
2022-04-10 15:30:41,015 - INFO - tqdm - accuracy: 0.6487, batch_loss: 0.7496, loss: 0.6141 ||: 100%|##########| 65/65 [12:23<00:00, 11.44s/it]
2022-04-10 15:30:43,907 - INFO - allennlp.training.trainer - Validating
2022-04-10 15:30:43,909 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-10 15:30:54,039 - INFO - tqdm - accuracy: 0.6500, batch_loss: 0.6269, loss: 0.7150 ||:  35%|###4      | 40/115 [00:10<00:19,  3.94it/s]
2022-04-10 15:31:04,080 - INFO - tqdm - accuracy: 0.6415, batch_loss: 0.8354, loss: 0.6612 ||:  70%|######9   | 80/115 [00:20<00:08,  3.98it/s]
2022-04-10 15:31:13,174 - INFO - tqdm - accuracy: 0.6419, batch_loss: 1.3639, loss: 0.6731 ||: 100%|##########| 115/115 [00:29<00:00,  3.84it/s]
2022-04-10 15:31:13,174 - INFO - tqdm - accuracy: 0.6419, batch_loss: 1.3639, loss: 0.6731 ||: 100%|##########| 115/115 [00:29<00:00,  3.93it/s]
2022-04-10 15:31:13,175 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-10 15:31:13,175 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.649  |     0.642
2022-04-10 15:31:13,176 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-10 15:31:13,177 - INFO - allennlp.training.tensorboard_writer - loss               |     0.614  |     0.673
2022-04-10 15:31:13,177 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5804.914  |       N/A
2022-04-10 15:31:18,849 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to './output_ir-ora-d_hyper4/best.th'.
2022-04-10 15:31:30,280 - INFO - allennlp.training.trainer - Epoch duration: 0:13:12.704884
2022-04-10 15:31:30,280 - INFO - allennlp.training.trainer - Estimated training time remaining: 2:37:38
2022-04-10 15:31:30,280 - INFO - allennlp.training.trainer - Epoch 3/14
2022-04-10 15:31:30,280 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-10 15:31:30,280 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-10 15:31:30,282 - INFO - allennlp.training.trainer - Training
2022-04-10 15:31:30,282 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-10 15:31:41,471 - INFO - tqdm - accuracy: 0.6562, batch_loss: 0.6152, loss: 0.6152 ||:   2%|1         | 1/65 [00:11<11:56, 11.19s/it]
2022-04-10 15:31:52,974 - INFO - tqdm - accuracy: 0.5938, batch_loss: 0.6845, loss: 0.6499 ||:   3%|3         | 2/65 [00:22<11:56, 11.37s/it]
2022-04-10 15:32:04,801 - INFO - tqdm - accuracy: 0.6250, batch_loss: 0.5929, loss: 0.6309 ||:   5%|4         | 3/65 [00:34<11:58, 11.58s/it]
2022-04-10 15:32:16,758 - INFO - tqdm - accuracy: 0.6562, batch_loss: 0.5281, loss: 0.6052 ||:   6%|6         | 4/65 [00:46<11:55, 11.73s/it]
2022-04-10 15:32:28,695 - INFO - tqdm - accuracy: 0.6813, batch_loss: 0.5370, loss: 0.5915 ||:   8%|7         | 5/65 [00:58<11:48, 11.80s/it]
2022-04-10 15:32:40,026 - INFO - tqdm - accuracy: 0.6979, batch_loss: 0.5479, loss: 0.5843 ||:   9%|9         | 6/65 [01:09<11:26, 11.64s/it]
2022-04-10 15:32:51,435 - INFO - tqdm - accuracy: 0.7009, batch_loss: 0.5157, loss: 0.5745 ||:  11%|#         | 7/65 [01:21<11:10, 11.57s/it]
2022-04-10 15:33:02,858 - INFO - tqdm - accuracy: 0.7109, batch_loss: 0.5112, loss: 0.5666 ||:  12%|#2        | 8/65 [01:32<10:56, 11.52s/it]
2022-04-10 15:33:14,489 - INFO - tqdm - accuracy: 0.7188, batch_loss: 0.4903, loss: 0.5581 ||:  14%|#3        | 9/65 [01:44<10:47, 11.56s/it]
2022-04-10 15:33:26,088 - INFO - tqdm - accuracy: 0.7250, batch_loss: 0.4918, loss: 0.5515 ||:  15%|#5        | 10/65 [01:55<10:36, 11.57s/it]
2022-04-10 15:33:37,503 - INFO - tqdm - accuracy: 0.7273, batch_loss: 0.5142, loss: 0.5481 ||:  17%|#6        | 11/65 [02:07<10:22, 11.52s/it]
2022-04-10 15:33:49,000 - INFO - tqdm - accuracy: 0.7370, batch_loss: 0.4262, loss: 0.5379 ||:  18%|#8        | 12/65 [02:18<10:10, 11.51s/it]
2022-04-10 15:34:00,492 - INFO - tqdm - accuracy: 0.7404, batch_loss: 0.4358, loss: 0.5301 ||:  20%|##        | 13/65 [02:30<09:58, 11.51s/it]
2022-04-10 15:34:12,026 - INFO - tqdm - accuracy: 0.7344, batch_loss: 0.5574, loss: 0.5320 ||:  22%|##1       | 14/65 [02:41<09:47, 11.52s/it]
2022-04-10 15:34:23,577 - INFO - tqdm - accuracy: 0.7375, batch_loss: 0.5032, loss: 0.5301 ||:  23%|##3       | 15/65 [02:53<09:36, 11.53s/it]
2022-04-10 15:34:35,148 - INFO - tqdm - accuracy: 0.7344, batch_loss: 0.5606, loss: 0.5320 ||:  25%|##4       | 16/65 [03:04<09:25, 11.54s/it]
2022-04-10 15:34:46,720 - INFO - tqdm - accuracy: 0.7316, batch_loss: 0.5251, loss: 0.5316 ||:  26%|##6       | 17/65 [03:16<09:14, 11.55s/it]
2022-04-10 15:34:58,332 - INFO - tqdm - accuracy: 0.7396, batch_loss: 0.3366, loss: 0.5208 ||:  28%|##7       | 18/65 [03:28<09:03, 11.57s/it]
2022-04-10 15:35:09,947 - INFO - tqdm - accuracy: 0.7270, batch_loss: 0.7226, loss: 0.5314 ||:  29%|##9       | 19/65 [03:39<08:52, 11.58s/it]
2022-04-10 15:35:21,538 - INFO - tqdm - accuracy: 0.7234, batch_loss: 0.5158, loss: 0.5306 ||:  31%|###       | 20/65 [03:51<08:41, 11.58s/it]
2022-04-10 15:35:33,119 - INFO - tqdm - accuracy: 0.7262, batch_loss: 0.4787, loss: 0.5281 ||:  32%|###2      | 21/65 [04:02<08:29, 11.58s/it]
2022-04-10 15:35:44,676 - INFO - tqdm - accuracy: 0.7244, batch_loss: 0.6043, loss: 0.5316 ||:  34%|###3      | 22/65 [04:14<08:17, 11.58s/it]
2022-04-10 15:35:56,198 - INFO - tqdm - accuracy: 0.7255, batch_loss: 0.4053, loss: 0.5261 ||:  35%|###5      | 23/65 [04:25<08:05, 11.56s/it]
2022-04-10 15:36:07,735 - INFO - tqdm - accuracy: 0.7279, batch_loss: 0.4232, loss: 0.5218 ||:  37%|###6      | 24/65 [04:37<07:53, 11.55s/it]
2022-04-10 15:36:19,283 - INFO - tqdm - accuracy: 0.7338, batch_loss: 0.3790, loss: 0.5161 ||:  38%|###8      | 25/65 [04:49<07:42, 11.55s/it]
2022-04-10 15:36:30,781 - INFO - tqdm - accuracy: 0.7380, batch_loss: 0.4407, loss: 0.5132 ||:  40%|####      | 26/65 [05:00<07:29, 11.54s/it]
2022-04-10 15:36:42,322 - INFO - tqdm - accuracy: 0.7361, batch_loss: 0.5682, loss: 0.5152 ||:  42%|####1     | 27/65 [05:12<07:18, 11.54s/it]
2022-04-10 15:36:53,833 - INFO - tqdm - accuracy: 0.7377, batch_loss: 0.4192, loss: 0.5118 ||:  43%|####3     | 28/65 [05:23<07:06, 11.53s/it]
2022-04-10 15:37:05,334 - INFO - tqdm - accuracy: 0.7392, batch_loss: 0.3798, loss: 0.5073 ||:  45%|####4     | 29/65 [05:35<06:54, 11.52s/it]
2022-04-10 15:37:16,818 - INFO - tqdm - accuracy: 0.7406, batch_loss: 0.4353, loss: 0.5049 ||:  46%|####6     | 30/65 [05:46<06:42, 11.51s/it]
2022-04-10 15:37:28,321 - INFO - tqdm - accuracy: 0.7450, batch_loss: 0.4260, loss: 0.5023 ||:  48%|####7     | 31/65 [05:58<06:31, 11.51s/it]
2022-04-10 15:37:39,730 - INFO - tqdm - accuracy: 0.7451, batch_loss: 0.5095, loss: 0.5025 ||:  49%|####9     | 32/65 [06:09<06:18, 11.48s/it]
2022-04-10 15:37:51,294 - INFO - tqdm - accuracy: 0.7424, batch_loss: 0.5278, loss: 0.5033 ||:  51%|#####     | 33/65 [06:21<06:08, 11.50s/it]
2022-04-10 15:38:02,875 - INFO - tqdm - accuracy: 0.7408, batch_loss: 0.5687, loss: 0.5052 ||:  52%|#####2    | 34/65 [06:32<05:57, 11.53s/it]
2022-04-10 15:38:14,455 - INFO - tqdm - accuracy: 0.7411, batch_loss: 0.5287, loss: 0.5059 ||:  54%|#####3    | 35/65 [06:44<05:46, 11.54s/it]
2022-04-10 15:38:26,036 - INFO - tqdm - accuracy: 0.7396, batch_loss: 0.5386, loss: 0.5068 ||:  55%|#####5    | 36/65 [06:55<05:35, 11.55s/it]
2022-04-10 15:38:37,626 - INFO - tqdm - accuracy: 0.7390, batch_loss: 0.4794, loss: 0.5061 ||:  57%|#####6    | 37/65 [07:07<05:23, 11.56s/it]
2022-04-10 15:38:49,247 - INFO - tqdm - accuracy: 0.7393, batch_loss: 0.5078, loss: 0.5061 ||:  58%|#####8    | 38/65 [07:18<05:12, 11.58s/it]
2022-04-10 15:39:00,833 - INFO - tqdm - accuracy: 0.7380, batch_loss: 0.6543, loss: 0.5099 ||:  60%|######    | 39/65 [07:30<05:01, 11.58s/it]
2022-04-10 15:39:12,404 - INFO - tqdm - accuracy: 0.7375, batch_loss: 0.5419, loss: 0.5107 ||:  62%|######1   | 40/65 [07:42<04:49, 11.58s/it]
2022-04-10 15:39:23,977 - INFO - tqdm - accuracy: 0.7393, batch_loss: 0.4957, loss: 0.5103 ||:  63%|######3   | 41/65 [07:53<04:37, 11.58s/it]
2022-04-10 15:39:35,539 - INFO - tqdm - accuracy: 0.7403, batch_loss: 0.4502, loss: 0.5089 ||:  65%|######4   | 42/65 [08:05<04:26, 11.57s/it]
2022-04-10 15:39:47,084 - INFO - tqdm - accuracy: 0.7435, batch_loss: 0.3881, loss: 0.5061 ||:  66%|######6   | 43/65 [08:16<04:14, 11.56s/it]
2022-04-10 15:39:58,631 - INFO - tqdm - accuracy: 0.7422, batch_loss: 0.5657, loss: 0.5075 ||:  68%|######7   | 44/65 [08:28<04:02, 11.56s/it]
2022-04-10 15:40:10,155 - INFO - tqdm - accuracy: 0.7403, batch_loss: 0.6026, loss: 0.5096 ||:  69%|######9   | 45/65 [08:39<03:50, 11.55s/it]
2022-04-10 15:40:21,678 - INFO - tqdm - accuracy: 0.7391, batch_loss: 0.4856, loss: 0.5090 ||:  71%|#######   | 46/65 [08:51<03:39, 11.54s/it]
2022-04-10 15:40:33,191 - INFO - tqdm - accuracy: 0.7387, batch_loss: 0.4855, loss: 0.5085 ||:  72%|#######2  | 47/65 [09:02<03:27, 11.53s/it]
2022-04-10 15:40:44,716 - INFO - tqdm - accuracy: 0.7409, batch_loss: 0.3612, loss: 0.5055 ||:  74%|#######3  | 48/65 [09:14<03:16, 11.53s/it]
2022-04-10 15:40:56,237 - INFO - tqdm - accuracy: 0.7398, batch_loss: 0.5897, loss: 0.5072 ||:  75%|#######5  | 49/65 [09:25<03:04, 11.53s/it]
2022-04-10 15:41:07,739 - INFO - tqdm - accuracy: 0.7406, batch_loss: 0.5529, loss: 0.5081 ||:  77%|#######6  | 50/65 [09:37<02:52, 11.52s/it]
2022-04-10 15:41:19,259 - INFO - tqdm - accuracy: 0.7402, batch_loss: 0.6258, loss: 0.5104 ||:  78%|#######8  | 51/65 [09:48<02:41, 11.52s/it]
2022-04-10 15:41:30,527 - INFO - tqdm - accuracy: 0.7392, batch_loss: 0.5218, loss: 0.5106 ||:  80%|########  | 52/65 [10:00<02:28, 11.44s/it]
2022-04-10 15:41:42,017 - INFO - tqdm - accuracy: 0.7376, batch_loss: 0.5110, loss: 0.5106 ||:  82%|########1 | 53/65 [10:11<02:17, 11.46s/it]
2022-04-10 15:41:53,551 - INFO - tqdm - accuracy: 0.7355, batch_loss: 0.5861, loss: 0.5120 ||:  83%|########3 | 54/65 [10:23<02:06, 11.48s/it]
2022-04-10 15:42:05,096 - INFO - tqdm - accuracy: 0.7347, batch_loss: 0.5418, loss: 0.5126 ||:  85%|########4 | 55/65 [10:34<01:55, 11.50s/it]
2022-04-10 15:42:16,661 - INFO - tqdm - accuracy: 0.7333, batch_loss: 0.6041, loss: 0.5142 ||:  86%|########6 | 56/65 [10:46<01:43, 11.52s/it]
2022-04-10 15:42:28,251 - INFO - tqdm - accuracy: 0.7336, batch_loss: 0.5045, loss: 0.5140 ||:  88%|########7 | 57/65 [10:57<01:32, 11.54s/it]
2022-04-10 15:42:39,839 - INFO - tqdm - accuracy: 0.7365, batch_loss: 0.4133, loss: 0.5123 ||:  89%|########9 | 58/65 [11:09<01:20, 11.55s/it]
2022-04-10 15:42:51,110 - INFO - tqdm - accuracy: 0.7366, batch_loss: 0.5114, loss: 0.5123 ||:  91%|######### | 59/65 [11:20<01:08, 11.47s/it]
2022-04-10 15:43:02,680 - INFO - tqdm - accuracy: 0.7384, batch_loss: 0.3958, loss: 0.5104 ||:  92%|#########2| 60/65 [11:32<00:57, 11.50s/it]
2022-04-10 15:43:14,264 - INFO - tqdm - accuracy: 0.7386, batch_loss: 0.4882, loss: 0.5100 ||:  94%|#########3| 61/65 [11:43<00:46, 11.53s/it]
2022-04-10 15:43:25,811 - INFO - tqdm - accuracy: 0.7398, batch_loss: 0.5024, loss: 0.5099 ||:  95%|#########5| 62/65 [11:55<00:34, 11.53s/it]
2022-04-10 15:43:37,358 - INFO - tqdm - accuracy: 0.7385, batch_loss: 0.5793, loss: 0.5110 ||:  97%|#########6| 63/65 [12:07<00:23, 11.54s/it]
2022-04-10 15:43:48,877 - INFO - tqdm - accuracy: 0.7391, batch_loss: 0.4583, loss: 0.5101 ||:  98%|#########8| 64/65 [12:18<00:11, 11.53s/it]
2022-04-10 15:43:53,987 - INFO - tqdm - accuracy: 0.7380, batch_loss: 0.5181, loss: 0.5103 ||: 100%|##########| 65/65 [12:23<00:00,  9.60s/it]
2022-04-10 15:43:53,989 - INFO - tqdm - accuracy: 0.7380, batch_loss: 0.5181, loss: 0.5103 ||: 100%|##########| 65/65 [12:23<00:00, 11.44s/it]
2022-04-10 15:43:56,892 - INFO - allennlp.training.trainer - Validating
2022-04-10 15:43:56,893 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-10 15:44:07,040 - INFO - tqdm - accuracy: 0.5875, batch_loss: 0.0939, loss: 0.9199 ||:  35%|###4      | 40/115 [00:10<00:19,  3.94it/s]
2022-04-10 15:44:17,173 - INFO - tqdm - accuracy: 0.6438, batch_loss: 1.1327, loss: 0.7813 ||:  70%|######9   | 80/115 [00:20<00:09,  3.88it/s]
2022-04-10 15:44:26,089 - INFO - tqdm - accuracy: 0.6507, batch_loss: 0.0699, loss: 0.7696 ||: 100%|##########| 115/115 [00:29<00:00,  3.94it/s]
2022-04-10 15:44:26,089 - INFO - tqdm - accuracy: 0.6507, batch_loss: 0.0699, loss: 0.7696 ||: 100%|##########| 115/115 [00:29<00:00,  3.94it/s]
2022-04-10 15:44:26,089 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-10 15:44:26,090 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.738  |     0.651
2022-04-10 15:44:26,091 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-10 15:44:26,091 - INFO - allennlp.training.tensorboard_writer - loss               |     0.510  |     0.770
2022-04-10 15:44:26,092 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5804.914  |       N/A
2022-04-10 15:44:31,886 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to './output_ir-ora-d_hyper4/best.th'.
2022-04-10 15:44:44,390 - INFO - allennlp.training.trainer - Epoch duration: 0:13:14.109486
2022-04-10 15:44:44,390 - INFO - allennlp.training.trainer - Estimated training time remaining: 2:24:46
2022-04-10 15:44:44,390 - INFO - allennlp.training.trainer - Epoch 4/14
2022-04-10 15:44:44,390 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-10 15:44:44,390 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-10 15:44:44,392 - INFO - allennlp.training.trainer - Training
2022-04-10 15:44:44,392 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-10 15:44:55,646 - INFO - tqdm - accuracy: 0.8438, batch_loss: 0.4439, loss: 0.4439 ||:   2%|1         | 1/65 [00:11<12:00, 11.25s/it]
2022-04-10 15:45:07,225 - INFO - tqdm - accuracy: 0.8438, batch_loss: 0.3588, loss: 0.4014 ||:   3%|3         | 2/65 [00:22<12:01, 11.45s/it]
2022-04-10 15:45:19,119 - INFO - tqdm - accuracy: 0.8021, batch_loss: 0.5513, loss: 0.4514 ||:   5%|4         | 3/65 [00:34<12:02, 11.65s/it]
2022-04-10 15:45:31,017 - INFO - tqdm - accuracy: 0.8125, batch_loss: 0.4947, loss: 0.4622 ||:   6%|6         | 4/65 [00:46<11:56, 11.75s/it]
2022-04-10 15:45:42,532 - INFO - tqdm - accuracy: 0.8125, batch_loss: 0.4482, loss: 0.4594 ||:   8%|7         | 5/65 [00:58<11:39, 11.66s/it]
2022-04-10 15:45:53,931 - INFO - tqdm - accuracy: 0.8229, batch_loss: 0.4535, loss: 0.4584 ||:   9%|9         | 6/65 [01:09<11:22, 11.57s/it]
2022-04-10 15:46:05,352 - INFO - tqdm - accuracy: 0.8170, batch_loss: 0.4208, loss: 0.4530 ||:  11%|#         | 7/65 [01:20<11:08, 11.52s/it]
2022-04-10 15:46:16,933 - INFO - tqdm - accuracy: 0.8359, batch_loss: 0.3078, loss: 0.4349 ||:  12%|#2        | 8/65 [01:32<10:57, 11.54s/it]
2022-04-10 15:46:28,623 - INFO - tqdm - accuracy: 0.8403, batch_loss: 0.3584, loss: 0.4264 ||:  14%|#3        | 9/65 [01:44<10:48, 11.59s/it]
2022-04-10 15:46:40,217 - INFO - tqdm - accuracy: 0.8375, batch_loss: 0.3548, loss: 0.4192 ||:  15%|#5        | 10/65 [01:55<10:37, 11.59s/it]
2022-04-10 15:46:51,760 - INFO - tqdm - accuracy: 0.8352, batch_loss: 0.3984, loss: 0.4173 ||:  17%|#6        | 11/65 [02:07<10:25, 11.58s/it]
2022-04-10 15:47:03,230 - INFO - tqdm - accuracy: 0.8281, batch_loss: 0.4695, loss: 0.4217 ||:  18%|#8        | 12/65 [02:18<10:11, 11.54s/it]
2022-04-10 15:47:14,737 - INFO - tqdm - accuracy: 0.8197, batch_loss: 0.4798, loss: 0.4261 ||:  20%|##        | 13/65 [02:30<09:59, 11.53s/it]
2022-04-10 15:47:26,275 - INFO - tqdm - accuracy: 0.8259, batch_loss: 0.3416, loss: 0.4201 ||:  22%|##1       | 14/65 [02:41<09:48, 11.53s/it]
2022-04-10 15:47:37,828 - INFO - tqdm - accuracy: 0.8271, batch_loss: 0.3377, loss: 0.4146 ||:  23%|##3       | 15/65 [02:53<09:36, 11.54s/it]
2022-04-10 15:47:49,408 - INFO - tqdm - accuracy: 0.8281, batch_loss: 0.3723, loss: 0.4120 ||:  25%|##4       | 16/65 [03:05<09:26, 11.55s/it]
2022-04-10 15:48:00,981 - INFO - tqdm - accuracy: 0.8290, batch_loss: 0.3589, loss: 0.4088 ||:  26%|##6       | 17/65 [03:16<09:14, 11.56s/it]
2022-04-10 15:48:12,566 - INFO - tqdm - accuracy: 0.8316, batch_loss: 0.3403, loss: 0.4050 ||:  28%|##7       | 18/65 [03:28<09:03, 11.57s/it]
2022-04-10 15:48:24,194 - INFO - tqdm - accuracy: 0.8322, batch_loss: 0.3976, loss: 0.4046 ||:  29%|##9       | 19/65 [03:39<08:52, 11.58s/it]
2022-04-10 15:48:35,763 - INFO - tqdm - accuracy: 0.8266, batch_loss: 0.5302, loss: 0.4109 ||:  31%|###       | 20/65 [03:51<08:41, 11.58s/it]
2022-04-10 15:48:47,330 - INFO - tqdm - accuracy: 0.8229, batch_loss: 0.5623, loss: 0.4181 ||:  32%|###2      | 21/65 [04:02<08:29, 11.58s/it]
2022-04-10 15:48:58,863 - INFO - tqdm - accuracy: 0.8210, batch_loss: 0.3469, loss: 0.4149 ||:  34%|###3      | 22/65 [04:14<08:17, 11.56s/it]
2022-04-10 15:49:10,350 - INFO - tqdm - accuracy: 0.8152, batch_loss: 0.5357, loss: 0.4201 ||:  35%|###5      | 23/65 [04:25<08:04, 11.54s/it]
2022-04-10 15:49:21,846 - INFO - tqdm - accuracy: 0.8125, batch_loss: 0.3870, loss: 0.4188 ||:  37%|###6      | 24/65 [04:37<07:52, 11.53s/it]
2022-04-10 15:49:33,363 - INFO - tqdm - accuracy: 0.8050, batch_loss: 0.5593, loss: 0.4244 ||:  38%|###8      | 25/65 [04:48<07:40, 11.52s/it]
2022-04-10 15:49:44,881 - INFO - tqdm - accuracy: 0.8077, batch_loss: 0.3230, loss: 0.4205 ||:  40%|####      | 26/65 [05:00<07:29, 11.52s/it]
2022-04-10 15:49:56,394 - INFO - tqdm - accuracy: 0.8125, batch_loss: 0.2675, loss: 0.4148 ||:  42%|####1     | 27/65 [05:12<07:17, 11.52s/it]
2022-04-10 15:50:07,932 - INFO - tqdm - accuracy: 0.8125, batch_loss: 0.4625, loss: 0.4165 ||:  43%|####3     | 28/65 [05:23<07:06, 11.52s/it]
2022-04-10 15:50:19,451 - INFO - tqdm - accuracy: 0.8157, batch_loss: 0.2996, loss: 0.4125 ||:  45%|####4     | 29/65 [05:35<06:54, 11.52s/it]
2022-04-10 15:50:30,838 - INFO - tqdm - accuracy: 0.8146, batch_loss: 0.4965, loss: 0.4153 ||:  46%|####6     | 30/65 [05:46<06:41, 11.48s/it]
2022-04-10 15:50:42,368 - INFO - tqdm - accuracy: 0.8185, batch_loss: 0.3499, loss: 0.4132 ||:  48%|####7     | 31/65 [05:57<06:30, 11.50s/it]
2022-04-10 15:50:53,901 - INFO - tqdm - accuracy: 0.8164, batch_loss: 0.3777, loss: 0.4121 ||:  49%|####9     | 32/65 [06:09<06:19, 11.51s/it]
2022-04-10 15:51:05,440 - INFO - tqdm - accuracy: 0.8172, batch_loss: 0.3931, loss: 0.4115 ||:  51%|#####     | 33/65 [06:21<06:08, 11.52s/it]
2022-04-10 15:51:17,002 - INFO - tqdm - accuracy: 0.8171, batch_loss: 0.3445, loss: 0.4095 ||:  52%|#####2    | 34/65 [06:32<05:57, 11.53s/it]
2022-04-10 15:51:28,537 - INFO - tqdm - accuracy: 0.8187, batch_loss: 0.2940, loss: 0.4062 ||:  54%|#####3    | 35/65 [06:44<05:45, 11.53s/it]
2022-04-10 15:51:40,117 - INFO - tqdm - accuracy: 0.8203, batch_loss: 0.3631, loss: 0.4050 ||:  55%|#####5    | 36/65 [06:55<05:34, 11.55s/it]
2022-04-10 15:51:51,358 - INFO - tqdm - accuracy: 0.8191, batch_loss: 0.4281, loss: 0.4056 ||:  57%|#####6    | 37/65 [07:06<05:20, 11.45s/it]
2022-04-10 15:52:02,916 - INFO - tqdm - accuracy: 0.8222, batch_loss: 0.2222, loss: 0.4008 ||:  58%|#####8    | 38/65 [07:18<05:10, 11.49s/it]
2022-04-10 15:52:14,524 - INFO - tqdm - accuracy: 0.8236, batch_loss: 0.3045, loss: 0.3983 ||:  60%|######    | 39/65 [07:30<04:59, 11.52s/it]
2022-04-10 15:52:26,655 - INFO - tqdm - accuracy: 0.8233, batch_loss: 0.4483, loss: 0.3996 ||:  62%|######1   | 40/65 [07:42<04:52, 11.70s/it]
2022-04-10 15:52:38,192 - INFO - tqdm - accuracy: 0.8238, batch_loss: 0.3588, loss: 0.3986 ||:  63%|######3   | 41/65 [07:53<04:39, 11.65s/it]
2022-04-10 15:52:49,789 - INFO - tqdm - accuracy: 0.8243, batch_loss: 0.3980, loss: 0.3986 ||:  65%|######4   | 42/65 [08:05<04:27, 11.64s/it]
2022-04-10 15:53:01,373 - INFO - tqdm - accuracy: 0.8233, batch_loss: 0.4171, loss: 0.3990 ||:  66%|######6   | 43/65 [08:16<04:15, 11.62s/it]
2022-04-10 15:53:12,946 - INFO - tqdm - accuracy: 0.8216, batch_loss: 0.3758, loss: 0.3985 ||:  68%|######7   | 44/65 [08:28<04:03, 11.61s/it]
2022-04-10 15:53:24,382 - INFO - tqdm - accuracy: 0.8228, batch_loss: 0.2483, loss: 0.3952 ||:  69%|######9   | 45/65 [08:39<03:51, 11.56s/it]
2022-04-10 15:53:35,912 - INFO - tqdm - accuracy: 0.8212, batch_loss: 0.5114, loss: 0.3977 ||:  71%|#######   | 46/65 [08:51<03:39, 11.55s/it]
2022-04-10 15:53:47,470 - INFO - tqdm - accuracy: 0.8204, batch_loss: 0.4281, loss: 0.3983 ||:  72%|#######2  | 47/65 [09:03<03:27, 11.55s/it]
2022-04-10 15:53:59,031 - INFO - tqdm - accuracy: 0.8189, batch_loss: 0.4599, loss: 0.3996 ||:  74%|#######3  | 48/65 [09:14<03:16, 11.55s/it]
2022-04-10 15:54:10,572 - INFO - tqdm - accuracy: 0.8200, batch_loss: 0.3117, loss: 0.3978 ||:  75%|#######5  | 49/65 [09:26<03:04, 11.55s/it]
2022-04-10 15:54:22,124 - INFO - tqdm - accuracy: 0.8199, batch_loss: 0.4508, loss: 0.3989 ||:  77%|#######6  | 50/65 [09:37<02:53, 11.55s/it]
2022-04-10 15:54:33,667 - INFO - tqdm - accuracy: 0.8216, batch_loss: 0.2485, loss: 0.3959 ||:  78%|#######8  | 51/65 [09:49<02:41, 11.55s/it]
2022-04-10 15:54:45,214 - INFO - tqdm - accuracy: 0.8220, batch_loss: 0.3065, loss: 0.3942 ||:  80%|########  | 52/65 [10:00<02:30, 11.55s/it]
2022-04-10 15:54:56,752 - INFO - tqdm - accuracy: 0.8224, batch_loss: 0.3754, loss: 0.3939 ||:  82%|########1 | 53/65 [10:12<02:18, 11.55s/it]
2022-04-10 15:55:08,283 - INFO - tqdm - accuracy: 0.8222, batch_loss: 0.3852, loss: 0.3937 ||:  83%|########3 | 54/65 [10:23<02:06, 11.54s/it]
2022-04-10 15:55:19,801 - INFO - tqdm - accuracy: 0.8226, batch_loss: 0.3557, loss: 0.3930 ||:  85%|########4 | 55/65 [10:35<01:55, 11.53s/it]
2022-04-10 15:55:31,341 - INFO - tqdm - accuracy: 0.8236, batch_loss: 0.3154, loss: 0.3916 ||:  86%|########6 | 56/65 [10:46<01:43, 11.54s/it]
2022-04-10 15:55:42,882 - INFO - tqdm - accuracy: 0.8228, batch_loss: 0.4899, loss: 0.3933 ||:  88%|########7 | 57/65 [10:58<01:32, 11.54s/it]
2022-04-10 15:55:54,416 - INFO - tqdm - accuracy: 0.8237, batch_loss: 0.3833, loss: 0.3932 ||:  89%|########9 | 58/65 [11:10<01:20, 11.54s/it]
2022-04-10 15:56:05,963 - INFO - tqdm - accuracy: 0.8251, batch_loss: 0.3118, loss: 0.3918 ||:  91%|######### | 59/65 [11:21<01:09, 11.54s/it]
2022-04-10 15:56:17,497 - INFO - tqdm - accuracy: 0.8244, batch_loss: 0.4223, loss: 0.3923 ||:  92%|#########2| 60/65 [11:33<00:57, 11.54s/it]
2022-04-10 15:56:29,038 - INFO - tqdm - accuracy: 0.8237, batch_loss: 0.4036, loss: 0.3925 ||:  94%|#########3| 61/65 [11:44<00:46, 11.54s/it]
2022-04-10 15:56:40,338 - INFO - tqdm - accuracy: 0.8240, batch_loss: 0.3322, loss: 0.3915 ||:  95%|#########5| 62/65 [11:55<00:34, 11.47s/it]
2022-04-10 15:56:51,880 - INFO - tqdm - accuracy: 0.8248, batch_loss: 0.2583, loss: 0.3894 ||:  97%|#########6| 63/65 [12:07<00:22, 11.49s/it]
2022-04-10 15:57:03,425 - INFO - tqdm - accuracy: 0.8251, batch_loss: 0.3619, loss: 0.3890 ||:  98%|#########8| 64/65 [12:19<00:11, 11.51s/it]
2022-04-10 15:57:08,555 - INFO - tqdm - accuracy: 0.8253, batch_loss: 0.3973, loss: 0.3891 ||: 100%|##########| 65/65 [12:24<00:00,  9.59s/it]
2022-04-10 15:57:08,555 - INFO - tqdm - accuracy: 0.8253, batch_loss: 0.3973, loss: 0.3891 ||: 100%|##########| 65/65 [12:24<00:00, 11.45s/it]
2022-04-10 15:57:11,460 - INFO - allennlp.training.trainer - Validating
2022-04-10 15:57:11,461 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-10 15:57:21,593 - INFO - tqdm - accuracy: 0.6500, batch_loss: 0.0062, loss: 1.0657 ||:  35%|###4      | 40/115 [00:10<00:19,  3.92it/s]
2022-04-10 15:57:31,831 - INFO - tqdm - accuracy: 0.6375, batch_loss: 0.0115, loss: 1.0480 ||:  70%|######9   | 80/115 [00:20<00:09,  3.88it/s]
2022-04-10 15:57:40,718 - INFO - tqdm - accuracy: 0.6419, batch_loss: 0.0499, loss: 1.0312 ||: 100%|##########| 115/115 [00:29<00:00,  3.85it/s]
2022-04-10 15:57:40,719 - INFO - tqdm - accuracy: 0.6419, batch_loss: 0.0499, loss: 1.0312 ||: 100%|##########| 115/115 [00:29<00:00,  3.93it/s]
2022-04-10 15:57:40,719 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-10 15:57:40,719 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.825  |     0.642
2022-04-10 15:57:40,720 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-10 15:57:40,721 - INFO - allennlp.training.tensorboard_writer - loss               |     0.389  |     1.031
2022-04-10 15:57:40,722 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5804.914  |       N/A
2022-04-10 15:57:46,769 - INFO - allennlp.training.trainer - Epoch duration: 0:13:02.378857
2022-04-10 15:57:46,769 - INFO - allennlp.training.trainer - Estimated training time remaining: 2:11:22
2022-04-10 15:57:46,769 - INFO - allennlp.training.trainer - Epoch 5/14
2022-04-10 15:57:46,769 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-10 15:57:46,770 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-10 15:57:46,771 - INFO - allennlp.training.trainer - Training
2022-04-10 15:57:46,772 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-10 15:57:58,007 - INFO - tqdm - accuracy: 0.9375, batch_loss: 0.1858, loss: 0.1858 ||:   2%|1         | 1/65 [00:11<11:59, 11.24s/it]
2022-04-10 15:58:09,578 - INFO - tqdm - accuracy: 0.9062, batch_loss: 0.3174, loss: 0.2516 ||:   3%|3         | 2/65 [00:22<12:00, 11.43s/it]
2022-04-10 15:58:21,389 - INFO - tqdm - accuracy: 0.8854, batch_loss: 0.3560, loss: 0.2864 ||:   5%|4         | 3/65 [00:34<11:59, 11.61s/it]
2022-04-10 15:58:33,020 - INFO - tqdm - accuracy: 0.8672, batch_loss: 0.3363, loss: 0.2989 ||:   6%|6         | 4/65 [00:46<11:48, 11.62s/it]
2022-04-10 15:58:44,485 - INFO - tqdm - accuracy: 0.8750, batch_loss: 0.2258, loss: 0.2843 ||:   8%|7         | 5/65 [00:57<11:33, 11.56s/it]
2022-04-10 15:58:55,983 - INFO - tqdm - accuracy: 0.8750, batch_loss: 0.3183, loss: 0.2899 ||:   9%|9         | 6/65 [01:09<11:20, 11.54s/it]
2022-04-10 15:59:07,359 - INFO - tqdm - accuracy: 0.8705, batch_loss: 0.2541, loss: 0.2848 ||:  11%|#         | 7/65 [01:20<11:06, 11.49s/it]
2022-04-10 15:59:18,876 - INFO - tqdm - accuracy: 0.8789, batch_loss: 0.2044, loss: 0.2748 ||:  12%|#2        | 8/65 [01:32<10:55, 11.50s/it]
2022-04-10 15:59:30,399 - INFO - tqdm - accuracy: 0.8889, batch_loss: 0.1986, loss: 0.2663 ||:  14%|#3        | 9/65 [01:43<10:44, 11.50s/it]
2022-04-10 15:59:41,937 - INFO - tqdm - accuracy: 0.8938, batch_loss: 0.2935, loss: 0.2690 ||:  15%|#5        | 10/65 [01:55<10:33, 11.51s/it]
2022-04-10 15:59:53,494 - INFO - tqdm - accuracy: 0.8920, batch_loss: 0.2841, loss: 0.2704 ||:  17%|#6        | 11/65 [02:06<10:22, 11.53s/it]
2022-04-10 16:00:05,054 - INFO - tqdm - accuracy: 0.8958, batch_loss: 0.1963, loss: 0.2642 ||:  18%|#8        | 12/65 [02:18<10:11, 11.54s/it]
2022-04-10 16:00:16,614 - INFO - tqdm - accuracy: 0.8942, batch_loss: 0.2676, loss: 0.2645 ||:  20%|##        | 13/65 [02:29<10:00, 11.54s/it]
2022-04-10 16:00:28,200 - INFO - tqdm - accuracy: 0.8906, batch_loss: 0.4381, loss: 0.2769 ||:  22%|##1       | 14/65 [02:41<09:49, 11.56s/it]
2022-04-10 16:00:39,779 - INFO - tqdm - accuracy: 0.8958, batch_loss: 0.1483, loss: 0.2683 ||:  23%|##3       | 15/65 [02:53<09:38, 11.56s/it]
2022-04-10 16:00:51,011 - INFO - tqdm - accuracy: 0.8904, batch_loss: 0.3281, loss: 0.2720 ||:  25%|##4       | 16/65 [03:04<09:21, 11.46s/it]
2022-04-10 16:01:02,573 - INFO - tqdm - accuracy: 0.8895, batch_loss: 0.3389, loss: 0.2760 ||:  26%|##6       | 17/65 [03:15<09:11, 11.49s/it]
2022-04-10 16:01:14,174 - INFO - tqdm - accuracy: 0.8887, batch_loss: 0.3229, loss: 0.2786 ||:  28%|##7       | 18/65 [03:27<09:01, 11.53s/it]
2022-04-10 16:01:25,756 - INFO - tqdm - accuracy: 0.8913, batch_loss: 0.1771, loss: 0.2732 ||:  29%|##9       | 19/65 [03:38<08:50, 11.54s/it]
2022-04-10 16:01:37,322 - INFO - tqdm - accuracy: 0.8920, batch_loss: 0.2005, loss: 0.2696 ||:  31%|###       | 20/65 [03:50<08:39, 11.55s/it]
2022-04-10 16:01:48,871 - INFO - tqdm - accuracy: 0.8927, batch_loss: 0.2711, loss: 0.2697 ||:  32%|###2      | 21/65 [04:02<08:28, 11.55s/it]
2022-04-10 16:02:00,409 - INFO - tqdm - accuracy: 0.8905, batch_loss: 0.3628, loss: 0.2739 ||:  34%|###3      | 22/65 [04:13<08:16, 11.55s/it]
2022-04-10 16:02:11,969 - INFO - tqdm - accuracy: 0.8871, batch_loss: 0.3146, loss: 0.2757 ||:  35%|###5      | 23/65 [04:25<08:05, 11.55s/it]
2022-04-10 16:02:23,490 - INFO - tqdm - accuracy: 0.8879, batch_loss: 0.2616, loss: 0.2751 ||:  37%|###6      | 24/65 [04:36<07:53, 11.54s/it]
2022-04-10 16:02:35,032 - INFO - tqdm - accuracy: 0.8861, batch_loss: 0.3254, loss: 0.2771 ||:  38%|###8      | 25/65 [04:48<07:41, 11.54s/it]
2022-04-10 16:02:46,558 - INFO - tqdm - accuracy: 0.8881, batch_loss: 0.2024, loss: 0.2742 ||:  40%|####      | 26/65 [04:59<07:29, 11.54s/it]
2022-04-10 16:02:58,072 - INFO - tqdm - accuracy: 0.8841, batch_loss: 0.3671, loss: 0.2777 ||:  42%|####1     | 27/65 [05:11<07:18, 11.53s/it]
2022-04-10 16:03:09,588 - INFO - tqdm - accuracy: 0.8849, batch_loss: 0.1860, loss: 0.2744 ||:  43%|####3     | 28/65 [05:22<07:06, 11.53s/it]
2022-04-10 16:03:21,117 - INFO - tqdm - accuracy: 0.8889, batch_loss: 0.1144, loss: 0.2689 ||:  45%|####4     | 29/65 [05:34<06:54, 11.53s/it]
2022-04-10 16:03:32,633 - INFO - tqdm - accuracy: 0.8895, batch_loss: 0.2894, loss: 0.2696 ||:  46%|####6     | 30/65 [05:45<06:43, 11.52s/it]
2022-04-10 16:03:44,134 - INFO - tqdm - accuracy: 0.8910, batch_loss: 0.1908, loss: 0.2670 ||:  48%|####7     | 31/65 [05:57<06:31, 11.52s/it]
2022-04-10 16:03:55,680 - INFO - tqdm - accuracy: 0.8915, batch_loss: 0.2658, loss: 0.2670 ||:  49%|####9     | 32/65 [06:08<06:20, 11.53s/it]
2022-04-10 16:04:07,248 - INFO - tqdm - accuracy: 0.8900, batch_loss: 0.2612, loss: 0.2668 ||:  51%|#####     | 33/65 [06:20<06:09, 11.54s/it]
2022-04-10 16:04:18,822 - INFO - tqdm - accuracy: 0.8896, batch_loss: 0.2831, loss: 0.2673 ||:  52%|#####2    | 34/65 [06:32<05:58, 11.55s/it]
2022-04-10 16:04:30,391 - INFO - tqdm - accuracy: 0.8901, batch_loss: 0.2340, loss: 0.2663 ||:  54%|#####3    | 35/65 [06:43<05:46, 11.55s/it]
2022-04-10 16:04:42,003 - INFO - tqdm - accuracy: 0.8914, batch_loss: 0.1884, loss: 0.2642 ||:  55%|#####5    | 36/65 [06:55<05:35, 11.57s/it]
2022-04-10 16:04:53,577 - INFO - tqdm - accuracy: 0.8901, batch_loss: 0.3036, loss: 0.2652 ||:  57%|#####6    | 37/65 [07:06<05:24, 11.57s/it]
2022-04-10 16:05:05,158 - INFO - tqdm - accuracy: 0.8897, batch_loss: 0.2423, loss: 0.2646 ||:  58%|#####8    | 38/65 [07:18<05:12, 11.58s/it]
2022-04-10 16:05:16,716 - INFO - tqdm - accuracy: 0.8909, batch_loss: 0.2202, loss: 0.2635 ||:  60%|######    | 39/65 [07:29<05:00, 11.57s/it]
2022-04-10 16:05:28,295 - INFO - tqdm - accuracy: 0.8898, batch_loss: 0.4289, loss: 0.2676 ||:  62%|######1   | 40/65 [07:41<04:49, 11.57s/it]
2022-04-10 16:05:39,853 - INFO - tqdm - accuracy: 0.8917, batch_loss: 0.1699, loss: 0.2652 ||:  63%|######3   | 41/65 [07:53<04:37, 11.57s/it]
2022-04-10 16:05:51,396 - INFO - tqdm - accuracy: 0.8905, batch_loss: 0.4573, loss: 0.2698 ||:  65%|######4   | 42/65 [08:04<04:25, 11.56s/it]
2022-04-10 16:06:02,943 - INFO - tqdm - accuracy: 0.8902, batch_loss: 0.3161, loss: 0.2709 ||:  66%|######6   | 43/65 [08:16<04:14, 11.56s/it]
2022-04-10 16:06:14,470 - INFO - tqdm - accuracy: 0.8863, batch_loss: 0.6151, loss: 0.2787 ||:  68%|######7   | 44/65 [08:27<04:02, 11.55s/it]
2022-04-10 16:06:25,743 - INFO - tqdm - accuracy: 0.8874, batch_loss: 0.2537, loss: 0.2782 ||:  69%|######9   | 45/65 [08:38<03:49, 11.47s/it]
2022-04-10 16:06:37,253 - INFO - tqdm - accuracy: 0.8878, batch_loss: 0.2407, loss: 0.2773 ||:  71%|#######   | 46/65 [08:50<03:38, 11.48s/it]
2022-04-10 16:06:48,798 - INFO - tqdm - accuracy: 0.8876, batch_loss: 0.3113, loss: 0.2781 ||:  72%|#######2  | 47/65 [09:02<03:26, 11.50s/it]
2022-04-10 16:07:00,327 - INFO - tqdm - accuracy: 0.8847, batch_loss: 0.4732, loss: 0.2821 ||:  74%|#######3  | 48/65 [09:13<03:15, 11.51s/it]
2022-04-10 16:07:11,840 - INFO - tqdm - accuracy: 0.8845, batch_loss: 0.2098, loss: 0.2807 ||:  75%|#######5  | 49/65 [09:25<03:04, 11.51s/it]
2022-04-10 16:07:23,379 - INFO - tqdm - accuracy: 0.8843, batch_loss: 0.2892, loss: 0.2808 ||:  77%|#######6  | 50/65 [09:36<02:52, 11.52s/it]
2022-04-10 16:07:34,904 - INFO - tqdm - accuracy: 0.8860, batch_loss: 0.1470, loss: 0.2782 ||:  78%|#######8  | 51/65 [09:48<02:41, 11.52s/it]
2022-04-10 16:07:46,468 - INFO - tqdm - accuracy: 0.8851, batch_loss: 0.3242, loss: 0.2791 ||:  80%|########  | 52/65 [09:59<02:29, 11.53s/it]
2022-04-10 16:07:58,050 - INFO - tqdm - accuracy: 0.8861, batch_loss: 0.2046, loss: 0.2777 ||:  82%|########1 | 53/65 [10:11<02:18, 11.55s/it]
2022-04-10 16:08:09,611 - INFO - tqdm - accuracy: 0.8865, batch_loss: 0.2632, loss: 0.2774 ||:  83%|########3 | 54/65 [10:22<02:07, 11.55s/it]
2022-04-10 16:08:21,205 - INFO - tqdm - accuracy: 0.8852, batch_loss: 0.3707, loss: 0.2791 ||:  85%|########4 | 55/65 [10:34<01:55, 11.56s/it]
2022-04-10 16:08:32,781 - INFO - tqdm - accuracy: 0.8844, batch_loss: 0.2782, loss: 0.2791 ||:  86%|########6 | 56/65 [10:46<01:44, 11.57s/it]
2022-04-10 16:08:44,374 - INFO - tqdm - accuracy: 0.8848, batch_loss: 0.2455, loss: 0.2785 ||:  88%|########7 | 57/65 [10:57<01:32, 11.58s/it]
2022-04-10 16:08:55,983 - INFO - tqdm - accuracy: 0.8836, batch_loss: 0.3560, loss: 0.2798 ||:  89%|########9 | 58/65 [11:09<01:21, 11.59s/it]
2022-04-10 16:09:07,453 - INFO - tqdm - accuracy: 0.8834, batch_loss: 0.3074, loss: 0.2803 ||:  91%|######### | 59/65 [11:20<01:09, 11.55s/it]
2022-04-10 16:09:19,047 - INFO - tqdm - accuracy: 0.8833, batch_loss: 0.3860, loss: 0.2821 ||:  92%|#########2| 60/65 [11:32<00:57, 11.56s/it]
2022-04-10 16:09:30,583 - INFO - tqdm - accuracy: 0.8831, batch_loss: 0.3204, loss: 0.2827 ||:  94%|#########3| 61/65 [11:43<00:46, 11.56s/it]
2022-04-10 16:09:42,132 - INFO - tqdm - accuracy: 0.8830, batch_loss: 0.2594, loss: 0.2823 ||:  95%|#########5| 62/65 [11:55<00:34, 11.55s/it]
2022-04-10 16:09:53,642 - INFO - tqdm - accuracy: 0.8834, batch_loss: 0.3040, loss: 0.2827 ||:  97%|#########6| 63/65 [12:06<00:23, 11.54s/it]
2022-04-10 16:10:05,135 - INFO - tqdm - accuracy: 0.8837, batch_loss: 0.2398, loss: 0.2820 ||:  98%|#########8| 64/65 [12:18<00:11, 11.53s/it]
2022-04-10 16:10:10,258 - INFO - tqdm - accuracy: 0.8826, batch_loss: 0.4233, loss: 0.2842 ||: 100%|##########| 65/65 [12:23<00:00,  9.61s/it]
2022-04-10 16:10:10,258 - INFO - tqdm - accuracy: 0.8826, batch_loss: 0.4233, loss: 0.2842 ||: 100%|##########| 65/65 [12:23<00:00, 11.44s/it]
2022-04-10 16:10:13,181 - INFO - allennlp.training.trainer - Validating
2022-04-10 16:10:13,183 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-10 16:10:23,314 - INFO - tqdm - accuracy: 0.6125, batch_loss: 4.5733, loss: 1.3972 ||:  35%|###4      | 40/115 [00:10<00:19,  3.93it/s]
2022-04-10 16:10:33,356 - INFO - tqdm - accuracy: 0.6415, batch_loss: 0.1475, loss: 1.1907 ||:  70%|######9   | 80/115 [00:20<00:08,  4.06it/s]
2022-04-10 16:10:42,486 - INFO - tqdm - accuracy: 0.6332, batch_loss: 0.1618, loss: 1.1945 ||: 100%|##########| 115/115 [00:29<00:00,  3.84it/s]
2022-04-10 16:10:42,486 - INFO - tqdm - accuracy: 0.6332, batch_loss: 0.1618, loss: 1.1945 ||: 100%|##########| 115/115 [00:29<00:00,  3.92it/s]
2022-04-10 16:10:42,486 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-10 16:10:42,487 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.883  |     0.633
2022-04-10 16:10:42,487 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-10 16:10:42,487 - INFO - allennlp.training.tensorboard_writer - loss               |     0.284  |     1.195
2022-04-10 16:10:42,487 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5804.914  |       N/A
2022-04-10 16:10:48,590 - INFO - allennlp.training.trainer - Epoch duration: 0:13:01.821079
2022-04-10 16:10:48,591 - INFO - allennlp.training.trainer - Estimated training time remaining: 1:58:04
2022-04-10 16:10:48,591 - INFO - allennlp.training.trainer - Epoch 6/14
2022-04-10 16:10:48,591 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-10 16:10:48,591 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-10 16:10:48,593 - INFO - allennlp.training.trainer - Training
2022-04-10 16:10:48,593 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-10 16:11:00,011 - INFO - tqdm - accuracy: 0.8750, batch_loss: 0.2666, loss: 0.2666 ||:   2%|1         | 1/65 [00:11<12:10, 11.42s/it]
2022-04-10 16:11:11,573 - INFO - tqdm - accuracy: 0.9062, batch_loss: 0.2070, loss: 0.2368 ||:   3%|3         | 2/65 [00:22<12:04, 11.50s/it]
2022-04-10 16:11:23,367 - INFO - tqdm - accuracy: 0.9375, batch_loss: 0.1536, loss: 0.2091 ||:   5%|4         | 3/65 [00:34<12:01, 11.64s/it]
2022-04-10 16:11:34,967 - INFO - tqdm - accuracy: 0.9297, batch_loss: 0.2323, loss: 0.2149 ||:   6%|6         | 4/65 [00:46<11:48, 11.62s/it]
2022-04-10 16:11:46,471 - INFO - tqdm - accuracy: 0.9187, batch_loss: 0.2437, loss: 0.2207 ||:   8%|7         | 5/65 [00:57<11:34, 11.58s/it]
2022-04-10 16:11:58,005 - INFO - tqdm - accuracy: 0.9323, batch_loss: 0.1232, loss: 0.2044 ||:   9%|9         | 6/65 [01:09<11:22, 11.56s/it]
2022-04-10 16:12:09,491 - INFO - tqdm - accuracy: 0.9196, batch_loss: 0.3128, loss: 0.2199 ||:  11%|#         | 7/65 [01:20<11:09, 11.54s/it]
2022-04-10 16:12:21,012 - INFO - tqdm - accuracy: 0.9258, batch_loss: 0.1849, loss: 0.2155 ||:  12%|#2        | 8/65 [01:32<10:57, 11.53s/it]
2022-04-10 16:12:32,576 - INFO - tqdm - accuracy: 0.9236, batch_loss: 0.2124, loss: 0.2152 ||:  14%|#3        | 9/65 [01:43<10:46, 11.54s/it]
2022-04-10 16:12:44,652 - INFO - tqdm - accuracy: 0.9219, batch_loss: 0.2807, loss: 0.2217 ||:  15%|#5        | 10/65 [01:56<10:43, 11.71s/it]
2022-04-10 16:12:56,200 - INFO - tqdm - accuracy: 0.9119, batch_loss: 0.3975, loss: 0.2377 ||:  17%|#6        | 11/65 [02:07<10:29, 11.66s/it]
2022-04-10 16:13:07,786 - INFO - tqdm - accuracy: 0.9141, batch_loss: 0.1461, loss: 0.2301 ||:  18%|#8        | 12/65 [02:19<10:16, 11.64s/it]
2022-04-10 16:13:19,382 - INFO - tqdm - accuracy: 0.9135, batch_loss: 0.2239, loss: 0.2296 ||:  20%|##        | 13/65 [02:30<10:04, 11.62s/it]
2022-04-10 16:13:30,973 - INFO - tqdm - accuracy: 0.9129, batch_loss: 0.1863, loss: 0.2265 ||:  22%|##1       | 14/65 [02:42<09:52, 11.61s/it]
2022-04-10 16:13:42,542 - INFO - tqdm - accuracy: 0.9146, batch_loss: 0.1800, loss: 0.2234 ||:  23%|##3       | 15/65 [02:53<09:40, 11.60s/it]
2022-04-10 16:13:54,119 - INFO - tqdm - accuracy: 0.9180, batch_loss: 0.1691, loss: 0.2200 ||:  25%|##4       | 16/65 [03:05<09:28, 11.59s/it]
2022-04-10 16:14:05,686 - INFO - tqdm - accuracy: 0.9210, batch_loss: 0.0921, loss: 0.2125 ||:  26%|##6       | 17/65 [03:17<09:16, 11.59s/it]
2022-04-10 16:14:17,246 - INFO - tqdm - accuracy: 0.9184, batch_loss: 0.2000, loss: 0.2118 ||:  28%|##7       | 18/65 [03:28<09:04, 11.58s/it]
2022-04-10 16:14:28,806 - INFO - tqdm - accuracy: 0.9178, batch_loss: 0.2392, loss: 0.2132 ||:  29%|##9       | 19/65 [03:40<08:52, 11.57s/it]
2022-04-10 16:14:40,344 - INFO - tqdm - accuracy: 0.9203, batch_loss: 0.1267, loss: 0.2089 ||:  31%|###       | 20/65 [03:51<08:40, 11.56s/it]
2022-04-10 16:14:51,895 - INFO - tqdm - accuracy: 0.9182, batch_loss: 0.2396, loss: 0.2104 ||:  32%|###2      | 21/65 [04:03<08:28, 11.56s/it]
2022-04-10 16:15:03,437 - INFO - tqdm - accuracy: 0.9190, batch_loss: 0.1874, loss: 0.2093 ||:  34%|###3      | 22/65 [04:14<08:16, 11.55s/it]
2022-04-10 16:15:14,982 - INFO - tqdm - accuracy: 0.9212, batch_loss: 0.2011, loss: 0.2090 ||:  35%|###5      | 23/65 [04:26<08:05, 11.55s/it]
2022-04-10 16:15:26,537 - INFO - tqdm - accuracy: 0.9193, batch_loss: 0.2842, loss: 0.2121 ||:  37%|###6      | 24/65 [04:37<07:53, 11.55s/it]
2022-04-10 16:15:38,100 - INFO - tqdm - accuracy: 0.9163, batch_loss: 0.4107, loss: 0.2201 ||:  38%|###8      | 25/65 [04:49<07:42, 11.56s/it]
2022-04-10 16:15:49,652 - INFO - tqdm - accuracy: 0.9171, batch_loss: 0.2170, loss: 0.2199 ||:  40%|####      | 26/65 [05:01<07:30, 11.55s/it]
2022-04-10 16:16:01,192 - INFO - tqdm - accuracy: 0.9201, batch_loss: 0.0410, loss: 0.2133 ||:  42%|####1     | 27/65 [05:12<07:18, 11.55s/it]
2022-04-10 16:16:12,756 - INFO - tqdm - accuracy: 0.9196, batch_loss: 0.2159, loss: 0.2134 ||:  43%|####3     | 28/65 [05:24<07:07, 11.55s/it]
2022-04-10 16:16:24,317 - INFO - tqdm - accuracy: 0.9224, batch_loss: 0.0594, loss: 0.2081 ||:  45%|####4     | 29/65 [05:35<06:56, 11.56s/it]
2022-04-10 16:16:35,726 - INFO - tqdm - accuracy: 0.9219, batch_loss: 0.1961, loss: 0.2077 ||:  46%|####6     | 30/65 [05:47<06:42, 11.51s/it]
2022-04-10 16:16:47,272 - INFO - tqdm - accuracy: 0.9234, batch_loss: 0.1150, loss: 0.2047 ||:  48%|####7     | 31/65 [05:58<06:31, 11.52s/it]
2022-04-10 16:16:58,817 - INFO - tqdm - accuracy: 0.9238, batch_loss: 0.1529, loss: 0.2031 ||:  49%|####9     | 32/65 [06:10<06:20, 11.53s/it]
2022-04-10 16:17:10,350 - INFO - tqdm - accuracy: 0.9233, batch_loss: 0.1567, loss: 0.2017 ||:  51%|#####     | 33/65 [06:21<06:08, 11.53s/it]
2022-04-10 16:17:21,875 - INFO - tqdm - accuracy: 0.9246, batch_loss: 0.1458, loss: 0.2000 ||:  52%|#####2    | 34/65 [06:33<05:57, 11.53s/it]
2022-04-10 16:17:33,408 - INFO - tqdm - accuracy: 0.9232, batch_loss: 0.1995, loss: 0.2000 ||:  54%|#####3    | 35/65 [06:44<05:45, 11.53s/it]
2022-04-10 16:17:44,947 - INFO - tqdm - accuracy: 0.9210, batch_loss: 0.2230, loss: 0.2007 ||:  55%|#####5    | 36/65 [06:56<05:34, 11.53s/it]
2022-04-10 16:17:56,477 - INFO - tqdm - accuracy: 0.9231, batch_loss: 0.1173, loss: 0.1984 ||:  57%|#####6    | 37/65 [07:07<05:22, 11.53s/it]
2022-04-10 16:18:08,009 - INFO - tqdm - accuracy: 0.9211, batch_loss: 0.3239, loss: 0.2017 ||:  58%|#####8    | 38/65 [07:19<05:11, 11.53s/it]
2022-04-10 16:18:19,531 - INFO - tqdm - accuracy: 0.9207, batch_loss: 0.2071, loss: 0.2018 ||:  60%|######    | 39/65 [07:30<04:59, 11.53s/it]
2022-04-10 16:18:31,059 - INFO - tqdm - accuracy: 0.9203, batch_loss: 0.2584, loss: 0.2033 ||:  62%|######1   | 40/65 [07:42<04:48, 11.53s/it]
2022-04-10 16:18:42,245 - INFO - tqdm - accuracy: 0.9207, batch_loss: 0.1078, loss: 0.2009 ||:  63%|######3   | 41/65 [07:53<04:34, 11.43s/it]
2022-04-10 16:18:53,789 - INFO - tqdm - accuracy: 0.9203, batch_loss: 0.3355, loss: 0.2041 ||:  65%|######4   | 42/65 [08:05<04:23, 11.46s/it]
2022-04-10 16:19:05,067 - INFO - tqdm - accuracy: 0.9215, batch_loss: 0.1330, loss: 0.2025 ||:  66%|######6   | 43/65 [08:16<04:10, 11.41s/it]
2022-04-10 16:19:16,585 - INFO - tqdm - accuracy: 0.9211, batch_loss: 0.2248, loss: 0.2030 ||:  68%|######7   | 44/65 [08:27<04:00, 11.44s/it]
2022-04-10 16:19:28,131 - INFO - tqdm - accuracy: 0.9201, batch_loss: 0.2773, loss: 0.2046 ||:  69%|######9   | 45/65 [08:39<03:49, 11.47s/it]
2022-04-10 16:19:39,674 - INFO - tqdm - accuracy: 0.9205, batch_loss: 0.2115, loss: 0.2048 ||:  71%|#######   | 46/65 [08:51<03:38, 11.49s/it]
2022-04-10 16:19:51,219 - INFO - tqdm - accuracy: 0.9208, batch_loss: 0.1399, loss: 0.2034 ||:  72%|#######2  | 47/65 [09:02<03:27, 11.51s/it]
2022-04-10 16:20:02,743 - INFO - tqdm - accuracy: 0.9212, batch_loss: 0.3287, loss: 0.2060 ||:  74%|#######3  | 48/65 [09:14<03:15, 11.51s/it]
2022-04-10 16:20:14,279 - INFO - tqdm - accuracy: 0.9221, batch_loss: 0.1219, loss: 0.2043 ||:  75%|#######5  | 49/65 [09:25<03:04, 11.52s/it]
2022-04-10 16:20:25,805 - INFO - tqdm - accuracy: 0.9231, batch_loss: 0.0588, loss: 0.2014 ||:  77%|#######6  | 50/65 [09:37<02:52, 11.52s/it]
2022-04-10 16:20:37,331 - INFO - tqdm - accuracy: 0.9227, batch_loss: 0.1246, loss: 0.1999 ||:  78%|#######8  | 51/65 [09:48<02:41, 11.52s/it]
2022-04-10 16:20:48,731 - INFO - tqdm - accuracy: 0.9230, batch_loss: 0.1514, loss: 0.1990 ||:  80%|########  | 52/65 [10:00<02:29, 11.49s/it]
2022-04-10 16:21:00,265 - INFO - tqdm - accuracy: 0.9233, batch_loss: 0.2018, loss: 0.1990 ||:  82%|########1 | 53/65 [10:11<02:18, 11.50s/it]
2022-04-10 16:21:11,788 - INFO - tqdm - accuracy: 0.9241, batch_loss: 0.1204, loss: 0.1975 ||:  83%|########3 | 54/65 [10:23<02:06, 11.51s/it]
2022-04-10 16:21:23,324 - INFO - tqdm - accuracy: 0.9255, batch_loss: 0.0633, loss: 0.1951 ||:  85%|########4 | 55/65 [10:34<01:55, 11.52s/it]
2022-04-10 16:21:34,741 - INFO - tqdm - accuracy: 0.9269, batch_loss: 0.0727, loss: 0.1929 ||:  86%|########6 | 56/65 [10:46<01:43, 11.49s/it]
2022-04-10 16:21:46,271 - INFO - tqdm - accuracy: 0.9248, batch_loss: 0.3341, loss: 0.1954 ||:  88%|########7 | 57/65 [10:57<01:31, 11.50s/it]
2022-04-10 16:21:57,783 - INFO - tqdm - accuracy: 0.9261, batch_loss: 0.0479, loss: 0.1929 ||:  89%|########9 | 58/65 [11:09<01:20, 11.50s/it]
2022-04-10 16:22:09,318 - INFO - tqdm - accuracy: 0.9269, batch_loss: 0.0961, loss: 0.1912 ||:  91%|######### | 59/65 [11:20<01:09, 11.51s/it]
2022-04-10 16:22:20,839 - INFO - tqdm - accuracy: 0.9270, batch_loss: 0.1200, loss: 0.1900 ||:  92%|#########2| 60/65 [11:32<00:57, 11.52s/it]
2022-04-10 16:22:32,372 - INFO - tqdm - accuracy: 0.9272, batch_loss: 0.1756, loss: 0.1898 ||:  94%|#########3| 61/65 [11:43<00:46, 11.52s/it]
2022-04-10 16:22:43,897 - INFO - tqdm - accuracy: 0.9264, batch_loss: 0.2624, loss: 0.1910 ||:  95%|#########5| 62/65 [11:55<00:34, 11.52s/it]
2022-04-10 16:22:55,425 - INFO - tqdm - accuracy: 0.9261, batch_loss: 0.1758, loss: 0.1907 ||:  97%|#########6| 63/65 [12:06<00:23, 11.52s/it]
2022-04-10 16:23:06,960 - INFO - tqdm - accuracy: 0.9243, batch_loss: 0.2422, loss: 0.1915 ||:  98%|#########8| 64/65 [12:18<00:11, 11.53s/it]
2022-04-10 16:23:12,083 - INFO - tqdm - accuracy: 0.9248, batch_loss: 0.0982, loss: 0.1901 ||: 100%|##########| 65/65 [12:23<00:00,  9.61s/it]
2022-04-10 16:23:12,083 - INFO - tqdm - accuracy: 0.9248, batch_loss: 0.0982, loss: 0.1901 ||: 100%|##########| 65/65 [12:23<00:00, 11.44s/it]
2022-04-10 16:23:14,976 - INFO - allennlp.training.trainer - Validating
2022-04-10 16:23:14,977 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-10 16:23:25,226 - INFO - tqdm - accuracy: 0.6173, batch_loss: 4.1361, loss: 1.4936 ||:  36%|###5      | 41/115 [00:10<00:18,  3.93it/s]
2022-04-10 16:23:35,376 - INFO - tqdm - accuracy: 0.6522, batch_loss: 0.0598, loss: 1.4818 ||:  70%|#######   | 81/115 [00:20<00:07,  4.31it/s]
2022-04-10 16:23:44,215 - INFO - tqdm - accuracy: 0.6594, batch_loss: 2.1144, loss: 1.3974 ||: 100%|##########| 115/115 [00:29<00:00,  3.82it/s]
2022-04-10 16:23:44,216 - INFO - tqdm - accuracy: 0.6594, batch_loss: 2.1144, loss: 1.3974 ||: 100%|##########| 115/115 [00:29<00:00,  3.93it/s]
2022-04-10 16:23:44,216 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-10 16:23:44,216 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.925  |     0.659
2022-04-10 16:23:44,217 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-10 16:23:44,218 - INFO - allennlp.training.tensorboard_writer - loss               |     0.190  |     1.397
2022-04-10 16:23:44,218 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5804.914  |       N/A
2022-04-10 16:23:50,048 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to './output_ir-ora-d_hyper4/best.th'.
2022-04-10 16:24:03,718 - INFO - allennlp.training.trainer - Epoch duration: 0:13:15.126753
2022-04-10 16:24:03,718 - INFO - allennlp.training.trainer - Estimated training time remaining: 1:45:06
2022-04-10 16:24:03,718 - INFO - allennlp.training.trainer - Epoch 7/14
2022-04-10 16:24:03,718 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-10 16:24:03,718 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-10 16:24:03,720 - INFO - allennlp.training.trainer - Training
2022-04-10 16:24:03,720 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-10 16:24:14,899 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.1225, loss: 0.1225 ||:   2%|1         | 1/65 [00:11<11:55, 11.18s/it]
2022-04-10 16:24:26,404 - INFO - tqdm - accuracy: 0.9531, batch_loss: 0.0978, loss: 0.1102 ||:   3%|3         | 2/65 [00:22<11:56, 11.37s/it]
2022-04-10 16:24:38,261 - INFO - tqdm - accuracy: 0.9479, batch_loss: 0.2199, loss: 0.1468 ||:   5%|4         | 3/65 [00:34<11:58, 11.59s/it]
2022-04-10 16:24:50,186 - INFO - tqdm - accuracy: 0.9297, batch_loss: 0.1695, loss: 0.1524 ||:   6%|6         | 4/65 [00:46<11:55, 11.72s/it]
2022-04-10 16:25:01,726 - INFO - tqdm - accuracy: 0.9125, batch_loss: 0.3378, loss: 0.1895 ||:   8%|7         | 5/65 [00:58<11:39, 11.66s/it]
2022-04-10 16:25:13,127 - INFO - tqdm - accuracy: 0.9115, batch_loss: 0.1424, loss: 0.1817 ||:   9%|9         | 6/65 [01:09<11:22, 11.57s/it]
2022-04-10 16:25:24,535 - INFO - tqdm - accuracy: 0.9241, batch_loss: 0.0373, loss: 0.1610 ||:  11%|#         | 7/65 [01:20<11:07, 11.52s/it]
2022-04-10 16:25:36,075 - INFO - tqdm - accuracy: 0.9258, batch_loss: 0.1491, loss: 0.1595 ||:  12%|#2        | 8/65 [01:32<10:56, 11.52s/it]
2022-04-10 16:25:47,690 - INFO - tqdm - accuracy: 0.9306, batch_loss: 0.0586, loss: 0.1483 ||:  14%|#3        | 9/65 [01:43<10:46, 11.55s/it]
2022-04-10 16:25:59,284 - INFO - tqdm - accuracy: 0.9344, batch_loss: 0.1517, loss: 0.1487 ||:  15%|#5        | 10/65 [01:55<10:36, 11.57s/it]
2022-04-10 16:26:10,818 - INFO - tqdm - accuracy: 0.9347, batch_loss: 0.1357, loss: 0.1475 ||:  17%|#6        | 11/65 [02:07<10:24, 11.56s/it]
2022-04-10 16:26:22,338 - INFO - tqdm - accuracy: 0.9271, batch_loss: 0.5666, loss: 0.1824 ||:  18%|#8        | 12/65 [02:18<10:11, 11.54s/it]
2022-04-10 16:26:33,831 - INFO - tqdm - accuracy: 0.9231, batch_loss: 0.3057, loss: 0.1919 ||:  20%|##        | 13/65 [02:30<09:59, 11.53s/it]
2022-04-10 16:26:45,314 - INFO - tqdm - accuracy: 0.9263, batch_loss: 0.1165, loss: 0.1865 ||:  22%|##1       | 14/65 [02:41<09:47, 11.52s/it]
2022-04-10 16:26:56,715 - INFO - tqdm - accuracy: 0.9271, batch_loss: 0.1296, loss: 0.1827 ||:  23%|##3       | 15/65 [02:52<09:34, 11.48s/it]
2022-04-10 16:27:08,298 - INFO - tqdm - accuracy: 0.9316, batch_loss: 0.0745, loss: 0.1759 ||:  25%|##4       | 16/65 [03:04<09:24, 11.51s/it]
2022-04-10 16:27:19,765 - INFO - tqdm - accuracy: 0.9320, batch_loss: 0.1458, loss: 0.1742 ||:  26%|##6       | 17/65 [03:16<09:11, 11.50s/it]
2022-04-10 16:27:31,346 - INFO - tqdm - accuracy: 0.9358, batch_loss: 0.0404, loss: 0.1667 ||:  28%|##7       | 18/65 [03:27<09:01, 11.52s/it]
2022-04-10 16:27:42,919 - INFO - tqdm - accuracy: 0.9342, batch_loss: 0.2046, loss: 0.1687 ||:  29%|##9       | 19/65 [03:39<08:50, 11.54s/it]
2022-04-10 16:27:54,362 - INFO - tqdm - accuracy: 0.9328, batch_loss: 0.2901, loss: 0.1748 ||:  31%|###       | 20/65 [03:50<08:37, 11.51s/it]
2022-04-10 16:28:05,935 - INFO - tqdm - accuracy: 0.9330, batch_loss: 0.1578, loss: 0.1740 ||:  32%|###2      | 21/65 [04:02<08:27, 11.53s/it]
2022-04-10 16:28:17,474 - INFO - tqdm - accuracy: 0.9318, batch_loss: 0.1607, loss: 0.1734 ||:  34%|###3      | 22/65 [04:13<08:15, 11.53s/it]
2022-04-10 16:28:28,995 - INFO - tqdm - accuracy: 0.9321, batch_loss: 0.1632, loss: 0.1729 ||:  35%|###5      | 23/65 [04:25<08:04, 11.53s/it]
2022-04-10 16:28:40,519 - INFO - tqdm - accuracy: 0.9349, batch_loss: 0.0601, loss: 0.1682 ||:  37%|###6      | 24/65 [04:36<07:52, 11.53s/it]
2022-04-10 16:28:52,050 - INFO - tqdm - accuracy: 0.9363, batch_loss: 0.1125, loss: 0.1660 ||:  38%|###8      | 25/65 [04:48<07:41, 11.53s/it]
2022-04-10 16:29:03,579 - INFO - tqdm - accuracy: 0.9375, batch_loss: 0.1323, loss: 0.1647 ||:  40%|####      | 26/65 [04:59<07:29, 11.53s/it]
2022-04-10 16:29:15,107 - INFO - tqdm - accuracy: 0.9398, batch_loss: 0.0455, loss: 0.1603 ||:  42%|####1     | 27/65 [05:11<07:18, 11.53s/it]
2022-04-10 16:29:26,663 - INFO - tqdm - accuracy: 0.9420, batch_loss: 0.0500, loss: 0.1564 ||:  43%|####3     | 28/65 [05:22<07:06, 11.54s/it]
2022-04-10 16:29:38,232 - INFO - tqdm - accuracy: 0.9418, batch_loss: 0.1631, loss: 0.1566 ||:  45%|####4     | 29/65 [05:34<06:55, 11.55s/it]
2022-04-10 16:29:49,833 - INFO - tqdm - accuracy: 0.9406, batch_loss: 0.1654, loss: 0.1569 ||:  46%|####6     | 30/65 [05:46<06:44, 11.56s/it]
2022-04-10 16:30:01,432 - INFO - tqdm - accuracy: 0.9405, batch_loss: 0.1274, loss: 0.1559 ||:  48%|####7     | 31/65 [05:57<06:33, 11.57s/it]
2022-04-10 16:30:13,041 - INFO - tqdm - accuracy: 0.9414, batch_loss: 0.1120, loss: 0.1546 ||:  49%|####9     | 32/65 [06:09<06:22, 11.58s/it]
2022-04-10 16:30:24,641 - INFO - tqdm - accuracy: 0.9432, batch_loss: 0.0622, loss: 0.1518 ||:  51%|#####     | 33/65 [06:20<06:10, 11.59s/it]
2022-04-10 16:30:36,248 - INFO - tqdm - accuracy: 0.9412, batch_loss: 0.3328, loss: 0.1571 ||:  52%|#####2    | 34/65 [06:32<05:59, 11.59s/it]
2022-04-10 16:30:47,863 - INFO - tqdm - accuracy: 0.9384, batch_loss: 0.2578, loss: 0.1600 ||:  54%|#####3    | 35/65 [06:44<05:48, 11.60s/it]
2022-04-10 16:30:59,443 - INFO - tqdm - accuracy: 0.9366, batch_loss: 0.1822, loss: 0.1606 ||:  55%|#####5    | 36/65 [06:55<05:36, 11.59s/it]
2022-04-10 16:31:11,034 - INFO - tqdm - accuracy: 0.9375, batch_loss: 0.0653, loss: 0.1580 ||:  57%|#####6    | 37/65 [07:07<05:24, 11.59s/it]
2022-04-10 16:31:22,610 - INFO - tqdm - accuracy: 0.9383, batch_loss: 0.0962, loss: 0.1564 ||:  58%|#####8    | 38/65 [07:18<05:12, 11.59s/it]
2022-04-10 16:31:34,179 - INFO - tqdm - accuracy: 0.9383, batch_loss: 0.1445, loss: 0.1561 ||:  60%|######    | 39/65 [07:30<05:01, 11.58s/it]
2022-04-10 16:31:45,747 - INFO - tqdm - accuracy: 0.9383, batch_loss: 0.1560, loss: 0.1561 ||:  62%|######1   | 40/65 [07:42<04:49, 11.58s/it]
2022-04-10 16:31:57,285 - INFO - tqdm - accuracy: 0.9375, batch_loss: 0.1860, loss: 0.1568 ||:  63%|######3   | 41/65 [07:53<04:37, 11.57s/it]
2022-04-10 16:32:08,836 - INFO - tqdm - accuracy: 0.9375, batch_loss: 0.2167, loss: 0.1582 ||:  65%|######4   | 42/65 [08:05<04:25, 11.56s/it]
2022-04-10 16:32:20,373 - INFO - tqdm - accuracy: 0.9375, batch_loss: 0.1292, loss: 0.1576 ||:  66%|######6   | 43/65 [08:16<04:14, 11.55s/it]
2022-04-10 16:32:31,916 - INFO - tqdm - accuracy: 0.9361, batch_loss: 0.1896, loss: 0.1583 ||:  68%|######7   | 44/65 [08:28<04:02, 11.55s/it]
2022-04-10 16:32:43,869 - INFO - tqdm - accuracy: 0.9368, batch_loss: 0.0484, loss: 0.1558 ||:  69%|######9   | 45/65 [08:40<03:53, 11.67s/it]
2022-04-10 16:32:55,370 - INFO - tqdm - accuracy: 0.9375, batch_loss: 0.1045, loss: 0.1547 ||:  71%|#######   | 46/65 [08:51<03:40, 11.62s/it]
2022-04-10 16:33:06,938 - INFO - tqdm - accuracy: 0.9382, batch_loss: 0.0747, loss: 0.1530 ||:  72%|#######2  | 47/65 [09:03<03:28, 11.60s/it]
2022-04-10 16:33:18,521 - INFO - tqdm - accuracy: 0.9388, batch_loss: 0.1609, loss: 0.1532 ||:  74%|#######3  | 48/65 [09:14<03:17, 11.60s/it]
2022-04-10 16:33:30,102 - INFO - tqdm - accuracy: 0.9401, batch_loss: 0.0366, loss: 0.1508 ||:  75%|#######5  | 49/65 [09:26<03:05, 11.59s/it]
2022-04-10 16:33:41,678 - INFO - tqdm - accuracy: 0.9406, batch_loss: 0.0929, loss: 0.1496 ||:  77%|#######6  | 50/65 [09:37<02:53, 11.59s/it]
2022-04-10 16:33:53,256 - INFO - tqdm - accuracy: 0.9412, batch_loss: 0.1536, loss: 0.1497 ||:  78%|#######8  | 51/65 [09:49<02:42, 11.58s/it]
2022-04-10 16:34:04,844 - INFO - tqdm - accuracy: 0.9393, batch_loss: 0.3227, loss: 0.1531 ||:  80%|########  | 52/65 [10:01<02:30, 11.59s/it]
2022-04-10 16:34:16,429 - INFO - tqdm - accuracy: 0.9399, batch_loss: 0.1681, loss: 0.1533 ||:  82%|########1 | 53/65 [10:12<02:19, 11.59s/it]
2022-04-10 16:34:28,015 - INFO - tqdm - accuracy: 0.9392, batch_loss: 0.1512, loss: 0.1533 ||:  83%|########3 | 54/65 [10:24<02:07, 11.59s/it]
2022-04-10 16:34:39,586 - INFO - tqdm - accuracy: 0.9392, batch_loss: 0.2401, loss: 0.1549 ||:  85%|########4 | 55/65 [10:35<01:55, 11.58s/it]
2022-04-10 16:34:51,159 - INFO - tqdm - accuracy: 0.9392, batch_loss: 0.1972, loss: 0.1556 ||:  86%|########6 | 56/65 [10:47<01:44, 11.58s/it]
2022-04-10 16:35:02,726 - INFO - tqdm - accuracy: 0.9402, batch_loss: 0.0513, loss: 0.1538 ||:  88%|########7 | 57/65 [10:59<01:32, 11.58s/it]
2022-04-10 16:35:14,304 - INFO - tqdm - accuracy: 0.9402, batch_loss: 0.2754, loss: 0.1559 ||:  89%|########9 | 58/65 [11:10<01:21, 11.58s/it]
2022-04-10 16:35:25,275 - INFO - tqdm - accuracy: 0.9406, batch_loss: 0.1594, loss: 0.1560 ||:  91%|######### | 59/65 [11:21<01:08, 11.39s/it]
2022-04-10 16:35:36,810 - INFO - tqdm - accuracy: 0.9416, batch_loss: 0.0543, loss: 0.1543 ||:  92%|#########2| 60/65 [11:33<00:57, 11.44s/it]
2022-04-10 16:35:48,369 - INFO - tqdm - accuracy: 0.9416, batch_loss: 0.1144, loss: 0.1536 ||:  94%|#########3| 61/65 [11:44<00:45, 11.47s/it]
2022-04-10 16:35:59,914 - INFO - tqdm - accuracy: 0.9415, batch_loss: 0.1828, loss: 0.1541 ||:  95%|#########5| 62/65 [11:56<00:34, 11.49s/it]
2022-04-10 16:36:11,469 - INFO - tqdm - accuracy: 0.9424, batch_loss: 0.0502, loss: 0.1524 ||:  97%|#########6| 63/65 [12:07<00:23, 11.51s/it]
2022-04-10 16:36:23,025 - INFO - tqdm - accuracy: 0.9424, batch_loss: 0.1303, loss: 0.1521 ||:  98%|#########8| 64/65 [12:19<00:11, 11.53s/it]
2022-04-10 16:36:28,151 - INFO - tqdm - accuracy: 0.9427, batch_loss: 0.0387, loss: 0.1503 ||: 100%|##########| 65/65 [12:24<00:00,  9.61s/it]
2022-04-10 16:36:28,152 - INFO - tqdm - accuracy: 0.9427, batch_loss: 0.0387, loss: 0.1503 ||: 100%|##########| 65/65 [12:24<00:00, 11.45s/it]
2022-04-10 16:36:31,073 - INFO - allennlp.training.trainer - Validating
2022-04-10 16:36:31,075 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-10 16:36:41,127 - INFO - tqdm - accuracy: 0.6750, batch_loss: 1.1121, loss: 1.1693 ||:  35%|###4      | 40/115 [00:10<00:18,  4.01it/s]
2022-04-10 16:36:51,241 - INFO - tqdm - accuracy: 0.6541, batch_loss: 2.8109, loss: 1.3614 ||:  70%|######9   | 80/115 [00:20<00:09,  3.87it/s]
2022-04-10 16:37:00,349 - INFO - tqdm - accuracy: 0.6376, batch_loss: 2.7452, loss: 1.4990 ||: 100%|##########| 115/115 [00:29<00:00,  3.84it/s]
2022-04-10 16:37:00,350 - INFO - tqdm - accuracy: 0.6376, batch_loss: 2.7452, loss: 1.4990 ||: 100%|##########| 115/115 [00:29<00:00,  3.93it/s]
2022-04-10 16:37:00,350 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-10 16:37:00,350 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.943  |     0.638
2022-04-10 16:37:00,351 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-10 16:37:00,351 - INFO - allennlp.training.tensorboard_writer - loss               |     0.150  |     1.499
2022-04-10 16:37:00,352 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5804.914  |       N/A
2022-04-10 16:37:06,670 - INFO - allennlp.training.trainer - Epoch duration: 0:13:02.951991
2022-04-10 16:37:06,670 - INFO - allennlp.training.trainer - Estimated training time remaining: 1:31:53
2022-04-10 16:37:06,670 - INFO - allennlp.training.trainer - Epoch 8/14
2022-04-10 16:37:06,670 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-10 16:37:06,671 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-10 16:37:06,672 - INFO - allennlp.training.trainer - Training
2022-04-10 16:37:06,673 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-10 16:37:18,056 - INFO - tqdm - accuracy: 0.9375, batch_loss: 0.1976, loss: 0.1976 ||:   2%|1         | 1/65 [00:11<12:08, 11.38s/it]
2022-04-10 16:37:29,640 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.0321, loss: 0.1149 ||:   3%|3         | 2/65 [00:22<12:04, 11.50s/it]
2022-04-10 16:37:41,472 - INFO - tqdm - accuracy: 0.9479, batch_loss: 0.1654, loss: 0.1317 ||:   5%|4         | 3/65 [00:34<12:02, 11.65s/it]
2022-04-10 16:37:53,093 - INFO - tqdm - accuracy: 0.9531, batch_loss: 0.0852, loss: 0.1201 ||:   6%|6         | 4/65 [00:46<11:50, 11.64s/it]
2022-04-10 16:38:04,548 - INFO - tqdm - accuracy: 0.9563, batch_loss: 0.0768, loss: 0.1114 ||:   8%|7         | 5/65 [00:57<11:34, 11.57s/it]
2022-04-10 16:38:16,056 - INFO - tqdm - accuracy: 0.9635, batch_loss: 0.0439, loss: 0.1002 ||:   9%|9         | 6/65 [01:09<11:21, 11.55s/it]
2022-04-10 16:38:27,580 - INFO - tqdm - accuracy: 0.9643, batch_loss: 0.0931, loss: 0.0992 ||:  11%|#         | 7/65 [01:20<11:09, 11.54s/it]
2022-04-10 16:38:39,090 - INFO - tqdm - accuracy: 0.9648, batch_loss: 0.0639, loss: 0.0948 ||:  12%|#2        | 8/65 [01:32<10:57, 11.53s/it]
2022-04-10 16:38:50,607 - INFO - tqdm - accuracy: 0.9618, batch_loss: 0.1486, loss: 0.1007 ||:  14%|#3        | 9/65 [01:43<10:45, 11.53s/it]
2022-04-10 16:39:02,132 - INFO - tqdm - accuracy: 0.9563, batch_loss: 0.1422, loss: 0.1049 ||:  15%|#5        | 10/65 [01:55<10:33, 11.53s/it]
2022-04-10 16:39:13,679 - INFO - tqdm - accuracy: 0.9545, batch_loss: 0.1246, loss: 0.1067 ||:  17%|#6        | 11/65 [02:07<10:22, 11.53s/it]
2022-04-10 16:39:25,217 - INFO - tqdm - accuracy: 0.9557, batch_loss: 0.1113, loss: 0.1071 ||:  18%|#8        | 12/65 [02:18<10:11, 11.53s/it]
2022-04-10 16:39:36,804 - INFO - tqdm - accuracy: 0.9567, batch_loss: 0.1051, loss: 0.1069 ||:  20%|##        | 13/65 [02:30<10:00, 11.55s/it]
2022-04-10 16:39:48,399 - INFO - tqdm - accuracy: 0.9576, batch_loss: 0.1005, loss: 0.1064 ||:  22%|##1       | 14/65 [02:41<09:49, 11.56s/it]
2022-04-10 16:39:59,974 - INFO - tqdm - accuracy: 0.9604, batch_loss: 0.0587, loss: 0.1033 ||:  23%|##3       | 15/65 [02:53<09:38, 11.57s/it]
2022-04-10 16:40:11,570 - INFO - tqdm - accuracy: 0.9629, batch_loss: 0.0563, loss: 0.1003 ||:  25%|##4       | 16/65 [03:04<09:27, 11.58s/it]
2022-04-10 16:40:23,133 - INFO - tqdm - accuracy: 0.9651, batch_loss: 0.0305, loss: 0.0962 ||:  26%|##6       | 17/65 [03:16<09:15, 11.57s/it]
2022-04-10 16:40:34,709 - INFO - tqdm - accuracy: 0.9583, batch_loss: 0.2318, loss: 0.1038 ||:  28%|##7       | 18/65 [03:28<09:03, 11.57s/it]
2022-04-10 16:40:46,300 - INFO - tqdm - accuracy: 0.9605, batch_loss: 0.0626, loss: 0.1016 ||:  29%|##9       | 19/65 [03:39<08:52, 11.58s/it]
2022-04-10 16:40:57,910 - INFO - tqdm - accuracy: 0.9578, batch_loss: 0.2180, loss: 0.1074 ||:  31%|###       | 20/65 [03:51<08:41, 11.59s/it]
2022-04-10 16:41:09,509 - INFO - tqdm - accuracy: 0.9583, batch_loss: 0.0863, loss: 0.1064 ||:  32%|###2      | 21/65 [04:02<08:30, 11.59s/it]
2022-04-10 16:41:21,131 - INFO - tqdm - accuracy: 0.9602, batch_loss: 0.0373, loss: 0.1033 ||:  34%|###3      | 22/65 [04:14<08:18, 11.60s/it]
2022-04-10 16:41:32,745 - INFO - tqdm - accuracy: 0.9620, batch_loss: 0.0317, loss: 0.1002 ||:  35%|###5      | 23/65 [04:26<08:07, 11.60s/it]
2022-04-10 16:41:44,338 - INFO - tqdm - accuracy: 0.9635, batch_loss: 0.0384, loss: 0.0976 ||:  37%|###6      | 24/65 [04:37<07:55, 11.60s/it]
2022-04-10 16:41:55,913 - INFO - tqdm - accuracy: 0.9613, batch_loss: 0.1994, loss: 0.1017 ||:  38%|###8      | 25/65 [04:49<07:43, 11.59s/it]
2022-04-10 16:42:07,483 - INFO - tqdm - accuracy: 0.9615, batch_loss: 0.1003, loss: 0.1016 ||:  40%|####      | 26/65 [05:00<07:31, 11.59s/it]
2022-04-10 16:42:18,804 - INFO - tqdm - accuracy: 0.9618, batch_loss: 0.1851, loss: 0.1047 ||:  42%|####1     | 27/65 [05:12<07:17, 11.51s/it]
2022-04-10 16:42:30,343 - INFO - tqdm - accuracy: 0.9621, batch_loss: 0.1511, loss: 0.1064 ||:  43%|####3     | 28/65 [05:23<07:06, 11.52s/it]
2022-04-10 16:42:41,876 - INFO - tqdm - accuracy: 0.9634, batch_loss: 0.0381, loss: 0.1040 ||:  45%|####4     | 29/65 [05:35<06:54, 11.52s/it]
2022-04-10 16:42:53,418 - INFO - tqdm - accuracy: 0.9646, batch_loss: 0.0458, loss: 0.1021 ||:  46%|####6     | 30/65 [05:46<06:43, 11.53s/it]
2022-04-10 16:43:04,957 - INFO - tqdm - accuracy: 0.9657, batch_loss: 0.0555, loss: 0.1006 ||:  48%|####7     | 31/65 [05:58<06:32, 11.53s/it]
2022-04-10 16:43:16,471 - INFO - tqdm - accuracy: 0.9658, batch_loss: 0.0660, loss: 0.0995 ||:  49%|####9     | 32/65 [06:09<06:20, 11.53s/it]
2022-04-10 16:43:27,980 - INFO - tqdm - accuracy: 0.9650, batch_loss: 0.0998, loss: 0.0995 ||:  51%|#####     | 33/65 [06:21<06:08, 11.52s/it]
2022-04-10 16:43:39,478 - INFO - tqdm - accuracy: 0.9623, batch_loss: 0.3097, loss: 0.1057 ||:  52%|#####2    | 34/65 [06:32<05:56, 11.51s/it]
2022-04-10 16:43:50,988 - INFO - tqdm - accuracy: 0.9634, batch_loss: 0.0235, loss: 0.1033 ||:  54%|#####3    | 35/65 [06:44<05:45, 11.51s/it]
2022-04-10 16:44:02,511 - INFO - tqdm - accuracy: 0.9644, batch_loss: 0.0479, loss: 0.1018 ||:  55%|#####5    | 36/65 [06:55<05:33, 11.52s/it]
2022-04-10 16:44:14,049 - INFO - tqdm - accuracy: 0.9645, batch_loss: 0.0929, loss: 0.1015 ||:  57%|#####6    | 37/65 [07:07<05:22, 11.52s/it]
2022-04-10 16:44:25,589 - INFO - tqdm - accuracy: 0.9646, batch_loss: 0.0681, loss: 0.1007 ||:  58%|#####8    | 38/65 [07:18<05:11, 11.53s/it]
2022-04-10 16:44:36,850 - INFO - tqdm - accuracy: 0.9631, batch_loss: 0.2503, loss: 0.1045 ||:  60%|######    | 39/65 [07:30<04:57, 11.45s/it]
2022-04-10 16:44:48,359 - INFO - tqdm - accuracy: 0.9633, batch_loss: 0.1159, loss: 0.1048 ||:  62%|######1   | 40/65 [07:41<04:46, 11.47s/it]
2022-04-10 16:44:59,881 - INFO - tqdm - accuracy: 0.9627, batch_loss: 0.1058, loss: 0.1048 ||:  63%|######3   | 41/65 [07:53<04:35, 11.48s/it]
2022-04-10 16:45:11,427 - INFO - tqdm - accuracy: 0.9628, batch_loss: 0.1569, loss: 0.1060 ||:  65%|######4   | 42/65 [08:04<04:24, 11.50s/it]
2022-04-10 16:45:22,983 - INFO - tqdm - accuracy: 0.9615, batch_loss: 0.1899, loss: 0.1080 ||:  66%|######6   | 43/65 [08:16<04:13, 11.52s/it]
2022-04-10 16:45:34,554 - INFO - tqdm - accuracy: 0.9602, batch_loss: 0.2110, loss: 0.1103 ||:  68%|######7   | 44/65 [08:27<04:02, 11.53s/it]
2022-04-10 16:45:46,134 - INFO - tqdm - accuracy: 0.9604, batch_loss: 0.0859, loss: 0.1098 ||:  69%|######9   | 45/65 [08:39<03:50, 11.55s/it]
2022-04-10 16:45:57,707 - INFO - tqdm - accuracy: 0.9599, batch_loss: 0.1701, loss: 0.1111 ||:  71%|#######   | 46/65 [08:51<03:39, 11.56s/it]
2022-04-10 16:46:09,284 - INFO - tqdm - accuracy: 0.9608, batch_loss: 0.0265, loss: 0.1093 ||:  72%|#######2  | 47/65 [09:02<03:28, 11.56s/it]
2022-04-10 16:46:20,884 - INFO - tqdm - accuracy: 0.9616, batch_loss: 0.0372, loss: 0.1078 ||:  74%|#######3  | 48/65 [09:14<03:16, 11.57s/it]
2022-04-10 16:46:32,486 - INFO - tqdm - accuracy: 0.9624, batch_loss: 0.0448, loss: 0.1065 ||:  75%|#######5  | 49/65 [09:25<03:05, 11.58s/it]
2022-04-10 16:46:44,084 - INFO - tqdm - accuracy: 0.9625, batch_loss: 0.0735, loss: 0.1059 ||:  77%|#######6  | 50/65 [09:37<02:53, 11.59s/it]
2022-04-10 16:46:55,699 - INFO - tqdm - accuracy: 0.9626, batch_loss: 0.1004, loss: 0.1058 ||:  78%|#######8  | 51/65 [09:49<02:42, 11.60s/it]
2022-04-10 16:47:07,317 - INFO - tqdm - accuracy: 0.9621, batch_loss: 0.1609, loss: 0.1068 ||:  80%|########  | 52/65 [10:00<02:30, 11.60s/it]
2022-04-10 16:47:18,915 - INFO - tqdm - accuracy: 0.9623, batch_loss: 0.0811, loss: 0.1063 ||:  82%|########1 | 53/65 [10:12<02:19, 11.60s/it]
2022-04-10 16:47:30,543 - INFO - tqdm - accuracy: 0.9624, batch_loss: 0.0997, loss: 0.1062 ||:  83%|########3 | 54/65 [10:23<02:07, 11.61s/it]
2022-04-10 16:47:42,153 - INFO - tqdm - accuracy: 0.9631, batch_loss: 0.0128, loss: 0.1045 ||:  85%|########4 | 55/65 [10:35<01:56, 11.61s/it]
2022-04-10 16:47:53,762 - INFO - tqdm - accuracy: 0.9609, batch_loss: 0.3126, loss: 0.1082 ||:  86%|########6 | 56/65 [10:47<01:44, 11.61s/it]
2022-04-10 16:48:05,348 - INFO - tqdm - accuracy: 0.9611, batch_loss: 0.0909, loss: 0.1079 ||:  88%|########7 | 57/65 [10:58<01:32, 11.60s/it]
2022-04-10 16:48:16,950 - INFO - tqdm - accuracy: 0.9607, batch_loss: 0.1021, loss: 0.1078 ||:  89%|########9 | 58/65 [11:10<01:21, 11.60s/it]
2022-04-10 16:48:28,576 - INFO - tqdm - accuracy: 0.9608, batch_loss: 0.0504, loss: 0.1068 ||:  91%|######### | 59/65 [11:21<01:09, 11.61s/it]
2022-04-10 16:48:39,807 - INFO - tqdm - accuracy: 0.9614, batch_loss: 0.0497, loss: 0.1059 ||:  92%|#########2| 60/65 [11:33<00:57, 11.50s/it]
2022-04-10 16:48:51,368 - INFO - tqdm - accuracy: 0.9616, batch_loss: 0.1036, loss: 0.1059 ||:  94%|#########3| 61/65 [11:44<00:46, 11.52s/it]
2022-04-10 16:49:02,964 - INFO - tqdm - accuracy: 0.9617, batch_loss: 0.1904, loss: 0.1072 ||:  95%|#########5| 62/65 [11:56<00:34, 11.54s/it]
2022-04-10 16:49:14,533 - INFO - tqdm - accuracy: 0.9613, batch_loss: 0.1128, loss: 0.1073 ||:  97%|#########6| 63/65 [12:07<00:23, 11.55s/it]
2022-04-10 16:49:26,085 - INFO - tqdm - accuracy: 0.9614, batch_loss: 0.0952, loss: 0.1071 ||:  98%|#########8| 64/65 [12:19<00:11, 11.55s/it]
2022-04-10 16:49:31,090 - INFO - tqdm - accuracy: 0.9617, batch_loss: 0.0112, loss: 0.1056 ||: 100%|##########| 65/65 [12:24<00:00,  9.59s/it]
2022-04-10 16:49:31,091 - INFO - tqdm - accuracy: 0.9617, batch_loss: 0.0112, loss: 0.1056 ||: 100%|##########| 65/65 [12:24<00:00, 11.45s/it]
2022-04-10 16:49:33,987 - INFO - allennlp.training.trainer - Validating
2022-04-10 16:49:33,989 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-10 16:49:44,109 - INFO - tqdm - accuracy: 0.6500, batch_loss: 0.0170, loss: 1.7616 ||:  35%|###4      | 40/115 [00:10<00:19,  3.94it/s]
2022-04-10 16:49:54,354 - INFO - tqdm - accuracy: 0.6500, batch_loss: 0.8371, loss: 1.6783 ||:  70%|######9   | 80/115 [00:20<00:09,  3.89it/s]
2022-04-10 16:50:03,113 - INFO - tqdm - accuracy: 0.6638, batch_loss: 0.2801, loss: 1.5664 ||: 100%|##########| 115/115 [00:29<00:00,  3.98it/s]
2022-04-10 16:50:03,113 - INFO - tqdm - accuracy: 0.6638, batch_loss: 0.2801, loss: 1.5664 ||: 100%|##########| 115/115 [00:29<00:00,  3.95it/s]
2022-04-10 16:50:03,113 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-10 16:50:03,114 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.962  |     0.664
2022-04-10 16:50:03,115 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-10 16:50:03,116 - INFO - allennlp.training.tensorboard_writer - loss               |     0.106  |     1.566
2022-04-10 16:50:03,116 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5804.914  |       N/A
2022-04-10 16:50:08,907 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to './output_ir-ora-d_hyper4/best.th'.
2022-04-10 16:50:23,208 - INFO - allennlp.training.trainer - Epoch duration: 0:13:16.537807
2022-04-10 16:50:23,208 - INFO - allennlp.training.trainer - Estimated training time remaining: 1:18:51
2022-04-10 16:50:23,209 - INFO - allennlp.training.trainer - Epoch 9/14
2022-04-10 16:50:23,209 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-10 16:50:23,209 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-10 16:50:23,210 - INFO - allennlp.training.trainer - Training
2022-04-10 16:50:23,210 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-10 16:50:34,463 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.0540, loss: 0.0540 ||:   2%|1         | 1/65 [00:11<12:00, 11.25s/it]
2022-04-10 16:50:46,026 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0471, loss: 0.0506 ||:   3%|3         | 2/65 [00:22<12:00, 11.44s/it]
2022-04-10 16:50:57,942 - INFO - tqdm - accuracy: 0.9583, batch_loss: 0.1692, loss: 0.0901 ||:   5%|4         | 3/65 [00:34<12:02, 11.65s/it]
2022-04-10 16:51:09,706 - INFO - tqdm - accuracy: 0.9531, batch_loss: 0.0766, loss: 0.0868 ||:   6%|6         | 4/65 [00:46<11:53, 11.70s/it]
2022-04-10 16:51:21,237 - INFO - tqdm - accuracy: 0.9500, batch_loss: 0.1511, loss: 0.0996 ||:   8%|7         | 5/65 [00:58<11:38, 11.64s/it]
2022-04-10 16:51:32,646 - INFO - tqdm - accuracy: 0.9479, batch_loss: 0.0904, loss: 0.0981 ||:   9%|9         | 6/65 [01:09<11:22, 11.56s/it]
2022-04-10 16:51:44,069 - INFO - tqdm - accuracy: 0.9554, batch_loss: 0.0313, loss: 0.0885 ||:  11%|#         | 7/65 [01:20<11:07, 11.52s/it]
2022-04-10 16:51:55,617 - INFO - tqdm - accuracy: 0.9531, batch_loss: 0.1121, loss: 0.0915 ||:  12%|#2        | 8/65 [01:32<10:56, 11.53s/it]
2022-04-10 16:52:07,244 - INFO - tqdm - accuracy: 0.9583, batch_loss: 0.0088, loss: 0.0823 ||:  14%|#3        | 9/65 [01:44<10:47, 11.56s/it]
2022-04-10 16:52:18,854 - INFO - tqdm - accuracy: 0.9625, batch_loss: 0.0297, loss: 0.0770 ||:  15%|#5        | 10/65 [01:55<10:36, 11.57s/it]
2022-04-10 16:52:30,412 - INFO - tqdm - accuracy: 0.9631, batch_loss: 0.0989, loss: 0.0790 ||:  17%|#6        | 11/65 [02:07<10:24, 11.57s/it]
2022-04-10 16:52:41,966 - INFO - tqdm - accuracy: 0.9635, batch_loss: 0.0717, loss: 0.0784 ||:  18%|#8        | 12/65 [02:18<10:12, 11.56s/it]
2022-04-10 16:52:53,481 - INFO - tqdm - accuracy: 0.9663, batch_loss: 0.0281, loss: 0.0745 ||:  20%|##        | 13/65 [02:30<10:00, 11.55s/it]
2022-04-10 16:53:04,958 - INFO - tqdm - accuracy: 0.9665, batch_loss: 0.0887, loss: 0.0756 ||:  22%|##1       | 14/65 [02:41<09:47, 11.53s/it]
2022-04-10 16:53:16,898 - INFO - tqdm - accuracy: 0.9646, batch_loss: 0.1046, loss: 0.0775 ||:  23%|##3       | 15/65 [02:53<09:42, 11.65s/it]
2022-04-10 16:53:28,432 - INFO - tqdm - accuracy: 0.9668, batch_loss: 0.0265, loss: 0.0743 ||:  25%|##4       | 16/65 [03:05<09:29, 11.62s/it]
2022-04-10 16:53:40,038 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.0060, loss: 0.0703 ||:  26%|##6       | 17/65 [03:16<09:17, 11.61s/it]
2022-04-10 16:53:51,630 - INFO - tqdm - accuracy: 0.9705, batch_loss: 0.0376, loss: 0.0685 ||:  28%|##7       | 18/65 [03:28<09:05, 11.61s/it]
2022-04-10 16:54:03,195 - INFO - tqdm - accuracy: 0.9720, batch_loss: 0.0271, loss: 0.0663 ||:  29%|##9       | 19/65 [03:39<08:53, 11.59s/it]
2022-04-10 16:54:14,751 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.2708, loss: 0.0765 ||:  31%|###       | 20/65 [03:51<08:41, 11.58s/it]
2022-04-10 16:54:26,273 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.0337, loss: 0.0745 ||:  32%|###2      | 21/65 [04:03<08:28, 11.56s/it]
2022-04-10 16:54:37,770 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.0614, loss: 0.0739 ||:  34%|###3      | 22/65 [04:14<08:16, 11.54s/it]
2022-04-10 16:54:49,285 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.0909, loss: 0.0746 ||:  35%|###5      | 23/65 [04:26<08:04, 11.54s/it]
2022-04-10 16:55:00,848 - INFO - tqdm - accuracy: 0.9661, batch_loss: 0.1688, loss: 0.0786 ||:  37%|###6      | 24/65 [04:37<07:53, 11.54s/it]
2022-04-10 16:55:12,405 - INFO - tqdm - accuracy: 0.9675, batch_loss: 0.0373, loss: 0.0769 ||:  38%|###8      | 25/65 [04:49<07:41, 11.55s/it]
2022-04-10 16:55:23,970 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.0218, loss: 0.0748 ||:  40%|####      | 26/65 [05:00<07:30, 11.55s/it]
2022-04-10 16:55:35,529 - INFO - tqdm - accuracy: 0.9699, batch_loss: 0.0275, loss: 0.0730 ||:  42%|####1     | 27/65 [05:12<07:19, 11.55s/it]
2022-04-10 16:55:47,099 - INFO - tqdm - accuracy: 0.9699, batch_loss: 0.2031, loss: 0.0777 ||:  43%|####3     | 28/65 [05:23<07:07, 11.56s/it]
2022-04-10 16:55:58,684 - INFO - tqdm - accuracy: 0.9698, batch_loss: 0.0657, loss: 0.0773 ||:  45%|####4     | 29/65 [05:35<06:56, 11.57s/it]
2022-04-10 16:56:10,157 - INFO - tqdm - accuracy: 0.9708, batch_loss: 0.0153, loss: 0.0752 ||:  46%|####6     | 30/65 [05:46<06:43, 11.54s/it]
2022-04-10 16:56:21,771 - INFO - tqdm - accuracy: 0.9708, batch_loss: 0.0437, loss: 0.0742 ||:  48%|####7     | 31/65 [05:58<06:33, 11.56s/it]
2022-04-10 16:56:33,361 - INFO - tqdm - accuracy: 0.9717, batch_loss: 0.0243, loss: 0.0726 ||:  49%|####9     | 32/65 [06:10<06:21, 11.57s/it]
2022-04-10 16:56:44,968 - INFO - tqdm - accuracy: 0.9716, batch_loss: 0.0876, loss: 0.0731 ||:  51%|#####     | 33/65 [06:21<06:10, 11.58s/it]
2022-04-10 16:56:56,577 - INFO - tqdm - accuracy: 0.9715, batch_loss: 0.0807, loss: 0.0733 ||:  52%|#####2    | 34/65 [06:33<05:59, 11.59s/it]
2022-04-10 16:57:07,815 - INFO - tqdm - accuracy: 0.9723, batch_loss: 0.0163, loss: 0.0717 ||:  54%|#####3    | 35/65 [06:44<05:44, 11.48s/it]
2022-04-10 16:57:19,398 - INFO - tqdm - accuracy: 0.9722, batch_loss: 0.0679, loss: 0.0716 ||:  55%|#####5    | 36/65 [06:56<05:33, 11.51s/it]
2022-04-10 16:57:30,946 - INFO - tqdm - accuracy: 0.9704, batch_loss: 0.1892, loss: 0.0747 ||:  57%|#####6    | 37/65 [07:07<05:22, 11.52s/it]
2022-04-10 16:57:42,528 - INFO - tqdm - accuracy: 0.9712, batch_loss: 0.0171, loss: 0.0732 ||:  58%|#####8    | 38/65 [07:19<05:11, 11.54s/it]
2022-04-10 16:57:54,088 - INFO - tqdm - accuracy: 0.9719, batch_loss: 0.0270, loss: 0.0720 ||:  60%|######    | 39/65 [07:30<05:00, 11.55s/it]
2022-04-10 16:58:05,680 - INFO - tqdm - accuracy: 0.9719, batch_loss: 0.0598, loss: 0.0717 ||:  62%|######1   | 40/65 [07:42<04:49, 11.56s/it]
2022-04-10 16:58:17,234 - INFO - tqdm - accuracy: 0.9725, batch_loss: 0.0276, loss: 0.0707 ||:  63%|######3   | 41/65 [07:54<04:37, 11.56s/it]
2022-04-10 16:58:28,800 - INFO - tqdm - accuracy: 0.9724, batch_loss: 0.0619, loss: 0.0705 ||:  65%|######4   | 42/65 [08:05<04:25, 11.56s/it]
2022-04-10 16:58:40,355 - INFO - tqdm - accuracy: 0.9731, batch_loss: 0.0314, loss: 0.0695 ||:  66%|######6   | 43/65 [08:17<04:14, 11.56s/it]
2022-04-10 16:58:51,919 - INFO - tqdm - accuracy: 0.9723, batch_loss: 0.1659, loss: 0.0717 ||:  68%|######7   | 44/65 [08:28<04:02, 11.56s/it]
2022-04-10 16:59:03,449 - INFO - tqdm - accuracy: 0.9722, batch_loss: 0.1620, loss: 0.0737 ||:  69%|######9   | 45/65 [08:40<03:51, 11.55s/it]
2022-04-10 16:59:15,000 - INFO - tqdm - accuracy: 0.9708, batch_loss: 0.2256, loss: 0.0770 ||:  71%|#######   | 46/65 [08:51<03:39, 11.55s/it]
2022-04-10 16:59:26,550 - INFO - tqdm - accuracy: 0.9707, batch_loss: 0.1514, loss: 0.0786 ||:  72%|#######2  | 47/65 [09:03<03:27, 11.55s/it]
2022-04-10 16:59:38,114 - INFO - tqdm - accuracy: 0.9713, batch_loss: 0.0517, loss: 0.0781 ||:  74%|#######3  | 48/65 [09:14<03:16, 11.55s/it]
2022-04-10 16:59:49,650 - INFO - tqdm - accuracy: 0.9706, batch_loss: 0.0880, loss: 0.0783 ||:  75%|#######5  | 49/65 [09:26<03:04, 11.55s/it]
2022-04-10 17:00:00,947 - INFO - tqdm - accuracy: 0.9712, batch_loss: 0.0575, loss: 0.0779 ||:  77%|#######6  | 50/65 [09:37<02:52, 11.47s/it]
2022-04-10 17:00:12,485 - INFO - tqdm - accuracy: 0.9718, batch_loss: 0.0335, loss: 0.0770 ||:  78%|#######8  | 51/65 [09:49<02:40, 11.49s/it]
2022-04-10 17:00:24,036 - INFO - tqdm - accuracy: 0.9723, batch_loss: 0.0269, loss: 0.0760 ||:  80%|########  | 52/65 [10:00<02:29, 11.51s/it]
2022-04-10 17:00:35,575 - INFO - tqdm - accuracy: 0.9723, batch_loss: 0.1096, loss: 0.0767 ||:  82%|########1 | 53/65 [10:12<02:18, 11.52s/it]
2022-04-10 17:00:47,110 - INFO - tqdm - accuracy: 0.9728, batch_loss: 0.0470, loss: 0.0761 ||:  83%|########3 | 54/65 [10:23<02:06, 11.52s/it]
2022-04-10 17:00:58,652 - INFO - tqdm - accuracy: 0.9733, batch_loss: 0.0187, loss: 0.0751 ||:  85%|########4 | 55/65 [10:35<01:55, 11.53s/it]
2022-04-10 17:01:10,204 - INFO - tqdm - accuracy: 0.9732, batch_loss: 0.0972, loss: 0.0755 ||:  86%|########6 | 56/65 [10:46<01:43, 11.54s/it]
2022-04-10 17:01:21,738 - INFO - tqdm - accuracy: 0.9731, batch_loss: 0.0787, loss: 0.0755 ||:  88%|########7 | 57/65 [10:58<01:32, 11.54s/it]
2022-04-10 17:01:33,262 - INFO - tqdm - accuracy: 0.9736, batch_loss: 0.0330, loss: 0.0748 ||:  89%|########9 | 58/65 [11:10<01:20, 11.53s/it]
2022-04-10 17:01:44,813 - INFO - tqdm - accuracy: 0.9735, batch_loss: 0.0680, loss: 0.0747 ||:  91%|######### | 59/65 [11:21<01:09, 11.54s/it]
2022-04-10 17:01:56,339 - INFO - tqdm - accuracy: 0.9739, batch_loss: 0.0400, loss: 0.0741 ||:  92%|#########2| 60/65 [11:33<00:57, 11.53s/it]
2022-04-10 17:02:07,865 - INFO - tqdm - accuracy: 0.9744, batch_loss: 0.0256, loss: 0.0733 ||:  94%|#########3| 61/65 [11:44<00:46, 11.53s/it]
2022-04-10 17:02:19,423 - INFO - tqdm - accuracy: 0.9743, batch_loss: 0.0436, loss: 0.0728 ||:  95%|#########5| 62/65 [11:56<00:34, 11.54s/it]
2022-04-10 17:02:30,960 - INFO - tqdm - accuracy: 0.9747, batch_loss: 0.0331, loss: 0.0722 ||:  97%|#########6| 63/65 [12:07<00:23, 11.54s/it]
2022-04-10 17:02:42,481 - INFO - tqdm - accuracy: 0.9751, batch_loss: 0.0359, loss: 0.0716 ||:  98%|#########8| 64/65 [12:19<00:11, 11.53s/it]
2022-04-10 17:02:47,602 - INFO - tqdm - accuracy: 0.9748, batch_loss: 0.1685, loss: 0.0731 ||: 100%|##########| 65/65 [12:24<00:00,  9.61s/it]
2022-04-10 17:02:47,602 - INFO - tqdm - accuracy: 0.9748, batch_loss: 0.1685, loss: 0.0731 ||: 100%|##########| 65/65 [12:24<00:00, 11.45s/it]
2022-04-10 17:02:50,510 - INFO - allennlp.training.trainer - Validating
2022-04-10 17:02:50,511 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-10 17:03:00,674 - INFO - tqdm - accuracy: 0.6667, batch_loss: 3.0546, loss: 1.8026 ||:  36%|###5      | 41/115 [00:10<00:18,  3.91it/s]
2022-04-10 17:03:10,910 - INFO - tqdm - accuracy: 0.6460, batch_loss: 3.0596, loss: 1.8158 ||:  70%|#######   | 81/115 [00:20<00:08,  3.87it/s]
2022-04-10 17:03:19,739 - INFO - tqdm - accuracy: 0.6419, batch_loss: 0.8406, loss: 1.6706 ||: 100%|##########| 115/115 [00:29<00:00,  3.84it/s]
2022-04-10 17:03:19,739 - INFO - tqdm - accuracy: 0.6419, batch_loss: 0.8406, loss: 1.6706 ||: 100%|##########| 115/115 [00:29<00:00,  3.93it/s]
2022-04-10 17:03:19,739 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-10 17:03:19,740 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.975  |     0.642
2022-04-10 17:03:19,741 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-10 17:03:19,741 - INFO - allennlp.training.tensorboard_writer - loss               |     0.073  |     1.671
2022-04-10 17:03:19,742 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5804.914  |       N/A
2022-04-10 17:03:25,999 - INFO - allennlp.training.trainer - Epoch duration: 0:13:02.790910
2022-04-10 17:03:26,000 - INFO - allennlp.training.trainer - Estimated training time remaining: 1:05:40
2022-04-10 17:03:26,000 - INFO - allennlp.training.trainer - Epoch 10/14
2022-04-10 17:03:26,000 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-10 17:03:26,000 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-10 17:03:26,002 - INFO - allennlp.training.trainer - Training
2022-04-10 17:03:26,002 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-10 17:03:37,404 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.0470, loss: 0.0470 ||:   2%|1         | 1/65 [00:11<12:09, 11.40s/it]
2022-04-10 17:03:48,724 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.0722, loss: 0.0596 ||:   3%|3         | 2/65 [00:22<11:55, 11.35s/it]
2022-04-10 17:04:00,556 - INFO - tqdm - accuracy: 0.9792, batch_loss: 0.0397, loss: 0.0529 ||:   5%|4         | 3/65 [00:34<11:57, 11.57s/it]
2022-04-10 17:04:12,244 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0198, loss: 0.0447 ||:   6%|6         | 4/65 [00:46<11:48, 11.62s/it]
2022-04-10 17:04:23,643 - INFO - tqdm - accuracy: 0.9812, batch_loss: 0.0485, loss: 0.0454 ||:   8%|7         | 5/65 [00:57<11:32, 11.54s/it]
2022-04-10 17:04:35,135 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0134, loss: 0.0401 ||:   9%|9         | 6/65 [01:09<11:19, 11.52s/it]
2022-04-10 17:04:46,689 - INFO - tqdm - accuracy: 0.9821, batch_loss: 0.0431, loss: 0.0405 ||:  11%|#         | 7/65 [01:20<11:08, 11.53s/it]
2022-04-10 17:04:58,281 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0140, loss: 0.0372 ||:  12%|#2        | 8/65 [01:32<10:58, 11.55s/it]
2022-04-10 17:05:09,921 - INFO - tqdm - accuracy: 0.9861, batch_loss: 0.0170, loss: 0.0350 ||:  14%|#3        | 9/65 [01:43<10:48, 11.58s/it]
2022-04-10 17:05:21,539 - INFO - tqdm - accuracy: 0.9875, batch_loss: 0.0322, loss: 0.0347 ||:  15%|#5        | 10/65 [01:55<10:37, 11.59s/it]
2022-04-10 17:05:33,107 - INFO - tqdm - accuracy: 0.9858, batch_loss: 0.1736, loss: 0.0473 ||:  17%|#6        | 11/65 [02:07<10:25, 11.58s/it]
2022-04-10 17:05:44,649 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0715, loss: 0.0493 ||:  18%|#8        | 12/65 [02:18<10:13, 11.57s/it]
2022-04-10 17:05:56,192 - INFO - tqdm - accuracy: 0.9832, batch_loss: 0.0522, loss: 0.0496 ||:  20%|##        | 13/65 [02:30<10:01, 11.56s/it]
2022-04-10 17:06:07,687 - INFO - tqdm - accuracy: 0.9799, batch_loss: 0.1734, loss: 0.0584 ||:  22%|##1       | 14/65 [02:41<09:48, 11.54s/it]
2022-04-10 17:06:19,208 - INFO - tqdm - accuracy: 0.9792, batch_loss: 0.0314, loss: 0.0566 ||:  23%|##3       | 15/65 [02:53<09:36, 11.54s/it]
2022-04-10 17:06:30,778 - INFO - tqdm - accuracy: 0.9805, batch_loss: 0.0251, loss: 0.0546 ||:  25%|##4       | 16/65 [03:04<09:25, 11.55s/it]
2022-04-10 17:06:42,409 - INFO - tqdm - accuracy: 0.9816, batch_loss: 0.0044, loss: 0.0517 ||:  26%|##6       | 17/65 [03:16<09:15, 11.57s/it]
2022-04-10 17:06:54,018 - INFO - tqdm - accuracy: 0.9809, batch_loss: 0.0922, loss: 0.0539 ||:  28%|##7       | 18/65 [03:28<09:04, 11.58s/it]
2022-04-10 17:07:05,554 - INFO - tqdm - accuracy: 0.9819, batch_loss: 0.0200, loss: 0.0521 ||:  29%|##9       | 19/65 [03:39<08:52, 11.57s/it]
2022-04-10 17:07:17,044 - INFO - tqdm - accuracy: 0.9828, batch_loss: 0.0755, loss: 0.0533 ||:  31%|###       | 20/65 [03:51<08:39, 11.55s/it]
2022-04-10 17:07:28,586 - INFO - tqdm - accuracy: 0.9836, batch_loss: 0.0334, loss: 0.0524 ||:  32%|###2      | 21/65 [04:02<08:27, 11.54s/it]
2022-04-10 17:07:40,168 - INFO - tqdm - accuracy: 0.9830, batch_loss: 0.0700, loss: 0.0532 ||:  34%|###3      | 22/65 [04:14<08:16, 11.56s/it]
2022-04-10 17:07:51,434 - INFO - tqdm - accuracy: 0.9837, batch_loss: 0.0215, loss: 0.0518 ||:  35%|###5      | 23/65 [04:25<08:01, 11.47s/it]
2022-04-10 17:08:02,980 - INFO - tqdm - accuracy: 0.9831, batch_loss: 0.0682, loss: 0.0525 ||:  37%|###6      | 24/65 [04:36<07:51, 11.49s/it]
2022-04-10 17:08:14,533 - INFO - tqdm - accuracy: 0.9812, batch_loss: 0.0653, loss: 0.0530 ||:  38%|###8      | 25/65 [04:48<07:40, 11.51s/it]
2022-04-10 17:08:25,941 - INFO - tqdm - accuracy: 0.9819, batch_loss: 0.0273, loss: 0.0520 ||:  40%|####      | 26/65 [04:59<07:27, 11.48s/it]
2022-04-10 17:08:37,466 - INFO - tqdm - accuracy: 0.9803, batch_loss: 0.1084, loss: 0.0541 ||:  42%|####1     | 27/65 [05:11<07:16, 11.49s/it]
2022-04-10 17:08:49,006 - INFO - tqdm - accuracy: 0.9788, batch_loss: 0.0870, loss: 0.0553 ||:  43%|####3     | 28/65 [05:23<07:05, 11.51s/it]
2022-04-10 17:09:00,568 - INFO - tqdm - accuracy: 0.9773, batch_loss: 0.1350, loss: 0.0580 ||:  45%|####4     | 29/65 [05:34<06:54, 11.52s/it]
2022-04-10 17:09:12,157 - INFO - tqdm - accuracy: 0.9781, batch_loss: 0.0291, loss: 0.0570 ||:  46%|####6     | 30/65 [05:46<06:44, 11.54s/it]
2022-04-10 17:09:23,755 - INFO - tqdm - accuracy: 0.9788, batch_loss: 0.0239, loss: 0.0560 ||:  48%|####7     | 31/65 [05:57<06:33, 11.56s/it]
2022-04-10 17:09:35,370 - INFO - tqdm - accuracy: 0.9795, batch_loss: 0.0140, loss: 0.0547 ||:  49%|####9     | 32/65 [06:09<06:22, 11.58s/it]
2022-04-10 17:09:46,980 - INFO - tqdm - accuracy: 0.9791, batch_loss: 0.0712, loss: 0.0552 ||:  51%|#####     | 33/65 [06:20<06:10, 11.59s/it]
2022-04-10 17:09:58,591 - INFO - tqdm - accuracy: 0.9798, batch_loss: 0.0215, loss: 0.0542 ||:  52%|#####2    | 34/65 [06:32<05:59, 11.59s/it]
2022-04-10 17:10:10,184 - INFO - tqdm - accuracy: 0.9803, batch_loss: 0.0451, loss: 0.0539 ||:  54%|#####3    | 35/65 [06:44<05:47, 11.59s/it]
2022-04-10 17:10:21,770 - INFO - tqdm - accuracy: 0.9809, batch_loss: 0.0098, loss: 0.0527 ||:  55%|#####5    | 36/65 [06:55<05:36, 11.59s/it]
2022-04-10 17:10:33,340 - INFO - tqdm - accuracy: 0.9814, batch_loss: 0.0087, loss: 0.0515 ||:  57%|#####6    | 37/65 [07:07<05:24, 11.59s/it]
2022-04-10 17:10:44,925 - INFO - tqdm - accuracy: 0.9819, batch_loss: 0.0355, loss: 0.0511 ||:  58%|#####8    | 38/65 [07:18<05:12, 11.58s/it]
2022-04-10 17:10:56,493 - INFO - tqdm - accuracy: 0.9816, batch_loss: 0.0928, loss: 0.0521 ||:  60%|######    | 39/65 [07:30<05:01, 11.58s/it]
2022-04-10 17:11:08,048 - INFO - tqdm - accuracy: 0.9812, batch_loss: 0.0435, loss: 0.0519 ||:  62%|######1   | 40/65 [07:42<04:49, 11.57s/it]
2022-04-10 17:11:19,630 - INFO - tqdm - accuracy: 0.9817, batch_loss: 0.0179, loss: 0.0511 ||:  63%|######3   | 41/65 [07:53<04:37, 11.58s/it]
2022-04-10 17:11:31,202 - INFO - tqdm - accuracy: 0.9806, batch_loss: 0.1381, loss: 0.0532 ||:  65%|######4   | 42/65 [08:05<04:26, 11.57s/it]
2022-04-10 17:11:42,762 - INFO - tqdm - accuracy: 0.9804, batch_loss: 0.1279, loss: 0.0549 ||:  66%|######6   | 43/65 [08:16<04:14, 11.57s/it]
2022-04-10 17:11:54,308 - INFO - tqdm - accuracy: 0.9808, batch_loss: 0.0187, loss: 0.0541 ||:  68%|######7   | 44/65 [08:28<04:02, 11.56s/it]
2022-04-10 17:12:05,847 - INFO - tqdm - accuracy: 0.9798, batch_loss: 0.0851, loss: 0.0548 ||:  69%|######9   | 45/65 [08:39<03:51, 11.56s/it]
2022-04-10 17:12:17,404 - INFO - tqdm - accuracy: 0.9796, batch_loss: 0.0754, loss: 0.0552 ||:  71%|#######   | 46/65 [08:51<03:39, 11.56s/it]
2022-04-10 17:12:28,949 - INFO - tqdm - accuracy: 0.9787, batch_loss: 0.1089, loss: 0.0564 ||:  72%|#######2  | 47/65 [09:02<03:27, 11.55s/it]
2022-04-10 17:12:40,498 - INFO - tqdm - accuracy: 0.9792, batch_loss: 0.0294, loss: 0.0558 ||:  74%|#######3  | 48/65 [09:14<03:16, 11.55s/it]
2022-04-10 17:12:52,038 - INFO - tqdm - accuracy: 0.9796, batch_loss: 0.0144, loss: 0.0550 ||:  75%|#######5  | 49/65 [09:26<03:04, 11.55s/it]
2022-04-10 17:13:03,985 - INFO - tqdm - accuracy: 0.9800, batch_loss: 0.0150, loss: 0.0542 ||:  77%|#######6  | 50/65 [09:37<02:55, 11.67s/it]
2022-04-10 17:13:15,485 - INFO - tqdm - accuracy: 0.9804, batch_loss: 0.0073, loss: 0.0532 ||:  78%|#######8  | 51/65 [09:49<02:42, 11.62s/it]
2022-04-10 17:13:27,021 - INFO - tqdm - accuracy: 0.9808, batch_loss: 0.0139, loss: 0.0525 ||:  80%|########  | 52/65 [10:01<02:30, 11.59s/it]
2022-04-10 17:13:38,571 - INFO - tqdm - accuracy: 0.9811, batch_loss: 0.0149, loss: 0.0518 ||:  82%|########1 | 53/65 [10:12<02:18, 11.58s/it]
2022-04-10 17:13:50,126 - INFO - tqdm - accuracy: 0.9815, batch_loss: 0.0364, loss: 0.0515 ||:  83%|########3 | 54/65 [10:24<02:07, 11.57s/it]
2022-04-10 17:14:01,667 - INFO - tqdm - accuracy: 0.9807, batch_loss: 0.1135, loss: 0.0526 ||:  85%|########4 | 55/65 [10:35<01:55, 11.56s/it]
2022-04-10 17:14:13,223 - INFO - tqdm - accuracy: 0.9799, batch_loss: 0.1947, loss: 0.0552 ||:  86%|########6 | 56/65 [10:47<01:44, 11.56s/it]
2022-04-10 17:14:24,754 - INFO - tqdm - accuracy: 0.9803, batch_loss: 0.0633, loss: 0.0553 ||:  88%|########7 | 57/65 [10:58<01:32, 11.55s/it]
2022-04-10 17:14:36,276 - INFO - tqdm - accuracy: 0.9806, batch_loss: 0.0242, loss: 0.0548 ||:  89%|########9 | 58/65 [11:10<01:20, 11.54s/it]
2022-04-10 17:14:47,827 - INFO - tqdm - accuracy: 0.9804, batch_loss: 0.0711, loss: 0.0550 ||:  91%|######### | 59/65 [11:21<01:09, 11.55s/it]
2022-04-10 17:14:59,351 - INFO - tqdm - accuracy: 0.9807, batch_loss: 0.0427, loss: 0.0548 ||:  92%|#########2| 60/65 [11:33<00:57, 11.54s/it]
2022-04-10 17:15:10,883 - INFO - tqdm - accuracy: 0.9810, batch_loss: 0.0163, loss: 0.0542 ||:  94%|#########3| 61/65 [11:44<00:46, 11.54s/it]
2022-04-10 17:15:22,421 - INFO - tqdm - accuracy: 0.9813, batch_loss: 0.0142, loss: 0.0536 ||:  95%|#########5| 62/65 [11:56<00:34, 11.54s/it]
2022-04-10 17:15:33,932 - INFO - tqdm - accuracy: 0.9811, batch_loss: 0.0518, loss: 0.0535 ||:  97%|#########6| 63/65 [12:07<00:23, 11.53s/it]
2022-04-10 17:15:45,482 - INFO - tqdm - accuracy: 0.9809, batch_loss: 0.0862, loss: 0.0540 ||:  98%|#########8| 64/65 [12:19<00:11, 11.54s/it]
2022-04-10 17:15:50,599 - INFO - tqdm - accuracy: 0.9811, batch_loss: 0.0301, loss: 0.0537 ||: 100%|##########| 65/65 [12:24<00:00,  9.61s/it]
2022-04-10 17:15:50,600 - INFO - tqdm - accuracy: 0.9811, batch_loss: 0.0301, loss: 0.0537 ||: 100%|##########| 65/65 [12:24<00:00, 11.46s/it]
2022-04-10 17:15:53,505 - INFO - allennlp.training.trainer - Validating
2022-04-10 17:15:53,506 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-10 17:16:03,621 - INFO - tqdm - accuracy: 0.6875, batch_loss: 0.0003, loss: 1.2480 ||:  35%|###4      | 40/115 [00:10<00:19,  3.94it/s]
2022-04-10 17:16:13,639 - INFO - tqdm - accuracy: 0.6038, batch_loss: 5.2530, loss: 1.7896 ||:  70%|######9   | 80/115 [00:20<00:08,  3.90it/s]
2022-04-10 17:16:22,761 - INFO - tqdm - accuracy: 0.6332, batch_loss: 3.9208, loss: 1.7812 ||: 100%|##########| 115/115 [00:29<00:00,  3.83it/s]
2022-04-10 17:16:22,761 - INFO - tqdm - accuracy: 0.6332, batch_loss: 3.9208, loss: 1.7812 ||: 100%|##########| 115/115 [00:29<00:00,  3.93it/s]
2022-04-10 17:16:22,762 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-10 17:16:22,762 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.981  |     0.633
2022-04-10 17:16:22,763 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-10 17:16:22,763 - INFO - allennlp.training.tensorboard_writer - loss               |     0.054  |     1.781
2022-04-10 17:16:22,764 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5804.914  |       N/A
2022-04-10 17:16:28,949 - INFO - allennlp.training.trainer - Epoch duration: 0:13:02.949358
2022-04-10 17:16:28,949 - INFO - allennlp.training.trainer - Estimated training time remaining: 0:52:30
2022-04-10 17:16:28,950 - INFO - allennlp.training.trainer - Epoch 11/14
2022-04-10 17:16:28,950 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-10 17:16:28,950 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-10 17:16:28,951 - INFO - allennlp.training.trainer - Training
2022-04-10 17:16:28,951 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-10 17:16:40,358 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0096, loss: 0.0096 ||:   2%|1         | 1/65 [00:11<12:09, 11.41s/it]
2022-04-10 17:16:51,933 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0245, loss: 0.0171 ||:   3%|3         | 2/65 [00:22<12:04, 11.51s/it]
2022-04-10 17:17:03,740 - INFO - tqdm - accuracy: 0.9792, batch_loss: 0.3154, loss: 0.1165 ||:   5%|4         | 3/65 [00:34<12:01, 11.64s/it]
2022-04-10 17:17:15,351 - INFO - tqdm - accuracy: 0.9766, batch_loss: 0.1930, loss: 0.1356 ||:   6%|6         | 4/65 [00:46<11:49, 11.63s/it]
2022-04-10 17:17:26,834 - INFO - tqdm - accuracy: 0.9812, batch_loss: 0.0294, loss: 0.1144 ||:   8%|7         | 5/65 [00:57<11:34, 11.58s/it]
2022-04-10 17:17:38,356 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0277, loss: 0.0999 ||:   9%|9         | 6/65 [01:09<11:21, 11.56s/it]
2022-04-10 17:17:49,855 - INFO - tqdm - accuracy: 0.9866, batch_loss: 0.0175, loss: 0.0881 ||:  11%|#         | 7/65 [01:20<11:09, 11.54s/it]
2022-04-10 17:18:01,377 - INFO - tqdm - accuracy: 0.9883, batch_loss: 0.0088, loss: 0.0782 ||:  12%|#2        | 8/65 [01:32<10:57, 11.53s/it]
2022-04-10 17:18:12,922 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.0077, loss: 0.0704 ||:  14%|#3        | 9/65 [01:43<10:46, 11.54s/it]
2022-04-10 17:18:24,513 - INFO - tqdm - accuracy: 0.9906, batch_loss: 0.0235, loss: 0.0657 ||:  15%|#5        | 10/65 [01:55<10:35, 11.55s/it]
2022-04-10 17:18:36,106 - INFO - tqdm - accuracy: 0.9915, batch_loss: 0.0263, loss: 0.0621 ||:  17%|#6        | 11/65 [02:07<10:24, 11.57s/it]
2022-04-10 17:18:47,681 - INFO - tqdm - accuracy: 0.9922, batch_loss: 0.0100, loss: 0.0578 ||:  18%|#8        | 12/65 [02:18<10:13, 11.57s/it]
2022-04-10 17:18:59,282 - INFO - tqdm - accuracy: 0.9904, batch_loss: 0.0861, loss: 0.0599 ||:  20%|##        | 13/65 [02:30<10:02, 11.58s/it]
2022-04-10 17:19:10,885 - INFO - tqdm - accuracy: 0.9888, batch_loss: 0.0938, loss: 0.0624 ||:  22%|##1       | 14/65 [02:41<09:50, 11.59s/it]
2022-04-10 17:19:22,486 - INFO - tqdm - accuracy: 0.9875, batch_loss: 0.0357, loss: 0.0606 ||:  23%|##3       | 15/65 [02:53<09:39, 11.59s/it]
2022-04-10 17:19:33,977 - INFO - tqdm - accuracy: 0.9883, batch_loss: 0.0074, loss: 0.0573 ||:  25%|##4       | 16/65 [03:05<09:26, 11.56s/it]
2022-04-10 17:19:45,574 - INFO - tqdm - accuracy: 0.9890, batch_loss: 0.0051, loss: 0.0542 ||:  26%|##6       | 17/65 [03:16<09:15, 11.57s/it]
2022-04-10 17:19:57,177 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.0245, loss: 0.0525 ||:  28%|##7       | 18/65 [03:28<09:04, 11.58s/it]
2022-04-10 17:20:08,758 - INFO - tqdm - accuracy: 0.9901, batch_loss: 0.0238, loss: 0.0510 ||:  29%|##9       | 19/65 [03:39<08:52, 11.58s/it]
2022-04-10 17:20:20,329 - INFO - tqdm - accuracy: 0.9906, batch_loss: 0.0368, loss: 0.0503 ||:  31%|###       | 20/65 [03:51<08:41, 11.58s/it]
2022-04-10 17:20:31,928 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.0377, loss: 0.0497 ||:  32%|###2      | 21/65 [04:02<08:29, 11.58s/it]
2022-04-10 17:20:43,514 - INFO - tqdm - accuracy: 0.9901, batch_loss: 0.0124, loss: 0.0480 ||:  34%|###3      | 22/65 [04:14<08:18, 11.58s/it]
2022-04-10 17:20:55,088 - INFO - tqdm - accuracy: 0.9891, batch_loss: 0.0914, loss: 0.0499 ||:  35%|###5      | 23/65 [04:26<08:06, 11.58s/it]
2022-04-10 17:21:06,647 - INFO - tqdm - accuracy: 0.9883, batch_loss: 0.0682, loss: 0.0507 ||:  37%|###6      | 24/65 [04:37<07:54, 11.57s/it]
2022-04-10 17:21:18,212 - INFO - tqdm - accuracy: 0.9888, batch_loss: 0.0217, loss: 0.0495 ||:  38%|###8      | 25/65 [04:49<07:42, 11.57s/it]
2022-04-10 17:21:29,771 - INFO - tqdm - accuracy: 0.9892, batch_loss: 0.0071, loss: 0.0479 ||:  40%|####      | 26/65 [05:00<07:31, 11.57s/it]
2022-04-10 17:21:41,322 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.0256, loss: 0.0471 ||:  42%|####1     | 27/65 [05:12<07:19, 11.56s/it]
2022-04-10 17:21:52,866 - INFO - tqdm - accuracy: 0.9888, batch_loss: 0.0430, loss: 0.0469 ||:  43%|####3     | 28/65 [05:23<07:07, 11.56s/it]
2022-04-10 17:22:04,429 - INFO - tqdm - accuracy: 0.9892, batch_loss: 0.0099, loss: 0.0456 ||:  45%|####4     | 29/65 [05:35<06:56, 11.56s/it]
2022-04-10 17:22:15,973 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.0095, loss: 0.0444 ||:  46%|####6     | 30/65 [05:47<06:44, 11.55s/it]
2022-04-10 17:22:27,519 - INFO - tqdm - accuracy: 0.9899, batch_loss: 0.0045, loss: 0.0431 ||:  48%|####7     | 31/65 [05:58<06:32, 11.55s/it]
2022-04-10 17:22:39,053 - INFO - tqdm - accuracy: 0.9902, batch_loss: 0.0218, loss: 0.0425 ||:  49%|####9     | 32/65 [06:10<06:21, 11.55s/it]
2022-04-10 17:22:50,593 - INFO - tqdm - accuracy: 0.9905, batch_loss: 0.0180, loss: 0.0417 ||:  51%|#####     | 33/65 [06:21<06:09, 11.54s/it]
2022-04-10 17:23:02,160 - INFO - tqdm - accuracy: 0.9890, batch_loss: 0.1048, loss: 0.0436 ||:  52%|#####2    | 34/65 [06:33<05:58, 11.55s/it]
2022-04-10 17:23:13,701 - INFO - tqdm - accuracy: 0.9884, batch_loss: 0.0546, loss: 0.0439 ||:  54%|#####3    | 35/65 [06:44<05:46, 11.55s/it]
2022-04-10 17:23:25,240 - INFO - tqdm - accuracy: 0.9887, batch_loss: 0.0058, loss: 0.0428 ||:  55%|#####5    | 36/65 [06:56<05:34, 11.55s/it]
2022-04-10 17:23:36,794 - INFO - tqdm - accuracy: 0.9882, batch_loss: 0.0752, loss: 0.0437 ||:  57%|#####6    | 37/65 [07:07<05:23, 11.55s/it]
2022-04-10 17:23:48,318 - INFO - tqdm - accuracy: 0.9877, batch_loss: 0.0380, loss: 0.0436 ||:  58%|#####8    | 38/65 [07:19<05:11, 11.54s/it]
2022-04-10 17:23:59,848 - INFO - tqdm - accuracy: 0.9880, batch_loss: 0.0626, loss: 0.0441 ||:  60%|######    | 39/65 [07:30<04:59, 11.54s/it]
2022-04-10 17:24:11,395 - INFO - tqdm - accuracy: 0.9875, batch_loss: 0.0559, loss: 0.0444 ||:  62%|######1   | 40/65 [07:42<04:48, 11.54s/it]
2022-04-10 17:24:22,961 - INFO - tqdm - accuracy: 0.9870, batch_loss: 0.0637, loss: 0.0448 ||:  63%|######3   | 41/65 [07:54<04:37, 11.55s/it]
2022-04-10 17:24:34,513 - INFO - tqdm - accuracy: 0.9866, batch_loss: 0.0613, loss: 0.0452 ||:  65%|######4   | 42/65 [08:05<04:25, 11.55s/it]
2022-04-10 17:24:45,956 - INFO - tqdm - accuracy: 0.9862, batch_loss: 0.0856, loss: 0.0462 ||:  66%|######6   | 43/65 [08:17<04:13, 11.52s/it]
2022-04-10 17:24:57,509 - INFO - tqdm - accuracy: 0.9865, batch_loss: 0.0294, loss: 0.0458 ||:  68%|######7   | 44/65 [08:28<04:02, 11.53s/it]
2022-04-10 17:25:08,740 - INFO - tqdm - accuracy: 0.9861, batch_loss: 0.1109, loss: 0.0472 ||:  69%|######9   | 45/65 [08:39<03:48, 11.44s/it]
2022-04-10 17:25:20,292 - INFO - tqdm - accuracy: 0.9864, batch_loss: 0.0324, loss: 0.0469 ||:  71%|#######   | 46/65 [08:51<03:37, 11.47s/it]
2022-04-10 17:25:31,853 - INFO - tqdm - accuracy: 0.9860, batch_loss: 0.0910, loss: 0.0478 ||:  72%|#######2  | 47/65 [09:02<03:26, 11.50s/it]
2022-04-10 17:25:43,414 - INFO - tqdm - accuracy: 0.9857, batch_loss: 0.0554, loss: 0.0480 ||:  74%|#######3  | 48/65 [09:14<03:15, 11.52s/it]
2022-04-10 17:25:54,965 - INFO - tqdm - accuracy: 0.9860, batch_loss: 0.0121, loss: 0.0473 ||:  75%|#######5  | 49/65 [09:26<03:04, 11.53s/it]
2022-04-10 17:26:06,511 - INFO - tqdm - accuracy: 0.9862, batch_loss: 0.0030, loss: 0.0464 ||:  77%|#######6  | 50/65 [09:37<02:52, 11.53s/it]
2022-04-10 17:26:18,061 - INFO - tqdm - accuracy: 0.9859, batch_loss: 0.1520, loss: 0.0484 ||:  78%|#######8  | 51/65 [09:49<02:41, 11.54s/it]
2022-04-10 17:26:29,616 - INFO - tqdm - accuracy: 0.9862, batch_loss: 0.0111, loss: 0.0477 ||:  80%|########  | 52/65 [10:00<02:30, 11.54s/it]
2022-04-10 17:26:41,160 - INFO - tqdm - accuracy: 0.9864, batch_loss: 0.0190, loss: 0.0472 ||:  82%|########1 | 53/65 [10:12<02:18, 11.54s/it]
2022-04-10 17:26:52,720 - INFO - tqdm - accuracy: 0.9867, batch_loss: 0.0075, loss: 0.0465 ||:  83%|########3 | 54/65 [10:23<02:07, 11.55s/it]
2022-04-10 17:27:04,252 - INFO - tqdm - accuracy: 0.9869, batch_loss: 0.0138, loss: 0.0459 ||:  85%|########4 | 55/65 [10:35<01:55, 11.54s/it]
2022-04-10 17:27:15,798 - INFO - tqdm - accuracy: 0.9872, batch_loss: 0.0215, loss: 0.0454 ||:  86%|########6 | 56/65 [10:46<01:43, 11.54s/it]
2022-04-10 17:27:27,105 - INFO - tqdm - accuracy: 0.9868, batch_loss: 0.0948, loss: 0.0463 ||:  88%|########7 | 57/65 [10:58<01:31, 11.47s/it]
2022-04-10 17:27:38,628 - INFO - tqdm - accuracy: 0.9865, batch_loss: 0.0773, loss: 0.0468 ||:  89%|########9 | 58/65 [11:09<01:20, 11.49s/it]
2022-04-10 17:27:50,178 - INFO - tqdm - accuracy: 0.9868, batch_loss: 0.0254, loss: 0.0465 ||:  91%|######### | 59/65 [11:21<01:09, 11.51s/it]
2022-04-10 17:28:01,711 - INFO - tqdm - accuracy: 0.9865, batch_loss: 0.0439, loss: 0.0464 ||:  92%|#########2| 60/65 [11:32<00:57, 11.51s/it]
2022-04-10 17:28:13,253 - INFO - tqdm - accuracy: 0.9867, batch_loss: 0.0177, loss: 0.0460 ||:  94%|#########3| 61/65 [11:44<00:46, 11.52s/it]
2022-04-10 17:28:24,785 - INFO - tqdm - accuracy: 0.9869, batch_loss: 0.0092, loss: 0.0454 ||:  95%|#########5| 62/65 [11:55<00:34, 11.53s/it]
2022-04-10 17:28:36,325 - INFO - tqdm - accuracy: 0.9871, batch_loss: 0.0075, loss: 0.0448 ||:  97%|#########6| 63/65 [12:07<00:23, 11.53s/it]
2022-04-10 17:28:47,867 - INFO - tqdm - accuracy: 0.9868, batch_loss: 0.0671, loss: 0.0451 ||:  98%|#########8| 64/65 [12:18<00:11, 11.53s/it]
2022-04-10 17:28:52,989 - INFO - tqdm - accuracy: 0.9864, batch_loss: 0.1171, loss: 0.0462 ||: 100%|##########| 65/65 [12:24<00:00,  9.61s/it]
2022-04-10 17:28:52,989 - INFO - tqdm - accuracy: 0.9864, batch_loss: 0.1171, loss: 0.0462 ||: 100%|##########| 65/65 [12:24<00:00, 11.45s/it]
2022-04-10 17:28:55,884 - INFO - allennlp.training.trainer - Validating
2022-04-10 17:28:55,886 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-10 17:29:06,005 - INFO - tqdm - accuracy: 0.6000, batch_loss: 0.0047, loss: 2.3343 ||:  35%|###4      | 40/115 [00:10<00:19,  3.92it/s]
2022-04-10 17:29:16,114 - INFO - tqdm - accuracy: 0.5975, batch_loss: 0.0444, loss: 2.0663 ||:  70%|######9   | 80/115 [00:20<00:07,  4.53it/s]
2022-04-10 17:29:25,149 - INFO - tqdm - accuracy: 0.6288, batch_loss: 0.8556, loss: 1.8406 ||: 100%|##########| 115/115 [00:29<00:00,  3.80it/s]
2022-04-10 17:29:25,149 - INFO - tqdm - accuracy: 0.6288, batch_loss: 0.8556, loss: 1.8406 ||: 100%|##########| 115/115 [00:29<00:00,  3.93it/s]
2022-04-10 17:29:25,149 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-10 17:29:25,150 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.986  |     0.629
2022-04-10 17:29:25,151 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-10 17:29:25,152 - INFO - allennlp.training.tensorboard_writer - loss               |     0.046  |     1.841
2022-04-10 17:29:25,152 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5804.914  |       N/A
2022-04-10 17:29:31,241 - INFO - allennlp.training.trainer - Epoch duration: 0:13:02.291181
2022-04-10 17:29:31,241 - INFO - allennlp.training.trainer - Estimated training time remaining: 0:39:21
2022-04-10 17:29:31,241 - INFO - allennlp.training.trainer - Epoch 12/14
2022-04-10 17:29:31,241 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-10 17:29:31,242 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-10 17:29:31,243 - INFO - allennlp.training.trainer - Training
2022-04-10 17:29:31,243 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-10 17:29:42,645 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0268, loss: 0.0268 ||:   2%|1         | 1/65 [00:11<12:09, 11.40s/it]
2022-04-10 17:29:54,217 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0102, loss: 0.0185 ||:   3%|3         | 2/65 [00:22<12:04, 11.50s/it]
2022-04-10 17:30:05,912 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0105, loss: 0.0159 ||:   5%|4         | 3/65 [00:34<11:58, 11.59s/it]
2022-04-10 17:30:17,543 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0127, loss: 0.0151 ||:   6%|6         | 4/65 [00:46<11:47, 11.61s/it]
2022-04-10 17:30:29,019 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0139, loss: 0.0148 ||:   8%|7         | 5/65 [00:57<11:33, 11.56s/it]
2022-04-10 17:30:40,545 - INFO - tqdm - accuracy: 0.9948, batch_loss: 0.0849, loss: 0.0265 ||:   9%|9         | 6/65 [01:09<11:21, 11.55s/it]
2022-04-10 17:30:52,107 - INFO - tqdm - accuracy: 0.9955, batch_loss: 0.0141, loss: 0.0247 ||:  11%|#         | 7/65 [01:20<11:10, 11.55s/it]
2022-04-10 17:31:03,747 - INFO - tqdm - accuracy: 0.9961, batch_loss: 0.0074, loss: 0.0226 ||:  12%|#2        | 8/65 [01:32<11:00, 11.58s/it]
2022-04-10 17:31:15,393 - INFO - tqdm - accuracy: 0.9965, batch_loss: 0.0058, loss: 0.0207 ||:  14%|#3        | 9/65 [01:44<10:49, 11.60s/it]
2022-04-10 17:31:26,979 - INFO - tqdm - accuracy: 0.9969, batch_loss: 0.0039, loss: 0.0190 ||:  15%|#5        | 10/65 [01:55<10:37, 11.60s/it]
2022-04-10 17:31:38,524 - INFO - tqdm - accuracy: 0.9972, batch_loss: 0.0140, loss: 0.0186 ||:  17%|#6        | 11/65 [02:07<10:25, 11.58s/it]
2022-04-10 17:31:50,067 - INFO - tqdm - accuracy: 0.9948, batch_loss: 0.0520, loss: 0.0214 ||:  18%|#8        | 12/65 [02:18<10:13, 11.57s/it]
2022-04-10 17:32:01,250 - INFO - tqdm - accuracy: 0.9952, batch_loss: 0.0233, loss: 0.0215 ||:  20%|##        | 13/65 [02:30<09:55, 11.45s/it]
2022-04-10 17:32:12,816 - INFO - tqdm - accuracy: 0.9911, batch_loss: 0.1080, loss: 0.0277 ||:  22%|##1       | 14/65 [02:41<09:45, 11.49s/it]
2022-04-10 17:32:24,371 - INFO - tqdm - accuracy: 0.9916, batch_loss: 0.0187, loss: 0.0271 ||:  23%|##3       | 15/65 [02:53<09:35, 11.51s/it]
2022-04-10 17:32:35,928 - INFO - tqdm - accuracy: 0.9902, batch_loss: 0.0772, loss: 0.0302 ||:  25%|##4       | 16/65 [03:04<09:24, 11.52s/it]
2022-04-10 17:32:47,506 - INFO - tqdm - accuracy: 0.9908, batch_loss: 0.0102, loss: 0.0290 ||:  26%|##6       | 17/65 [03:16<09:13, 11.54s/it]
2022-04-10 17:32:59,071 - INFO - tqdm - accuracy: 0.9913, batch_loss: 0.0240, loss: 0.0288 ||:  28%|##7       | 18/65 [03:27<09:02, 11.55s/it]
2022-04-10 17:33:10,665 - INFO - tqdm - accuracy: 0.9918, batch_loss: 0.0075, loss: 0.0276 ||:  29%|##9       | 19/65 [03:39<08:51, 11.56s/it]
2022-04-10 17:33:22,685 - INFO - tqdm - accuracy: 0.9922, batch_loss: 0.0356, loss: 0.0280 ||:  31%|###       | 20/65 [03:51<08:46, 11.70s/it]
2022-04-10 17:33:34,219 - INFO - tqdm - accuracy: 0.9911, batch_loss: 0.0515, loss: 0.0292 ||:  32%|###2      | 21/65 [04:02<08:32, 11.65s/it]
2022-04-10 17:33:45,839 - INFO - tqdm - accuracy: 0.9900, batch_loss: 0.0757, loss: 0.0313 ||:  34%|###3      | 22/65 [04:14<08:20, 11.64s/it]
2022-04-10 17:33:57,453 - INFO - tqdm - accuracy: 0.9905, batch_loss: 0.0150, loss: 0.0306 ||:  35%|###5      | 23/65 [04:26<08:08, 11.63s/it]
2022-04-10 17:34:09,062 - INFO - tqdm - accuracy: 0.9870, batch_loss: 0.1392, loss: 0.0351 ||:  37%|###6      | 24/65 [04:37<07:56, 11.63s/it]
2022-04-10 17:34:20,412 - INFO - tqdm - accuracy: 0.9875, batch_loss: 0.0267, loss: 0.0348 ||:  38%|###8      | 25/65 [04:49<07:41, 11.54s/it]
2022-04-10 17:34:32,052 - INFO - tqdm - accuracy: 0.9880, batch_loss: 0.0100, loss: 0.0338 ||:  40%|####      | 26/65 [05:00<07:31, 11.57s/it]
2022-04-10 17:34:43,651 - INFO - tqdm - accuracy: 0.9884, batch_loss: 0.0049, loss: 0.0327 ||:  42%|####1     | 27/65 [05:12<07:20, 11.58s/it]
2022-04-10 17:34:55,244 - INFO - tqdm - accuracy: 0.9888, batch_loss: 0.0146, loss: 0.0321 ||:  43%|####3     | 28/65 [05:24<07:08, 11.58s/it]
2022-04-10 17:35:06,854 - INFO - tqdm - accuracy: 0.9892, batch_loss: 0.0279, loss: 0.0319 ||:  45%|####4     | 29/65 [05:35<06:57, 11.59s/it]
2022-04-10 17:35:18,453 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.0185, loss: 0.0315 ||:  46%|####6     | 30/65 [05:47<06:45, 11.59s/it]
2022-04-10 17:35:30,048 - INFO - tqdm - accuracy: 0.9899, batch_loss: 0.0031, loss: 0.0306 ||:  48%|####7     | 31/65 [05:58<06:34, 11.59s/it]
2022-04-10 17:35:41,526 - INFO - tqdm - accuracy: 0.9892, batch_loss: 0.0602, loss: 0.0315 ||:  49%|####9     | 32/65 [06:10<06:21, 11.56s/it]
2022-04-10 17:35:53,102 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.0131, loss: 0.0309 ||:  51%|#####     | 33/65 [06:21<06:10, 11.56s/it]
2022-04-10 17:36:04,699 - INFO - tqdm - accuracy: 0.9899, batch_loss: 0.0115, loss: 0.0304 ||:  52%|#####2    | 34/65 [06:33<05:58, 11.57s/it]
2022-04-10 17:36:16,264 - INFO - tqdm - accuracy: 0.9902, batch_loss: 0.0114, loss: 0.0298 ||:  54%|#####3    | 35/65 [06:45<05:47, 11.57s/it]
2022-04-10 17:36:27,811 - INFO - tqdm - accuracy: 0.9887, batch_loss: 0.1433, loss: 0.0330 ||:  55%|#####5    | 36/65 [06:56<05:35, 11.56s/it]
2022-04-10 17:36:39,373 - INFO - tqdm - accuracy: 0.9890, batch_loss: 0.0052, loss: 0.0322 ||:  57%|#####6    | 37/65 [07:08<05:23, 11.56s/it]
2022-04-10 17:36:50,922 - INFO - tqdm - accuracy: 0.9885, batch_loss: 0.0633, loss: 0.0330 ||:  58%|#####8    | 38/65 [07:19<05:12, 11.56s/it]
2022-04-10 17:37:02,473 - INFO - tqdm - accuracy: 0.9880, batch_loss: 0.1042, loss: 0.0349 ||:  60%|######    | 39/65 [07:31<05:00, 11.56s/it]
2022-04-10 17:37:14,006 - INFO - tqdm - accuracy: 0.9883, batch_loss: 0.0234, loss: 0.0346 ||:  62%|######1   | 40/65 [07:42<04:48, 11.55s/it]
2022-04-10 17:37:25,540 - INFO - tqdm - accuracy: 0.9870, batch_loss: 0.2379, loss: 0.0395 ||:  63%|######3   | 41/65 [07:54<04:37, 11.54s/it]
2022-04-10 17:37:37,066 - INFO - tqdm - accuracy: 0.9873, batch_loss: 0.0282, loss: 0.0393 ||:  65%|######4   | 42/65 [08:05<04:25, 11.54s/it]
2022-04-10 17:37:48,607 - INFO - tqdm - accuracy: 0.9876, batch_loss: 0.0028, loss: 0.0384 ||:  66%|######6   | 43/65 [08:17<04:13, 11.54s/it]
2022-04-10 17:38:00,126 - INFO - tqdm - accuracy: 0.9879, batch_loss: 0.0094, loss: 0.0378 ||:  68%|######7   | 44/65 [08:28<04:02, 11.53s/it]
2022-04-10 17:38:11,651 - INFO - tqdm - accuracy: 0.9875, batch_loss: 0.0529, loss: 0.0381 ||:  69%|######9   | 45/65 [08:40<03:50, 11.53s/it]
2022-04-10 17:38:23,172 - INFO - tqdm - accuracy: 0.9878, batch_loss: 0.0106, loss: 0.0375 ||:  71%|#######   | 46/65 [08:51<03:39, 11.53s/it]
2022-04-10 17:38:34,716 - INFO - tqdm - accuracy: 0.9880, batch_loss: 0.0038, loss: 0.0368 ||:  72%|#######2  | 47/65 [09:03<03:27, 11.53s/it]
2022-04-10 17:38:46,243 - INFO - tqdm - accuracy: 0.9883, batch_loss: 0.0268, loss: 0.0366 ||:  74%|#######3  | 48/65 [09:14<03:16, 11.53s/it]
2022-04-10 17:38:57,768 - INFO - tqdm - accuracy: 0.9885, batch_loss: 0.0036, loss: 0.0359 ||:  75%|#######5  | 49/65 [09:26<03:04, 11.53s/it]
2022-04-10 17:39:09,329 - INFO - tqdm - accuracy: 0.9887, batch_loss: 0.0045, loss: 0.0353 ||:  77%|#######6  | 50/65 [09:38<02:53, 11.54s/it]
2022-04-10 17:39:20,892 - INFO - tqdm - accuracy: 0.9890, batch_loss: 0.0127, loss: 0.0348 ||:  78%|#######8  | 51/65 [09:49<02:41, 11.55s/it]
2022-04-10 17:39:32,464 - INFO - tqdm - accuracy: 0.9886, batch_loss: 0.0922, loss: 0.0359 ||:  80%|########  | 52/65 [10:01<02:30, 11.55s/it]
2022-04-10 17:39:44,038 - INFO - tqdm - accuracy: 0.9888, batch_loss: 0.0119, loss: 0.0355 ||:  82%|########1 | 53/65 [10:12<02:18, 11.56s/it]
2022-04-10 17:39:55,630 - INFO - tqdm - accuracy: 0.9884, batch_loss: 0.0746, loss: 0.0362 ||:  83%|########3 | 54/65 [10:24<02:07, 11.57s/it]
2022-04-10 17:40:07,250 - INFO - tqdm - accuracy: 0.9886, batch_loss: 0.0159, loss: 0.0358 ||:  85%|########4 | 55/65 [10:36<01:55, 11.58s/it]
2022-04-10 17:40:18,846 - INFO - tqdm - accuracy: 0.9883, batch_loss: 0.0463, loss: 0.0360 ||:  86%|########6 | 56/65 [10:47<01:44, 11.59s/it]
2022-04-10 17:40:30,448 - INFO - tqdm - accuracy: 0.9885, batch_loss: 0.0255, loss: 0.0358 ||:  88%|########7 | 57/65 [10:59<01:32, 11.59s/it]
2022-04-10 17:40:42,058 - INFO - tqdm - accuracy: 0.9876, batch_loss: 0.0605, loss: 0.0363 ||:  89%|########9 | 58/65 [11:10<01:21, 11.60s/it]
2022-04-10 17:40:53,650 - INFO - tqdm - accuracy: 0.9873, batch_loss: 0.1014, loss: 0.0374 ||:  91%|######### | 59/65 [11:22<01:09, 11.60s/it]
2022-04-10 17:41:05,282 - INFO - tqdm - accuracy: 0.9875, batch_loss: 0.0163, loss: 0.0370 ||:  92%|#########2| 60/65 [11:34<00:58, 11.61s/it]
2022-04-10 17:41:16,848 - INFO - tqdm - accuracy: 0.9877, batch_loss: 0.0053, loss: 0.0365 ||:  94%|#########3| 61/65 [11:45<00:46, 11.59s/it]
2022-04-10 17:41:28,442 - INFO - tqdm - accuracy: 0.9879, batch_loss: 0.0336, loss: 0.0365 ||:  95%|#########5| 62/65 [11:57<00:34, 11.59s/it]
2022-04-10 17:41:40,011 - INFO - tqdm - accuracy: 0.9881, batch_loss: 0.0464, loss: 0.0366 ||:  97%|#########6| 63/65 [12:08<00:23, 11.59s/it]
2022-04-10 17:41:51,451 - INFO - tqdm - accuracy: 0.9873, batch_loss: 0.1566, loss: 0.0385 ||:  98%|#########8| 64/65 [12:20<00:11, 11.54s/it]
2022-04-10 17:41:56,592 - INFO - tqdm - accuracy: 0.9874, batch_loss: 0.0019, loss: 0.0379 ||: 100%|##########| 65/65 [12:25<00:00,  9.62s/it]
2022-04-10 17:41:56,593 - INFO - tqdm - accuracy: 0.9874, batch_loss: 0.0019, loss: 0.0379 ||: 100%|##########| 65/65 [12:25<00:00, 11.47s/it]
2022-04-10 17:41:59,501 - INFO - allennlp.training.trainer - Validating
2022-04-10 17:41:59,503 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-10 17:42:09,730 - INFO - tqdm - accuracy: 0.7037, batch_loss: 4.5237, loss: 1.6887 ||:  36%|###5      | 41/115 [00:10<00:18,  3.94it/s]
2022-04-10 17:42:19,747 - INFO - tqdm - accuracy: 0.6604, batch_loss: 0.0004, loss: 1.7446 ||:  70%|######9   | 80/115 [00:20<00:08,  3.89it/s]
2022-04-10 17:42:28,791 - INFO - tqdm - accuracy: 0.6288, batch_loss: 1.8889, loss: 1.8666 ||: 100%|##########| 115/115 [00:29<00:00,  3.86it/s]
2022-04-10 17:42:28,791 - INFO - tqdm - accuracy: 0.6288, batch_loss: 1.8889, loss: 1.8666 ||: 100%|##########| 115/115 [00:29<00:00,  3.93it/s]
2022-04-10 17:42:28,791 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-10 17:42:28,792 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.987  |     0.629
2022-04-10 17:42:28,793 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-10 17:42:28,794 - INFO - allennlp.training.tensorboard_writer - loss               |     0.038  |     1.867
2022-04-10 17:42:28,795 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5804.914  |       N/A
2022-04-10 17:42:34,921 - INFO - allennlp.training.trainer - Epoch duration: 0:13:03.679445
2022-04-10 17:42:34,921 - INFO - allennlp.training.trainer - Estimated training time remaining: 0:26:13
2022-04-10 17:42:34,921 - INFO - allennlp.training.trainer - Epoch 13/14
2022-04-10 17:42:34,921 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-10 17:42:34,922 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-10 17:42:34,923 - INFO - allennlp.training.trainer - Training
2022-04-10 17:42:34,924 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-10 17:42:46,329 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.0447, loss: 0.0447 ||:   2%|1         | 1/65 [00:11<12:09, 11.40s/it]
2022-04-10 17:42:57,973 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0082, loss: 0.0264 ||:   3%|3         | 2/65 [00:23<12:07, 11.55s/it]
2022-04-10 17:43:09,857 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.0224, loss: 0.0251 ||:   5%|4         | 3/65 [00:34<12:05, 11.70s/it]
2022-04-10 17:43:21,464 - INFO - tqdm - accuracy: 0.9922, batch_loss: 0.0237, loss: 0.0248 ||:   6%|6         | 4/65 [00:46<11:51, 11.66s/it]
2022-04-10 17:43:32,930 - INFO - tqdm - accuracy: 0.9812, batch_loss: 0.1357, loss: 0.0469 ||:   8%|7         | 5/65 [00:58<11:35, 11.59s/it]
2022-04-10 17:43:44,425 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0063, loss: 0.0402 ||:   9%|9         | 6/65 [01:09<11:21, 11.56s/it]
2022-04-10 17:43:55,994 - INFO - tqdm - accuracy: 0.9866, batch_loss: 0.0220, loss: 0.0376 ||:  11%|#         | 7/65 [01:21<11:10, 11.56s/it]
2022-04-10 17:44:07,609 - INFO - tqdm - accuracy: 0.9883, batch_loss: 0.0059, loss: 0.0336 ||:  12%|#2        | 8/65 [01:32<11:00, 11.58s/it]
2022-04-10 17:44:19,231 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.0141, loss: 0.0314 ||:  14%|#3        | 9/65 [01:44<10:49, 11.59s/it]
2022-04-10 17:44:30,779 - INFO - tqdm - accuracy: 0.9906, batch_loss: 0.0195, loss: 0.0302 ||:  15%|#5        | 10/65 [01:55<10:36, 11.58s/it]
2022-04-10 17:44:42,322 - INFO - tqdm - accuracy: 0.9915, batch_loss: 0.0044, loss: 0.0279 ||:  17%|#6        | 11/65 [02:07<10:24, 11.57s/it]
2022-04-10 17:44:53,830 - INFO - tqdm - accuracy: 0.9922, batch_loss: 0.0073, loss: 0.0262 ||:  18%|#8        | 12/65 [02:18<10:12, 11.55s/it]
2022-04-10 17:45:05,349 - INFO - tqdm - accuracy: 0.9928, batch_loss: 0.0118, loss: 0.0251 ||:  20%|##        | 13/65 [02:30<10:00, 11.54s/it]
2022-04-10 17:45:16,917 - INFO - tqdm - accuracy: 0.9911, batch_loss: 0.0544, loss: 0.0272 ||:  22%|##1       | 14/65 [02:41<09:48, 11.55s/it]
2022-04-10 17:45:28,485 - INFO - tqdm - accuracy: 0.9917, batch_loss: 0.0024, loss: 0.0255 ||:  23%|##3       | 15/65 [02:53<09:37, 11.55s/it]
2022-04-10 17:45:40,080 - INFO - tqdm - accuracy: 0.9922, batch_loss: 0.0187, loss: 0.0251 ||:  25%|##4       | 16/65 [03:05<09:26, 11.57s/it]
2022-04-10 17:45:51,694 - INFO - tqdm - accuracy: 0.9926, batch_loss: 0.0032, loss: 0.0238 ||:  26%|##6       | 17/65 [03:16<09:15, 11.58s/it]
2022-04-10 17:46:02,975 - INFO - tqdm - accuracy: 0.9930, batch_loss: 0.0153, loss: 0.0233 ||:  28%|##7       | 18/65 [03:28<09:00, 11.49s/it]
2022-04-10 17:46:14,591 - INFO - tqdm - accuracy: 0.9934, batch_loss: 0.0117, loss: 0.0227 ||:  29%|##9       | 19/65 [03:39<08:50, 11.53s/it]
2022-04-10 17:46:26,213 - INFO - tqdm - accuracy: 0.9922, batch_loss: 0.0819, loss: 0.0257 ||:  31%|###       | 20/65 [03:51<08:40, 11.56s/it]
2022-04-10 17:46:37,827 - INFO - tqdm - accuracy: 0.9925, batch_loss: 0.0101, loss: 0.0249 ||:  32%|###2      | 21/65 [04:02<08:29, 11.57s/it]
2022-04-10 17:46:49,432 - INFO - tqdm - accuracy: 0.9929, batch_loss: 0.0284, loss: 0.0251 ||:  34%|###3      | 22/65 [04:14<08:18, 11.58s/it]
2022-04-10 17:47:01,015 - INFO - tqdm - accuracy: 0.9918, batch_loss: 0.0575, loss: 0.0265 ||:  35%|###5      | 23/65 [04:26<08:06, 11.58s/it]
2022-04-10 17:47:12,543 - INFO - tqdm - accuracy: 0.9922, batch_loss: 0.0051, loss: 0.0256 ||:  37%|###6      | 24/65 [04:37<07:54, 11.57s/it]
2022-04-10 17:47:24,105 - INFO - tqdm - accuracy: 0.9925, batch_loss: 0.0067, loss: 0.0249 ||:  38%|###8      | 25/65 [04:49<07:42, 11.57s/it]
2022-04-10 17:47:35,656 - INFO - tqdm - accuracy: 0.9916, batch_loss: 0.0961, loss: 0.0276 ||:  40%|####      | 26/65 [05:00<07:30, 11.56s/it]
2022-04-10 17:47:47,200 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.1255, loss: 0.0312 ||:  42%|####1     | 27/65 [05:12<07:19, 11.56s/it]
2022-04-10 17:47:58,753 - INFO - tqdm - accuracy: 0.9877, batch_loss: 0.2327, loss: 0.0384 ||:  43%|####3     | 28/65 [05:23<07:07, 11.56s/it]
2022-04-10 17:48:10,311 - INFO - tqdm - accuracy: 0.9881, batch_loss: 0.0187, loss: 0.0377 ||:  45%|####4     | 29/65 [05:35<06:56, 11.56s/it]
2022-04-10 17:48:21,864 - INFO - tqdm - accuracy: 0.9875, batch_loss: 0.1598, loss: 0.0418 ||:  46%|####6     | 30/65 [05:46<06:44, 11.55s/it]
2022-04-10 17:48:33,415 - INFO - tqdm - accuracy: 0.9879, batch_loss: 0.0055, loss: 0.0406 ||:  48%|####7     | 31/65 [05:58<06:32, 11.55s/it]
2022-04-10 17:48:44,962 - INFO - tqdm - accuracy: 0.9883, batch_loss: 0.0109, loss: 0.0397 ||:  49%|####9     | 32/65 [06:10<06:21, 11.55s/it]
2022-04-10 17:48:56,512 - INFO - tqdm - accuracy: 0.9877, batch_loss: 0.0825, loss: 0.0410 ||:  51%|#####     | 33/65 [06:21<06:09, 11.55s/it]
2022-04-10 17:49:07,909 - INFO - tqdm - accuracy: 0.9880, batch_loss: 0.0377, loss: 0.0409 ||:  52%|#####2    | 34/65 [06:32<05:56, 11.50s/it]
2022-04-10 17:49:19,432 - INFO - tqdm - accuracy: 0.9866, batch_loss: 0.1148, loss: 0.0430 ||:  54%|#####3    | 35/65 [06:44<05:45, 11.51s/it]
2022-04-10 17:49:30,961 - INFO - tqdm - accuracy: 0.9870, batch_loss: 0.0437, loss: 0.0430 ||:  55%|#####5    | 36/65 [06:56<05:33, 11.52s/it]
2022-04-10 17:49:42,525 - INFO - tqdm - accuracy: 0.9873, batch_loss: 0.0029, loss: 0.0420 ||:  57%|#####6    | 37/65 [07:07<05:22, 11.53s/it]
2022-04-10 17:49:54,103 - INFO - tqdm - accuracy: 0.9868, batch_loss: 0.0923, loss: 0.0433 ||:  58%|#####8    | 38/65 [07:19<05:11, 11.54s/it]
2022-04-10 17:50:05,670 - INFO - tqdm - accuracy: 0.9864, batch_loss: 0.0853, loss: 0.0444 ||:  60%|######    | 39/65 [07:30<05:00, 11.55s/it]
2022-04-10 17:50:17,246 - INFO - tqdm - accuracy: 0.9867, batch_loss: 0.0171, loss: 0.0437 ||:  62%|######1   | 40/65 [07:42<04:48, 11.56s/it]
2022-04-10 17:50:28,857 - INFO - tqdm - accuracy: 0.9863, batch_loss: 0.0360, loss: 0.0435 ||:  63%|######3   | 41/65 [07:53<04:37, 11.57s/it]
2022-04-10 17:50:40,456 - INFO - tqdm - accuracy: 0.9866, batch_loss: 0.0084, loss: 0.0427 ||:  65%|######4   | 42/65 [08:05<04:26, 11.58s/it]
2022-04-10 17:50:52,071 - INFO - tqdm - accuracy: 0.9869, batch_loss: 0.0196, loss: 0.0421 ||:  66%|######6   | 43/65 [08:17<04:15, 11.59s/it]
2022-04-10 17:51:03,690 - INFO - tqdm - accuracy: 0.9872, batch_loss: 0.0303, loss: 0.0418 ||:  68%|######7   | 44/65 [08:28<04:03, 11.60s/it]
2022-04-10 17:51:15,306 - INFO - tqdm - accuracy: 0.9875, batch_loss: 0.0104, loss: 0.0411 ||:  69%|######9   | 45/65 [08:40<03:52, 11.60s/it]
2022-04-10 17:51:26,647 - INFO - tqdm - accuracy: 0.9878, batch_loss: 0.0066, loss: 0.0404 ||:  71%|#######   | 46/65 [08:51<03:38, 11.53s/it]
2022-04-10 17:51:38,258 - INFO - tqdm - accuracy: 0.9880, batch_loss: 0.0111, loss: 0.0398 ||:  72%|#######2  | 47/65 [09:03<03:27, 11.55s/it]
2022-04-10 17:51:49,839 - INFO - tqdm - accuracy: 0.9883, batch_loss: 0.0043, loss: 0.0390 ||:  74%|#######3  | 48/65 [09:14<03:16, 11.56s/it]
2022-04-10 17:52:01,411 - INFO - tqdm - accuracy: 0.9879, batch_loss: 0.0386, loss: 0.0390 ||:  75%|#######5  | 49/65 [09:26<03:05, 11.56s/it]
2022-04-10 17:52:12,860 - INFO - tqdm - accuracy: 0.9875, batch_loss: 0.0548, loss: 0.0393 ||:  77%|#######6  | 50/65 [09:37<02:52, 11.53s/it]
2022-04-10 17:52:24,429 - INFO - tqdm - accuracy: 0.9877, batch_loss: 0.0190, loss: 0.0389 ||:  78%|#######8  | 51/65 [09:49<02:41, 11.54s/it]
2022-04-10 17:52:35,986 - INFO - tqdm - accuracy: 0.9874, batch_loss: 0.0646, loss: 0.0394 ||:  80%|########  | 52/65 [10:01<02:30, 11.55s/it]
2022-04-10 17:52:47,531 - INFO - tqdm - accuracy: 0.9870, batch_loss: 0.0916, loss: 0.0404 ||:  82%|########1 | 53/65 [10:12<02:18, 11.55s/it]
2022-04-10 17:52:59,061 - INFO - tqdm - accuracy: 0.9873, batch_loss: 0.0046, loss: 0.0398 ||:  83%|########3 | 54/65 [10:24<02:06, 11.54s/it]
2022-04-10 17:53:11,199 - INFO - tqdm - accuracy: 0.9858, batch_loss: 0.1361, loss: 0.0415 ||:  85%|########4 | 55/65 [10:36<01:57, 11.72s/it]
2022-04-10 17:53:22,689 - INFO - tqdm - accuracy: 0.9860, batch_loss: 0.0392, loss: 0.0415 ||:  86%|########6 | 56/65 [10:47<01:44, 11.65s/it]
2022-04-10 17:53:34,229 - INFO - tqdm - accuracy: 0.9857, batch_loss: 0.1255, loss: 0.0429 ||:  88%|########7 | 57/65 [10:59<01:32, 11.62s/it]
2022-04-10 17:53:45,788 - INFO - tqdm - accuracy: 0.9860, batch_loss: 0.0080, loss: 0.0423 ||:  89%|########9 | 58/65 [11:10<01:21, 11.60s/it]
2022-04-10 17:53:57,368 - INFO - tqdm - accuracy: 0.9862, batch_loss: 0.0055, loss: 0.0417 ||:  91%|######### | 59/65 [11:22<01:09, 11.59s/it]
2022-04-10 17:54:08,934 - INFO - tqdm - accuracy: 0.9865, batch_loss: 0.0052, loss: 0.0411 ||:  92%|#########2| 60/65 [11:34<00:57, 11.59s/it]
2022-04-10 17:54:20,544 - INFO - tqdm - accuracy: 0.9867, batch_loss: 0.0171, loss: 0.0407 ||:  94%|#########3| 61/65 [11:45<00:46, 11.59s/it]
2022-04-10 17:54:32,156 - INFO - tqdm - accuracy: 0.9869, batch_loss: 0.0124, loss: 0.0403 ||:  95%|#########5| 62/65 [11:57<00:34, 11.60s/it]
2022-04-10 17:54:43,774 - INFO - tqdm - accuracy: 0.9871, batch_loss: 0.0098, loss: 0.0398 ||:  97%|#########6| 63/65 [12:08<00:23, 11.60s/it]
2022-04-10 17:54:55,362 - INFO - tqdm - accuracy: 0.9873, batch_loss: 0.0170, loss: 0.0394 ||:  98%|#########8| 64/65 [12:20<00:11, 11.60s/it]
2022-04-10 17:55:00,497 - INFO - tqdm - accuracy: 0.9864, batch_loss: 0.1653, loss: 0.0414 ||: 100%|##########| 65/65 [12:25<00:00,  9.66s/it]
2022-04-10 17:55:00,497 - INFO - tqdm - accuracy: 0.9864, batch_loss: 0.1653, loss: 0.0414 ||: 100%|##########| 65/65 [12:25<00:00, 11.47s/it]
2022-04-10 17:55:03,557 - INFO - allennlp.training.trainer - Validating
2022-04-10 17:55:03,559 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-10 17:55:13,585 - INFO - tqdm - accuracy: 0.7215, batch_loss: 0.0764, loss: 1.3813 ||:  35%|###4      | 40/115 [00:10<00:19,  3.93it/s]
2022-04-10 17:55:23,835 - INFO - tqdm - accuracy: 0.6352, batch_loss: 1.6312, loss: 2.0036 ||:  70%|######9   | 80/115 [00:20<00:09,  3.87it/s]
2022-04-10 17:55:32,832 - INFO - tqdm - accuracy: 0.6288, batch_loss: 0.2311, loss: 1.9191 ||: 100%|##########| 115/115 [00:29<00:00,  3.86it/s]
2022-04-10 17:55:32,832 - INFO - tqdm - accuracy: 0.6288, batch_loss: 0.2311, loss: 1.9191 ||: 100%|##########| 115/115 [00:29<00:00,  3.93it/s]
2022-04-10 17:55:32,832 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-10 17:55:32,833 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.986  |     0.629
2022-04-10 17:55:32,833 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-10 17:55:32,834 - INFO - allennlp.training.tensorboard_writer - loss               |     0.041  |     1.919
2022-04-10 17:55:32,835 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5804.914  |       N/A
2022-04-10 17:55:39,077 - INFO - allennlp.training.trainer - Epoch duration: 0:13:04.156355
2022-04-10 17:55:39,078 - INFO - allennlp.training.trainer - Estimated training time remaining: 0:13:06
2022-04-10 17:55:39,078 - INFO - allennlp.training.trainer - Epoch 14/14
2022-04-10 17:55:39,078 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-10 17:55:39,078 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-10 17:55:39,080 - INFO - allennlp.training.trainer - Training
2022-04-10 17:55:39,080 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-10 17:55:50,437 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0050, loss: 0.0050 ||:   2%|1         | 1/65 [00:11<12:06, 11.36s/it]
2022-04-10 17:56:02,058 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0210, loss: 0.0130 ||:   3%|3         | 2/65 [00:22<12:05, 11.51s/it]
2022-04-10 17:56:13,873 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0132, loss: 0.0131 ||:   5%|4         | 3/65 [00:34<12:02, 11.65s/it]
2022-04-10 17:56:25,498 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0164, loss: 0.0139 ||:   6%|6         | 4/65 [00:46<11:50, 11.64s/it]
2022-04-10 17:56:36,981 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0076, loss: 0.0126 ||:   8%|7         | 5/65 [00:57<11:35, 11.58s/it]
2022-04-10 17:56:48,513 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0047, loss: 0.0113 ||:   9%|9         | 6/65 [01:09<11:22, 11.57s/it]
2022-04-10 17:57:00,037 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0442, loss: 0.0160 ||:  11%|#         | 7/65 [01:20<11:10, 11.55s/it]
2022-04-10 17:57:11,567 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0089, loss: 0.0151 ||:  12%|#2        | 8/65 [01:32<10:58, 11.55s/it]
2022-04-10 17:57:23,103 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0037, loss: 0.0139 ||:  14%|#3        | 9/65 [01:44<10:46, 11.54s/it]
2022-04-10 17:57:34,627 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0115, loss: 0.0136 ||:  15%|#5        | 10/65 [01:55<10:34, 11.54s/it]
2022-04-10 17:57:46,125 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0075, loss: 0.0131 ||:  17%|#6        | 11/65 [02:07<10:22, 11.52s/it]
2022-04-10 17:57:57,645 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0088, loss: 0.0127 ||:  18%|#8        | 12/65 [02:18<10:10, 11.52s/it]
2022-04-10 17:58:09,169 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0194, loss: 0.0132 ||:  20%|##        | 13/65 [02:30<09:59, 11.52s/it]
2022-04-10 17:58:20,712 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0147, loss: 0.0133 ||:  22%|##1       | 14/65 [02:41<09:48, 11.53s/it]
2022-04-10 17:58:32,263 - INFO - tqdm - accuracy: 0.9979, batch_loss: 0.0309, loss: 0.0145 ||:  23%|##3       | 15/65 [02:53<09:36, 11.54s/it]
2022-04-10 17:58:43,809 - INFO - tqdm - accuracy: 0.9980, batch_loss: 0.0138, loss: 0.0145 ||:  25%|##4       | 16/65 [03:04<09:25, 11.54s/it]
2022-04-10 17:58:55,380 - INFO - tqdm - accuracy: 0.9982, batch_loss: 0.0299, loss: 0.0154 ||:  26%|##6       | 17/65 [03:16<09:14, 11.55s/it]
2022-04-10 17:59:06,927 - INFO - tqdm - accuracy: 0.9983, batch_loss: 0.0040, loss: 0.0147 ||:  28%|##7       | 18/65 [03:27<09:02, 11.55s/it]
2022-04-10 17:59:18,491 - INFO - tqdm - accuracy: 0.9984, batch_loss: 0.0026, loss: 0.0141 ||:  29%|##9       | 19/65 [03:39<08:51, 11.55s/it]
2022-04-10 17:59:30,055 - INFO - tqdm - accuracy: 0.9984, batch_loss: 0.0039, loss: 0.0136 ||:  31%|###       | 20/65 [03:50<08:40, 11.56s/it]
2022-04-10 17:59:41,615 - INFO - tqdm - accuracy: 0.9985, batch_loss: 0.0240, loss: 0.0141 ||:  32%|###2      | 21/65 [04:02<08:28, 11.56s/it]
2022-04-10 17:59:53,029 - INFO - tqdm - accuracy: 0.9986, batch_loss: 0.0238, loss: 0.0145 ||:  34%|###3      | 22/65 [04:13<08:15, 11.51s/it]
2022-04-10 18:00:04,583 - INFO - tqdm - accuracy: 0.9986, batch_loss: 0.0118, loss: 0.0144 ||:  35%|###5      | 23/65 [04:25<08:04, 11.53s/it]
2022-04-10 18:00:16,140 - INFO - tqdm - accuracy: 0.9974, batch_loss: 0.0404, loss: 0.0155 ||:  37%|###6      | 24/65 [04:37<07:52, 11.54s/it]
2022-04-10 18:00:27,676 - INFO - tqdm - accuracy: 0.9962, batch_loss: 0.0378, loss: 0.0164 ||:  38%|###8      | 25/65 [04:48<07:41, 11.54s/it]
2022-04-10 18:00:39,228 - INFO - tqdm - accuracy: 0.9964, batch_loss: 0.0038, loss: 0.0159 ||:  40%|####      | 26/65 [05:00<07:30, 11.54s/it]
2022-04-10 18:00:50,762 - INFO - tqdm - accuracy: 0.9954, batch_loss: 0.0451, loss: 0.0170 ||:  42%|####1     | 27/65 [05:11<07:18, 11.54s/it]
2022-04-10 18:01:02,310 - INFO - tqdm - accuracy: 0.9955, batch_loss: 0.0062, loss: 0.0166 ||:  43%|####3     | 28/65 [05:23<07:07, 11.54s/it]
2022-04-10 18:01:13,843 - INFO - tqdm - accuracy: 0.9957, batch_loss: 0.0043, loss: 0.0162 ||:  45%|####4     | 29/65 [05:34<06:55, 11.54s/it]
2022-04-10 18:01:25,389 - INFO - tqdm - accuracy: 0.9958, batch_loss: 0.0197, loss: 0.0163 ||:  46%|####6     | 30/65 [05:46<06:43, 11.54s/it]
2022-04-10 18:01:36,947 - INFO - tqdm - accuracy: 0.9960, batch_loss: 0.0175, loss: 0.0163 ||:  48%|####7     | 31/65 [05:57<06:32, 11.55s/it]
2022-04-10 18:01:48,492 - INFO - tqdm - accuracy: 0.9961, batch_loss: 0.0137, loss: 0.0162 ||:  49%|####9     | 32/65 [06:09<06:21, 11.55s/it]
2022-04-10 18:02:00,045 - INFO - tqdm - accuracy: 0.9962, batch_loss: 0.0105, loss: 0.0161 ||:  51%|#####     | 33/65 [06:20<06:09, 11.55s/it]
2022-04-10 18:02:11,584 - INFO - tqdm - accuracy: 0.9963, batch_loss: 0.0028, loss: 0.0157 ||:  52%|#####2    | 34/65 [06:32<05:57, 11.55s/it]
2022-04-10 18:02:23,115 - INFO - tqdm - accuracy: 0.9964, batch_loss: 0.0245, loss: 0.0159 ||:  54%|#####3    | 35/65 [06:44<05:46, 11.54s/it]
2022-04-10 18:02:34,642 - INFO - tqdm - accuracy: 0.9965, batch_loss: 0.0204, loss: 0.0161 ||:  55%|#####5    | 36/65 [06:55<05:34, 11.54s/it]
2022-04-10 18:02:46,183 - INFO - tqdm - accuracy: 0.9958, batch_loss: 0.0412, loss: 0.0167 ||:  57%|#####6    | 37/65 [07:07<05:23, 11.54s/it]
2022-04-10 18:02:57,724 - INFO - tqdm - accuracy: 0.9959, batch_loss: 0.0080, loss: 0.0165 ||:  58%|#####8    | 38/65 [07:18<05:11, 11.54s/it]
2022-04-10 18:03:09,251 - INFO - tqdm - accuracy: 0.9960, batch_loss: 0.0051, loss: 0.0162 ||:  60%|######    | 39/65 [07:30<04:59, 11.54s/it]
2022-04-10 18:03:20,776 - INFO - tqdm - accuracy: 0.9961, batch_loss: 0.0099, loss: 0.0161 ||:  62%|######1   | 40/65 [07:41<04:48, 11.53s/it]
2022-04-10 18:03:32,337 - INFO - tqdm - accuracy: 0.9962, batch_loss: 0.0135, loss: 0.0160 ||:  63%|######3   | 41/65 [07:53<04:36, 11.54s/it]
2022-04-10 18:03:43,888 - INFO - tqdm - accuracy: 0.9955, batch_loss: 0.0871, loss: 0.0177 ||:  65%|######4   | 42/65 [08:04<04:25, 11.54s/it]
2022-04-10 18:03:55,446 - INFO - tqdm - accuracy: 0.9956, batch_loss: 0.0158, loss: 0.0176 ||:  66%|######6   | 43/65 [08:16<04:14, 11.55s/it]
2022-04-10 18:04:07,026 - INFO - tqdm - accuracy: 0.9957, batch_loss: 0.0048, loss: 0.0173 ||:  68%|######7   | 44/65 [08:27<04:02, 11.56s/it]
2022-04-10 18:04:18,602 - INFO - tqdm - accuracy: 0.9958, batch_loss: 0.0142, loss: 0.0173 ||:  69%|######9   | 45/65 [08:39<03:51, 11.56s/it]
2022-04-10 18:04:30,199 - INFO - tqdm - accuracy: 0.9959, batch_loss: 0.0064, loss: 0.0170 ||:  71%|#######   | 46/65 [08:51<03:39, 11.57s/it]
2022-04-10 18:04:41,811 - INFO - tqdm - accuracy: 0.9960, batch_loss: 0.0211, loss: 0.0171 ||:  72%|#######2  | 47/65 [09:02<03:28, 11.58s/it]
2022-04-10 18:04:53,401 - INFO - tqdm - accuracy: 0.9954, batch_loss: 0.0714, loss: 0.0183 ||:  74%|#######3  | 48/65 [09:14<03:16, 11.59s/it]
2022-04-10 18:05:04,997 - INFO - tqdm - accuracy: 0.9955, batch_loss: 0.0055, loss: 0.0180 ||:  75%|#######5  | 49/65 [09:25<03:05, 11.59s/it]
2022-04-10 18:05:16,613 - INFO - tqdm - accuracy: 0.9956, batch_loss: 0.0115, loss: 0.0179 ||:  77%|#######6  | 50/65 [09:37<02:53, 11.60s/it]
2022-04-10 18:05:28,199 - INFO - tqdm - accuracy: 0.9957, batch_loss: 0.0073, loss: 0.0177 ||:  78%|#######8  | 51/65 [09:49<02:42, 11.59s/it]
2022-04-10 18:05:39,312 - INFO - tqdm - accuracy: 0.9958, batch_loss: 0.0186, loss: 0.0177 ||:  80%|########  | 52/65 [10:00<02:28, 11.45s/it]
2022-04-10 18:05:50,865 - INFO - tqdm - accuracy: 0.9959, batch_loss: 0.0165, loss: 0.0177 ||:  82%|########1 | 53/65 [10:11<02:17, 11.48s/it]
2022-04-10 18:06:02,442 - INFO - tqdm - accuracy: 0.9959, batch_loss: 0.0053, loss: 0.0174 ||:  83%|########3 | 54/65 [10:23<02:06, 11.51s/it]
2022-04-10 18:06:14,005 - INFO - tqdm - accuracy: 0.9960, batch_loss: 0.0045, loss: 0.0172 ||:  85%|########4 | 55/65 [10:34<01:55, 11.53s/it]
2022-04-10 18:06:25,309 - INFO - tqdm - accuracy: 0.9961, batch_loss: 0.0102, loss: 0.0171 ||:  86%|########6 | 56/65 [10:46<01:43, 11.46s/it]
2022-04-10 18:06:36,807 - INFO - tqdm - accuracy: 0.9956, batch_loss: 0.0312, loss: 0.0173 ||:  88%|########7 | 57/65 [10:57<01:31, 11.47s/it]
2022-04-10 18:06:48,346 - INFO - tqdm - accuracy: 0.9951, batch_loss: 0.0440, loss: 0.0178 ||:  89%|########9 | 58/65 [11:09<01:20, 11.49s/it]
2022-04-10 18:06:59,883 - INFO - tqdm - accuracy: 0.9952, batch_loss: 0.0066, loss: 0.0176 ||:  91%|######### | 59/65 [11:20<01:09, 11.50s/it]
2022-04-10 18:07:11,403 - INFO - tqdm - accuracy: 0.9953, batch_loss: 0.0221, loss: 0.0177 ||:  92%|#########2| 60/65 [11:32<00:57, 11.51s/it]
2022-04-10 18:07:22,921 - INFO - tqdm - accuracy: 0.9954, batch_loss: 0.0274, loss: 0.0178 ||:  94%|#########3| 61/65 [11:43<00:46, 11.51s/it]
2022-04-10 18:07:34,455 - INFO - tqdm - accuracy: 0.9955, batch_loss: 0.0038, loss: 0.0176 ||:  95%|#########5| 62/65 [11:55<00:34, 11.52s/it]
2022-04-10 18:07:45,969 - INFO - tqdm - accuracy: 0.9955, batch_loss: 0.0085, loss: 0.0174 ||:  97%|#########6| 63/65 [12:06<00:23, 11.52s/it]
2022-04-10 18:07:57,478 - INFO - tqdm - accuracy: 0.9956, batch_loss: 0.0028, loss: 0.0172 ||:  98%|#########8| 64/65 [12:18<00:11, 11.51s/it]
2022-04-10 18:08:02,583 - INFO - tqdm - accuracy: 0.9956, batch_loss: 0.0392, loss: 0.0176 ||: 100%|##########| 65/65 [12:23<00:00,  9.59s/it]
2022-04-10 18:08:02,584 - INFO - tqdm - accuracy: 0.9956, batch_loss: 0.0392, loss: 0.0176 ||: 100%|##########| 65/65 [12:23<00:00, 11.44s/it]
2022-04-10 18:08:05,469 - INFO - allennlp.training.trainer - Validating
2022-04-10 18:08:05,471 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-10 18:08:15,563 - INFO - tqdm - accuracy: 0.6375, batch_loss: 3.4393, loss: 1.6670 ||:  35%|###4      | 40/115 [00:10<00:18,  3.96it/s]
2022-04-10 18:08:25,796 - INFO - tqdm - accuracy: 0.6625, batch_loss: 4.3265, loss: 1.7913 ||:  70%|######9   | 80/115 [00:20<00:09,  3.87it/s]
2022-04-10 18:08:34,696 - INFO - tqdm - accuracy: 0.6376, batch_loss: 3.2426, loss: 1.9322 ||: 100%|##########| 115/115 [00:29<00:00,  3.82it/s]
2022-04-10 18:08:34,696 - INFO - tqdm - accuracy: 0.6376, batch_loss: 3.2426, loss: 1.9322 ||: 100%|##########| 115/115 [00:29<00:00,  3.93it/s]
2022-04-10 18:08:34,696 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-10 18:08:34,697 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.996  |     0.638
2022-04-10 18:08:34,698 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-10 18:08:34,699 - INFO - allennlp.training.tensorboard_writer - loss               |     0.018  |     1.932
2022-04-10 18:08:34,699 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5804.914  |       N/A
2022-04-10 18:08:40,998 - INFO - allennlp.training.trainer - Epoch duration: 0:13:01.920617
2022-04-10 18:08:40,999 - INFO - allennlp.training.checkpointer - loading best weights
2022-04-10 18:08:42,009 - INFO - allennlp.common.util - Metrics: {
  "best_epoch": 8,
  "peak_worker_0_memory_MB": 5804.9140625,
  "peak_gpu_0_memory_MB": 9580.5712890625,
  "training_duration": "3:16:29.081795",
  "training_start_epoch": 0,
  "training_epochs": 14,
  "epoch": 14,
  "training_accuracy": 0.9956331877729258,
  "training_loss": 0.01755571285111028,
  "training_worker_0_memory_MB": 5804.9140625,
  "training_gpu_0_memory_MB": 9580.5712890625,
  "validation_accuracy": 0.6375545851528385,
  "validation_loss": 1.9322430861701319,
  "best_validation_accuracy": 0.6637554585152838,
  "best_validation_loss": 1.566425569042685
}
2022-04-10 18:08:42,010 - INFO - allennlp.models.archival - archiving weights and vocabulary to ./output_ir-ora-d_hyper4/model.tar.gz
