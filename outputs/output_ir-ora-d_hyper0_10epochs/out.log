2022-04-09 14:57:11,943 - INFO - allennlp.common.params - random_seed = 42
2022-04-09 14:57:11,943 - INFO - allennlp.common.params - numpy_seed = 42
2022-04-09 14:57:11,943 - INFO - allennlp.common.params - pytorch_seed = 42
2022-04-09 14:57:12,616 - INFO - allennlp.common.checks - Pytorch version: 1.7.1
2022-04-09 14:57:12,616 - INFO - allennlp.common.params - type = default
2022-04-09 14:57:12,617 - INFO - allennlp.common.params - dataset_reader.type = strategy_qa_reader
2022-04-09 14:57:12,617 - INFO - allennlp.common.params - dataset_reader.lazy = False
2022-04-09 14:57:12,617 - INFO - allennlp.common.params - dataset_reader.cache_directory = None
2022-04-09 14:57:12,617 - INFO - allennlp.common.params - dataset_reader.max_instances = None
2022-04-09 14:57:12,617 - INFO - allennlp.common.params - dataset_reader.manual_distributed_sharding = False
2022-04-09 14:57:12,617 - INFO - allennlp.common.params - dataset_reader.manual_multi_process_sharding = False
2022-04-09 14:57:12,618 - INFO - allennlp.common.params - dataset_reader.tokenizer_wrapper.type = default
2022-04-09 14:57:12,618 - INFO - allennlp.common.params - dataset_reader.tokenizer_wrapper.pretrained_model = roberta-large
2022-04-09 14:57:12,618 - INFO - allennlp.common.params - dataset_reader.tokenizer_wrapper.call_kwargs.return_offsets_mapping = True
2022-04-09 14:57:12,618 - INFO - allennlp.common.params - dataset_reader.tokenizer_wrapper.call_kwargs.return_special_tokens_mask = True
2022-04-09 14:57:12,618 - INFO - allennlp.common.params - dataset_reader.tokenizer_wrapper.call_kwargs.truncation = only_first
2022-04-09 14:57:13,806 - INFO - allennlp.common.params - dataset_reader.save_tokenizer = True
2022-04-09 14:57:13,806 - INFO - allennlp.common.params - dataset_reader.is_training = True
2022-04-09 14:57:13,806 - INFO - allennlp.common.params - dataset_reader.pickle.action = None
2022-04-09 14:57:13,806 - INFO - allennlp.common.params - dataset_reader.pickle.file_name = None
2022-04-09 14:57:13,806 - INFO - allennlp.common.params - dataset_reader.pickle.path = ../pickle
2022-04-09 14:57:13,807 - INFO - allennlp.common.params - dataset_reader.pickle.save_even_when_max_instances = False
2022-04-09 14:57:13,807 - INFO - allennlp.common.params - dataset_reader.paragraphs_source = IR-ORA-D
2022-04-09 14:57:13,807 - INFO - allennlp.common.params - dataset_reader.answer_last_decomposition_step = False
2022-04-09 14:57:13,807 - INFO - allennlp.common.params - dataset_reader.skip_if_context_missing = True
2022-04-09 14:57:13,807 - INFO - allennlp.common.params - dataset_reader.paragraphs_limit = 10
2022-04-09 14:57:13,807 - INFO - allennlp.common.params - dataset_reader.generated_decompositions_paths = None
2022-04-09 14:57:13,807 - INFO - allennlp.common.params - dataset_reader.save_elasticsearch_cache = True
2022-04-09 14:57:14,260 - INFO - filelock - Lock 140295502546256 acquired on cache.lock
2022-04-09 14:57:14,940 - INFO - filelock - Lock 140295502546256 released on cache.lock
2022-04-09 14:57:14,941 - INFO - allennlp.common.params - train_data_path = data/strategyqa/train.json
2022-04-09 14:57:14,942 - INFO - allennlp.common.params - vocabulary = <allennlp.common.lazy.Lazy object at 0x7f99225a4050>
2022-04-09 14:57:14,943 - INFO - allennlp.common.params - datasets_for_vocab_creation = None
2022-04-09 14:57:14,943 - INFO - allennlp.common.params - validation_dataset_reader.type = strategy_qa_reader
2022-04-09 14:57:14,943 - INFO - allennlp.common.params - validation_dataset_reader.lazy = False
2022-04-09 14:57:14,943 - INFO - allennlp.common.params - validation_dataset_reader.cache_directory = None
2022-04-09 14:57:14,943 - INFO - allennlp.common.params - validation_dataset_reader.max_instances = None
2022-04-09 14:57:14,943 - INFO - allennlp.common.params - validation_dataset_reader.manual_distributed_sharding = False
2022-04-09 14:57:14,944 - INFO - allennlp.common.params - validation_dataset_reader.manual_multi_process_sharding = False
2022-04-09 14:57:14,944 - INFO - allennlp.common.params - validation_dataset_reader.tokenizer_wrapper.type = default
2022-04-09 14:57:14,944 - INFO - allennlp.common.params - validation_dataset_reader.tokenizer_wrapper.pretrained_model = roberta-large
2022-04-09 14:57:14,944 - INFO - allennlp.common.params - validation_dataset_reader.tokenizer_wrapper.call_kwargs.return_offsets_mapping = True
2022-04-09 14:57:14,944 - INFO - allennlp.common.params - validation_dataset_reader.tokenizer_wrapper.call_kwargs.return_special_tokens_mask = True
2022-04-09 14:57:14,944 - INFO - allennlp.common.params - validation_dataset_reader.tokenizer_wrapper.call_kwargs.truncation = only_first
2022-04-09 14:57:16,129 - INFO - allennlp.common.params - validation_dataset_reader.save_tokenizer = False
2022-04-09 14:57:16,129 - INFO - allennlp.common.params - validation_dataset_reader.is_training = False
2022-04-09 14:57:16,129 - INFO - allennlp.common.params - validation_dataset_reader.pickle.action = None
2022-04-09 14:57:16,129 - INFO - allennlp.common.params - validation_dataset_reader.pickle.file_name = None
2022-04-09 14:57:16,129 - INFO - allennlp.common.params - validation_dataset_reader.pickle.path = ../pickle
2022-04-09 14:57:16,129 - INFO - allennlp.common.params - validation_dataset_reader.pickle.save_even_when_max_instances = False
2022-04-09 14:57:16,130 - INFO - allennlp.common.params - validation_dataset_reader.paragraphs_source = IR-ORA-D
2022-04-09 14:57:16,130 - INFO - allennlp.common.params - validation_dataset_reader.answer_last_decomposition_step = False
2022-04-09 14:57:16,130 - INFO - allennlp.common.params - validation_dataset_reader.skip_if_context_missing = True
2022-04-09 14:57:16,130 - INFO - allennlp.common.params - validation_dataset_reader.paragraphs_limit = 10
2022-04-09 14:57:16,130 - INFO - allennlp.common.params - validation_dataset_reader.generated_decompositions_paths = None
2022-04-09 14:57:16,130 - INFO - allennlp.common.params - validation_dataset_reader.save_elasticsearch_cache = True
2022-04-09 14:57:16,130 - INFO - filelock - Lock 140295681637904 acquired on cache.lock
2022-04-09 14:57:16,955 - INFO - filelock - Lock 140295681637904 released on cache.lock
2022-04-09 14:57:16,955 - INFO - allennlp.common.params - validation_data_path = data/strategyqa/dev.json
2022-04-09 14:57:16,956 - INFO - allennlp.common.params - validation_data_loader = None
2022-04-09 14:57:16,956 - INFO - allennlp.common.params - test_data_path = None
2022-04-09 14:57:16,956 - INFO - allennlp.common.params - evaluate_on_test = True
2022-04-09 14:57:16,956 - INFO - allennlp.common.params - batch_weight_key = 
2022-04-09 14:57:16,956 - INFO - allennlp.training.util - Reading training data from data/strategyqa/train.json
2022-04-09 14:57:16,957 - INFO - tqdm - reading instances: 0it [00:00, ?it/s]
2022-04-09 14:57:16,957 - INFO - src.data.dataset_readers.base.base_dataset_reader - Reading the dataset from scratch
2022-04-09 14:57:16,957 - INFO - src.data.dataset_readers.strategy_qa_reader - Reading the dataset:
2022-04-09 14:57:16,957 - INFO - src.data.dataset_readers.strategy_qa_reader - Reading file at data/strategyqa/train.json
2022-04-09 14:57:24,112 - INFO - filelock - Lock 140294860269456 acquired on cache.lock
2022-04-09 14:57:24,903 - INFO - filelock - Lock 140294860269456 released on cache.lock
2022-04-09 14:57:24,938 - INFO - allennlp.training.util - Reading validation data from data/strategyqa/dev.json
2022-04-09 14:57:24,939 - INFO - tqdm - reading instances: 0it [00:00, ?it/s]
2022-04-09 14:57:24,939 - INFO - src.data.dataset_readers.base.base_dataset_reader - Reading the dataset from scratch
2022-04-09 14:57:24,939 - INFO - src.data.dataset_readers.strategy_qa_reader - Reading the dataset:
2022-04-09 14:57:24,939 - INFO - src.data.dataset_readers.strategy_qa_reader - Reading file at data/strategyqa/dev.json
2022-04-09 14:57:25,375 - INFO - src.data.dataset_readers.utils.elasticsearch_utils - Empty results from Elasticsearch for: actions commercial pilots take concerning engine noise arriving departing Orange County, California?
2022-04-09 14:57:25,689 - INFO - filelock - Lock 140294867690896 acquired on cache.lock
2022-04-09 14:57:26,450 - INFO - filelock - Lock 140294867690896 released on cache.lock
2022-04-09 14:57:26,485 - INFO - allennlp.common.params - type = from_instances
2022-04-09 14:57:26,485 - INFO - allennlp.common.params - min_count = None
2022-04-09 14:57:26,485 - INFO - allennlp.common.params - max_vocab_size = None
2022-04-09 14:57:26,485 - INFO - allennlp.common.params - non_padded_namespaces = ('*tags', '*labels')
2022-04-09 14:57:26,485 - INFO - allennlp.common.params - pretrained_files = None
2022-04-09 14:57:26,485 - INFO - allennlp.common.params - only_include_pretrained_words = False
2022-04-09 14:57:26,485 - INFO - allennlp.common.params - tokens_to_add = None
2022-04-09 14:57:26,485 - INFO - allennlp.common.params - min_pretrained_embeddings = None
2022-04-09 14:57:26,485 - INFO - allennlp.common.params - padding_token = @@PADDING@@
2022-04-09 14:57:26,486 - INFO - allennlp.common.params - oov_token = @@UNKNOWN@@
2022-04-09 14:57:26,486 - INFO - allennlp.data.vocabulary - Fitting token dictionary from dataset.
2022-04-09 14:57:26,486 - INFO - tqdm - building vocab: 0it [00:00, ?it/s]
2022-04-09 14:57:26,491 - INFO - allennlp.common.params - model.type = hf_classifier
2022-04-09 14:57:26,492 - INFO - allennlp.common.params - model.regularizer = None
2022-04-09 14:57:26,492 - INFO - allennlp.common.params - model.pretrained_model = roberta-large
2022-04-09 14:57:26,492 - INFO - allennlp.common.params - model.tokenizer_wrapper.type = default
2022-04-09 14:57:26,492 - INFO - allennlp.common.params - model.tokenizer_wrapper.pretrained_model = roberta-large
2022-04-09 14:57:26,492 - INFO - allennlp.common.params - model.tokenizer_wrapper.call_kwargs.return_offsets_mapping = True
2022-04-09 14:57:26,492 - INFO - allennlp.common.params - model.tokenizer_wrapper.call_kwargs.return_special_tokens_mask = True
2022-04-09 14:57:26,492 - INFO - allennlp.common.params - model.tokenizer_wrapper.call_kwargs.truncation = only_first
2022-04-09 14:57:27,664 - INFO - allennlp.common.params - model.num_labels = 2
2022-04-09 14:57:27,664 - INFO - allennlp.common.params - model.label_namespace = labels
2022-04-09 14:57:27,664 - INFO - allennlp.common.params - model.transformer_weights_path = None
2022-04-09 14:57:27,665 - INFO - allennlp.common.params - model.initializer.regexes.0.1.type = pretrained
2022-04-09 14:57:27,665 - INFO - allennlp.common.params - model.initializer.regexes.0.1.weights_file_path = output_roberta-star/model.tar.gz
2022-04-09 14:57:27,667 - INFO - allennlp.models.archival - extracting archive file output_roberta-star/model.tar.gz to temp dir /tmp/tmppjug2brq
2022-04-09 14:57:39,573 - INFO - allennlp.models.archival - removing temporary unarchived model dir at /tmp/tmppjug2brq
2022-04-09 14:57:39,745 - INFO - allennlp.common.params - model.initializer.prevent_regexes = None
2022-04-09 14:58:03,298 - INFO - allennlp.nn.initializers - Initializing parameters
2022-04-09 14:58:03,298 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.embeddings.word_embeddings.weight using .* initializer
2022-04-09 14:58:03,307 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.embeddings.position_embeddings.weight using .* initializer
2022-04-09 14:58:03,308 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.embeddings.token_type_embeddings.weight using .* initializer
2022-04-09 14:58:03,308 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.embeddings.LayerNorm.weight using .* initializer
2022-04-09 14:58:03,308 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.embeddings.LayerNorm.bias using .* initializer
2022-04-09 14:58:03,308 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.self.query.weight using .* initializer
2022-04-09 14:58:03,309 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.self.query.bias using .* initializer
2022-04-09 14:58:03,309 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.self.key.weight using .* initializer
2022-04-09 14:58:03,309 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.self.key.bias using .* initializer
2022-04-09 14:58:03,309 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.self.value.weight using .* initializer
2022-04-09 14:58:03,310 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.self.value.bias using .* initializer
2022-04-09 14:58:03,310 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.output.dense.weight using .* initializer
2022-04-09 14:58:03,310 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.output.dense.bias using .* initializer
2022-04-09 14:58:03,311 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.output.LayerNorm.weight using .* initializer
2022-04-09 14:58:03,311 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.output.LayerNorm.bias using .* initializer
2022-04-09 14:58:03,311 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.intermediate.dense.weight using .* initializer
2022-04-09 14:58:03,312 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.intermediate.dense.bias using .* initializer
2022-04-09 14:58:03,312 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.output.dense.weight using .* initializer
2022-04-09 14:58:03,313 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.output.dense.bias using .* initializer
2022-04-09 14:58:03,313 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.output.LayerNorm.weight using .* initializer
2022-04-09 14:58:03,313 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.output.LayerNorm.bias using .* initializer
2022-04-09 14:58:03,314 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.self.query.weight using .* initializer
2022-04-09 14:58:03,314 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.self.query.bias using .* initializer
2022-04-09 14:58:03,314 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.self.key.weight using .* initializer
2022-04-09 14:58:03,315 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.self.key.bias using .* initializer
2022-04-09 14:58:03,315 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.self.value.weight using .* initializer
2022-04-09 14:58:03,315 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.self.value.bias using .* initializer
2022-04-09 14:58:03,316 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.output.dense.weight using .* initializer
2022-04-09 14:58:03,316 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.output.dense.bias using .* initializer
2022-04-09 14:58:03,316 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.output.LayerNorm.weight using .* initializer
2022-04-09 14:58:03,316 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.output.LayerNorm.bias using .* initializer
2022-04-09 14:58:03,316 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.intermediate.dense.weight using .* initializer
2022-04-09 14:58:03,317 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.intermediate.dense.bias using .* initializer
2022-04-09 14:58:03,318 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.output.dense.weight using .* initializer
2022-04-09 14:58:03,319 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.output.dense.bias using .* initializer
2022-04-09 14:58:03,319 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.output.LayerNorm.weight using .* initializer
2022-04-09 14:58:03,319 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.output.LayerNorm.bias using .* initializer
2022-04-09 14:58:03,319 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.self.query.weight using .* initializer
2022-04-09 14:58:03,320 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.self.query.bias using .* initializer
2022-04-09 14:58:03,320 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.self.key.weight using .* initializer
2022-04-09 14:58:03,320 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.self.key.bias using .* initializer
2022-04-09 14:58:03,320 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.self.value.weight using .* initializer
2022-04-09 14:58:03,321 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.self.value.bias using .* initializer
2022-04-09 14:58:03,321 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.output.dense.weight using .* initializer
2022-04-09 14:58:03,321 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.output.dense.bias using .* initializer
2022-04-09 14:58:03,322 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.output.LayerNorm.weight using .* initializer
2022-04-09 14:58:03,322 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.output.LayerNorm.bias using .* initializer
2022-04-09 14:58:03,322 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.intermediate.dense.weight using .* initializer
2022-04-09 14:58:03,323 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.intermediate.dense.bias using .* initializer
2022-04-09 14:58:03,323 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.output.dense.weight using .* initializer
2022-04-09 14:58:03,324 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.output.dense.bias using .* initializer
2022-04-09 14:58:03,324 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.output.LayerNorm.weight using .* initializer
2022-04-09 14:58:03,324 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.output.LayerNorm.bias using .* initializer
2022-04-09 14:58:03,325 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.self.query.weight using .* initializer
2022-04-09 14:58:03,325 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.self.query.bias using .* initializer
2022-04-09 14:58:03,325 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.self.key.weight using .* initializer
2022-04-09 14:58:03,326 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.self.key.bias using .* initializer
2022-04-09 14:58:03,326 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.self.value.weight using .* initializer
2022-04-09 14:58:03,326 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.self.value.bias using .* initializer
2022-04-09 14:58:03,326 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.output.dense.weight using .* initializer
2022-04-09 14:58:03,327 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.output.dense.bias using .* initializer
2022-04-09 14:58:03,327 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.output.LayerNorm.weight using .* initializer
2022-04-09 14:58:03,327 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.output.LayerNorm.bias using .* initializer
2022-04-09 14:58:03,327 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.intermediate.dense.weight using .* initializer
2022-04-09 14:58:03,328 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.intermediate.dense.bias using .* initializer
2022-04-09 14:58:03,328 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.output.dense.weight using .* initializer
2022-04-09 14:58:03,329 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.output.dense.bias using .* initializer
2022-04-09 14:58:03,330 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.output.LayerNorm.weight using .* initializer
2022-04-09 14:58:03,330 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.output.LayerNorm.bias using .* initializer
2022-04-09 14:58:03,330 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.self.query.weight using .* initializer
2022-04-09 14:58:03,330 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.self.query.bias using .* initializer
2022-04-09 14:58:03,331 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.self.key.weight using .* initializer
2022-04-09 14:58:03,331 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.self.key.bias using .* initializer
2022-04-09 14:58:03,331 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.self.value.weight using .* initializer
2022-04-09 14:58:03,332 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.self.value.bias using .* initializer
2022-04-09 14:58:03,332 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.output.dense.weight using .* initializer
2022-04-09 14:58:03,332 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.output.dense.bias using .* initializer
2022-04-09 14:58:03,332 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.output.LayerNorm.weight using .* initializer
2022-04-09 14:58:03,333 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.output.LayerNorm.bias using .* initializer
2022-04-09 14:58:03,333 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.intermediate.dense.weight using .* initializer
2022-04-09 14:58:03,334 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.intermediate.dense.bias using .* initializer
2022-04-09 14:58:03,334 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.output.dense.weight using .* initializer
2022-04-09 14:58:03,335 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.output.dense.bias using .* initializer
2022-04-09 14:58:03,335 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.output.LayerNorm.weight using .* initializer
2022-04-09 14:58:03,335 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.output.LayerNorm.bias using .* initializer
2022-04-09 14:58:03,335 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.self.query.weight using .* initializer
2022-04-09 14:58:03,336 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.self.query.bias using .* initializer
2022-04-09 14:58:03,336 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.self.key.weight using .* initializer
2022-04-09 14:58:03,336 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.self.key.bias using .* initializer
2022-04-09 14:58:03,337 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.self.value.weight using .* initializer
2022-04-09 14:58:03,337 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.self.value.bias using .* initializer
2022-04-09 14:58:03,337 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.output.dense.weight using .* initializer
2022-04-09 14:58:03,338 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.output.dense.bias using .* initializer
2022-04-09 14:58:03,338 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.output.LayerNorm.weight using .* initializer
2022-04-09 14:58:03,338 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.output.LayerNorm.bias using .* initializer
2022-04-09 14:58:03,338 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.intermediate.dense.weight using .* initializer
2022-04-09 14:58:03,339 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.intermediate.dense.bias using .* initializer
2022-04-09 14:58:03,339 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.output.dense.weight using .* initializer
2022-04-09 14:58:03,340 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.output.dense.bias using .* initializer
2022-04-09 14:58:03,340 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.output.LayerNorm.weight using .* initializer
2022-04-09 14:58:03,341 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.output.LayerNorm.bias using .* initializer
2022-04-09 14:58:03,341 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.self.query.weight using .* initializer
2022-04-09 14:58:03,341 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.self.query.bias using .* initializer
2022-04-09 14:58:03,341 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.self.key.weight using .* initializer
2022-04-09 14:58:03,342 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.self.key.bias using .* initializer
2022-04-09 14:58:03,342 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.self.value.weight using .* initializer
2022-04-09 14:58:03,342 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.self.value.bias using .* initializer
2022-04-09 14:58:03,343 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.output.dense.weight using .* initializer
2022-04-09 14:58:03,343 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.output.dense.bias using .* initializer
2022-04-09 14:58:03,343 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.output.LayerNorm.weight using .* initializer
2022-04-09 14:58:03,343 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.output.LayerNorm.bias using .* initializer
2022-04-09 14:58:03,344 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.intermediate.dense.weight using .* initializer
2022-04-09 14:58:03,345 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.intermediate.dense.bias using .* initializer
2022-04-09 14:58:03,345 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.output.dense.weight using .* initializer
2022-04-09 14:58:03,346 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.output.dense.bias using .* initializer
2022-04-09 14:58:03,346 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.output.LayerNorm.weight using .* initializer
2022-04-09 14:58:03,346 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.output.LayerNorm.bias using .* initializer
2022-04-09 14:58:03,346 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.self.query.weight using .* initializer
2022-04-09 14:58:03,347 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.self.query.bias using .* initializer
2022-04-09 14:58:03,347 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.self.key.weight using .* initializer
2022-04-09 14:58:03,347 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.self.key.bias using .* initializer
2022-04-09 14:58:03,347 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.self.value.weight using .* initializer
2022-04-09 14:58:03,348 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.self.value.bias using .* initializer
2022-04-09 14:58:03,348 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.output.dense.weight using .* initializer
2022-04-09 14:58:03,348 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.output.dense.bias using .* initializer
2022-04-09 14:58:03,349 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.output.LayerNorm.weight using .* initializer
2022-04-09 14:58:03,349 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.output.LayerNorm.bias using .* initializer
2022-04-09 14:58:03,349 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.intermediate.dense.weight using .* initializer
2022-04-09 14:58:03,350 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.intermediate.dense.bias using .* initializer
2022-04-09 14:58:03,350 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.output.dense.weight using .* initializer
2022-04-09 14:58:03,351 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.output.dense.bias using .* initializer
2022-04-09 14:58:03,351 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.output.LayerNorm.weight using .* initializer
2022-04-09 14:58:03,351 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.output.LayerNorm.bias using .* initializer
2022-04-09 14:58:03,351 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.self.query.weight using .* initializer
2022-04-09 14:58:03,352 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.self.query.bias using .* initializer
2022-04-09 14:58:03,352 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.self.key.weight using .* initializer
2022-04-09 14:58:03,352 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.self.key.bias using .* initializer
2022-04-09 14:58:03,353 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.self.value.weight using .* initializer
2022-04-09 14:58:03,353 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.self.value.bias using .* initializer
2022-04-09 14:58:03,353 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.output.dense.weight using .* initializer
2022-04-09 14:58:03,354 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.output.dense.bias using .* initializer
2022-04-09 14:58:03,354 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.output.LayerNorm.weight using .* initializer
2022-04-09 14:58:03,354 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.output.LayerNorm.bias using .* initializer
2022-04-09 14:58:03,354 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.intermediate.dense.weight using .* initializer
2022-04-09 14:58:03,355 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.intermediate.dense.bias using .* initializer
2022-04-09 14:58:03,355 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.output.dense.weight using .* initializer
2022-04-09 14:58:03,356 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.output.dense.bias using .* initializer
2022-04-09 14:58:03,356 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.output.LayerNorm.weight using .* initializer
2022-04-09 14:58:03,357 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.output.LayerNorm.bias using .* initializer
2022-04-09 14:58:03,357 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.self.query.weight using .* initializer
2022-04-09 14:58:03,357 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.self.query.bias using .* initializer
2022-04-09 14:58:03,357 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.self.key.weight using .* initializer
2022-04-09 14:58:03,358 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.self.key.bias using .* initializer
2022-04-09 14:58:03,358 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.self.value.weight using .* initializer
2022-04-09 14:58:03,358 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.self.value.bias using .* initializer
2022-04-09 14:58:03,359 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.output.dense.weight using .* initializer
2022-04-09 14:58:03,359 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.output.dense.bias using .* initializer
2022-04-09 14:58:03,359 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.output.LayerNorm.weight using .* initializer
2022-04-09 14:58:03,359 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.output.LayerNorm.bias using .* initializer
2022-04-09 14:58:03,360 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.intermediate.dense.weight using .* initializer
2022-04-09 14:58:03,361 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.intermediate.dense.bias using .* initializer
2022-04-09 14:58:03,361 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.output.dense.weight using .* initializer
2022-04-09 14:58:03,362 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.output.dense.bias using .* initializer
2022-04-09 14:58:03,362 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.output.LayerNorm.weight using .* initializer
2022-04-09 14:58:03,362 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.output.LayerNorm.bias using .* initializer
2022-04-09 14:58:03,362 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.self.query.weight using .* initializer
2022-04-09 14:58:03,362 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.self.query.bias using .* initializer
2022-04-09 14:58:03,363 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.self.key.weight using .* initializer
2022-04-09 14:58:03,363 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.self.key.bias using .* initializer
2022-04-09 14:58:03,363 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.self.value.weight using .* initializer
2022-04-09 14:58:03,363 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.self.value.bias using .* initializer
2022-04-09 14:58:03,364 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.output.dense.weight using .* initializer
2022-04-09 14:58:03,364 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.output.dense.bias using .* initializer
2022-04-09 14:58:03,364 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.output.LayerNorm.weight using .* initializer
2022-04-09 14:58:03,364 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.output.LayerNorm.bias using .* initializer
2022-04-09 14:58:03,364 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.intermediate.dense.weight using .* initializer
2022-04-09 14:58:03,365 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.intermediate.dense.bias using .* initializer
2022-04-09 14:58:03,365 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.output.dense.weight using .* initializer
2022-04-09 14:58:03,366 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.output.dense.bias using .* initializer
2022-04-09 14:58:03,366 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.output.LayerNorm.weight using .* initializer
2022-04-09 14:58:03,367 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.output.LayerNorm.bias using .* initializer
2022-04-09 14:58:03,367 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.self.query.weight using .* initializer
2022-04-09 14:58:03,367 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.self.query.bias using .* initializer
2022-04-09 14:58:03,367 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.self.key.weight using .* initializer
2022-04-09 14:58:03,368 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.self.key.bias using .* initializer
2022-04-09 14:58:03,368 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.self.value.weight using .* initializer
2022-04-09 14:58:03,368 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.self.value.bias using .* initializer
2022-04-09 14:58:03,368 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.output.dense.weight using .* initializer
2022-04-09 14:58:03,368 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.output.dense.bias using .* initializer
2022-04-09 14:58:03,369 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.output.LayerNorm.weight using .* initializer
2022-04-09 14:58:03,369 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.output.LayerNorm.bias using .* initializer
2022-04-09 14:58:03,369 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.intermediate.dense.weight using .* initializer
2022-04-09 14:58:03,370 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.intermediate.dense.bias using .* initializer
2022-04-09 14:58:03,370 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.output.dense.weight using .* initializer
2022-04-09 14:58:03,371 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.output.dense.bias using .* initializer
2022-04-09 14:58:03,371 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.output.LayerNorm.weight using .* initializer
2022-04-09 14:58:03,371 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.output.LayerNorm.bias using .* initializer
2022-04-09 14:58:03,371 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.self.query.weight using .* initializer
2022-04-09 14:58:03,372 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.self.query.bias using .* initializer
2022-04-09 14:58:03,372 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.self.key.weight using .* initializer
2022-04-09 14:58:03,372 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.self.key.bias using .* initializer
2022-04-09 14:58:03,372 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.self.value.weight using .* initializer
2022-04-09 14:58:03,372 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.self.value.bias using .* initializer
2022-04-09 14:58:03,373 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.output.dense.weight using .* initializer
2022-04-09 14:58:03,373 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.output.dense.bias using .* initializer
2022-04-09 14:58:03,373 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.output.LayerNorm.weight using .* initializer
2022-04-09 14:58:03,373 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.output.LayerNorm.bias using .* initializer
2022-04-09 14:58:03,373 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.intermediate.dense.weight using .* initializer
2022-04-09 14:58:03,374 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.intermediate.dense.bias using .* initializer
2022-04-09 14:58:03,374 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.output.dense.weight using .* initializer
2022-04-09 14:58:03,375 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.output.dense.bias using .* initializer
2022-04-09 14:58:03,375 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.output.LayerNorm.weight using .* initializer
2022-04-09 14:58:03,376 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.output.LayerNorm.bias using .* initializer
2022-04-09 14:58:03,376 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.self.query.weight using .* initializer
2022-04-09 14:58:03,376 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.self.query.bias using .* initializer
2022-04-09 14:58:03,376 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.self.key.weight using .* initializer
2022-04-09 14:58:03,377 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.self.key.bias using .* initializer
2022-04-09 14:58:03,377 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.self.value.weight using .* initializer
2022-04-09 14:58:03,377 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.self.value.bias using .* initializer
2022-04-09 14:58:03,377 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.output.dense.weight using .* initializer
2022-04-09 14:58:03,378 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.output.dense.bias using .* initializer
2022-04-09 14:58:03,378 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.output.LayerNorm.weight using .* initializer
2022-04-09 14:58:03,378 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.output.LayerNorm.bias using .* initializer
2022-04-09 14:58:03,378 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.intermediate.dense.weight using .* initializer
2022-04-09 14:58:03,379 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.intermediate.dense.bias using .* initializer
2022-04-09 14:58:03,379 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.output.dense.weight using .* initializer
2022-04-09 14:58:03,380 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.output.dense.bias using .* initializer
2022-04-09 14:58:03,380 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.output.LayerNorm.weight using .* initializer
2022-04-09 14:58:03,380 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.output.LayerNorm.bias using .* initializer
2022-04-09 14:58:03,380 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.self.query.weight using .* initializer
2022-04-09 14:58:03,381 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.self.query.bias using .* initializer
2022-04-09 14:58:03,381 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.self.key.weight using .* initializer
2022-04-09 14:58:03,381 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.self.key.bias using .* initializer
2022-04-09 14:58:03,381 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.self.value.weight using .* initializer
2022-04-09 14:58:03,382 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.self.value.bias using .* initializer
2022-04-09 14:58:03,382 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.output.dense.weight using .* initializer
2022-04-09 14:58:03,382 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.output.dense.bias using .* initializer
2022-04-09 14:58:03,382 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.output.LayerNorm.weight using .* initializer
2022-04-09 14:58:03,382 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.output.LayerNorm.bias using .* initializer
2022-04-09 14:58:03,382 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.intermediate.dense.weight using .* initializer
2022-04-09 14:58:03,383 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.intermediate.dense.bias using .* initializer
2022-04-09 14:58:03,384 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.output.dense.weight using .* initializer
2022-04-09 14:58:03,384 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.output.dense.bias using .* initializer
2022-04-09 14:58:03,385 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.output.LayerNorm.weight using .* initializer
2022-04-09 14:58:03,385 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.output.LayerNorm.bias using .* initializer
2022-04-09 14:58:03,385 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.self.query.weight using .* initializer
2022-04-09 14:58:03,385 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.self.query.bias using .* initializer
2022-04-09 14:58:03,386 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.self.key.weight using .* initializer
2022-04-09 14:58:03,386 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.self.key.bias using .* initializer
2022-04-09 14:58:03,386 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.self.value.weight using .* initializer
2022-04-09 14:58:03,387 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.self.value.bias using .* initializer
2022-04-09 14:58:03,387 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.output.dense.weight using .* initializer
2022-04-09 14:58:03,387 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.output.dense.bias using .* initializer
2022-04-09 14:58:03,387 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.output.LayerNorm.weight using .* initializer
2022-04-09 14:58:03,388 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.output.LayerNorm.bias using .* initializer
2022-04-09 14:58:03,388 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.intermediate.dense.weight using .* initializer
2022-04-09 14:58:03,389 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.intermediate.dense.bias using .* initializer
2022-04-09 14:58:03,389 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.output.dense.weight using .* initializer
2022-04-09 14:58:03,390 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.output.dense.bias using .* initializer
2022-04-09 14:58:03,390 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.output.LayerNorm.weight using .* initializer
2022-04-09 14:58:03,390 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.output.LayerNorm.bias using .* initializer
2022-04-09 14:58:03,390 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.self.query.weight using .* initializer
2022-04-09 14:58:03,391 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.self.query.bias using .* initializer
2022-04-09 14:58:03,391 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.self.key.weight using .* initializer
2022-04-09 14:58:03,391 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.self.key.bias using .* initializer
2022-04-09 14:58:03,392 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.self.value.weight using .* initializer
2022-04-09 14:58:03,392 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.self.value.bias using .* initializer
2022-04-09 14:58:03,392 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.output.dense.weight using .* initializer
2022-04-09 14:58:03,393 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.output.dense.bias using .* initializer
2022-04-09 14:58:03,393 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.output.LayerNorm.weight using .* initializer
2022-04-09 14:58:03,393 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.output.LayerNorm.bias using .* initializer
2022-04-09 14:58:03,393 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.intermediate.dense.weight using .* initializer
2022-04-09 14:58:03,394 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.intermediate.dense.bias using .* initializer
2022-04-09 14:58:03,394 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.output.dense.weight using .* initializer
2022-04-09 14:58:03,395 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.output.dense.bias using .* initializer
2022-04-09 14:58:03,395 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.output.LayerNorm.weight using .* initializer
2022-04-09 14:58:03,396 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.output.LayerNorm.bias using .* initializer
2022-04-09 14:58:03,396 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.self.query.weight using .* initializer
2022-04-09 14:58:03,396 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.self.query.bias using .* initializer
2022-04-09 14:58:03,396 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.self.key.weight using .* initializer
2022-04-09 14:58:03,397 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.self.key.bias using .* initializer
2022-04-09 14:58:03,397 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.self.value.weight using .* initializer
2022-04-09 14:58:03,397 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.self.value.bias using .* initializer
2022-04-09 14:58:03,398 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.output.dense.weight using .* initializer
2022-04-09 14:58:03,398 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.output.dense.bias using .* initializer
2022-04-09 14:58:03,398 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.output.LayerNorm.weight using .* initializer
2022-04-09 14:58:03,398 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.output.LayerNorm.bias using .* initializer
2022-04-09 14:58:03,399 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.intermediate.dense.weight using .* initializer
2022-04-09 14:58:03,400 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.intermediate.dense.bias using .* initializer
2022-04-09 14:58:03,400 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.output.dense.weight using .* initializer
2022-04-09 14:58:03,401 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.output.dense.bias using .* initializer
2022-04-09 14:58:03,401 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.output.LayerNorm.weight using .* initializer
2022-04-09 14:58:03,401 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.output.LayerNorm.bias using .* initializer
2022-04-09 14:58:03,401 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.self.query.weight using .* initializer
2022-04-09 14:58:03,402 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.self.query.bias using .* initializer
2022-04-09 14:58:03,402 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.self.key.weight using .* initializer
2022-04-09 14:58:03,402 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.self.key.bias using .* initializer
2022-04-09 14:58:03,402 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.self.value.weight using .* initializer
2022-04-09 14:58:03,403 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.self.value.bias using .* initializer
2022-04-09 14:58:03,403 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.output.dense.weight using .* initializer
2022-04-09 14:58:03,403 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.output.dense.bias using .* initializer
2022-04-09 14:58:03,403 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.output.LayerNorm.weight using .* initializer
2022-04-09 14:58:03,404 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.output.LayerNorm.bias using .* initializer
2022-04-09 14:58:03,404 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.intermediate.dense.weight using .* initializer
2022-04-09 14:58:03,405 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.intermediate.dense.bias using .* initializer
2022-04-09 14:58:03,405 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.output.dense.weight using .* initializer
2022-04-09 14:58:03,406 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.output.dense.bias using .* initializer
2022-04-09 14:58:03,406 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.output.LayerNorm.weight using .* initializer
2022-04-09 14:58:03,406 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.output.LayerNorm.bias using .* initializer
2022-04-09 14:58:03,406 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.self.query.weight using .* initializer
2022-04-09 14:58:03,407 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.self.query.bias using .* initializer
2022-04-09 14:58:03,407 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.self.key.weight using .* initializer
2022-04-09 14:58:03,407 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.self.key.bias using .* initializer
2022-04-09 14:58:03,408 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.self.value.weight using .* initializer
2022-04-09 14:58:03,408 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.self.value.bias using .* initializer
2022-04-09 14:58:03,408 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.output.dense.weight using .* initializer
2022-04-09 14:58:03,409 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.output.dense.bias using .* initializer
2022-04-09 14:58:03,409 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.output.LayerNorm.weight using .* initializer
2022-04-09 14:58:03,409 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.output.LayerNorm.bias using .* initializer
2022-04-09 14:58:03,409 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.intermediate.dense.weight using .* initializer
2022-04-09 14:58:03,410 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.intermediate.dense.bias using .* initializer
2022-04-09 14:58:03,410 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.output.dense.weight using .* initializer
2022-04-09 14:58:03,411 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.output.dense.bias using .* initializer
2022-04-09 14:58:03,411 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.output.LayerNorm.weight using .* initializer
2022-04-09 14:58:03,412 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.output.LayerNorm.bias using .* initializer
2022-04-09 14:58:03,412 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.self.query.weight using .* initializer
2022-04-09 14:58:03,412 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.self.query.bias using .* initializer
2022-04-09 14:58:03,412 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.self.key.weight using .* initializer
2022-04-09 14:58:03,413 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.self.key.bias using .* initializer
2022-04-09 14:58:03,413 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.self.value.weight using .* initializer
2022-04-09 14:58:03,413 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.self.value.bias using .* initializer
2022-04-09 14:58:03,414 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.output.dense.weight using .* initializer
2022-04-09 14:58:03,414 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.output.dense.bias using .* initializer
2022-04-09 14:58:03,414 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.output.LayerNorm.weight using .* initializer
2022-04-09 14:58:03,414 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.output.LayerNorm.bias using .* initializer
2022-04-09 14:58:03,414 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.intermediate.dense.weight using .* initializer
2022-04-09 14:58:03,415 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.intermediate.dense.bias using .* initializer
2022-04-09 14:58:03,416 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.output.dense.weight using .* initializer
2022-04-09 14:58:03,417 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.output.dense.bias using .* initializer
2022-04-09 14:58:03,417 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.output.LayerNorm.weight using .* initializer
2022-04-09 14:58:03,417 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.output.LayerNorm.bias using .* initializer
2022-04-09 14:58:03,417 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.self.query.weight using .* initializer
2022-04-09 14:58:03,418 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.self.query.bias using .* initializer
2022-04-09 14:58:03,418 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.self.key.weight using .* initializer
2022-04-09 14:58:03,418 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.self.key.bias using .* initializer
2022-04-09 14:58:03,418 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.self.value.weight using .* initializer
2022-04-09 14:58:03,419 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.self.value.bias using .* initializer
2022-04-09 14:58:03,419 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.output.dense.weight using .* initializer
2022-04-09 14:58:03,419 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.output.dense.bias using .* initializer
2022-04-09 14:58:03,420 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.output.LayerNorm.weight using .* initializer
2022-04-09 14:58:03,420 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.output.LayerNorm.bias using .* initializer
2022-04-09 14:58:03,420 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.intermediate.dense.weight using .* initializer
2022-04-09 14:58:03,421 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.intermediate.dense.bias using .* initializer
2022-04-09 14:58:03,421 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.output.dense.weight using .* initializer
2022-04-09 14:58:03,422 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.output.dense.bias using .* initializer
2022-04-09 14:58:03,422 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.output.LayerNorm.weight using .* initializer
2022-04-09 14:58:03,422 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.output.LayerNorm.bias using .* initializer
2022-04-09 14:58:03,423 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.self.query.weight using .* initializer
2022-04-09 14:58:03,423 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.self.query.bias using .* initializer
2022-04-09 14:58:03,423 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.self.key.weight using .* initializer
2022-04-09 14:58:03,424 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.self.key.bias using .* initializer
2022-04-09 14:58:03,424 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.self.value.weight using .* initializer
2022-04-09 14:58:03,424 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.self.value.bias using .* initializer
2022-04-09 14:58:03,424 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.output.dense.weight using .* initializer
2022-04-09 14:58:03,425 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.output.dense.bias using .* initializer
2022-04-09 14:58:03,425 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.output.LayerNorm.weight using .* initializer
2022-04-09 14:58:03,425 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.output.LayerNorm.bias using .* initializer
2022-04-09 14:58:03,425 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.intermediate.dense.weight using .* initializer
2022-04-09 14:58:03,426 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.intermediate.dense.bias using .* initializer
2022-04-09 14:58:03,426 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.output.dense.weight using .* initializer
2022-04-09 14:58:03,427 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.output.dense.bias using .* initializer
2022-04-09 14:58:03,428 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.output.LayerNorm.weight using .* initializer
2022-04-09 14:58:03,428 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.output.LayerNorm.bias using .* initializer
2022-04-09 14:58:03,428 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.self.query.weight using .* initializer
2022-04-09 14:58:03,428 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.self.query.bias using .* initializer
2022-04-09 14:58:03,429 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.self.key.weight using .* initializer
2022-04-09 14:58:03,429 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.self.key.bias using .* initializer
2022-04-09 14:58:03,429 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.self.value.weight using .* initializer
2022-04-09 14:58:03,430 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.self.value.bias using .* initializer
2022-04-09 14:58:03,430 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.output.dense.weight using .* initializer
2022-04-09 14:58:03,430 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.output.dense.bias using .* initializer
2022-04-09 14:58:03,430 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.output.LayerNorm.weight using .* initializer
2022-04-09 14:58:03,431 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.output.LayerNorm.bias using .* initializer
2022-04-09 14:58:03,431 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.intermediate.dense.weight using .* initializer
2022-04-09 14:58:03,432 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.intermediate.dense.bias using .* initializer
2022-04-09 14:58:03,432 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.output.dense.weight using .* initializer
2022-04-09 14:58:03,433 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.output.dense.bias using .* initializer
2022-04-09 14:58:03,433 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.output.LayerNorm.weight using .* initializer
2022-04-09 14:58:03,433 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.output.LayerNorm.bias using .* initializer
2022-04-09 14:58:03,433 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.pooler.dense.weight using .* initializer
2022-04-09 14:58:03,434 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.pooler.dense.bias using .* initializer
2022-04-09 14:58:03,434 - INFO - allennlp.nn.initializers - Initializing _classifier.classifier.dense.weight using .* initializer
2022-04-09 14:58:03,434 - INFO - allennlp.nn.initializers - Initializing _classifier.classifier.dense.bias using .* initializer
2022-04-09 14:58:03,435 - INFO - allennlp.nn.initializers - Initializing _classifier.classifier.out_proj.weight using .* initializer
2022-04-09 14:58:03,435 - INFO - allennlp.nn.initializers - Initializing _classifier.classifier.out_proj.bias using .* initializer
2022-04-09 14:58:03,435 - INFO - allennlp.nn.initializers - Done initializing parameters; the following parameters are using their default initialization from their code
2022-04-09 14:58:03,984 - INFO - filelock - Lock 140294860268368 acquired on ./output_ir-ora-d_hyper0/vocabulary/.lock
2022-04-09 14:58:03,985 - INFO - filelock - Lock 140294860268368 released on ./output_ir-ora-d_hyper0/vocabulary/.lock
2022-04-09 14:58:03,985 - INFO - allennlp.common.params - data_loader.type = pytorch_dataloader
2022-04-09 14:58:03,985 - INFO - allennlp.common.params - data_loader.batch_size = 1
2022-04-09 14:58:03,986 - INFO - allennlp.common.params - data_loader.shuffle = False
2022-04-09 14:58:03,986 - INFO - allennlp.common.params - data_loader.sampler = None
2022-04-09 14:58:03,986 - INFO - allennlp.common.params - data_loader.num_workers = 0
2022-04-09 14:58:03,986 - INFO - allennlp.common.params - data_loader.pin_memory = False
2022-04-09 14:58:03,986 - INFO - allennlp.common.params - data_loader.drop_last = False
2022-04-09 14:58:03,986 - INFO - allennlp.common.params - data_loader.timeout = 0
2022-04-09 14:58:03,987 - INFO - allennlp.common.params - data_loader.worker_init_fn = None
2022-04-09 14:58:03,987 - INFO - allennlp.common.params - data_loader.multiprocessing_context = None
2022-04-09 14:58:03,987 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None
2022-04-09 14:58:03,987 - INFO - allennlp.common.params - data_loader.batch_sampler.type = bucket
2022-04-09 14:58:03,987 - INFO - allennlp.common.params - data_loader.batch_sampler.batch_size = 2
2022-04-09 14:58:03,988 - INFO - allennlp.common.params - data_loader.batch_sampler.sorting_keys = None
2022-04-09 14:58:03,988 - INFO - allennlp.common.params - data_loader.batch_sampler.padding_noise = 0.1
2022-04-09 14:58:03,988 - INFO - allennlp.common.params - data_loader.batch_sampler.drop_last = False
2022-04-09 14:58:03,992 - INFO - allennlp.common.params - data_loader.type = pytorch_dataloader
2022-04-09 14:58:03,992 - INFO - allennlp.common.params - data_loader.batch_size = 1
2022-04-09 14:58:03,992 - INFO - allennlp.common.params - data_loader.shuffle = False
2022-04-09 14:58:03,992 - INFO - allennlp.common.params - data_loader.sampler = None
2022-04-09 14:58:03,993 - INFO - allennlp.common.params - data_loader.num_workers = 0
2022-04-09 14:58:03,993 - INFO - allennlp.common.params - data_loader.pin_memory = False
2022-04-09 14:58:03,993 - INFO - allennlp.common.params - data_loader.drop_last = False
2022-04-09 14:58:03,993 - INFO - allennlp.common.params - data_loader.timeout = 0
2022-04-09 14:58:03,993 - INFO - allennlp.common.params - data_loader.worker_init_fn = None
2022-04-09 14:58:03,993 - INFO - allennlp.common.params - data_loader.multiprocessing_context = None
2022-04-09 14:58:03,993 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None
2022-04-09 14:58:03,994 - INFO - allennlp.common.params - data_loader.batch_sampler.type = bucket
2022-04-09 14:58:03,994 - INFO - allennlp.common.params - data_loader.batch_sampler.batch_size = 2
2022-04-09 14:58:03,994 - INFO - allennlp.common.params - data_loader.batch_sampler.sorting_keys = None
2022-04-09 14:58:03,994 - INFO - allennlp.common.params - data_loader.batch_sampler.padding_noise = 0.1
2022-04-09 14:58:03,994 - INFO - allennlp.common.params - data_loader.batch_sampler.drop_last = False
2022-04-09 14:58:03,995 - INFO - allennlp.common.params - trainer.type = gradient_descent
2022-04-09 14:58:03,995 - INFO - allennlp.common.params - trainer.patience = None
2022-04-09 14:58:03,995 - INFO - allennlp.common.params - trainer.validation_metric = +accuracy
2022-04-09 14:58:03,995 - INFO - allennlp.common.params - trainer.num_epochs = 10
2022-04-09 14:58:03,996 - INFO - allennlp.common.params - trainer.cuda_device = 0
2022-04-09 14:58:03,996 - INFO - allennlp.common.params - trainer.grad_norm = None
2022-04-09 14:58:03,996 - INFO - allennlp.common.params - trainer.grad_clipping = None
2022-04-09 14:58:03,996 - INFO - allennlp.common.params - trainer.distributed = False
2022-04-09 14:58:03,996 - INFO - allennlp.common.params - trainer.world_size = 1
2022-04-09 14:58:03,996 - INFO - allennlp.common.params - trainer.num_gradient_accumulation_steps = 16
2022-04-09 14:58:03,996 - INFO - allennlp.common.params - trainer.use_amp = False
2022-04-09 14:58:03,997 - INFO - allennlp.common.params - trainer.no_grad = None
2022-04-09 14:58:03,997 - INFO - allennlp.common.params - trainer.momentum_scheduler = None
2022-04-09 14:58:03,997 - INFO - allennlp.common.params - trainer.tensorboard_writer = <allennlp.common.lazy.Lazy object at 0x7f9918154310>
2022-04-09 14:58:03,997 - INFO - allennlp.common.params - trainer.moving_average = None
2022-04-09 14:58:03,997 - INFO - allennlp.common.params - trainer.checkpointer = <allennlp.common.lazy.Lazy object at 0x7f9918154390>
2022-04-09 14:58:03,997 - INFO - allennlp.common.params - trainer.batch_callbacks = None
2022-04-09 14:58:03,998 - INFO - allennlp.common.params - trainer.epoch_callbacks = None
2022-04-09 14:58:03,998 - INFO - allennlp.common.params - trainer.end_callbacks = None
2022-04-09 14:58:03,998 - INFO - allennlp.common.params - trainer.trainer_callbacks = None
2022-04-09 14:58:08,109 - INFO - allennlp.common.params - trainer.optimizer.type = huggingface_adamw
2022-04-09 14:58:08,109 - INFO - allennlp.common.params - trainer.optimizer.parameter_groups = None
2022-04-09 14:58:08,109 - INFO - allennlp.common.params - trainer.optimizer.lr = 1e-05
2022-04-09 14:58:08,110 - INFO - allennlp.common.params - trainer.optimizer.betas = [0.9, 0.98]
2022-04-09 14:58:08,110 - INFO - allennlp.common.params - trainer.optimizer.eps = 1e-06
2022-04-09 14:58:08,110 - INFO - allennlp.common.params - trainer.optimizer.weight_decay = 0.2
2022-04-09 14:58:08,110 - INFO - allennlp.common.params - trainer.optimizer.correct_bias = True
2022-04-09 14:58:08,111 - INFO - allennlp.training.optimizers - Number of trainable parameters: 356411394
2022-04-09 14:58:08,113 - INFO - allennlp.common.util - The following parameters are Frozen (without gradient):
2022-04-09 14:58:08,116 - INFO - allennlp.common.util - The following parameters are Tunable (with gradient):
2022-04-09 14:58:08,116 - INFO - allennlp.common.util - _classifier.roberta.embeddings.word_embeddings.weight
2022-04-09 14:58:08,116 - INFO - allennlp.common.util - _classifier.roberta.embeddings.position_embeddings.weight
2022-04-09 14:58:08,116 - INFO - allennlp.common.util - _classifier.roberta.embeddings.token_type_embeddings.weight
2022-04-09 14:58:08,116 - INFO - allennlp.common.util - _classifier.roberta.embeddings.LayerNorm.weight
2022-04-09 14:58:08,116 - INFO - allennlp.common.util - _classifier.roberta.embeddings.LayerNorm.bias
2022-04-09 14:58:08,116 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.self.query.weight
2022-04-09 14:58:08,117 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.self.query.bias
2022-04-09 14:58:08,117 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.self.key.weight
2022-04-09 14:58:08,117 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.self.key.bias
2022-04-09 14:58:08,117 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.self.value.weight
2022-04-09 14:58:08,117 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.self.value.bias
2022-04-09 14:58:08,117 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.output.dense.weight
2022-04-09 14:58:08,117 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.output.dense.bias
2022-04-09 14:58:08,117 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.output.LayerNorm.weight
2022-04-09 14:58:08,118 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.output.LayerNorm.bias
2022-04-09 14:58:08,118 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.intermediate.dense.weight
2022-04-09 14:58:08,118 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.intermediate.dense.bias
2022-04-09 14:58:08,118 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.output.dense.weight
2022-04-09 14:58:08,118 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.output.dense.bias
2022-04-09 14:58:08,118 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.output.LayerNorm.weight
2022-04-09 14:58:08,118 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.output.LayerNorm.bias
2022-04-09 14:58:08,119 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.self.query.weight
2022-04-09 14:58:08,119 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.self.query.bias
2022-04-09 14:58:08,119 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.self.key.weight
2022-04-09 14:58:08,119 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.self.key.bias
2022-04-09 14:58:08,119 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.self.value.weight
2022-04-09 14:58:08,119 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.self.value.bias
2022-04-09 14:58:08,119 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.output.dense.weight
2022-04-09 14:58:08,119 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.output.dense.bias
2022-04-09 14:58:08,120 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.output.LayerNorm.weight
2022-04-09 14:58:08,120 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.output.LayerNorm.bias
2022-04-09 14:58:08,120 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.intermediate.dense.weight
2022-04-09 14:58:08,120 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.intermediate.dense.bias
2022-04-09 14:58:08,120 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.output.dense.weight
2022-04-09 14:58:08,120 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.output.dense.bias
2022-04-09 14:58:08,120 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.output.LayerNorm.weight
2022-04-09 14:58:08,121 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.output.LayerNorm.bias
2022-04-09 14:58:08,121 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.self.query.weight
2022-04-09 14:58:08,121 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.self.query.bias
2022-04-09 14:58:08,121 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.self.key.weight
2022-04-09 14:58:08,121 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.self.key.bias
2022-04-09 14:58:08,121 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.self.value.weight
2022-04-09 14:58:08,121 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.self.value.bias
2022-04-09 14:58:08,121 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.output.dense.weight
2022-04-09 14:58:08,122 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.output.dense.bias
2022-04-09 14:58:08,122 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.output.LayerNorm.weight
2022-04-09 14:58:08,122 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.output.LayerNorm.bias
2022-04-09 14:58:08,122 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.intermediate.dense.weight
2022-04-09 14:58:08,122 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.intermediate.dense.bias
2022-04-09 14:58:08,122 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.output.dense.weight
2022-04-09 14:58:08,122 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.output.dense.bias
2022-04-09 14:58:08,122 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.output.LayerNorm.weight
2022-04-09 14:58:08,123 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.output.LayerNorm.bias
2022-04-09 14:58:08,123 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.self.query.weight
2022-04-09 14:58:08,123 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.self.query.bias
2022-04-09 14:58:08,123 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.self.key.weight
2022-04-09 14:58:08,123 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.self.key.bias
2022-04-09 14:58:08,123 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.self.value.weight
2022-04-09 14:58:08,123 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.self.value.bias
2022-04-09 14:58:08,124 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.output.dense.weight
2022-04-09 14:58:08,124 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.output.dense.bias
2022-04-09 14:58:08,124 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.output.LayerNorm.weight
2022-04-09 14:58:08,124 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.output.LayerNorm.bias
2022-04-09 14:58:08,124 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.intermediate.dense.weight
2022-04-09 14:58:08,124 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.intermediate.dense.bias
2022-04-09 14:58:08,124 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.output.dense.weight
2022-04-09 14:58:08,125 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.output.dense.bias
2022-04-09 14:58:08,125 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.output.LayerNorm.weight
2022-04-09 14:58:08,125 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.output.LayerNorm.bias
2022-04-09 14:58:08,125 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.self.query.weight
2022-04-09 14:58:08,125 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.self.query.bias
2022-04-09 14:58:08,125 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.self.key.weight
2022-04-09 14:58:08,125 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.self.key.bias
2022-04-09 14:58:08,125 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.self.value.weight
2022-04-09 14:58:08,126 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.self.value.bias
2022-04-09 14:58:08,126 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.output.dense.weight
2022-04-09 14:58:08,126 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.output.dense.bias
2022-04-09 14:58:08,126 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.output.LayerNorm.weight
2022-04-09 14:58:08,126 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.output.LayerNorm.bias
2022-04-09 14:58:08,126 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.intermediate.dense.weight
2022-04-09 14:58:08,126 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.intermediate.dense.bias
2022-04-09 14:58:08,126 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.output.dense.weight
2022-04-09 14:58:08,127 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.output.dense.bias
2022-04-09 14:58:08,127 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.output.LayerNorm.weight
2022-04-09 14:58:08,127 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.output.LayerNorm.bias
2022-04-09 14:58:08,127 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.self.query.weight
2022-04-09 14:58:08,127 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.self.query.bias
2022-04-09 14:58:08,127 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.self.key.weight
2022-04-09 14:58:08,127 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.self.key.bias
2022-04-09 14:58:08,128 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.self.value.weight
2022-04-09 14:58:08,128 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.self.value.bias
2022-04-09 14:58:08,128 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.output.dense.weight
2022-04-09 14:58:08,128 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.output.dense.bias
2022-04-09 14:58:08,128 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.output.LayerNorm.weight
2022-04-09 14:58:08,128 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.output.LayerNorm.bias
2022-04-09 14:58:08,128 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.intermediate.dense.weight
2022-04-09 14:58:08,128 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.intermediate.dense.bias
2022-04-09 14:58:08,129 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.output.dense.weight
2022-04-09 14:58:08,129 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.output.dense.bias
2022-04-09 14:58:08,129 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.output.LayerNorm.weight
2022-04-09 14:58:08,129 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.output.LayerNorm.bias
2022-04-09 14:58:08,129 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.self.query.weight
2022-04-09 14:58:08,129 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.self.query.bias
2022-04-09 14:58:08,129 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.self.key.weight
2022-04-09 14:58:08,130 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.self.key.bias
2022-04-09 14:58:08,130 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.self.value.weight
2022-04-09 14:58:08,130 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.self.value.bias
2022-04-09 14:58:08,130 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.output.dense.weight
2022-04-09 14:58:08,130 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.output.dense.bias
2022-04-09 14:58:08,130 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.output.LayerNorm.weight
2022-04-09 14:58:08,130 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.output.LayerNorm.bias
2022-04-09 14:58:08,130 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.intermediate.dense.weight
2022-04-09 14:58:08,131 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.intermediate.dense.bias
2022-04-09 14:58:08,131 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.output.dense.weight
2022-04-09 14:58:08,131 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.output.dense.bias
2022-04-09 14:58:08,131 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.output.LayerNorm.weight
2022-04-09 14:58:08,131 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.output.LayerNorm.bias
2022-04-09 14:58:08,131 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.self.query.weight
2022-04-09 14:58:08,131 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.self.query.bias
2022-04-09 14:58:08,131 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.self.key.weight
2022-04-09 14:58:08,131 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.self.key.bias
2022-04-09 14:58:08,131 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.self.value.weight
2022-04-09 14:58:08,131 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.self.value.bias
2022-04-09 14:58:08,132 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.output.dense.weight
2022-04-09 14:58:08,132 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.output.dense.bias
2022-04-09 14:58:08,132 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.output.LayerNorm.weight
2022-04-09 14:58:08,132 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.output.LayerNorm.bias
2022-04-09 14:58:08,132 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.intermediate.dense.weight
2022-04-09 14:58:08,132 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.intermediate.dense.bias
2022-04-09 14:58:08,132 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.output.dense.weight
2022-04-09 14:58:08,132 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.output.dense.bias
2022-04-09 14:58:08,132 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.output.LayerNorm.weight
2022-04-09 14:58:08,132 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.output.LayerNorm.bias
2022-04-09 14:58:08,132 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.self.query.weight
2022-04-09 14:58:08,132 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.self.query.bias
2022-04-09 14:58:08,132 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.self.key.weight
2022-04-09 14:58:08,132 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.self.key.bias
2022-04-09 14:58:08,132 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.self.value.weight
2022-04-09 14:58:08,132 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.self.value.bias
2022-04-09 14:58:08,132 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.output.dense.weight
2022-04-09 14:58:08,133 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.output.dense.bias
2022-04-09 14:58:08,133 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.output.LayerNorm.weight
2022-04-09 14:58:08,133 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.output.LayerNorm.bias
2022-04-09 14:58:08,133 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.intermediate.dense.weight
2022-04-09 14:58:08,133 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.intermediate.dense.bias
2022-04-09 14:58:08,133 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.output.dense.weight
2022-04-09 14:58:08,133 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.output.dense.bias
2022-04-09 14:58:08,133 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.output.LayerNorm.weight
2022-04-09 14:58:08,133 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.output.LayerNorm.bias
2022-04-09 14:58:08,133 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.self.query.weight
2022-04-09 14:58:08,133 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.self.query.bias
2022-04-09 14:58:08,133 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.self.key.weight
2022-04-09 14:58:08,133 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.self.key.bias
2022-04-09 14:58:08,133 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.self.value.weight
2022-04-09 14:58:08,133 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.self.value.bias
2022-04-09 14:58:08,133 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.output.dense.weight
2022-04-09 14:58:08,133 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.output.dense.bias
2022-04-09 14:58:08,134 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.output.LayerNorm.weight
2022-04-09 14:58:08,134 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.output.LayerNorm.bias
2022-04-09 14:58:08,134 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.intermediate.dense.weight
2022-04-09 14:58:08,134 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.intermediate.dense.bias
2022-04-09 14:58:08,134 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.output.dense.weight
2022-04-09 14:58:08,134 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.output.dense.bias
2022-04-09 14:58:08,134 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.output.LayerNorm.weight
2022-04-09 14:58:08,134 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.output.LayerNorm.bias
2022-04-09 14:58:08,134 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.self.query.weight
2022-04-09 14:58:08,134 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.self.query.bias
2022-04-09 14:58:08,134 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.self.key.weight
2022-04-09 14:58:08,134 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.self.key.bias
2022-04-09 14:58:08,134 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.self.value.weight
2022-04-09 14:58:08,134 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.self.value.bias
2022-04-09 14:58:08,134 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.output.dense.weight
2022-04-09 14:58:08,134 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.output.dense.bias
2022-04-09 14:58:08,134 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.output.LayerNorm.weight
2022-04-09 14:58:08,135 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.output.LayerNorm.bias
2022-04-09 14:58:08,135 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.intermediate.dense.weight
2022-04-09 14:58:08,135 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.intermediate.dense.bias
2022-04-09 14:58:08,135 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.output.dense.weight
2022-04-09 14:58:08,135 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.output.dense.bias
2022-04-09 14:58:08,135 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.output.LayerNorm.weight
2022-04-09 14:58:08,135 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.output.LayerNorm.bias
2022-04-09 14:58:08,135 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.self.query.weight
2022-04-09 14:58:08,135 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.self.query.bias
2022-04-09 14:58:08,135 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.self.key.weight
2022-04-09 14:58:08,135 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.self.key.bias
2022-04-09 14:58:08,135 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.self.value.weight
2022-04-09 14:58:08,135 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.self.value.bias
2022-04-09 14:58:08,135 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.output.dense.weight
2022-04-09 14:58:08,135 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.output.dense.bias
2022-04-09 14:58:08,135 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.output.LayerNorm.weight
2022-04-09 14:58:08,136 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.output.LayerNorm.bias
2022-04-09 14:58:08,136 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.intermediate.dense.weight
2022-04-09 14:58:08,136 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.intermediate.dense.bias
2022-04-09 14:58:08,136 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.output.dense.weight
2022-04-09 14:58:08,136 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.output.dense.bias
2022-04-09 14:58:08,136 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.output.LayerNorm.weight
2022-04-09 14:58:08,136 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.output.LayerNorm.bias
2022-04-09 14:58:08,136 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.self.query.weight
2022-04-09 14:58:08,136 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.self.query.bias
2022-04-09 14:58:08,136 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.self.key.weight
2022-04-09 14:58:08,136 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.self.key.bias
2022-04-09 14:58:08,136 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.self.value.weight
2022-04-09 14:58:08,136 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.self.value.bias
2022-04-09 14:58:08,136 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.output.dense.weight
2022-04-09 14:58:08,136 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.output.dense.bias
2022-04-09 14:58:08,136 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.output.LayerNorm.weight
2022-04-09 14:58:08,136 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.output.LayerNorm.bias
2022-04-09 14:58:08,137 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.intermediate.dense.weight
2022-04-09 14:58:08,137 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.intermediate.dense.bias
2022-04-09 14:58:08,137 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.output.dense.weight
2022-04-09 14:58:08,137 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.output.dense.bias
2022-04-09 14:58:08,137 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.output.LayerNorm.weight
2022-04-09 14:58:08,137 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.output.LayerNorm.bias
2022-04-09 14:58:08,137 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.self.query.weight
2022-04-09 14:58:08,137 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.self.query.bias
2022-04-09 14:58:08,137 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.self.key.weight
2022-04-09 14:58:08,137 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.self.key.bias
2022-04-09 14:58:08,137 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.self.value.weight
2022-04-09 14:58:08,137 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.self.value.bias
2022-04-09 14:58:08,137 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.output.dense.weight
2022-04-09 14:58:08,137 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.output.dense.bias
2022-04-09 14:58:08,137 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.output.LayerNorm.weight
2022-04-09 14:58:08,137 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.output.LayerNorm.bias
2022-04-09 14:58:08,138 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.intermediate.dense.weight
2022-04-09 14:58:08,138 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.intermediate.dense.bias
2022-04-09 14:58:08,138 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.output.dense.weight
2022-04-09 14:58:08,138 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.output.dense.bias
2022-04-09 14:58:08,138 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.output.LayerNorm.weight
2022-04-09 14:58:08,138 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.output.LayerNorm.bias
2022-04-09 14:58:08,138 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.self.query.weight
2022-04-09 14:58:08,138 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.self.query.bias
2022-04-09 14:58:08,138 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.self.key.weight
2022-04-09 14:58:08,138 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.self.key.bias
2022-04-09 14:58:08,138 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.self.value.weight
2022-04-09 14:58:08,138 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.self.value.bias
2022-04-09 14:58:08,138 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.output.dense.weight
2022-04-09 14:58:08,138 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.output.dense.bias
2022-04-09 14:58:08,138 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.output.LayerNorm.weight
2022-04-09 14:58:08,138 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.output.LayerNorm.bias
2022-04-09 14:58:08,138 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.intermediate.dense.weight
2022-04-09 14:58:08,139 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.intermediate.dense.bias
2022-04-09 14:58:08,139 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.output.dense.weight
2022-04-09 14:58:08,139 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.output.dense.bias
2022-04-09 14:58:08,139 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.output.LayerNorm.weight
2022-04-09 14:58:08,139 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.output.LayerNorm.bias
2022-04-09 14:58:08,139 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.self.query.weight
2022-04-09 14:58:08,139 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.self.query.bias
2022-04-09 14:58:08,139 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.self.key.weight
2022-04-09 14:58:08,139 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.self.key.bias
2022-04-09 14:58:08,139 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.self.value.weight
2022-04-09 14:58:08,139 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.self.value.bias
2022-04-09 14:58:08,139 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.output.dense.weight
2022-04-09 14:58:08,139 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.output.dense.bias
2022-04-09 14:58:08,139 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.output.LayerNorm.weight
2022-04-09 14:58:08,139 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.output.LayerNorm.bias
2022-04-09 14:58:08,139 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.intermediate.dense.weight
2022-04-09 14:58:08,139 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.intermediate.dense.bias
2022-04-09 14:58:08,140 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.output.dense.weight
2022-04-09 14:58:08,140 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.output.dense.bias
2022-04-09 14:58:08,140 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.output.LayerNorm.weight
2022-04-09 14:58:08,140 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.output.LayerNorm.bias
2022-04-09 14:58:08,140 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.self.query.weight
2022-04-09 14:58:08,140 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.self.query.bias
2022-04-09 14:58:08,140 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.self.key.weight
2022-04-09 14:58:08,140 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.self.key.bias
2022-04-09 14:58:08,140 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.self.value.weight
2022-04-09 14:58:08,140 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.self.value.bias
2022-04-09 14:58:08,140 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.output.dense.weight
2022-04-09 14:58:08,140 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.output.dense.bias
2022-04-09 14:58:08,140 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.output.LayerNorm.weight
2022-04-09 14:58:08,140 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.output.LayerNorm.bias
2022-04-09 14:58:08,140 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.intermediate.dense.weight
2022-04-09 14:58:08,140 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.intermediate.dense.bias
2022-04-09 14:58:08,140 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.output.dense.weight
2022-04-09 14:58:08,141 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.output.dense.bias
2022-04-09 14:58:08,141 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.output.LayerNorm.weight
2022-04-09 14:58:08,141 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.output.LayerNorm.bias
2022-04-09 14:58:08,141 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.self.query.weight
2022-04-09 14:58:08,141 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.self.query.bias
2022-04-09 14:58:08,141 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.self.key.weight
2022-04-09 14:58:08,141 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.self.key.bias
2022-04-09 14:58:08,141 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.self.value.weight
2022-04-09 14:58:08,141 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.self.value.bias
2022-04-09 14:58:08,141 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.output.dense.weight
2022-04-09 14:58:08,141 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.output.dense.bias
2022-04-09 14:58:08,141 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.output.LayerNorm.weight
2022-04-09 14:58:08,141 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.output.LayerNorm.bias
2022-04-09 14:58:08,141 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.intermediate.dense.weight
2022-04-09 14:58:08,141 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.intermediate.dense.bias
2022-04-09 14:58:08,141 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.output.dense.weight
2022-04-09 14:58:08,141 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.output.dense.bias
2022-04-09 14:58:08,142 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.output.LayerNorm.weight
2022-04-09 14:58:08,142 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.output.LayerNorm.bias
2022-04-09 14:58:08,142 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.self.query.weight
2022-04-09 14:58:08,142 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.self.query.bias
2022-04-09 14:58:08,142 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.self.key.weight
2022-04-09 14:58:08,142 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.self.key.bias
2022-04-09 14:58:08,142 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.self.value.weight
2022-04-09 14:58:08,142 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.self.value.bias
2022-04-09 14:58:08,142 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.output.dense.weight
2022-04-09 14:58:08,142 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.output.dense.bias
2022-04-09 14:58:08,142 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.output.LayerNorm.weight
2022-04-09 14:58:08,142 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.output.LayerNorm.bias
2022-04-09 14:58:08,142 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.intermediate.dense.weight
2022-04-09 14:58:08,142 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.intermediate.dense.bias
2022-04-09 14:58:08,142 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.output.dense.weight
2022-04-09 14:58:08,142 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.output.dense.bias
2022-04-09 14:58:08,142 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.output.LayerNorm.weight
2022-04-09 14:58:08,143 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.output.LayerNorm.bias
2022-04-09 14:58:08,143 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.self.query.weight
2022-04-09 14:58:08,143 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.self.query.bias
2022-04-09 14:58:08,143 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.self.key.weight
2022-04-09 14:58:08,143 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.self.key.bias
2022-04-09 14:58:08,143 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.self.value.weight
2022-04-09 14:58:08,143 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.self.value.bias
2022-04-09 14:58:08,143 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.output.dense.weight
2022-04-09 14:58:08,143 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.output.dense.bias
2022-04-09 14:58:08,143 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.output.LayerNorm.weight
2022-04-09 14:58:08,143 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.output.LayerNorm.bias
2022-04-09 14:58:08,143 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.intermediate.dense.weight
2022-04-09 14:58:08,143 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.intermediate.dense.bias
2022-04-09 14:58:08,143 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.output.dense.weight
2022-04-09 14:58:08,143 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.output.dense.bias
2022-04-09 14:58:08,143 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.output.LayerNorm.weight
2022-04-09 14:58:08,144 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.output.LayerNorm.bias
2022-04-09 14:58:08,144 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.self.query.weight
2022-04-09 14:58:08,144 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.self.query.bias
2022-04-09 14:58:08,144 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.self.key.weight
2022-04-09 14:58:08,144 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.self.key.bias
2022-04-09 14:58:08,144 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.self.value.weight
2022-04-09 14:58:08,144 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.self.value.bias
2022-04-09 14:58:08,144 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.output.dense.weight
2022-04-09 14:58:08,144 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.output.dense.bias
2022-04-09 14:58:08,144 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.output.LayerNorm.weight
2022-04-09 14:58:08,144 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.output.LayerNorm.bias
2022-04-09 14:58:08,144 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.intermediate.dense.weight
2022-04-09 14:58:08,144 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.intermediate.dense.bias
2022-04-09 14:58:08,144 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.output.dense.weight
2022-04-09 14:58:08,144 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.output.dense.bias
2022-04-09 14:58:08,144 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.output.LayerNorm.weight
2022-04-09 14:58:08,144 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.output.LayerNorm.bias
2022-04-09 14:58:08,145 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.self.query.weight
2022-04-09 14:58:08,145 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.self.query.bias
2022-04-09 14:58:08,145 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.self.key.weight
2022-04-09 14:58:08,145 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.self.key.bias
2022-04-09 14:58:08,145 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.self.value.weight
2022-04-09 14:58:08,145 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.self.value.bias
2022-04-09 14:58:08,145 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.output.dense.weight
2022-04-09 14:58:08,145 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.output.dense.bias
2022-04-09 14:58:08,145 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.output.LayerNorm.weight
2022-04-09 14:58:08,145 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.output.LayerNorm.bias
2022-04-09 14:58:08,145 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.intermediate.dense.weight
2022-04-09 14:58:08,145 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.intermediate.dense.bias
2022-04-09 14:58:08,145 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.output.dense.weight
2022-04-09 14:58:08,145 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.output.dense.bias
2022-04-09 14:58:08,145 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.output.LayerNorm.weight
2022-04-09 14:58:08,145 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.output.LayerNorm.bias
2022-04-09 14:58:08,145 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.self.query.weight
2022-04-09 14:58:08,146 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.self.query.bias
2022-04-09 14:58:08,146 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.self.key.weight
2022-04-09 14:58:08,146 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.self.key.bias
2022-04-09 14:58:08,146 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.self.value.weight
2022-04-09 14:58:08,146 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.self.value.bias
2022-04-09 14:58:08,146 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.output.dense.weight
2022-04-09 14:58:08,146 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.output.dense.bias
2022-04-09 14:58:08,146 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.output.LayerNorm.weight
2022-04-09 14:58:08,146 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.output.LayerNorm.bias
2022-04-09 14:58:08,146 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.intermediate.dense.weight
2022-04-09 14:58:08,146 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.intermediate.dense.bias
2022-04-09 14:58:08,146 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.output.dense.weight
2022-04-09 14:58:08,146 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.output.dense.bias
2022-04-09 14:58:08,146 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.output.LayerNorm.weight
2022-04-09 14:58:08,146 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.output.LayerNorm.bias
2022-04-09 14:58:08,146 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.self.query.weight
2022-04-09 14:58:08,146 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.self.query.bias
2022-04-09 14:58:08,147 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.self.key.weight
2022-04-09 14:58:08,147 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.self.key.bias
2022-04-09 14:58:08,147 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.self.value.weight
2022-04-09 14:58:08,147 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.self.value.bias
2022-04-09 14:58:08,147 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.output.dense.weight
2022-04-09 14:58:08,147 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.output.dense.bias
2022-04-09 14:58:08,147 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.output.LayerNorm.weight
2022-04-09 14:58:08,147 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.output.LayerNorm.bias
2022-04-09 14:58:08,147 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.intermediate.dense.weight
2022-04-09 14:58:08,147 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.intermediate.dense.bias
2022-04-09 14:58:08,147 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.output.dense.weight
2022-04-09 14:58:08,147 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.output.dense.bias
2022-04-09 14:58:08,147 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.output.LayerNorm.weight
2022-04-09 14:58:08,147 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.output.LayerNorm.bias
2022-04-09 14:58:08,147 - INFO - allennlp.common.util - _classifier.roberta.pooler.dense.weight
2022-04-09 14:58:08,147 - INFO - allennlp.common.util - _classifier.roberta.pooler.dense.bias
2022-04-09 14:58:08,147 - INFO - allennlp.common.util - _classifier.classifier.dense.weight
2022-04-09 14:58:08,148 - INFO - allennlp.common.util - _classifier.classifier.dense.bias
2022-04-09 14:58:08,148 - INFO - allennlp.common.util - _classifier.classifier.out_proj.weight
2022-04-09 14:58:08,148 - INFO - allennlp.common.util - _classifier.classifier.out_proj.bias
2022-04-09 14:58:08,148 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.type = slanted_triangular
2022-04-09 14:58:08,148 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.cut_frac = 0.06
2022-04-09 14:58:08,148 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.ratio = 32
2022-04-09 14:58:08,148 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.last_epoch = -1
2022-04-09 14:58:08,148 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.gradual_unfreezing = False
2022-04-09 14:58:08,148 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.discriminative_fine_tuning = False
2022-04-09 14:58:08,149 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.decay_factor = 0.38
2022-04-09 14:58:08,149 - INFO - allennlp.common.params - type = default
2022-04-09 14:58:08,149 - INFO - allennlp.common.params - keep_serialized_model_every_num_seconds = None
2022-04-09 14:58:08,149 - INFO - allennlp.common.params - num_serialized_models_to_keep = 2
2022-04-09 14:58:08,149 - INFO - allennlp.common.params - model_save_interval = None
2022-04-09 14:58:08,149 - INFO - allennlp.common.params - summary_interval = 100
2022-04-09 14:58:08,149 - INFO - allennlp.common.params - histogram_interval = None
2022-04-09 14:58:08,149 - INFO - allennlp.common.params - batch_size_interval = None
2022-04-09 14:58:08,149 - INFO - allennlp.common.params - should_log_parameter_statistics = True
2022-04-09 14:58:08,149 - INFO - allennlp.common.params - should_log_learning_rate = False
2022-04-09 14:58:08,149 - INFO - allennlp.common.params - get_batch_num_total = None
2022-04-09 14:58:08,152 - WARNING - allennlp.training.trainer - You provided a validation dataset but patience was set to None, meaning that early stopping is disabled
2022-04-09 14:58:08,247 - INFO - allennlp.training.trainer - Beginning training.
2022-04-09 14:58:08,247 - INFO - allennlp.training.trainer - Epoch 0/9
2022-04-09 14:58:08,248 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-09 14:58:08,248 - INFO - allennlp.training.trainer - GPU 0 memory usage: 1.3G
2022-04-09 14:58:08,249 - INFO - allennlp.training.trainer - Training
2022-04-09 14:58:08,250 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-09 14:58:08,250 - INFO - allennlp.data.samplers.bucket_batch_sampler - No sorting keys given; trying to guess a good one
2022-04-09 14:58:08,250 - INFO - allennlp.data.samplers.bucket_batch_sampler - Using ['tokens'] as the sorting keys
2022-04-09 14:58:27,990 - INFO - tqdm - accuracy: 0.5156, batch_loss: 1.5443, loss: 1.5467 ||:   3%|3         | 2/65 [00:19<10:21,  9.86s/it]
2022-04-09 14:58:48,001 - INFO - tqdm - accuracy: 0.5469, batch_loss: 1.3315, loss: 1.6098 ||:   6%|6         | 4/65 [00:39<10:07,  9.97s/it]
2022-04-09 14:58:58,076 - INFO - tqdm - accuracy: 0.5312, batch_loss: 1.1892, loss: 1.5257 ||:   8%|7         | 5/65 [00:49<10:00, 10.01s/it]
2022-04-09 14:59:08,247 - INFO - tqdm - accuracy: 0.5104, batch_loss: 1.6636, loss: 1.5487 ||:   9%|9         | 6/65 [00:59<09:53, 10.06s/it]
2022-04-09 14:59:18,515 - INFO - tqdm - accuracy: 0.5223, batch_loss: 1.4124, loss: 1.5292 ||:  11%|#         | 7/65 [01:10<09:47, 10.13s/it]
2022-04-09 14:59:28,901 - INFO - tqdm - accuracy: 0.5156, batch_loss: 1.4263, loss: 1.5163 ||:  12%|#2        | 8/65 [01:20<09:42, 10.21s/it]
2022-04-09 14:59:39,382 - INFO - tqdm - accuracy: 0.5104, batch_loss: 1.5718, loss: 1.5225 ||:  14%|#3        | 9/65 [01:31<09:36, 10.30s/it]
2022-04-09 14:59:49,807 - INFO - tqdm - accuracy: 0.4938, batch_loss: 1.3915, loss: 1.5094 ||:  15%|#5        | 10/65 [01:41<09:28, 10.34s/it]
2022-04-09 15:00:00,285 - INFO - tqdm - accuracy: 0.4915, batch_loss: 1.3244, loss: 1.4926 ||:  17%|#6        | 11/65 [01:52<09:20, 10.38s/it]
2022-04-09 15:00:10,868 - INFO - tqdm - accuracy: 0.5104, batch_loss: 0.8429, loss: 1.4384 ||:  18%|#8        | 12/65 [02:02<09:13, 10.44s/it]
2022-04-09 15:00:21,560 - INFO - tqdm - accuracy: 0.5168, batch_loss: 0.7627, loss: 1.3865 ||:  20%|##        | 13/65 [02:13<09:06, 10.52s/it]
2022-04-09 15:00:32,237 - INFO - tqdm - accuracy: 0.5112, batch_loss: 1.0931, loss: 1.3655 ||:  22%|##1       | 14/65 [02:23<08:58, 10.57s/it]
2022-04-09 15:00:43,106 - INFO - tqdm - accuracy: 0.5167, batch_loss: 0.8413, loss: 1.3306 ||:  23%|##3       | 15/65 [02:34<08:52, 10.66s/it]
2022-04-09 15:00:54,039 - INFO - tqdm - accuracy: 0.5000, batch_loss: 0.9007, loss: 1.3037 ||:  25%|##4       | 16/65 [02:45<08:46, 10.74s/it]
2022-04-09 15:01:05,013 - INFO - tqdm - accuracy: 0.5092, batch_loss: 0.5947, loss: 1.2620 ||:  26%|##6       | 17/65 [02:56<08:38, 10.81s/it]
2022-04-09 15:01:16,068 - INFO - tqdm - accuracy: 0.5104, batch_loss: 0.7556, loss: 1.2338 ||:  28%|##7       | 18/65 [03:07<08:31, 10.88s/it]
2022-04-09 15:01:27,159 - INFO - tqdm - accuracy: 0.5082, batch_loss: 0.7583, loss: 1.2088 ||:  29%|##9       | 19/65 [03:18<08:23, 10.95s/it]
2022-04-09 15:01:38,118 - INFO - tqdm - accuracy: 0.5188, batch_loss: 0.6246, loss: 1.1796 ||:  31%|###       | 20/65 [03:29<08:12, 10.95s/it]
2022-04-09 15:01:49,187 - INFO - tqdm - accuracy: 0.5164, batch_loss: 0.8138, loss: 1.1622 ||:  32%|###2      | 21/65 [03:40<08:03, 10.99s/it]
2022-04-09 15:02:00,170 - INFO - tqdm - accuracy: 0.5156, batch_loss: 0.7160, loss: 1.1419 ||:  34%|###3      | 22/65 [03:51<07:52, 10.98s/it]
2022-04-09 15:02:11,353 - INFO - tqdm - accuracy: 0.5109, batch_loss: 0.7427, loss: 1.1246 ||:  35%|###5      | 23/65 [04:03<07:43, 11.04s/it]
2022-04-09 15:02:22,605 - INFO - tqdm - accuracy: 0.5143, batch_loss: 0.7018, loss: 1.1069 ||:  37%|###6      | 24/65 [04:14<07:35, 11.11s/it]
2022-04-09 15:02:33,911 - INFO - tqdm - accuracy: 0.5125, batch_loss: 0.7341, loss: 1.0920 ||:  38%|###8      | 25/65 [04:25<07:26, 11.17s/it]
2022-04-09 15:02:45,273 - INFO - tqdm - accuracy: 0.5060, batch_loss: 0.7159, loss: 1.0776 ||:  40%|####      | 26/65 [04:37<07:17, 11.23s/it]
2022-04-09 15:02:56,675 - INFO - tqdm - accuracy: 0.5058, batch_loss: 0.6892, loss: 1.0632 ||:  42%|####1     | 27/65 [04:48<07:08, 11.28s/it]
2022-04-09 15:03:08,125 - INFO - tqdm - accuracy: 0.5045, batch_loss: 0.7190, loss: 1.0509 ||:  43%|####3     | 28/65 [04:59<06:59, 11.33s/it]
2022-04-09 15:03:19,580 - INFO - tqdm - accuracy: 0.5011, batch_loss: 0.7606, loss: 1.0409 ||:  45%|####4     | 29/65 [05:11<06:49, 11.37s/it]
2022-04-09 15:03:31,011 - INFO - tqdm - accuracy: 0.4979, batch_loss: 0.7187, loss: 1.0301 ||:  46%|####6     | 30/65 [05:22<06:38, 11.39s/it]
2022-04-09 15:03:42,411 - INFO - tqdm - accuracy: 0.4970, batch_loss: 0.7045, loss: 1.0196 ||:  48%|####7     | 31/65 [05:34<06:27, 11.39s/it]
2022-04-09 15:03:53,810 - INFO - tqdm - accuracy: 0.4941, batch_loss: 0.7326, loss: 1.0107 ||:  49%|####9     | 32/65 [05:45<06:15, 11.39s/it]
2022-04-09 15:04:05,218 - INFO - tqdm - accuracy: 0.4943, batch_loss: 0.7277, loss: 1.0021 ||:  51%|#####     | 33/65 [05:56<06:04, 11.40s/it]
2022-04-09 15:04:16,628 - INFO - tqdm - accuracy: 0.4954, batch_loss: 0.6975, loss: 0.9931 ||:  52%|#####2    | 34/65 [06:08<05:53, 11.40s/it]
2022-04-09 15:04:28,015 - INFO - tqdm - accuracy: 0.4946, batch_loss: 0.7122, loss: 0.9851 ||:  54%|#####3    | 35/65 [06:19<05:41, 11.40s/it]
2022-04-09 15:04:39,415 - INFO - tqdm - accuracy: 0.4948, batch_loss: 0.6832, loss: 0.9767 ||:  55%|#####5    | 36/65 [06:31<05:30, 11.40s/it]
2022-04-09 15:04:50,804 - INFO - tqdm - accuracy: 0.4916, batch_loss: 0.7351, loss: 0.9702 ||:  57%|#####6    | 37/65 [06:42<05:19, 11.40s/it]
2022-04-09 15:05:02,210 - INFO - tqdm - accuracy: 0.4942, batch_loss: 0.7008, loss: 0.9631 ||:  58%|#####8    | 38/65 [06:53<05:07, 11.40s/it]
2022-04-09 15:05:13,589 - INFO - tqdm - accuracy: 0.4952, batch_loss: 0.6969, loss: 0.9563 ||:  60%|######    | 39/65 [07:05<04:56, 11.39s/it]
2022-04-09 15:05:24,966 - INFO - tqdm - accuracy: 0.4945, batch_loss: 0.6927, loss: 0.9497 ||:  62%|######1   | 40/65 [07:16<04:44, 11.39s/it]
2022-04-09 15:05:36,329 - INFO - tqdm - accuracy: 0.4916, batch_loss: 0.7160, loss: 0.9440 ||:  63%|######3   | 41/65 [07:28<04:33, 11.38s/it]
2022-04-09 15:05:47,725 - INFO - tqdm - accuracy: 0.4955, batch_loss: 0.6673, loss: 0.9374 ||:  65%|######4   | 42/65 [07:39<04:21, 11.39s/it]
2022-04-09 15:05:59,122 - INFO - tqdm - accuracy: 0.4927, batch_loss: 0.7091, loss: 0.9321 ||:  66%|######6   | 43/65 [07:50<04:10, 11.39s/it]
2022-04-09 15:06:10,516 - INFO - tqdm - accuracy: 0.4950, batch_loss: 0.6547, loss: 0.9258 ||:  68%|######7   | 44/65 [08:02<03:59, 11.39s/it]
2022-04-09 15:06:21,917 - INFO - tqdm - accuracy: 0.4951, batch_loss: 0.7010, loss: 0.9208 ||:  69%|######9   | 45/65 [08:13<03:47, 11.39s/it]
2022-04-09 15:06:33,315 - INFO - tqdm - accuracy: 0.4959, batch_loss: 0.6817, loss: 0.9156 ||:  71%|#######   | 46/65 [08:25<03:36, 11.39s/it]
2022-04-09 15:06:44,723 - INFO - tqdm - accuracy: 0.4980, batch_loss: 0.6784, loss: 0.9105 ||:  72%|#######2  | 47/65 [08:36<03:25, 11.40s/it]
2022-04-09 15:06:56,135 - INFO - tqdm - accuracy: 0.4967, batch_loss: 0.7284, loss: 0.9067 ||:  74%|#######3  | 48/65 [08:47<03:13, 11.40s/it]
2022-04-09 15:07:07,539 - INFO - tqdm - accuracy: 0.4981, batch_loss: 0.6834, loss: 0.9022 ||:  75%|#######5  | 49/65 [08:59<03:02, 11.40s/it]
2022-04-09 15:07:18,945 - INFO - tqdm - accuracy: 0.4988, batch_loss: 0.6937, loss: 0.8980 ||:  77%|#######6  | 50/65 [09:10<02:51, 11.40s/it]
2022-04-09 15:07:30,360 - INFO - tqdm - accuracy: 0.4982, batch_loss: 0.7266, loss: 0.8947 ||:  78%|#######8  | 51/65 [09:22<02:39, 11.41s/it]
2022-04-09 15:07:41,761 - INFO - tqdm - accuracy: 0.5030, batch_loss: 0.6339, loss: 0.8896 ||:  80%|########  | 52/65 [09:33<02:28, 11.41s/it]
2022-04-09 15:07:53,181 - INFO - tqdm - accuracy: 0.5035, batch_loss: 0.6888, loss: 0.8859 ||:  82%|########1 | 53/65 [09:44<02:16, 11.41s/it]
2022-04-09 15:08:04,244 - INFO - tqdm - accuracy: 0.5090, batch_loss: 0.6090, loss: 0.8807 ||:  83%|########3 | 54/65 [09:55<02:04, 11.31s/it]
2022-04-09 15:08:15,651 - INFO - tqdm - accuracy: 0.5071, batch_loss: 0.7377, loss: 0.8781 ||:  85%|########4 | 55/65 [10:07<01:53, 11.34s/it]
2022-04-09 15:08:27,065 - INFO - tqdm - accuracy: 0.5075, batch_loss: 0.6915, loss: 0.8748 ||:  86%|########6 | 56/65 [10:18<01:42, 11.36s/it]
2022-04-09 15:08:38,470 - INFO - tqdm - accuracy: 0.5041, batch_loss: 0.8090, loss: 0.8736 ||:  88%|########7 | 57/65 [10:30<01:30, 11.37s/it]
2022-04-09 15:08:49,885 - INFO - tqdm - accuracy: 0.5040, batch_loss: 0.6825, loss: 0.8703 ||:  89%|########9 | 58/65 [10:41<01:19, 11.39s/it]
2022-04-09 15:09:01,318 - INFO - tqdm - accuracy: 0.5045, batch_loss: 0.7138, loss: 0.8677 ||:  91%|######### | 59/65 [10:53<01:08, 11.40s/it]
2022-04-09 15:09:12,742 - INFO - tqdm - accuracy: 0.5044, batch_loss: 0.6772, loss: 0.8645 ||:  92%|#########2| 60/65 [11:04<00:57, 11.41s/it]
2022-04-09 15:09:24,136 - INFO - tqdm - accuracy: 0.5044, batch_loss: 0.6903, loss: 0.8617 ||:  94%|#########3| 61/65 [11:15<00:45, 11.40s/it]
2022-04-09 15:09:35,546 - INFO - tqdm - accuracy: 0.5053, batch_loss: 0.6840, loss: 0.8588 ||:  95%|#########5| 62/65 [11:27<00:34, 11.41s/it]
2022-04-09 15:09:46,963 - INFO - tqdm - accuracy: 0.5032, batch_loss: 0.7114, loss: 0.8565 ||:  97%|#########6| 63/65 [11:38<00:22, 11.41s/it]
2022-04-09 15:09:58,384 - INFO - tqdm - accuracy: 0.5051, batch_loss: 0.6919, loss: 0.8539 ||:  98%|#########8| 64/65 [11:50<00:11, 11.41s/it]
2022-04-09 15:10:03,459 - INFO - tqdm - accuracy: 0.5041, batch_loss: 0.7191, loss: 0.8518 ||: 100%|##########| 65/65 [11:55<00:00,  9.51s/it]
2022-04-09 15:10:03,461 - INFO - tqdm - accuracy: 0.5041, batch_loss: 0.7191, loss: 0.8518 ||: 100%|##########| 65/65 [11:55<00:00, 11.00s/it]
2022-04-09 15:10:06,338 - INFO - allennlp.training.trainer - Validating
2022-04-09 15:10:06,340 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-09 15:10:06,340 - INFO - allennlp.data.samplers.bucket_batch_sampler - No sorting keys given; trying to guess a good one
2022-04-09 15:10:06,340 - INFO - allennlp.data.samplers.bucket_batch_sampler - Using ['tokens'] as the sorting keys
2022-04-09 15:10:16,349 - INFO - tqdm - accuracy: 0.5000, batch_loss: 0.6764, loss: 0.6958 ||:  35%|###4      | 40/115 [00:10<00:18,  3.97it/s]
2022-04-09 15:10:26,382 - INFO - tqdm - accuracy: 0.4688, batch_loss: 0.6700, loss: 0.7007 ||:  70%|######9   | 80/115 [00:20<00:08,  4.01it/s]
2022-04-09 15:10:35,195 - INFO - tqdm - accuracy: 0.4760, batch_loss: 0.7825, loss: 0.7003 ||: 100%|##########| 115/115 [00:28<00:00,  3.91it/s]
2022-04-09 15:10:35,196 - INFO - tqdm - accuracy: 0.4760, batch_loss: 0.7825, loss: 0.7003 ||: 100%|##########| 115/115 [00:28<00:00,  3.99it/s]
2022-04-09 15:10:35,196 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-09 15:10:35,197 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.504  |     0.476
2022-04-09 15:10:35,198 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  1359.606  |       N/A
2022-04-09 15:10:35,199 - INFO - allennlp.training.tensorboard_writer - loss               |     0.852  |     0.700
2022-04-09 15:10:35,199 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5804.656  |       N/A
2022-04-09 15:10:40,883 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to './output_ir-ora-d_hyper0/best.th'.
2022-04-09 15:10:42,119 - INFO - allennlp.training.trainer - Epoch duration: 0:12:33.871791
2022-04-09 15:10:42,120 - INFO - allennlp.training.trainer - Estimated training time remaining: 1:53:04
2022-04-09 15:10:42,120 - INFO - allennlp.training.trainer - Epoch 1/9
2022-04-09 15:10:42,120 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-09 15:10:42,120 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-09 15:10:42,122 - INFO - allennlp.training.trainer - Training
2022-04-09 15:10:42,122 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-09 15:10:53,375 - INFO - tqdm - accuracy: 0.3438, batch_loss: 0.7162, loss: 0.7162 ||:   2%|1         | 1/65 [00:11<12:00, 11.25s/it]
2022-04-09 15:11:04,873 - INFO - tqdm - accuracy: 0.4219, batch_loss: 0.6912, loss: 0.7037 ||:   3%|3         | 2/65 [00:22<11:58, 11.40s/it]
2022-04-09 15:11:16,564 - INFO - tqdm - accuracy: 0.4062, batch_loss: 0.7244, loss: 0.7106 ||:   5%|4         | 3/65 [00:34<11:54, 11.53s/it]
2022-04-09 15:11:28,016 - INFO - tqdm - accuracy: 0.4531, batch_loss: 0.6841, loss: 0.7040 ||:   6%|6         | 4/65 [00:45<11:41, 11.50s/it]
2022-04-09 15:11:39,383 - INFO - tqdm - accuracy: 0.4750, batch_loss: 0.6870, loss: 0.7006 ||:   8%|7         | 5/65 [00:57<11:27, 11.45s/it]
2022-04-09 15:11:50,719 - INFO - tqdm - accuracy: 0.5000, batch_loss: 0.6807, loss: 0.6973 ||:   9%|9         | 6/65 [01:08<11:13, 11.41s/it]
2022-04-09 15:12:02,115 - INFO - tqdm - accuracy: 0.4955, batch_loss: 0.7005, loss: 0.6977 ||:  11%|#         | 7/65 [01:19<11:01, 11.41s/it]
2022-04-09 15:12:13,560 - INFO - tqdm - accuracy: 0.5000, batch_loss: 0.6957, loss: 0.6975 ||:  12%|#2        | 8/65 [01:31<10:50, 11.42s/it]
2022-04-09 15:12:25,023 - INFO - tqdm - accuracy: 0.5104, batch_loss: 0.6808, loss: 0.6956 ||:  14%|#3        | 9/65 [01:42<10:40, 11.43s/it]
2022-04-09 15:12:36,463 - INFO - tqdm - accuracy: 0.5031, batch_loss: 0.6722, loss: 0.6933 ||:  15%|#5        | 10/65 [01:54<10:28, 11.44s/it]
2022-04-09 15:12:47,830 - INFO - tqdm - accuracy: 0.5028, batch_loss: 0.7302, loss: 0.6966 ||:  17%|#6        | 11/65 [02:05<10:16, 11.41s/it]
2022-04-09 15:12:59,189 - INFO - tqdm - accuracy: 0.5104, batch_loss: 0.6859, loss: 0.6957 ||:  18%|#8        | 12/65 [02:17<10:04, 11.40s/it]
2022-04-09 15:13:10,536 - INFO - tqdm - accuracy: 0.5168, batch_loss: 0.6699, loss: 0.6937 ||:  20%|##        | 13/65 [02:28<09:51, 11.38s/it]
2022-04-09 15:13:21,810 - INFO - tqdm - accuracy: 0.5134, batch_loss: 0.7212, loss: 0.6957 ||:  22%|##1       | 14/65 [02:39<09:38, 11.35s/it]
2022-04-09 15:13:33,243 - INFO - tqdm - accuracy: 0.5104, batch_loss: 0.6969, loss: 0.6958 ||:  23%|##3       | 15/65 [02:51<09:28, 11.37s/it]
2022-04-09 15:13:44,652 - INFO - tqdm - accuracy: 0.5156, batch_loss: 0.6758, loss: 0.6945 ||:  25%|##4       | 16/65 [03:02<09:17, 11.39s/it]
2022-04-09 15:13:56,090 - INFO - tqdm - accuracy: 0.5165, batch_loss: 0.7088, loss: 0.6954 ||:  26%|##6       | 17/65 [03:13<09:07, 11.40s/it]
2022-04-09 15:14:07,517 - INFO - tqdm - accuracy: 0.5208, batch_loss: 0.6355, loss: 0.6921 ||:  28%|##7       | 18/65 [03:25<08:56, 11.41s/it]
2022-04-09 15:14:18,960 - INFO - tqdm - accuracy: 0.5197, batch_loss: 0.6985, loss: 0.6924 ||:  29%|##9       | 19/65 [03:36<08:45, 11.42s/it]
2022-04-09 15:14:30,377 - INFO - tqdm - accuracy: 0.5250, batch_loss: 0.6890, loss: 0.6922 ||:  31%|###       | 20/65 [03:48<08:33, 11.42s/it]
2022-04-09 15:14:41,814 - INFO - tqdm - accuracy: 0.5268, batch_loss: 0.6719, loss: 0.6913 ||:  32%|###2      | 21/65 [03:59<08:22, 11.42s/it]
2022-04-09 15:14:53,121 - INFO - tqdm - accuracy: 0.5270, batch_loss: 0.6935, loss: 0.6914 ||:  34%|###3      | 22/65 [04:10<08:09, 11.39s/it]
2022-04-09 15:15:04,557 - INFO - tqdm - accuracy: 0.5326, batch_loss: 0.6605, loss: 0.6900 ||:  35%|###5      | 23/65 [04:22<07:58, 11.40s/it]
2022-04-09 15:15:15,954 - INFO - tqdm - accuracy: 0.5286, batch_loss: 0.7320, loss: 0.6918 ||:  37%|###6      | 24/65 [04:33<07:47, 11.40s/it]
2022-04-09 15:15:27,363 - INFO - tqdm - accuracy: 0.5312, batch_loss: 0.6532, loss: 0.6902 ||:  38%|###8      | 25/65 [04:45<07:36, 11.40s/it]
2022-04-09 15:15:38,751 - INFO - tqdm - accuracy: 0.5337, batch_loss: 0.6784, loss: 0.6898 ||:  40%|####      | 26/65 [04:56<07:24, 11.40s/it]
2022-04-09 15:15:50,160 - INFO - tqdm - accuracy: 0.5359, batch_loss: 0.6781, loss: 0.6893 ||:  42%|####1     | 27/65 [05:08<07:13, 11.40s/it]
2022-04-09 15:16:01,567 - INFO - tqdm - accuracy: 0.5379, batch_loss: 0.6728, loss: 0.6887 ||:  43%|####3     | 28/65 [05:19<07:01, 11.40s/it]
2022-04-09 15:16:12,962 - INFO - tqdm - accuracy: 0.5388, batch_loss: 0.6915, loss: 0.6888 ||:  45%|####4     | 29/65 [05:30<06:50, 11.40s/it]
2022-04-09 15:16:24,335 - INFO - tqdm - accuracy: 0.5333, batch_loss: 0.7183, loss: 0.6898 ||:  46%|####6     | 30/65 [05:42<06:38, 11.39s/it]
2022-04-09 15:16:35,726 - INFO - tqdm - accuracy: 0.5343, batch_loss: 0.6865, loss: 0.6897 ||:  48%|####7     | 31/65 [05:53<06:27, 11.39s/it]
2022-04-09 15:16:47,102 - INFO - tqdm - accuracy: 0.5312, batch_loss: 0.6641, loss: 0.6889 ||:  49%|####9     | 32/65 [06:04<06:15, 11.39s/it]
2022-04-09 15:16:58,479 - INFO - tqdm - accuracy: 0.5303, batch_loss: 0.6942, loss: 0.6891 ||:  51%|#####     | 33/65 [06:16<06:04, 11.38s/it]
2022-04-09 15:17:09,855 - INFO - tqdm - accuracy: 0.5322, batch_loss: 0.6967, loss: 0.6893 ||:  52%|#####2    | 34/65 [06:27<05:52, 11.38s/it]
2022-04-09 15:17:21,616 - INFO - tqdm - accuracy: 0.5339, batch_loss: 0.6723, loss: 0.6888 ||:  54%|#####3    | 35/65 [06:39<05:44, 11.50s/it]
2022-04-09 15:17:32,957 - INFO - tqdm - accuracy: 0.5391, batch_loss: 0.6464, loss: 0.6876 ||:  55%|#####5    | 36/65 [06:50<05:32, 11.45s/it]
2022-04-09 15:17:44,361 - INFO - tqdm - accuracy: 0.5397, batch_loss: 0.6565, loss: 0.6868 ||:  57%|#####6    | 37/65 [07:02<05:20, 11.44s/it]
2022-04-09 15:17:55,776 - INFO - tqdm - accuracy: 0.5411, batch_loss: 0.6736, loss: 0.6864 ||:  58%|#####8    | 38/65 [07:13<05:08, 11.43s/it]
2022-04-09 15:18:07,194 - INFO - tqdm - accuracy: 0.5393, batch_loss: 0.6941, loss: 0.6866 ||:  60%|######    | 39/65 [07:25<04:57, 11.43s/it]
2022-04-09 15:18:18,635 - INFO - tqdm - accuracy: 0.5406, batch_loss: 0.6627, loss: 0.6860 ||:  62%|######1   | 40/65 [07:36<04:45, 11.43s/it]
2022-04-09 15:18:30,071 - INFO - tqdm - accuracy: 0.5396, batch_loss: 0.6689, loss: 0.6856 ||:  63%|######3   | 41/65 [07:47<04:34, 11.43s/it]
2022-04-09 15:18:41,498 - INFO - tqdm - accuracy: 0.5387, batch_loss: 0.6567, loss: 0.6849 ||:  65%|######4   | 42/65 [07:59<04:22, 11.43s/it]
2022-04-09 15:18:52,928 - INFO - tqdm - accuracy: 0.5407, batch_loss: 0.6797, loss: 0.6848 ||:  66%|######6   | 43/65 [08:10<04:11, 11.43s/it]
2022-04-09 15:19:04,375 - INFO - tqdm - accuracy: 0.5412, batch_loss: 0.6753, loss: 0.6846 ||:  68%|######7   | 44/65 [08:22<04:00, 11.44s/it]
2022-04-09 15:19:15,817 - INFO - tqdm - accuracy: 0.5403, batch_loss: 0.6505, loss: 0.6838 ||:  69%|######9   | 45/65 [08:33<03:48, 11.44s/it]
2022-04-09 15:19:27,272 - INFO - tqdm - accuracy: 0.5387, batch_loss: 0.7227, loss: 0.6847 ||:  71%|#######   | 46/65 [08:45<03:37, 11.44s/it]
2022-04-09 15:19:38,730 - INFO - tqdm - accuracy: 0.5392, batch_loss: 0.7091, loss: 0.6852 ||:  72%|#######2  | 47/65 [08:56<03:26, 11.45s/it]
2022-04-09 15:19:50,175 - INFO - tqdm - accuracy: 0.5391, batch_loss: 0.7106, loss: 0.6857 ||:  74%|#######3  | 48/65 [09:08<03:14, 11.45s/it]
2022-04-09 15:20:01,663 - INFO - tqdm - accuracy: 0.5383, batch_loss: 0.7346, loss: 0.6867 ||:  75%|#######5  | 49/65 [09:19<03:03, 11.46s/it]
2022-04-09 15:20:13,109 - INFO - tqdm - accuracy: 0.5413, batch_loss: 0.6624, loss: 0.6862 ||:  77%|#######6  | 50/65 [09:30<02:51, 11.45s/it]
2022-04-09 15:20:24,564 - INFO - tqdm - accuracy: 0.5398, batch_loss: 0.7102, loss: 0.6867 ||:  78%|#######8  | 51/65 [09:42<02:40, 11.46s/it]
2022-04-09 15:20:36,012 - INFO - tqdm - accuracy: 0.5403, batch_loss: 0.6688, loss: 0.6864 ||:  80%|########  | 52/65 [09:53<02:28, 11.45s/it]
2022-04-09 15:20:47,450 - INFO - tqdm - accuracy: 0.5425, batch_loss: 0.6454, loss: 0.6856 ||:  82%|########1 | 53/65 [10:05<02:17, 11.45s/it]
2022-04-09 15:20:58,873 - INFO - tqdm - accuracy: 0.5428, batch_loss: 0.6970, loss: 0.6858 ||:  83%|########3 | 54/65 [10:16<02:05, 11.44s/it]
2022-04-09 15:21:10,281 - INFO - tqdm - accuracy: 0.5455, batch_loss: 0.6158, loss: 0.6845 ||:  85%|########4 | 55/65 [10:28<01:54, 11.43s/it]
2022-04-09 15:21:21,664 - INFO - tqdm - accuracy: 0.5452, batch_loss: 0.7119, loss: 0.6850 ||:  86%|########6 | 56/65 [10:39<01:42, 11.42s/it]
2022-04-09 15:21:32,793 - INFO - tqdm - accuracy: 0.5493, batch_loss: 0.5968, loss: 0.6835 ||:  88%|########7 | 57/65 [10:50<01:30, 11.33s/it]
2022-04-09 15:21:44,201 - INFO - tqdm - accuracy: 0.5474, batch_loss: 0.7143, loss: 0.6840 ||:  89%|########9 | 58/65 [11:02<01:19, 11.35s/it]
2022-04-09 15:21:55,579 - INFO - tqdm - accuracy: 0.5466, batch_loss: 0.6589, loss: 0.6836 ||:  91%|######### | 59/65 [11:13<01:08, 11.36s/it]
2022-04-09 15:22:06,992 - INFO - tqdm - accuracy: 0.5474, batch_loss: 0.7034, loss: 0.6839 ||:  92%|#########2| 60/65 [11:24<00:56, 11.38s/it]
2022-04-09 15:22:18,412 - INFO - tqdm - accuracy: 0.5502, batch_loss: 0.5461, loss: 0.6817 ||:  94%|#########3| 61/65 [11:36<00:45, 11.39s/it]
2022-04-09 15:22:29,846 - INFO - tqdm - accuracy: 0.5524, batch_loss: 0.5460, loss: 0.6795 ||:  95%|#########5| 62/65 [11:47<00:34, 11.40s/it]
2022-04-09 15:22:41,291 - INFO - tqdm - accuracy: 0.5531, batch_loss: 0.6312, loss: 0.6787 ||:  97%|#########6| 63/65 [11:59<00:22, 11.42s/it]
2022-04-09 15:22:52,391 - INFO - tqdm - accuracy: 0.5525, batch_loss: 0.7310, loss: 0.6795 ||:  98%|#########8| 64/65 [12:10<00:11, 11.32s/it]
2022-04-09 15:22:57,480 - INFO - tqdm - accuracy: 0.5531, batch_loss: 0.7066, loss: 0.6799 ||: 100%|##########| 65/65 [12:15<00:00,  9.45s/it]
2022-04-09 15:22:57,480 - INFO - tqdm - accuracy: 0.5531, batch_loss: 0.7066, loss: 0.6799 ||: 100%|##########| 65/65 [12:15<00:00, 11.31s/it]
2022-04-09 15:23:00,399 - INFO - allennlp.training.trainer - Validating
2022-04-09 15:23:00,401 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-09 15:23:10,638 - INFO - tqdm - accuracy: 0.5854, batch_loss: 0.7917, loss: 0.6778 ||:  36%|###5      | 41/115 [00:10<00:18,  3.97it/s]
2022-04-09 15:23:20,769 - INFO - tqdm - accuracy: 0.5741, batch_loss: 0.5328, loss: 0.7074 ||:  70%|#######   | 81/115 [00:20<00:08,  3.96it/s]
2022-04-09 15:23:29,275 - INFO - tqdm - accuracy: 0.5677, batch_loss: 0.5356, loss: 0.7243 ||: 100%|##########| 115/115 [00:28<00:00,  3.96it/s]
2022-04-09 15:23:29,275 - INFO - tqdm - accuracy: 0.5677, batch_loss: 0.5356, loss: 0.7243 ||: 100%|##########| 115/115 [00:28<00:00,  3.98it/s]
2022-04-09 15:23:29,275 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-09 15:23:29,276 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.553  |     0.568
2022-04-09 15:23:29,277 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-09 15:23:29,277 - INFO - allennlp.training.tensorboard_writer - loss               |     0.680  |     0.724
2022-04-09 15:23:29,277 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5804.656  |       N/A
2022-04-09 15:23:34,911 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to './output_ir-ora-d_hyper0/best.th'.
2022-04-09 15:23:47,668 - INFO - allennlp.training.trainer - Epoch duration: 0:13:05.548166
2022-04-09 15:23:47,668 - INFO - allennlp.training.trainer - Estimated training time remaining: 1:42:37
2022-04-09 15:23:47,668 - INFO - allennlp.training.trainer - Epoch 2/9
2022-04-09 15:23:47,668 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-09 15:23:47,669 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-09 15:23:47,670 - INFO - allennlp.training.trainer - Training
2022-04-09 15:23:47,670 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-09 15:23:58,756 - INFO - tqdm - accuracy: 0.5625, batch_loss: 0.6945, loss: 0.6945 ||:   2%|1         | 1/65 [00:11<11:49, 11.09s/it]
2022-04-09 15:24:10,189 - INFO - tqdm - accuracy: 0.7031, batch_loss: 0.5047, loss: 0.5996 ||:   3%|3         | 2/65 [00:22<11:51, 11.29s/it]
2022-04-09 15:24:21,909 - INFO - tqdm - accuracy: 0.6979, batch_loss: 0.5993, loss: 0.5995 ||:   5%|4         | 3/65 [00:34<11:52, 11.49s/it]
2022-04-09 15:24:33,708 - INFO - tqdm - accuracy: 0.6797, batch_loss: 0.5682, loss: 0.5917 ||:   6%|6         | 4/65 [00:46<11:48, 11.61s/it]
2022-04-09 15:24:45,151 - INFO - tqdm - accuracy: 0.6813, batch_loss: 0.5702, loss: 0.5874 ||:   8%|7         | 5/65 [00:57<11:32, 11.55s/it]
2022-04-09 15:24:56,298 - INFO - tqdm - accuracy: 0.6979, batch_loss: 0.5580, loss: 0.5825 ||:   9%|9         | 6/65 [01:08<11:13, 11.41s/it]
2022-04-09 15:25:07,558 - INFO - tqdm - accuracy: 0.6964, batch_loss: 0.5977, loss: 0.5847 ||:  11%|#         | 7/65 [01:19<10:59, 11.36s/it]
2022-04-09 15:25:18,959 - INFO - tqdm - accuracy: 0.6836, batch_loss: 0.6378, loss: 0.5913 ||:  12%|#2        | 8/65 [01:31<10:48, 11.37s/it]
2022-04-09 15:25:30,485 - INFO - tqdm - accuracy: 0.6944, batch_loss: 0.5026, loss: 0.5815 ||:  14%|#3        | 9/65 [01:42<10:39, 11.42s/it]
2022-04-09 15:25:42,003 - INFO - tqdm - accuracy: 0.6906, batch_loss: 0.6126, loss: 0.5846 ||:  15%|#5        | 10/65 [01:54<10:29, 11.45s/it]
2022-04-09 15:25:53,424 - INFO - tqdm - accuracy: 0.6818, batch_loss: 0.6602, loss: 0.5914 ||:  17%|#6        | 11/65 [02:05<10:17, 11.44s/it]
2022-04-09 15:26:04,802 - INFO - tqdm - accuracy: 0.6875, batch_loss: 0.5870, loss: 0.5911 ||:  18%|#8        | 12/65 [02:17<10:05, 11.42s/it]
2022-04-09 15:26:15,836 - INFO - tqdm - accuracy: 0.6651, batch_loss: 0.7911, loss: 0.6065 ||:  20%|##        | 13/65 [02:28<09:47, 11.30s/it]
2022-04-09 15:26:27,284 - INFO - tqdm - accuracy: 0.6667, batch_loss: 0.5963, loss: 0.6057 ||:  22%|##1       | 14/65 [02:39<09:38, 11.35s/it]
2022-04-09 15:26:38,735 - INFO - tqdm - accuracy: 0.6639, batch_loss: 0.6682, loss: 0.6099 ||:  23%|##3       | 15/65 [02:51<09:28, 11.38s/it]
2022-04-09 15:26:50,192 - INFO - tqdm - accuracy: 0.6693, batch_loss: 0.5165, loss: 0.6041 ||:  25%|##4       | 16/65 [03:02<09:18, 11.40s/it]
2022-04-09 15:27:01,658 - INFO - tqdm - accuracy: 0.6611, batch_loss: 0.6921, loss: 0.6092 ||:  26%|##6       | 17/65 [03:13<09:08, 11.42s/it]
2022-04-09 15:27:13,107 - INFO - tqdm - accuracy: 0.6522, batch_loss: 0.8271, loss: 0.6213 ||:  28%|##7       | 18/65 [03:25<08:57, 11.43s/it]
2022-04-09 15:27:24,557 - INFO - tqdm - accuracy: 0.6540, batch_loss: 0.6455, loss: 0.6226 ||:  29%|##9       | 19/65 [03:36<08:46, 11.44s/it]
2022-04-09 15:27:36,001 - INFO - tqdm - accuracy: 0.6573, batch_loss: 0.6214, loss: 0.6225 ||:  31%|###       | 20/65 [03:48<08:34, 11.44s/it]
2022-04-09 15:27:47,432 - INFO - tqdm - accuracy: 0.6513, batch_loss: 0.6491, loss: 0.6238 ||:  32%|###2      | 21/65 [03:59<08:23, 11.44s/it]
2022-04-09 15:27:58,875 - INFO - tqdm - accuracy: 0.6515, batch_loss: 0.5734, loss: 0.6215 ||:  34%|###3      | 22/65 [04:11<08:11, 11.44s/it]
2022-04-09 15:28:10,296 - INFO - tqdm - accuracy: 0.6503, batch_loss: 0.5542, loss: 0.6186 ||:  35%|###5      | 23/65 [04:22<08:00, 11.43s/it]
2022-04-09 15:28:21,598 - INFO - tqdm - accuracy: 0.6532, batch_loss: 0.6310, loss: 0.6191 ||:  37%|###6      | 24/65 [04:33<07:47, 11.39s/it]
2022-04-09 15:28:33,011 - INFO - tqdm - accuracy: 0.6546, batch_loss: 0.5879, loss: 0.6179 ||:  38%|###8      | 25/65 [04:45<07:35, 11.40s/it]
2022-04-09 15:28:44,417 - INFO - tqdm - accuracy: 0.6558, batch_loss: 0.5578, loss: 0.6156 ||:  40%|####      | 26/65 [04:56<07:24, 11.40s/it]
2022-04-09 15:28:55,824 - INFO - tqdm - accuracy: 0.6616, batch_loss: 0.5301, loss: 0.6124 ||:  42%|####1     | 27/65 [05:08<07:13, 11.40s/it]
2022-04-09 15:29:07,223 - INFO - tqdm - accuracy: 0.6682, batch_loss: 0.5170, loss: 0.6090 ||:  43%|####3     | 28/65 [05:19<07:01, 11.40s/it]
2022-04-09 15:29:18,638 - INFO - tqdm - accuracy: 0.6667, batch_loss: 0.6168, loss: 0.6093 ||:  45%|####4     | 29/65 [05:30<06:50, 11.41s/it]
2022-04-09 15:29:30,021 - INFO - tqdm - accuracy: 0.6674, batch_loss: 0.5419, loss: 0.6070 ||:  46%|####6     | 30/65 [05:42<06:38, 11.40s/it]
2022-04-09 15:29:41,410 - INFO - tqdm - accuracy: 0.6640, batch_loss: 0.5999, loss: 0.6068 ||:  48%|####7     | 31/65 [05:53<06:27, 11.40s/it]
2022-04-09 15:29:52,792 - INFO - tqdm - accuracy: 0.6686, batch_loss: 0.5329, loss: 0.6045 ||:  49%|####9     | 32/65 [06:05<06:15, 11.39s/it]
2022-04-09 15:30:04,182 - INFO - tqdm - accuracy: 0.6673, batch_loss: 0.6806, loss: 0.6068 ||:  51%|#####     | 33/65 [06:16<06:04, 11.39s/it]
2022-04-09 15:30:15,590 - INFO - tqdm - accuracy: 0.6661, batch_loss: 0.6401, loss: 0.6078 ||:  52%|#####2    | 34/65 [06:27<05:53, 11.40s/it]
2022-04-09 15:30:26,993 - INFO - tqdm - accuracy: 0.6676, batch_loss: 0.4880, loss: 0.6043 ||:  54%|#####3    | 35/65 [06:39<05:41, 11.40s/it]
2022-04-09 15:30:38,403 - INFO - tqdm - accuracy: 0.6699, batch_loss: 0.5727, loss: 0.6035 ||:  55%|#####5    | 36/65 [06:50<05:30, 11.40s/it]
2022-04-09 15:30:49,798 - INFO - tqdm - accuracy: 0.6695, batch_loss: 0.6287, loss: 0.6041 ||:  57%|#####6    | 37/65 [07:02<05:19, 11.40s/it]
2022-04-09 15:31:01,220 - INFO - tqdm - accuracy: 0.6749, batch_loss: 0.3989, loss: 0.5987 ||:  58%|#####8    | 38/65 [07:13<05:07, 11.41s/it]
2022-04-09 15:31:12,635 - INFO - tqdm - accuracy: 0.6744, batch_loss: 0.5309, loss: 0.5970 ||:  60%|######    | 39/65 [07:24<04:56, 11.41s/it]
2022-04-09 15:31:24,055 - INFO - tqdm - accuracy: 0.6740, batch_loss: 0.6400, loss: 0.5981 ||:  62%|######1   | 40/65 [07:36<04:45, 11.41s/it]
2022-04-09 15:31:35,478 - INFO - tqdm - accuracy: 0.6751, batch_loss: 0.5384, loss: 0.5966 ||:  63%|######3   | 41/65 [07:47<04:33, 11.42s/it]
2022-04-09 15:31:46,788 - INFO - tqdm - accuracy: 0.6768, batch_loss: 0.5408, loss: 0.5953 ||:  65%|######4   | 42/65 [07:59<04:21, 11.38s/it]
2022-04-09 15:31:58,224 - INFO - tqdm - accuracy: 0.6742, batch_loss: 0.6271, loss: 0.5960 ||:  66%|######6   | 43/65 [08:10<04:10, 11.40s/it]
2022-04-09 15:32:09,671 - INFO - tqdm - accuracy: 0.6745, batch_loss: 0.5315, loss: 0.5946 ||:  68%|######7   | 44/65 [08:22<03:59, 11.41s/it]
2022-04-09 15:32:21,115 - INFO - tqdm - accuracy: 0.6762, batch_loss: 0.6163, loss: 0.5950 ||:  69%|######9   | 45/65 [08:33<03:48, 11.42s/it]
2022-04-09 15:32:32,553 - INFO - tqdm - accuracy: 0.6785, batch_loss: 0.4958, loss: 0.5929 ||:  71%|#######   | 46/65 [08:44<03:37, 11.43s/it]
2022-04-09 15:32:44,006 - INFO - tqdm - accuracy: 0.6766, batch_loss: 0.6680, loss: 0.5945 ||:  72%|#######2  | 47/65 [08:56<03:25, 11.44s/it]
2022-04-09 15:32:55,455 - INFO - tqdm - accuracy: 0.6756, batch_loss: 0.6745, loss: 0.5961 ||:  74%|#######3  | 48/65 [09:07<03:14, 11.44s/it]
2022-04-09 15:33:06,895 - INFO - tqdm - accuracy: 0.6752, batch_loss: 0.5973, loss: 0.5962 ||:  75%|#######5  | 49/65 [09:19<03:03, 11.44s/it]
2022-04-09 15:33:18,332 - INFO - tqdm - accuracy: 0.6748, batch_loss: 0.6232, loss: 0.5967 ||:  77%|#######6  | 50/65 [09:30<02:51, 11.44s/it]
2022-04-09 15:33:29,774 - INFO - tqdm - accuracy: 0.6732, batch_loss: 0.6985, loss: 0.5987 ||:  78%|#######8  | 51/65 [09:42<02:40, 11.44s/it]
2022-04-09 15:33:41,205 - INFO - tqdm - accuracy: 0.6729, batch_loss: 0.5497, loss: 0.5978 ||:  80%|########  | 52/65 [09:53<02:28, 11.44s/it]
2022-04-09 15:33:52,640 - INFO - tqdm - accuracy: 0.6743, batch_loss: 0.5250, loss: 0.5964 ||:  82%|########1 | 53/65 [10:04<02:17, 11.44s/it]
2022-04-09 15:34:04,061 - INFO - tqdm - accuracy: 0.6752, batch_loss: 0.5588, loss: 0.5957 ||:  83%|########3 | 54/65 [10:16<02:05, 11.43s/it]
2022-04-09 15:34:15,482 - INFO - tqdm - accuracy: 0.6754, batch_loss: 0.6260, loss: 0.5963 ||:  85%|########4 | 55/65 [10:27<01:54, 11.43s/it]
2022-04-09 15:34:26,898 - INFO - tqdm - accuracy: 0.6745, batch_loss: 0.5790, loss: 0.5959 ||:  86%|########6 | 56/65 [10:39<01:42, 11.42s/it]
2022-04-09 15:34:38,318 - INFO - tqdm - accuracy: 0.6725, batch_loss: 0.7374, loss: 0.5984 ||:  88%|########7 | 57/65 [10:50<01:31, 11.42s/it]
2022-04-09 15:34:49,505 - INFO - tqdm - accuracy: 0.6733, batch_loss: 0.5386, loss: 0.5974 ||:  89%|########9 | 58/65 [11:01<01:19, 11.35s/it]
2022-04-09 15:35:00,921 - INFO - tqdm - accuracy: 0.6762, batch_loss: 0.4676, loss: 0.5952 ||:  91%|######### | 59/65 [11:13<01:08, 11.37s/it]
2022-04-09 15:35:12,335 - INFO - tqdm - accuracy: 0.6759, batch_loss: 0.6742, loss: 0.5965 ||:  92%|#########2| 60/65 [11:24<00:56, 11.38s/it]
2022-04-09 15:35:23,747 - INFO - tqdm - accuracy: 0.6761, batch_loss: 0.4920, loss: 0.5948 ||:  94%|#########3| 61/65 [11:36<00:45, 11.39s/it]
2022-04-09 15:35:35,157 - INFO - tqdm - accuracy: 0.6742, batch_loss: 0.7095, loss: 0.5966 ||:  95%|#########5| 62/65 [11:47<00:34, 11.40s/it]
2022-04-09 15:35:46,579 - INFO - tqdm - accuracy: 0.6739, batch_loss: 0.5624, loss: 0.5961 ||:  97%|#########6| 63/65 [11:58<00:22, 11.41s/it]
2022-04-09 15:35:58,017 - INFO - tqdm - accuracy: 0.6727, batch_loss: 0.6761, loss: 0.5974 ||:  98%|#########8| 64/65 [12:10<00:11, 11.41s/it]
2022-04-09 15:36:03,075 - INFO - tqdm - accuracy: 0.6730, batch_loss: 0.6546, loss: 0.5982 ||: 100%|##########| 65/65 [12:15<00:00,  9.51s/it]
2022-04-09 15:36:03,076 - INFO - tqdm - accuracy: 0.6730, batch_loss: 0.6546, loss: 0.5982 ||: 100%|##########| 65/65 [12:15<00:00, 11.31s/it]
2022-04-09 15:36:05,935 - INFO - allennlp.training.trainer - Validating
2022-04-09 15:36:05,937 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-09 15:36:15,970 - INFO - tqdm - accuracy: 0.6000, batch_loss: 0.7423, loss: 0.7442 ||:  35%|###4      | 40/115 [00:10<00:18,  3.99it/s]
2022-04-09 15:36:26,133 - INFO - tqdm - accuracy: 0.5901, batch_loss: 0.3310, loss: 0.6846 ||:  70%|#######   | 81/115 [00:20<00:08,  3.99it/s]
2022-04-09 15:36:34,777 - INFO - tqdm - accuracy: 0.6245, batch_loss: 1.3337, loss: 0.6803 ||: 100%|##########| 115/115 [00:28<00:00,  3.95it/s]
2022-04-09 15:36:34,777 - INFO - tqdm - accuracy: 0.6245, batch_loss: 1.3337, loss: 0.6803 ||: 100%|##########| 115/115 [00:28<00:00,  3.99it/s]
2022-04-09 15:36:34,777 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-09 15:36:34,778 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.673  |     0.624
2022-04-09 15:36:34,779 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-09 15:36:34,780 - INFO - allennlp.training.tensorboard_writer - loss               |     0.598  |     0.680
2022-04-09 15:36:34,780 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5804.656  |       N/A
2022-04-09 15:36:40,596 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to './output_ir-ora-d_hyper0/best.th'.
2022-04-09 15:36:53,438 - INFO - allennlp.training.trainer - Epoch duration: 0:13:05.769916
2022-04-09 15:36:53,438 - INFO - allennlp.training.trainer - Estimated training time remaining: 1:30:25
2022-04-09 15:36:53,439 - INFO - allennlp.training.trainer - Epoch 3/9
2022-04-09 15:36:53,439 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-09 15:36:53,439 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-09 15:36:53,440 - INFO - allennlp.training.trainer - Training
2022-04-09 15:36:53,441 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-09 15:37:04,556 - INFO - tqdm - accuracy: 0.7500, batch_loss: 0.5359, loss: 0.5359 ||:   2%|1         | 1/65 [00:11<11:51, 11.11s/it]
2022-04-09 15:37:15,998 - INFO - tqdm - accuracy: 0.6719, batch_loss: 0.6289, loss: 0.5824 ||:   3%|3         | 2/65 [00:22<11:52, 11.31s/it]
2022-04-09 15:37:27,677 - INFO - tqdm - accuracy: 0.7188, batch_loss: 0.5124, loss: 0.5591 ||:   5%|4         | 3/65 [00:34<11:51, 11.48s/it]
2022-04-09 15:37:39,337 - INFO - tqdm - accuracy: 0.7344, batch_loss: 0.4710, loss: 0.5370 ||:   6%|6         | 4/65 [00:45<11:44, 11.55s/it]
2022-04-09 15:37:51,183 - INFO - tqdm - accuracy: 0.7500, batch_loss: 0.4920, loss: 0.5280 ||:   8%|7         | 5/65 [00:57<11:39, 11.66s/it]
2022-04-09 15:38:02,409 - INFO - tqdm - accuracy: 0.7656, batch_loss: 0.4430, loss: 0.5139 ||:   9%|9         | 6/65 [01:08<11:19, 11.51s/it]
2022-04-09 15:38:13,700 - INFO - tqdm - accuracy: 0.7723, batch_loss: 0.4576, loss: 0.5058 ||:  11%|#         | 7/65 [01:20<11:03, 11.44s/it]
2022-04-09 15:38:24,971 - INFO - tqdm - accuracy: 0.7891, batch_loss: 0.3668, loss: 0.4884 ||:  12%|#2        | 8/65 [01:31<10:48, 11.38s/it]
2022-04-09 15:38:36,472 - INFO - tqdm - accuracy: 0.7917, batch_loss: 0.4250, loss: 0.4814 ||:  14%|#3        | 9/65 [01:43<10:39, 11.42s/it]
2022-04-09 15:38:47,967 - INFO - tqdm - accuracy: 0.7969, batch_loss: 0.3846, loss: 0.4717 ||:  15%|#5        | 10/65 [01:54<10:29, 11.44s/it]
2022-04-09 15:38:59,240 - INFO - tqdm - accuracy: 0.7983, batch_loss: 0.4026, loss: 0.4654 ||:  17%|#6        | 11/65 [02:05<10:15, 11.39s/it]
2022-04-09 15:39:10,596 - INFO - tqdm - accuracy: 0.8047, batch_loss: 0.3116, loss: 0.4526 ||:  18%|#8        | 12/65 [02:17<10:03, 11.38s/it]
2022-04-09 15:39:21,995 - INFO - tqdm - accuracy: 0.8077, batch_loss: 0.3160, loss: 0.4421 ||:  20%|##        | 13/65 [02:28<09:52, 11.39s/it]
2022-04-09 15:39:33,444 - INFO - tqdm - accuracy: 0.8036, batch_loss: 0.4856, loss: 0.4452 ||:  22%|##1       | 14/65 [02:40<09:41, 11.41s/it]
2022-04-09 15:39:44,903 - INFO - tqdm - accuracy: 0.8021, batch_loss: 0.4403, loss: 0.4449 ||:  23%|##3       | 15/65 [02:51<09:31, 11.42s/it]
2022-04-09 15:39:56,354 - INFO - tqdm - accuracy: 0.8008, batch_loss: 0.4111, loss: 0.4428 ||:  25%|##4       | 16/65 [03:02<09:20, 11.43s/it]
2022-04-09 15:40:07,811 - INFO - tqdm - accuracy: 0.7941, batch_loss: 0.5744, loss: 0.4505 ||:  26%|##6       | 17/65 [03:14<09:09, 11.44s/it]
2022-04-09 15:40:19,273 - INFO - tqdm - accuracy: 0.8038, batch_loss: 0.2155, loss: 0.4375 ||:  28%|##7       | 18/65 [03:25<08:57, 11.45s/it]
2022-04-09 15:40:30,733 - INFO - tqdm - accuracy: 0.7993, batch_loss: 0.6083, loss: 0.4464 ||:  29%|##9       | 19/65 [03:37<08:46, 11.45s/it]
2022-04-09 15:40:42,180 - INFO - tqdm - accuracy: 0.7922, batch_loss: 0.6410, loss: 0.4562 ||:  31%|###       | 20/65 [03:48<08:35, 11.45s/it]
2022-04-09 15:40:53,624 - INFO - tqdm - accuracy: 0.7917, batch_loss: 0.3653, loss: 0.4518 ||:  32%|###2      | 21/65 [04:00<08:23, 11.45s/it]
2022-04-09 15:41:05,057 - INFO - tqdm - accuracy: 0.7926, batch_loss: 0.4217, loss: 0.4505 ||:  34%|###3      | 22/65 [04:11<08:12, 11.44s/it]
2022-04-09 15:41:16,465 - INFO - tqdm - accuracy: 0.7962, batch_loss: 0.2758, loss: 0.4429 ||:  35%|###5      | 23/65 [04:23<08:00, 11.43s/it]
2022-04-09 15:41:27,885 - INFO - tqdm - accuracy: 0.7969, batch_loss: 0.3677, loss: 0.4397 ||:  37%|###6      | 24/65 [04:34<07:48, 11.43s/it]
2022-04-09 15:41:39,306 - INFO - tqdm - accuracy: 0.8013, batch_loss: 0.2869, loss: 0.4336 ||:  38%|###8      | 25/65 [04:45<07:37, 11.43s/it]
2022-04-09 15:41:50,733 - INFO - tqdm - accuracy: 0.8053, batch_loss: 0.2953, loss: 0.4283 ||:  40%|####      | 26/65 [04:57<07:25, 11.43s/it]
2022-04-09 15:42:02,158 - INFO - tqdm - accuracy: 0.8056, batch_loss: 0.4201, loss: 0.4280 ||:  42%|####1     | 27/65 [05:08<07:14, 11.43s/it]
2022-04-09 15:42:13,542 - INFO - tqdm - accuracy: 0.8047, batch_loss: 0.3486, loss: 0.4252 ||:  43%|####3     | 28/65 [05:20<07:02, 11.41s/it]
2022-04-09 15:42:24,946 - INFO - tqdm - accuracy: 0.8050, batch_loss: 0.3292, loss: 0.4219 ||:  45%|####4     | 29/65 [05:31<06:50, 11.41s/it]
2022-04-09 15:42:36,350 - INFO - tqdm - accuracy: 0.8052, batch_loss: 0.3330, loss: 0.4189 ||:  46%|####6     | 30/65 [05:42<06:39, 11.41s/it]
2022-04-09 15:42:47,753 - INFO - tqdm - accuracy: 0.8085, batch_loss: 0.4169, loss: 0.4188 ||:  48%|####7     | 31/65 [05:54<06:27, 11.41s/it]
2022-04-09 15:42:59,035 - INFO - tqdm - accuracy: 0.8096, batch_loss: 0.4365, loss: 0.4194 ||:  49%|####9     | 32/65 [06:05<06:15, 11.37s/it]
2022-04-09 15:43:10,449 - INFO - tqdm - accuracy: 0.8097, batch_loss: 0.4004, loss: 0.4188 ||:  51%|#####     | 33/65 [06:17<06:04, 11.38s/it]
2022-04-09 15:43:21,852 - INFO - tqdm - accuracy: 0.8107, batch_loss: 0.3875, loss: 0.4179 ||:  52%|#####2    | 34/65 [06:28<05:53, 11.39s/it]
2022-04-09 15:43:33,272 - INFO - tqdm - accuracy: 0.8107, batch_loss: 0.4024, loss: 0.4175 ||:  54%|#####3    | 35/65 [06:39<05:41, 11.40s/it]
2022-04-09 15:43:44,693 - INFO - tqdm - accuracy: 0.8090, batch_loss: 0.4233, loss: 0.4176 ||:  55%|#####5    | 36/65 [06:51<05:30, 11.40s/it]
2022-04-09 15:43:56,111 - INFO - tqdm - accuracy: 0.8083, batch_loss: 0.3606, loss: 0.4161 ||:  57%|#####6    | 37/65 [07:02<05:19, 11.41s/it]
2022-04-09 15:44:07,516 - INFO - tqdm - accuracy: 0.8092, batch_loss: 0.3197, loss: 0.4135 ||:  58%|#####8    | 38/65 [07:14<05:08, 11.41s/it]
2022-04-09 15:44:18,925 - INFO - tqdm - accuracy: 0.8085, batch_loss: 0.5529, loss: 0.4171 ||:  60%|######    | 39/65 [07:25<04:56, 11.41s/it]
2022-04-09 15:44:30,304 - INFO - tqdm - accuracy: 0.8086, batch_loss: 0.4443, loss: 0.4178 ||:  62%|######1   | 40/65 [07:36<04:44, 11.40s/it]
2022-04-09 15:44:41,706 - INFO - tqdm - accuracy: 0.8102, batch_loss: 0.3388, loss: 0.4159 ||:  63%|######3   | 41/65 [07:48<04:33, 11.40s/it]
2022-04-09 15:44:53,113 - INFO - tqdm - accuracy: 0.8118, batch_loss: 0.3570, loss: 0.4145 ||:  65%|######4   | 42/65 [07:59<04:22, 11.40s/it]
2022-04-09 15:45:04,491 - INFO - tqdm - accuracy: 0.8140, batch_loss: 0.3213, loss: 0.4123 ||:  66%|######6   | 43/65 [08:11<04:10, 11.40s/it]
2022-04-09 15:45:15,896 - INFO - tqdm - accuracy: 0.8146, batch_loss: 0.5126, loss: 0.4146 ||:  68%|######7   | 44/65 [08:22<03:59, 11.40s/it]
2022-04-09 15:45:27,298 - INFO - tqdm - accuracy: 0.8125, batch_loss: 0.5250, loss: 0.4170 ||:  69%|######9   | 45/65 [08:33<03:47, 11.40s/it]
2022-04-09 15:45:38,696 - INFO - tqdm - accuracy: 0.8132, batch_loss: 0.3665, loss: 0.4159 ||:  71%|#######   | 46/65 [08:45<03:36, 11.40s/it]
2022-04-09 15:45:50,085 - INFO - tqdm - accuracy: 0.8138, batch_loss: 0.3870, loss: 0.4153 ||:  72%|#######2  | 47/65 [08:56<03:25, 11.40s/it]
2022-04-09 15:46:01,481 - INFO - tqdm - accuracy: 0.8164, batch_loss: 0.1982, loss: 0.4108 ||:  74%|#######3  | 48/65 [09:08<03:13, 11.40s/it]
2022-04-09 15:46:12,886 - INFO - tqdm - accuracy: 0.8157, batch_loss: 0.4336, loss: 0.4113 ||:  75%|#######5  | 49/65 [09:19<03:02, 11.40s/it]
2022-04-09 15:46:24,260 - INFO - tqdm - accuracy: 0.8150, batch_loss: 0.5253, loss: 0.4135 ||:  77%|#######6  | 50/65 [09:30<02:50, 11.39s/it]
2022-04-09 15:46:35,661 - INFO - tqdm - accuracy: 0.8137, batch_loss: 0.5711, loss: 0.4166 ||:  78%|#######8  | 51/65 [09:42<02:39, 11.39s/it]
2022-04-09 15:46:46,816 - INFO - tqdm - accuracy: 0.8119, batch_loss: 0.5149, loss: 0.4185 ||:  80%|########  | 52/65 [09:53<02:27, 11.32s/it]
2022-04-09 15:46:58,233 - INFO - tqdm - accuracy: 0.8113, batch_loss: 0.4390, loss: 0.4189 ||:  82%|########1 | 53/65 [10:04<02:16, 11.35s/it]
2022-04-09 15:47:09,654 - INFO - tqdm - accuracy: 0.8096, batch_loss: 0.5208, loss: 0.4208 ||:  83%|########3 | 54/65 [10:16<02:05, 11.37s/it]
2022-04-09 15:47:21,046 - INFO - tqdm - accuracy: 0.8085, batch_loss: 0.4371, loss: 0.4211 ||:  85%|########4 | 55/65 [10:27<01:53, 11.38s/it]
2022-04-09 15:47:32,438 - INFO - tqdm - accuracy: 0.8075, batch_loss: 0.4554, loss: 0.4217 ||:  86%|########6 | 56/65 [10:38<01:42, 11.38s/it]
2022-04-09 15:47:43,839 - INFO - tqdm - accuracy: 0.8081, batch_loss: 0.3790, loss: 0.4209 ||:  88%|########7 | 57/65 [10:50<01:31, 11.39s/it]
2022-04-09 15:47:55,237 - INFO - tqdm - accuracy: 0.8103, batch_loss: 0.2803, loss: 0.4185 ||:  89%|########9 | 58/65 [11:01<01:19, 11.39s/it]
2022-04-09 15:48:06,294 - INFO - tqdm - accuracy: 0.8103, batch_loss: 0.4338, loss: 0.4188 ||:  91%|######### | 59/65 [11:12<01:07, 11.29s/it]
2022-04-09 15:48:17,699 - INFO - tqdm - accuracy: 0.8114, batch_loss: 0.3031, loss: 0.4169 ||:  92%|#########2| 60/65 [11:24<00:56, 11.32s/it]
2022-04-09 15:48:29,100 - INFO - tqdm - accuracy: 0.8104, batch_loss: 0.3958, loss: 0.4165 ||:  94%|#########3| 61/65 [11:35<00:45, 11.35s/it]
2022-04-09 15:48:40,490 - INFO - tqdm - accuracy: 0.8109, batch_loss: 0.3575, loss: 0.4156 ||:  95%|#########5| 62/65 [11:47<00:34, 11.36s/it]
2022-04-09 15:48:51,879 - INFO - tqdm - accuracy: 0.8109, batch_loss: 0.4132, loss: 0.4155 ||:  97%|#########6| 63/65 [11:58<00:22, 11.37s/it]
2022-04-09 15:49:03,288 - INFO - tqdm - accuracy: 0.8105, batch_loss: 0.3749, loss: 0.4149 ||:  98%|#########8| 64/65 [12:09<00:11, 11.38s/it]
2022-04-09 15:49:08,333 - INFO - tqdm - accuracy: 0.8103, batch_loss: 0.4563, loss: 0.4155 ||: 100%|##########| 65/65 [12:14<00:00,  9.48s/it]
2022-04-09 15:49:08,333 - INFO - tqdm - accuracy: 0.8103, batch_loss: 0.4563, loss: 0.4155 ||: 100%|##########| 65/65 [12:14<00:00, 11.31s/it]
2022-04-09 15:49:11,287 - INFO - allennlp.training.trainer - Validating
2022-04-09 15:49:11,289 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-09 15:49:21,300 - INFO - tqdm - accuracy: 0.6125, batch_loss: 0.0660, loss: 1.0196 ||:  35%|###4      | 40/115 [00:10<00:18,  4.00it/s]
2022-04-09 15:49:31,324 - INFO - tqdm - accuracy: 0.6438, batch_loss: 0.5075, loss: 0.8870 ||:  70%|######9   | 80/115 [00:20<00:08,  3.93it/s]
2022-04-09 15:49:40,146 - INFO - tqdm - accuracy: 0.6376, batch_loss: 0.0287, loss: 0.8721 ||: 100%|##########| 115/115 [00:28<00:00,  3.97it/s]
2022-04-09 15:49:40,146 - INFO - tqdm - accuracy: 0.6376, batch_loss: 0.0287, loss: 0.8721 ||: 100%|##########| 115/115 [00:28<00:00,  3.99it/s]
2022-04-09 15:49:40,146 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-09 15:49:40,147 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.810  |     0.638
2022-04-09 15:49:40,148 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-09 15:49:40,148 - INFO - allennlp.training.tensorboard_writer - loss               |     0.416  |     0.872
2022-04-09 15:49:40,149 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5804.656  |       N/A
2022-04-09 15:49:45,947 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to './output_ir-ora-d_hyper0/best.th'.
2022-04-09 15:50:01,671 - INFO - allennlp.training.trainer - Epoch duration: 0:13:08.232078
2022-04-09 15:50:01,671 - INFO - allennlp.training.trainer - Estimated training time remaining: 1:17:50
2022-04-09 15:50:01,671 - INFO - allennlp.training.trainer - Epoch 4/9
2022-04-09 15:50:01,671 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-09 15:50:01,672 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-09 15:50:01,673 - INFO - allennlp.training.trainer - Training
2022-04-09 15:50:01,673 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-09 15:50:12,774 - INFO - tqdm - accuracy: 0.9375, batch_loss: 0.3117, loss: 0.3117 ||:   2%|1         | 1/65 [00:11<11:50, 11.10s/it]
2022-04-09 15:50:24,178 - INFO - tqdm - accuracy: 0.9375, batch_loss: 0.1840, loss: 0.2479 ||:   3%|3         | 2/65 [00:22<11:50, 11.28s/it]
2022-04-09 15:50:35,614 - INFO - tqdm - accuracy: 0.9167, batch_loss: 0.3601, loss: 0.2853 ||:   5%|4         | 3/65 [00:33<11:43, 11.35s/it]
2022-04-09 15:50:47,013 - INFO - tqdm - accuracy: 0.9141, batch_loss: 0.3095, loss: 0.2913 ||:   6%|6         | 4/65 [00:45<11:33, 11.37s/it]
2022-04-09 15:50:58,370 - INFO - tqdm - accuracy: 0.9125, batch_loss: 0.2449, loss: 0.2820 ||:   8%|7         | 5/65 [00:56<11:21, 11.37s/it]
2022-04-09 15:51:09,758 - INFO - tqdm - accuracy: 0.9167, batch_loss: 0.2566, loss: 0.2778 ||:   9%|9         | 6/65 [01:08<11:10, 11.37s/it]
2022-04-09 15:51:21,203 - INFO - tqdm - accuracy: 0.9152, batch_loss: 0.2375, loss: 0.2720 ||:  11%|#         | 7/65 [01:19<11:01, 11.40s/it]
2022-04-09 15:51:32,663 - INFO - tqdm - accuracy: 0.9180, batch_loss: 0.1682, loss: 0.2591 ||:  12%|#2        | 8/65 [01:30<10:50, 11.42s/it]
2022-04-09 15:51:44,154 - INFO - tqdm - accuracy: 0.9097, batch_loss: 0.2865, loss: 0.2621 ||:  14%|#3        | 9/65 [01:42<10:40, 11.44s/it]
2022-04-09 15:51:55,640 - INFO - tqdm - accuracy: 0.9125, batch_loss: 0.1694, loss: 0.2528 ||:  15%|#5        | 10/65 [01:53<10:29, 11.45s/it]
2022-04-09 15:52:07,120 - INFO - tqdm - accuracy: 0.9119, batch_loss: 0.2565, loss: 0.2532 ||:  17%|#6        | 11/65 [02:05<10:18, 11.46s/it]
2022-04-09 15:52:18,522 - INFO - tqdm - accuracy: 0.9089, batch_loss: 0.2439, loss: 0.2524 ||:  18%|#8        | 12/65 [02:16<10:06, 11.44s/it]
2022-04-09 15:52:29,967 - INFO - tqdm - accuracy: 0.9062, batch_loss: 0.3358, loss: 0.2588 ||:  20%|##        | 13/65 [02:28<09:55, 11.44s/it]
2022-04-09 15:52:41,419 - INFO - tqdm - accuracy: 0.9129, batch_loss: 0.1006, loss: 0.2475 ||:  22%|##1       | 14/65 [02:39<09:43, 11.45s/it]
2022-04-09 15:52:52,862 - INFO - tqdm - accuracy: 0.9125, batch_loss: 0.2106, loss: 0.2451 ||:  23%|##3       | 15/65 [02:51<09:32, 11.45s/it]
2022-04-09 15:53:04,284 - INFO - tqdm - accuracy: 0.9121, batch_loss: 0.1680, loss: 0.2402 ||:  25%|##4       | 16/65 [03:02<09:20, 11.44s/it]
2022-04-09 15:53:15,714 - INFO - tqdm - accuracy: 0.9136, batch_loss: 0.2043, loss: 0.2381 ||:  26%|##6       | 17/65 [03:14<09:08, 11.44s/it]
2022-04-09 15:53:27,158 - INFO - tqdm - accuracy: 0.9167, batch_loss: 0.1704, loss: 0.2344 ||:  28%|##7       | 18/65 [03:25<08:57, 11.44s/it]
2022-04-09 15:53:38,575 - INFO - tqdm - accuracy: 0.9194, batch_loss: 0.1644, loss: 0.2307 ||:  29%|##9       | 19/65 [03:36<08:45, 11.43s/it]
2022-04-09 15:53:50,001 - INFO - tqdm - accuracy: 0.9141, batch_loss: 0.4547, loss: 0.2419 ||:  31%|###       | 20/65 [03:48<08:34, 11.43s/it]
2022-04-09 15:54:01,410 - INFO - tqdm - accuracy: 0.9152, batch_loss: 0.2297, loss: 0.2413 ||:  32%|###2      | 21/65 [03:59<08:22, 11.42s/it]
2022-04-09 15:54:12,829 - INFO - tqdm - accuracy: 0.9162, batch_loss: 0.2140, loss: 0.2401 ||:  34%|###3      | 22/65 [04:11<08:11, 11.42s/it]
2022-04-09 15:54:24,223 - INFO - tqdm - accuracy: 0.9130, batch_loss: 0.3438, loss: 0.2446 ||:  35%|###5      | 23/65 [04:22<07:59, 11.41s/it]
2022-04-09 15:54:35,626 - INFO - tqdm - accuracy: 0.9115, batch_loss: 0.1795, loss: 0.2419 ||:  37%|###6      | 24/65 [04:33<07:47, 11.41s/it]
2022-04-09 15:54:47,029 - INFO - tqdm - accuracy: 0.9050, batch_loss: 0.4487, loss: 0.2501 ||:  38%|###8      | 25/65 [04:45<07:36, 11.41s/it]
2022-04-09 15:54:58,462 - INFO - tqdm - accuracy: 0.9075, batch_loss: 0.1399, loss: 0.2459 ||:  40%|####      | 26/65 [04:56<07:25, 11.42s/it]
2022-04-09 15:55:09,879 - INFO - tqdm - accuracy: 0.9097, batch_loss: 0.0837, loss: 0.2399 ||:  42%|####1     | 27/65 [05:08<07:13, 11.42s/it]
2022-04-09 15:55:21,301 - INFO - tqdm - accuracy: 0.9107, batch_loss: 0.3697, loss: 0.2445 ||:  43%|####3     | 28/65 [05:19<07:02, 11.42s/it]
2022-04-09 15:55:32,744 - INFO - tqdm - accuracy: 0.9127, batch_loss: 0.1806, loss: 0.2423 ||:  45%|####4     | 29/65 [05:31<06:51, 11.43s/it]
2022-04-09 15:55:44,055 - INFO - tqdm - accuracy: 0.9135, batch_loss: 0.1521, loss: 0.2393 ||:  46%|####6     | 30/65 [05:42<06:38, 11.39s/it]
2022-04-09 15:55:55,495 - INFO - tqdm - accuracy: 0.9163, batch_loss: 0.1011, loss: 0.2349 ||:  48%|####7     | 31/65 [05:53<06:27, 11.41s/it]
2022-04-09 15:56:06,927 - INFO - tqdm - accuracy: 0.9170, batch_loss: 0.1868, loss: 0.2334 ||:  49%|####9     | 32/65 [06:05<06:16, 11.41s/it]
2022-04-09 15:56:18,366 - INFO - tqdm - accuracy: 0.9186, batch_loss: 0.1679, loss: 0.2314 ||:  51%|#####     | 33/65 [06:16<06:05, 11.42s/it]
2022-04-09 15:56:29,816 - INFO - tqdm - accuracy: 0.9182, batch_loss: 0.3893, loss: 0.2360 ||:  52%|#####2    | 34/65 [06:28<05:54, 11.43s/it]
2022-04-09 15:56:41,278 - INFO - tqdm - accuracy: 0.9196, batch_loss: 0.1521, loss: 0.2336 ||:  54%|#####3    | 35/65 [06:39<05:43, 11.44s/it]
2022-04-09 15:56:52,717 - INFO - tqdm - accuracy: 0.9201, batch_loss: 0.1984, loss: 0.2326 ||:  55%|#####5    | 36/65 [06:51<05:31, 11.44s/it]
2022-04-09 15:57:03,819 - INFO - tqdm - accuracy: 0.9189, batch_loss: 0.2599, loss: 0.2334 ||:  57%|#####6    | 37/65 [07:02<05:17, 11.34s/it]
2022-04-09 15:57:15,267 - INFO - tqdm - accuracy: 0.9210, batch_loss: 0.0653, loss: 0.2290 ||:  58%|#####8    | 38/65 [07:13<05:07, 11.37s/it]
2022-04-09 15:57:26,710 - INFO - tqdm - accuracy: 0.9214, batch_loss: 0.1633, loss: 0.2273 ||:  60%|######    | 39/65 [07:25<04:56, 11.39s/it]
2022-04-09 15:57:38,589 - INFO - tqdm - accuracy: 0.9203, batch_loss: 0.3109, loss: 0.2294 ||:  62%|######1   | 40/65 [07:36<04:48, 11.54s/it]
2022-04-09 15:57:50,000 - INFO - tqdm - accuracy: 0.9214, batch_loss: 0.1077, loss: 0.2264 ||:  63%|######3   | 41/65 [07:48<04:36, 11.50s/it]
2022-04-09 15:58:01,447 - INFO - tqdm - accuracy: 0.9218, batch_loss: 0.2027, loss: 0.2258 ||:  65%|######4   | 42/65 [07:59<04:24, 11.48s/it]
2022-04-09 15:58:12,889 - INFO - tqdm - accuracy: 0.9236, batch_loss: 0.1052, loss: 0.2230 ||:  66%|######6   | 43/65 [08:11<04:12, 11.47s/it]
2022-04-09 15:58:24,346 - INFO - tqdm - accuracy: 0.9218, batch_loss: 0.3367, loss: 0.2256 ||:  68%|######7   | 44/65 [08:22<04:00, 11.47s/it]
2022-04-09 15:58:35,671 - INFO - tqdm - accuracy: 0.9222, batch_loss: 0.1399, loss: 0.2237 ||:  69%|######9   | 45/65 [08:33<03:48, 11.42s/it]
2022-04-09 15:58:47,127 - INFO - tqdm - accuracy: 0.9191, batch_loss: 0.4943, loss: 0.2296 ||:  71%|#######   | 46/65 [08:45<03:37, 11.43s/it]
2022-04-09 15:58:58,584 - INFO - tqdm - accuracy: 0.9182, batch_loss: 0.3512, loss: 0.2322 ||:  72%|#######2  | 47/65 [08:56<03:25, 11.44s/it]
2022-04-09 15:59:10,044 - INFO - tqdm - accuracy: 0.9179, batch_loss: 0.2164, loss: 0.2318 ||:  74%|#######3  | 48/65 [09:08<03:14, 11.45s/it]
2022-04-09 15:59:21,486 - INFO - tqdm - accuracy: 0.9190, batch_loss: 0.1564, loss: 0.2303 ||:  75%|#######5  | 49/65 [09:19<03:03, 11.45s/it]
2022-04-09 15:59:32,928 - INFO - tqdm - accuracy: 0.9187, batch_loss: 0.2403, loss: 0.2305 ||:  77%|#######6  | 50/65 [09:31<02:51, 11.44s/it]
2022-04-09 15:59:44,368 - INFO - tqdm - accuracy: 0.9197, batch_loss: 0.1177, loss: 0.2283 ||:  78%|#######8  | 51/65 [09:42<02:40, 11.44s/it]
2022-04-09 15:59:55,800 - INFO - tqdm - accuracy: 0.9200, batch_loss: 0.1952, loss: 0.2277 ||:  80%|########  | 52/65 [09:54<02:28, 11.44s/it]
2022-04-09 16:00:07,198 - INFO - tqdm - accuracy: 0.9186, batch_loss: 0.2792, loss: 0.2286 ||:  82%|########1 | 53/65 [10:05<02:17, 11.43s/it]
2022-04-09 16:00:18,601 - INFO - tqdm - accuracy: 0.9189, batch_loss: 0.2493, loss: 0.2290 ||:  83%|########3 | 54/65 [10:16<02:05, 11.42s/it]
2022-04-09 16:00:29,992 - INFO - tqdm - accuracy: 0.9181, batch_loss: 0.2869, loss: 0.2301 ||:  85%|########4 | 55/65 [10:28<01:54, 11.41s/it]
2022-04-09 16:00:41,391 - INFO - tqdm - accuracy: 0.9185, batch_loss: 0.1122, loss: 0.2280 ||:  86%|########6 | 56/65 [10:39<01:42, 11.41s/it]
2022-04-09 16:00:52,798 - INFO - tqdm - accuracy: 0.9172, batch_loss: 0.2992, loss: 0.2292 ||:  88%|########7 | 57/65 [10:51<01:31, 11.41s/it]
2022-04-09 16:01:04,202 - INFO - tqdm - accuracy: 0.9175, batch_loss: 0.2576, loss: 0.2297 ||:  89%|########9 | 58/65 [11:02<01:19, 11.41s/it]
2022-04-09 16:01:15,586 - INFO - tqdm - accuracy: 0.9173, batch_loss: 0.1755, loss: 0.2288 ||:  91%|######### | 59/65 [11:13<01:08, 11.40s/it]
2022-04-09 16:01:26,973 - INFO - tqdm - accuracy: 0.9161, batch_loss: 0.2969, loss: 0.2299 ||:  92%|#########2| 60/65 [11:25<00:56, 11.40s/it]
2022-04-09 16:01:38,357 - INFO - tqdm - accuracy: 0.9175, batch_loss: 0.1138, loss: 0.2280 ||:  94%|#########3| 61/65 [11:36<00:45, 11.39s/it]
2022-04-09 16:01:49,516 - INFO - tqdm - accuracy: 0.9178, batch_loss: 0.1438, loss: 0.2267 ||:  95%|#########5| 62/65 [11:47<00:33, 11.32s/it]
2022-04-09 16:02:00,914 - INFO - tqdm - accuracy: 0.9186, batch_loss: 0.1490, loss: 0.2254 ||:  97%|#########6| 63/65 [11:59<00:22, 11.35s/it]
2022-04-09 16:02:12,312 - INFO - tqdm - accuracy: 0.9165, batch_loss: 0.3718, loss: 0.2277 ||:  98%|#########8| 64/65 [12:10<00:11, 11.36s/it]
2022-04-09 16:02:17,360 - INFO - tqdm - accuracy: 0.9165, batch_loss: 0.1999, loss: 0.2273 ||: 100%|##########| 65/65 [12:15<00:00,  9.47s/it]
2022-04-09 16:02:17,360 - INFO - tqdm - accuracy: 0.9165, batch_loss: 0.1999, loss: 0.2273 ||: 100%|##########| 65/65 [12:15<00:00, 11.32s/it]
2022-04-09 16:02:20,268 - INFO - allennlp.training.trainer - Validating
2022-04-09 16:02:20,269 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-09 16:02:30,520 - INFO - tqdm - accuracy: 0.6341, batch_loss: 0.0164, loss: 1.2958 ||:  36%|###5      | 41/115 [00:10<00:18,  3.98it/s]
2022-04-09 16:02:40,634 - INFO - tqdm - accuracy: 0.6049, batch_loss: 0.1528, loss: 1.2322 ||:  70%|#######   | 81/115 [00:20<00:08,  3.93it/s]
2022-04-09 16:02:49,112 - INFO - tqdm - accuracy: 0.6157, batch_loss: 0.0069, loss: 1.2480 ||: 100%|##########| 115/115 [00:28<00:00,  3.92it/s]
2022-04-09 16:02:49,113 - INFO - tqdm - accuracy: 0.6157, batch_loss: 0.0069, loss: 1.2480 ||: 100%|##########| 115/115 [00:28<00:00,  3.99it/s]
2022-04-09 16:02:49,113 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-09 16:02:49,113 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.917  |     0.616
2022-04-09 16:02:49,114 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-09 16:02:49,115 - INFO - allennlp.training.tensorboard_writer - loss               |     0.227  |     1.248
2022-04-09 16:02:49,115 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5804.656  |       N/A
2022-04-09 16:02:55,904 - INFO - allennlp.training.trainer - Epoch duration: 0:12:54.233113
2022-04-09 16:02:55,905 - INFO - allennlp.training.trainer - Estimated training time remaining: 1:04:47
2022-04-09 16:02:55,905 - INFO - allennlp.training.trainer - Epoch 5/9
2022-04-09 16:02:55,905 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-09 16:02:55,905 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-09 16:02:55,907 - INFO - allennlp.training.trainer - Training
2022-04-09 16:02:55,907 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-09 16:03:07,058 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0683, loss: 0.0683 ||:   2%|1         | 1/65 [00:11<11:53, 11.15s/it]
2022-04-09 16:03:18,575 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.1013, loss: 0.0848 ||:   3%|3         | 2/65 [00:22<11:56, 11.37s/it]
2022-04-09 16:03:30,281 - INFO - tqdm - accuracy: 0.9583, batch_loss: 0.1891, loss: 0.1196 ||:   5%|4         | 3/65 [00:34<11:54, 11.52s/it]
2022-04-09 16:03:41,712 - INFO - tqdm - accuracy: 0.9531, batch_loss: 0.1188, loss: 0.1194 ||:   6%|6         | 4/65 [00:45<11:40, 11.49s/it]
2022-04-09 16:03:53,065 - INFO - tqdm - accuracy: 0.9563, batch_loss: 0.1007, loss: 0.1156 ||:   8%|7         | 5/65 [00:57<11:26, 11.44s/it]
2022-04-09 16:04:04,405 - INFO - tqdm - accuracy: 0.9531, batch_loss: 0.1365, loss: 0.1191 ||:   9%|9         | 6/65 [01:08<11:12, 11.40s/it]
2022-04-09 16:04:15,708 - INFO - tqdm - accuracy: 0.9509, batch_loss: 0.0883, loss: 0.1147 ||:  11%|#         | 7/65 [01:19<10:59, 11.37s/it]
2022-04-09 16:04:27,214 - INFO - tqdm - accuracy: 0.9570, batch_loss: 0.0669, loss: 0.1087 ||:  12%|#2        | 8/65 [01:31<10:50, 11.41s/it]
2022-04-09 16:04:38,701 - INFO - tqdm - accuracy: 0.9618, batch_loss: 0.0319, loss: 0.1002 ||:  14%|#3        | 9/65 [01:42<10:40, 11.44s/it]
2022-04-09 16:04:50,119 - INFO - tqdm - accuracy: 0.9656, batch_loss: 0.0724, loss: 0.0974 ||:  15%|#5        | 10/65 [01:54<10:28, 11.43s/it]
2022-04-09 16:05:01,551 - INFO - tqdm - accuracy: 0.9631, batch_loss: 0.1214, loss: 0.0996 ||:  17%|#6        | 11/65 [02:05<10:17, 11.43s/it]
2022-04-09 16:05:12,913 - INFO - tqdm - accuracy: 0.9609, batch_loss: 0.1080, loss: 0.1003 ||:  18%|#8        | 12/65 [02:17<10:04, 11.41s/it]
2022-04-09 16:05:24,334 - INFO - tqdm - accuracy: 0.9591, batch_loss: 0.1438, loss: 0.1036 ||:  20%|##        | 13/65 [02:28<09:53, 11.41s/it]
2022-04-09 16:05:35,756 - INFO - tqdm - accuracy: 0.9554, batch_loss: 0.3134, loss: 0.1186 ||:  22%|##1       | 14/65 [02:39<09:42, 11.42s/it]
2022-04-09 16:05:47,214 - INFO - tqdm - accuracy: 0.9583, batch_loss: 0.0259, loss: 0.1124 ||:  23%|##3       | 15/65 [02:51<09:31, 11.43s/it]
2022-04-09 16:05:58,312 - INFO - tqdm - accuracy: 0.9569, batch_loss: 0.0986, loss: 0.1116 ||:  25%|##4       | 16/65 [03:02<09:15, 11.33s/it]
2022-04-09 16:06:09,757 - INFO - tqdm - accuracy: 0.9576, batch_loss: 0.1040, loss: 0.1111 ||:  26%|##6       | 17/65 [03:13<09:05, 11.36s/it]
2022-04-09 16:06:21,201 - INFO - tqdm - accuracy: 0.9513, batch_loss: 0.3398, loss: 0.1238 ||:  28%|##7       | 18/65 [03:25<08:55, 11.39s/it]
2022-04-09 16:06:32,647 - INFO - tqdm - accuracy: 0.9539, batch_loss: 0.0838, loss: 0.1217 ||:  29%|##9       | 19/65 [03:36<08:44, 11.41s/it]
2022-04-09 16:06:44,108 - INFO - tqdm - accuracy: 0.9562, batch_loss: 0.0285, loss: 0.1171 ||:  31%|###       | 20/65 [03:48<08:33, 11.42s/it]
2022-04-09 16:06:55,539 - INFO - tqdm - accuracy: 0.9568, batch_loss: 0.1411, loss: 0.1182 ||:  32%|###2      | 21/65 [03:59<08:22, 11.42s/it]
2022-04-09 16:07:06,978 - INFO - tqdm - accuracy: 0.9559, batch_loss: 0.1538, loss: 0.1198 ||:  34%|###3      | 22/65 [04:11<08:11, 11.43s/it]
2022-04-09 16:07:18,422 - INFO - tqdm - accuracy: 0.9510, batch_loss: 0.2849, loss: 0.1270 ||:  35%|###5      | 23/65 [04:22<08:00, 11.43s/it]
2022-04-09 16:07:29,863 - INFO - tqdm - accuracy: 0.9518, batch_loss: 0.0911, loss: 0.1255 ||:  37%|###6      | 24/65 [04:33<07:48, 11.44s/it]
2022-04-09 16:07:41,306 - INFO - tqdm - accuracy: 0.9512, batch_loss: 0.1050, loss: 0.1247 ||:  38%|###8      | 25/65 [04:45<07:37, 11.44s/it]
2022-04-09 16:07:52,721 - INFO - tqdm - accuracy: 0.9531, batch_loss: 0.0429, loss: 0.1215 ||:  40%|####      | 26/65 [04:56<07:25, 11.43s/it]
2022-04-09 16:08:04,149 - INFO - tqdm - accuracy: 0.9537, batch_loss: 0.1442, loss: 0.1224 ||:  42%|####1     | 27/65 [05:08<07:14, 11.43s/it]
2022-04-09 16:08:15,594 - INFO - tqdm - accuracy: 0.9542, batch_loss: 0.0693, loss: 0.1205 ||:  43%|####3     | 28/65 [05:19<07:03, 11.43s/it]
2022-04-09 16:08:27,000 - INFO - tqdm - accuracy: 0.9558, batch_loss: 0.0356, loss: 0.1176 ||:  45%|####4     | 29/65 [05:31<06:51, 11.43s/it]
2022-04-09 16:08:38,434 - INFO - tqdm - accuracy: 0.9541, batch_loss: 0.2064, loss: 0.1205 ||:  46%|####6     | 30/65 [05:42<06:39, 11.43s/it]
2022-04-09 16:08:49,849 - INFO - tqdm - accuracy: 0.9546, batch_loss: 0.0955, loss: 0.1197 ||:  48%|####7     | 31/65 [05:53<06:28, 11.42s/it]
2022-04-09 16:09:01,266 - INFO - tqdm - accuracy: 0.9541, batch_loss: 0.1544, loss: 0.1208 ||:  49%|####9     | 32/65 [06:05<06:16, 11.42s/it]
2022-04-09 16:09:12,665 - INFO - tqdm - accuracy: 0.9536, batch_loss: 0.1113, loss: 0.1205 ||:  51%|#####     | 33/65 [06:16<06:05, 11.42s/it]
2022-04-09 16:09:24,080 - INFO - tqdm - accuracy: 0.9531, batch_loss: 0.1158, loss: 0.1204 ||:  52%|#####2    | 34/65 [06:28<05:53, 11.42s/it]
2022-04-09 16:09:35,505 - INFO - tqdm - accuracy: 0.9544, batch_loss: 0.0446, loss: 0.1182 ||:  54%|#####3    | 35/65 [06:39<05:42, 11.42s/it]
2022-04-09 16:09:46,928 - INFO - tqdm - accuracy: 0.9548, batch_loss: 0.0766, loss: 0.1171 ||:  55%|#####5    | 36/65 [06:51<05:31, 11.42s/it]
2022-04-09 16:09:58,352 - INFO - tqdm - accuracy: 0.9560, batch_loss: 0.0434, loss: 0.1151 ||:  57%|#####6    | 37/65 [07:02<05:19, 11.42s/it]
2022-04-09 16:10:09,788 - INFO - tqdm - accuracy: 0.9564, batch_loss: 0.0763, loss: 0.1141 ||:  58%|#####8    | 38/65 [07:13<05:08, 11.43s/it]
2022-04-09 16:10:21,187 - INFO - tqdm - accuracy: 0.9559, batch_loss: 0.1498, loss: 0.1150 ||:  60%|######    | 39/65 [07:25<04:56, 11.42s/it]
2022-04-09 16:10:32,615 - INFO - tqdm - accuracy: 0.9539, batch_loss: 0.2838, loss: 0.1192 ||:  62%|######1   | 40/65 [07:36<04:45, 11.42s/it]
2022-04-09 16:10:44,024 - INFO - tqdm - accuracy: 0.9550, batch_loss: 0.0278, loss: 0.1170 ||:  63%|######3   | 41/65 [07:48<04:34, 11.42s/it]
2022-04-09 16:10:55,425 - INFO - tqdm - accuracy: 0.9553, batch_loss: 0.0809, loss: 0.1161 ||:  65%|######4   | 42/65 [07:59<04:22, 11.41s/it]
2022-04-09 16:11:06,835 - INFO - tqdm - accuracy: 0.9542, batch_loss: 0.2212, loss: 0.1185 ||:  66%|######6   | 43/65 [08:10<04:11, 11.41s/it]
2022-04-09 16:11:18,250 - INFO - tqdm - accuracy: 0.9538, batch_loss: 0.3271, loss: 0.1233 ||:  68%|######7   | 44/65 [08:22<03:59, 11.41s/it]
2022-04-09 16:11:29,409 - INFO - tqdm - accuracy: 0.9541, batch_loss: 0.1474, loss: 0.1238 ||:  69%|######9   | 45/65 [08:33<03:46, 11.34s/it]
2022-04-09 16:11:40,835 - INFO - tqdm - accuracy: 0.9538, batch_loss: 0.1504, loss: 0.1244 ||:  71%|#######   | 46/65 [08:44<03:35, 11.36s/it]
2022-04-09 16:11:52,258 - INFO - tqdm - accuracy: 0.9541, batch_loss: 0.1152, loss: 0.1242 ||:  72%|#######2  | 47/65 [08:56<03:24, 11.38s/it]
2022-04-09 16:12:03,657 - INFO - tqdm - accuracy: 0.9531, batch_loss: 0.1955, loss: 0.1257 ||:  74%|#######3  | 48/65 [09:07<03:13, 11.39s/it]
2022-04-09 16:12:15,073 - INFO - tqdm - accuracy: 0.9534, batch_loss: 0.1002, loss: 0.1252 ||:  75%|#######5  | 49/65 [09:19<03:02, 11.40s/it]
2022-04-09 16:12:26,502 - INFO - tqdm - accuracy: 0.9543, batch_loss: 0.0389, loss: 0.1234 ||:  77%|#######6  | 50/65 [09:30<02:51, 11.41s/it]
2022-04-09 16:12:37,912 - INFO - tqdm - accuracy: 0.9546, batch_loss: 0.0868, loss: 0.1227 ||:  78%|#######8  | 51/65 [09:42<02:39, 11.41s/it]
2022-04-09 16:12:49,344 - INFO - tqdm - accuracy: 0.9531, batch_loss: 0.1942, loss: 0.1241 ||:  80%|########  | 52/65 [09:53<02:28, 11.41s/it]
2022-04-09 16:13:00,773 - INFO - tqdm - accuracy: 0.9534, batch_loss: 0.0852, loss: 0.1234 ||:  82%|########1 | 53/65 [10:04<02:17, 11.42s/it]
2022-04-09 16:13:12,185 - INFO - tqdm - accuracy: 0.9537, batch_loss: 0.1055, loss: 0.1230 ||:  83%|########3 | 54/65 [10:16<02:05, 11.42s/it]
2022-04-09 16:13:23,589 - INFO - tqdm - accuracy: 0.9534, batch_loss: 0.1357, loss: 0.1233 ||:  85%|########4 | 55/65 [10:27<01:54, 11.41s/it]
2022-04-09 16:13:35,005 - INFO - tqdm - accuracy: 0.9542, batch_loss: 0.0763, loss: 0.1224 ||:  86%|########6 | 56/65 [10:39<01:42, 11.41s/it]
2022-04-09 16:13:46,396 - INFO - tqdm - accuracy: 0.9545, batch_loss: 0.1089, loss: 0.1222 ||:  88%|########7 | 57/65 [10:50<01:31, 11.41s/it]
2022-04-09 16:13:57,802 - INFO - tqdm - accuracy: 0.9531, batch_loss: 0.4206, loss: 0.1273 ||:  89%|########9 | 58/65 [11:01<01:19, 11.41s/it]
2022-04-09 16:14:09,100 - INFO - tqdm - accuracy: 0.9528, batch_loss: 0.1376, loss: 0.1275 ||:  91%|######### | 59/65 [11:13<01:08, 11.37s/it]
2022-04-09 16:14:20,509 - INFO - tqdm - accuracy: 0.9536, batch_loss: 0.1140, loss: 0.1273 ||:  92%|#########2| 60/65 [11:24<00:56, 11.38s/it]
2022-04-09 16:14:31,933 - INFO - tqdm - accuracy: 0.9528, batch_loss: 0.1389, loss: 0.1275 ||:  94%|#########3| 61/65 [11:36<00:45, 11.40s/it]
2022-04-09 16:14:43,351 - INFO - tqdm - accuracy: 0.9521, batch_loss: 0.1256, loss: 0.1274 ||:  95%|#########5| 62/65 [11:47<00:34, 11.40s/it]
2022-04-09 16:14:54,782 - INFO - tqdm - accuracy: 0.9524, batch_loss: 0.0748, loss: 0.1266 ||:  97%|#########6| 63/65 [11:58<00:22, 11.41s/it]
2022-04-09 16:15:06,199 - INFO - tqdm - accuracy: 0.9521, batch_loss: 0.1176, loss: 0.1265 ||:  98%|#########8| 64/65 [12:10<00:11, 11.41s/it]
2022-04-09 16:15:11,257 - INFO - tqdm - accuracy: 0.9515, batch_loss: 0.2527, loss: 0.1284 ||: 100%|##########| 65/65 [12:15<00:00,  9.51s/it]
2022-04-09 16:15:11,258 - INFO - tqdm - accuracy: 0.9515, batch_loss: 0.2527, loss: 0.1284 ||: 100%|##########| 65/65 [12:15<00:00, 11.31s/it]
2022-04-09 16:15:14,203 - INFO - allennlp.training.trainer - Validating
2022-04-09 16:15:14,204 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-09 16:15:24,219 - INFO - tqdm - accuracy: 0.6250, batch_loss: 6.9553, loss: 1.7923 ||:  35%|###4      | 40/115 [00:10<00:18,  3.99it/s]
2022-04-09 16:15:34,384 - INFO - tqdm - accuracy: 0.6584, batch_loss: 1.9267, loss: 1.4999 ||:  70%|#######   | 81/115 [00:20<00:08,  4.07it/s]
2022-04-09 16:15:43,053 - INFO - tqdm - accuracy: 0.6332, batch_loss: 0.0066, loss: 1.5985 ||: 100%|##########| 115/115 [00:28<00:00,  3.92it/s]
2022-04-09 16:15:43,053 - INFO - tqdm - accuracy: 0.6332, batch_loss: 0.0066, loss: 1.5985 ||: 100%|##########| 115/115 [00:28<00:00,  3.99it/s]
2022-04-09 16:15:43,053 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-09 16:15:43,054 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.951  |     0.633
2022-04-09 16:15:43,055 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-09 16:15:43,055 - INFO - allennlp.training.tensorboard_writer - loss               |     0.128  |     1.598
2022-04-09 16:15:43,056 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5804.656  |       N/A
2022-04-09 16:15:49,224 - INFO - allennlp.training.trainer - Epoch duration: 0:12:53.319137
2022-04-09 16:15:49,224 - INFO - allennlp.training.trainer - Estimated training time remaining: 0:51:47
2022-04-09 16:15:49,224 - INFO - allennlp.training.trainer - Epoch 6/9
2022-04-09 16:15:49,224 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-09 16:15:49,225 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-09 16:15:49,226 - INFO - allennlp.training.trainer - Training
2022-04-09 16:15:49,227 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-09 16:16:00,495 - INFO - tqdm - accuracy: 0.9062, batch_loss: 0.1145, loss: 0.1145 ||:   2%|1         | 1/65 [00:11<12:01, 11.27s/it]
2022-04-09 16:16:11,989 - INFO - tqdm - accuracy: 0.9531, batch_loss: 0.0477, loss: 0.0811 ||:   3%|3         | 2/65 [00:22<11:58, 11.40s/it]
2022-04-09 16:16:23,670 - INFO - tqdm - accuracy: 0.9583, batch_loss: 0.0797, loss: 0.0806 ||:   5%|4         | 3/65 [00:34<11:54, 11.53s/it]
2022-04-09 16:16:35,098 - INFO - tqdm - accuracy: 0.9531, batch_loss: 0.1481, loss: 0.0975 ||:   6%|6         | 4/65 [00:45<11:40, 11.49s/it]
2022-04-09 16:16:46,469 - INFO - tqdm - accuracy: 0.9500, batch_loss: 0.0954, loss: 0.0971 ||:   8%|7         | 5/65 [00:57<11:26, 11.45s/it]
2022-04-09 16:16:57,881 - INFO - tqdm - accuracy: 0.9583, batch_loss: 0.0401, loss: 0.0876 ||:   9%|9         | 6/65 [01:08<11:14, 11.43s/it]
2022-04-09 16:17:09,265 - INFO - tqdm - accuracy: 0.9643, batch_loss: 0.0671, loss: 0.0847 ||:  11%|#         | 7/65 [01:20<11:02, 11.42s/it]
2022-04-09 16:17:20,698 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.0475, loss: 0.0800 ||:  12%|#2        | 8/65 [01:31<10:51, 11.42s/it]
2022-04-09 16:17:32,124 - INFO - tqdm - accuracy: 0.9722, batch_loss: 0.0297, loss: 0.0744 ||:  14%|#3        | 9/65 [01:42<10:39, 11.42s/it]
2022-04-09 16:17:44,081 - INFO - tqdm - accuracy: 0.9656, batch_loss: 0.3245, loss: 0.0994 ||:  15%|#5        | 10/65 [01:54<10:37, 11.59s/it]
2022-04-09 16:17:55,473 - INFO - tqdm - accuracy: 0.9659, batch_loss: 0.0838, loss: 0.0980 ||:  17%|#6        | 11/65 [02:06<10:22, 11.53s/it]
2022-04-09 16:18:06,911 - INFO - tqdm - accuracy: 0.9609, batch_loss: 0.1382, loss: 0.1014 ||:  18%|#8        | 12/65 [02:17<10:09, 11.50s/it]
2022-04-09 16:18:18,375 - INFO - tqdm - accuracy: 0.9615, batch_loss: 0.1013, loss: 0.1014 ||:  20%|##        | 13/65 [02:29<09:57, 11.49s/it]
2022-04-09 16:18:29,808 - INFO - tqdm - accuracy: 0.9598, batch_loss: 0.1893, loss: 0.1076 ||:  22%|##1       | 14/65 [02:40<09:45, 11.47s/it]
2022-04-09 16:18:41,259 - INFO - tqdm - accuracy: 0.9604, batch_loss: 0.0805, loss: 0.1058 ||:  23%|##3       | 15/65 [02:52<09:33, 11.47s/it]
2022-04-09 16:18:52,731 - INFO - tqdm - accuracy: 0.9629, batch_loss: 0.0331, loss: 0.1013 ||:  25%|##4       | 16/65 [03:03<09:21, 11.47s/it]
2022-04-09 16:19:04,179 - INFO - tqdm - accuracy: 0.9632, batch_loss: 0.0497, loss: 0.0982 ||:  26%|##6       | 17/65 [03:14<09:10, 11.46s/it]
2022-04-09 16:19:15,652 - INFO - tqdm - accuracy: 0.9635, batch_loss: 0.0757, loss: 0.0970 ||:  28%|##7       | 18/65 [03:26<08:58, 11.47s/it]
2022-04-09 16:19:27,102 - INFO - tqdm - accuracy: 0.9622, batch_loss: 0.0979, loss: 0.0970 ||:  29%|##9       | 19/65 [03:37<08:47, 11.46s/it]
2022-04-09 16:19:38,561 - INFO - tqdm - accuracy: 0.9641, batch_loss: 0.0188, loss: 0.0931 ||:  31%|###       | 20/65 [03:49<08:35, 11.46s/it]
2022-04-09 16:19:50,012 - INFO - tqdm - accuracy: 0.9658, batch_loss: 0.0631, loss: 0.0917 ||:  32%|###2      | 21/65 [04:00<08:24, 11.46s/it]
2022-04-09 16:20:01,487 - INFO - tqdm - accuracy: 0.9659, batch_loss: 0.0571, loss: 0.0901 ||:  34%|###3      | 22/65 [04:12<08:12, 11.46s/it]
2022-04-09 16:20:12,933 - INFO - tqdm - accuracy: 0.9647, batch_loss: 0.1357, loss: 0.0921 ||:  35%|###5      | 23/65 [04:23<08:01, 11.46s/it]
2022-04-09 16:20:24,393 - INFO - tqdm - accuracy: 0.9635, batch_loss: 0.3394, loss: 0.1024 ||:  37%|###6      | 24/65 [04:35<07:49, 11.46s/it]
2022-04-09 16:20:35,839 - INFO - tqdm - accuracy: 0.9600, batch_loss: 0.2317, loss: 0.1076 ||:  38%|###8      | 25/65 [04:46<07:38, 11.45s/it]
2022-04-09 16:20:47,285 - INFO - tqdm - accuracy: 0.9603, batch_loss: 0.0515, loss: 0.1054 ||:  40%|####      | 26/65 [04:58<07:26, 11.45s/it]
2022-04-09 16:20:58,748 - INFO - tqdm - accuracy: 0.9618, batch_loss: 0.0118, loss: 0.1020 ||:  42%|####1     | 27/65 [05:09<07:15, 11.46s/it]
2022-04-09 16:21:10,194 - INFO - tqdm - accuracy: 0.9632, batch_loss: 0.0234, loss: 0.0992 ||:  43%|####3     | 28/65 [05:20<07:03, 11.45s/it]
2022-04-09 16:21:21,650 - INFO - tqdm - accuracy: 0.9634, batch_loss: 0.0667, loss: 0.0980 ||:  45%|####4     | 29/65 [05:32<06:52, 11.45s/it]
2022-04-09 16:21:32,957 - INFO - tqdm - accuracy: 0.9635, batch_loss: 0.1632, loss: 0.1002 ||:  46%|####6     | 30/65 [05:43<06:39, 11.41s/it]
2022-04-09 16:21:44,376 - INFO - tqdm - accuracy: 0.9637, batch_loss: 0.0814, loss: 0.0996 ||:  48%|####7     | 31/65 [05:55<06:28, 11.41s/it]
2022-04-09 16:21:55,804 - INFO - tqdm - accuracy: 0.9648, batch_loss: 0.0202, loss: 0.0971 ||:  49%|####9     | 32/65 [06:06<06:16, 11.42s/it]
2022-04-09 16:22:07,227 - INFO - tqdm - accuracy: 0.9650, batch_loss: 0.0973, loss: 0.0971 ||:  51%|#####     | 33/65 [06:17<06:05, 11.42s/it]
2022-04-09 16:22:18,656 - INFO - tqdm - accuracy: 0.9660, batch_loss: 0.0289, loss: 0.0951 ||:  52%|#####2    | 34/65 [06:29<05:54, 11.42s/it]
2022-04-09 16:22:30,077 - INFO - tqdm - accuracy: 0.9661, batch_loss: 0.0457, loss: 0.0937 ||:  54%|#####3    | 35/65 [06:40<05:42, 11.42s/it]
2022-04-09 16:22:41,499 - INFO - tqdm - accuracy: 0.9670, batch_loss: 0.0343, loss: 0.0921 ||:  55%|#####5    | 36/65 [06:52<05:31, 11.42s/it]
2022-04-09 16:22:52,927 - INFO - tqdm - accuracy: 0.9671, batch_loss: 0.0769, loss: 0.0916 ||:  57%|#####6    | 37/65 [07:03<05:19, 11.42s/it]
2022-04-09 16:23:04,349 - INFO - tqdm - accuracy: 0.9671, batch_loss: 0.0653, loss: 0.0910 ||:  58%|#####8    | 38/65 [07:15<05:08, 11.42s/it]
2022-04-09 16:23:15,761 - INFO - tqdm - accuracy: 0.9671, batch_loss: 0.0781, loss: 0.0906 ||:  60%|######    | 39/65 [07:26<04:56, 11.42s/it]
2022-04-09 16:23:27,172 - INFO - tqdm - accuracy: 0.9664, batch_loss: 0.2170, loss: 0.0938 ||:  62%|######1   | 40/65 [07:37<04:45, 11.42s/it]
2022-04-09 16:23:38,257 - INFO - tqdm - accuracy: 0.9664, batch_loss: 0.0658, loss: 0.0931 ||:  63%|######3   | 41/65 [07:49<04:31, 11.32s/it]
2022-04-09 16:23:49,660 - INFO - tqdm - accuracy: 0.9672, batch_loss: 0.0283, loss: 0.0916 ||:  65%|######4   | 42/65 [08:00<04:20, 11.34s/it]
2022-04-09 16:24:00,823 - INFO - tqdm - accuracy: 0.9665, batch_loss: 0.1376, loss: 0.0926 ||:  66%|######6   | 43/65 [08:11<04:08, 11.29s/it]
2022-04-09 16:24:12,258 - INFO - tqdm - accuracy: 0.9666, batch_loss: 0.0588, loss: 0.0919 ||:  68%|######7   | 44/65 [08:23<03:57, 11.33s/it]
2022-04-09 16:24:23,666 - INFO - tqdm - accuracy: 0.9666, batch_loss: 0.1212, loss: 0.0925 ||:  69%|######9   | 45/65 [08:34<03:47, 11.36s/it]
2022-04-09 16:24:35,071 - INFO - tqdm - accuracy: 0.9674, batch_loss: 0.0456, loss: 0.0915 ||:  71%|#######   | 46/65 [08:45<03:36, 11.37s/it]
2022-04-09 16:24:46,459 - INFO - tqdm - accuracy: 0.9674, batch_loss: 0.0538, loss: 0.0907 ||:  72%|#######2  | 47/65 [08:57<03:24, 11.38s/it]
2022-04-09 16:24:57,861 - INFO - tqdm - accuracy: 0.9668, batch_loss: 0.0846, loss: 0.0906 ||:  74%|#######3  | 48/65 [09:08<03:13, 11.38s/it]
2022-04-09 16:25:09,270 - INFO - tqdm - accuracy: 0.9668, batch_loss: 0.0597, loss: 0.0899 ||:  75%|#######5  | 49/65 [09:20<03:02, 11.39s/it]
2022-04-09 16:25:20,684 - INFO - tqdm - accuracy: 0.9675, batch_loss: 0.0086, loss: 0.0883 ||:  77%|#######6  | 50/65 [09:31<02:50, 11.40s/it]
2022-04-09 16:25:32,110 - INFO - tqdm - accuracy: 0.9681, batch_loss: 0.0169, loss: 0.0869 ||:  78%|#######8  | 51/65 [09:42<02:39, 11.41s/it]
2022-04-09 16:25:43,395 - INFO - tqdm - accuracy: 0.9681, batch_loss: 0.0710, loss: 0.0866 ||:  80%|########  | 52/65 [09:54<02:27, 11.37s/it]
2022-04-09 16:25:54,802 - INFO - tqdm - accuracy: 0.9681, batch_loss: 0.0958, loss: 0.0868 ||:  82%|########1 | 53/65 [10:05<02:16, 11.38s/it]
2022-04-09 16:26:06,221 - INFO - tqdm - accuracy: 0.9687, batch_loss: 0.0248, loss: 0.0856 ||:  83%|########3 | 54/65 [10:16<02:05, 11.39s/it]
2022-04-09 16:26:17,623 - INFO - tqdm - accuracy: 0.9693, batch_loss: 0.0128, loss: 0.0843 ||:  85%|########4 | 55/65 [10:28<01:53, 11.40s/it]
2022-04-09 16:26:28,901 - INFO - tqdm - accuracy: 0.9687, batch_loss: 0.1730, loss: 0.0859 ||:  86%|########6 | 56/65 [10:39<01:42, 11.36s/it]
2022-04-09 16:26:40,327 - INFO - tqdm - accuracy: 0.9682, batch_loss: 0.1933, loss: 0.0878 ||:  88%|########7 | 57/65 [10:51<01:31, 11.38s/it]
2022-04-09 16:26:51,776 - INFO - tqdm - accuracy: 0.9687, batch_loss: 0.0132, loss: 0.0865 ||:  89%|########9 | 58/65 [11:02<01:19, 11.40s/it]
2022-04-09 16:27:03,224 - INFO - tqdm - accuracy: 0.9693, batch_loss: 0.0281, loss: 0.0855 ||:  91%|######### | 59/65 [11:13<01:08, 11.41s/it]
2022-04-09 16:27:14,660 - INFO - tqdm - accuracy: 0.9698, batch_loss: 0.0192, loss: 0.0844 ||:  92%|#########2| 60/65 [11:25<00:57, 11.42s/it]
2022-04-09 16:27:26,088 - INFO - tqdm - accuracy: 0.9692, batch_loss: 0.1796, loss: 0.0860 ||:  94%|#########3| 61/65 [11:36<00:45, 11.42s/it]
2022-04-09 16:27:37,514 - INFO - tqdm - accuracy: 0.9687, batch_loss: 0.1167, loss: 0.0864 ||:  95%|#########5| 62/65 [11:48<00:34, 11.42s/it]
2022-04-09 16:27:48,941 - INFO - tqdm - accuracy: 0.9687, batch_loss: 0.0602, loss: 0.0860 ||:  97%|#########6| 63/65 [11:59<00:22, 11.42s/it]
2022-04-09 16:28:00,376 - INFO - tqdm - accuracy: 0.9692, batch_loss: 0.0239, loss: 0.0851 ||:  98%|#########8| 64/65 [12:11<00:11, 11.43s/it]
2022-04-09 16:28:05,467 - INFO - tqdm - accuracy: 0.9694, batch_loss: 0.0363, loss: 0.0843 ||: 100%|##########| 65/65 [12:16<00:00,  9.53s/it]
2022-04-09 16:28:05,467 - INFO - tqdm - accuracy: 0.9694, batch_loss: 0.0363, loss: 0.0843 ||: 100%|##########| 65/65 [12:16<00:00, 11.33s/it]
2022-04-09 16:28:08,376 - INFO - allennlp.training.trainer - Validating
2022-04-09 16:28:08,377 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-09 16:28:18,546 - INFO - tqdm - accuracy: 0.6420, batch_loss: 5.6216, loss: 1.7074 ||:  36%|###5      | 41/115 [00:10<00:18,  3.97it/s]
2022-04-09 16:28:28,568 - INFO - tqdm - accuracy: 0.6460, batch_loss: 0.0081, loss: 1.6796 ||:  70%|#######   | 81/115 [00:20<00:07,  4.40it/s]
2022-04-09 16:28:37,201 - INFO - tqdm - accuracy: 0.6332, batch_loss: 2.6576, loss: 1.6603 ||: 100%|##########| 115/115 [00:28<00:00,  3.95it/s]
2022-04-09 16:28:37,201 - INFO - tqdm - accuracy: 0.6332, batch_loss: 2.6576, loss: 1.6603 ||: 100%|##########| 115/115 [00:28<00:00,  3.99it/s]
2022-04-09 16:28:37,202 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-09 16:28:37,202 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.969  |     0.633
2022-04-09 16:28:37,202 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-09 16:28:37,203 - INFO - allennlp.training.tensorboard_writer - loss               |     0.084  |     1.660
2022-04-09 16:28:37,204 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5804.656  |       N/A
2022-04-09 16:28:43,341 - INFO - allennlp.training.trainer - Epoch duration: 0:12:54.117024
2022-04-09 16:28:43,342 - INFO - allennlp.training.trainer - Estimated training time remaining: 0:38:49
2022-04-09 16:28:43,342 - INFO - allennlp.training.trainer - Epoch 7/9
2022-04-09 16:28:43,342 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-09 16:28:43,342 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-09 16:28:43,343 - INFO - allennlp.training.trainer - Training
2022-04-09 16:28:43,344 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-09 16:28:54,621 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0389, loss: 0.0389 ||:   2%|1         | 1/65 [00:11<12:01, 11.28s/it]
2022-04-09 16:29:06,149 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.1035, loss: 0.0712 ||:   3%|3         | 2/65 [00:22<11:59, 11.42s/it]
2022-04-09 16:29:17,824 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.0240, loss: 0.0555 ||:   5%|4         | 3/65 [00:34<11:55, 11.54s/it]
2022-04-09 16:29:29,258 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0747, loss: 0.0603 ||:   6%|6         | 4/65 [00:45<11:41, 11.50s/it]
2022-04-09 16:29:40,622 - INFO - tqdm - accuracy: 0.9750, batch_loss: 0.1487, loss: 0.0779 ||:   8%|7         | 5/65 [00:57<11:26, 11.45s/it]
2022-04-09 16:29:51,990 - INFO - tqdm - accuracy: 0.9792, batch_loss: 0.0160, loss: 0.0676 ||:   9%|9         | 6/65 [01:08<11:13, 11.42s/it]
2022-04-09 16:30:03,398 - INFO - tqdm - accuracy: 0.9777, batch_loss: 0.0849, loss: 0.0701 ||:  11%|#         | 7/65 [01:20<11:02, 11.42s/it]
2022-04-09 16:30:14,764 - INFO - tqdm - accuracy: 0.9805, batch_loss: 0.0153, loss: 0.0632 ||:  12%|#2        | 8/65 [01:31<10:49, 11.40s/it]
2022-04-09 16:30:26,157 - INFO - tqdm - accuracy: 0.9792, batch_loss: 0.0405, loss: 0.0607 ||:  14%|#3        | 9/65 [01:42<10:38, 11.40s/it]
2022-04-09 16:30:37,538 - INFO - tqdm - accuracy: 0.9781, batch_loss: 0.1088, loss: 0.0655 ||:  15%|#5        | 10/65 [01:54<10:26, 11.39s/it]
2022-04-09 16:30:48,930 - INFO - tqdm - accuracy: 0.9801, batch_loss: 0.0325, loss: 0.0625 ||:  17%|#6        | 11/65 [02:05<10:15, 11.39s/it]
2022-04-09 16:31:00,325 - INFO - tqdm - accuracy: 0.9740, batch_loss: 0.1295, loss: 0.0681 ||:  18%|#8        | 12/65 [02:16<10:03, 11.39s/it]
2022-04-09 16:31:11,709 - INFO - tqdm - accuracy: 0.9736, batch_loss: 0.0606, loss: 0.0675 ||:  20%|##        | 13/65 [02:28<09:52, 11.39s/it]
2022-04-09 16:31:23,103 - INFO - tqdm - accuracy: 0.9732, batch_loss: 0.0632, loss: 0.0672 ||:  22%|##1       | 14/65 [02:39<09:40, 11.39s/it]
2022-04-09 16:31:34,381 - INFO - tqdm - accuracy: 0.9729, batch_loss: 0.0781, loss: 0.0679 ||:  23%|##3       | 15/65 [02:51<09:27, 11.36s/it]
2022-04-09 16:31:45,797 - INFO - tqdm - accuracy: 0.9746, batch_loss: 0.0394, loss: 0.0662 ||:  25%|##4       | 16/65 [03:02<09:17, 11.38s/it]
2022-04-09 16:31:57,098 - INFO - tqdm - accuracy: 0.9761, batch_loss: 0.0174, loss: 0.0633 ||:  26%|##6       | 17/65 [03:13<09:04, 11.35s/it]
2022-04-09 16:32:08,503 - INFO - tqdm - accuracy: 0.9774, batch_loss: 0.0106, loss: 0.0604 ||:  28%|##7       | 18/65 [03:25<08:54, 11.37s/it]
2022-04-09 16:32:19,907 - INFO - tqdm - accuracy: 0.9786, batch_loss: 0.0294, loss: 0.0587 ||:  29%|##9       | 19/65 [03:36<08:43, 11.38s/it]
2022-04-09 16:32:31,200 - INFO - tqdm - accuracy: 0.9750, batch_loss: 0.1390, loss: 0.0627 ||:  31%|###       | 20/65 [03:47<08:30, 11.35s/it]
2022-04-09 16:32:42,622 - INFO - tqdm - accuracy: 0.9762, batch_loss: 0.0203, loss: 0.0607 ||:  32%|###2      | 21/65 [03:59<08:20, 11.37s/it]
2022-04-09 16:32:54,030 - INFO - tqdm - accuracy: 0.9730, batch_loss: 0.0917, loss: 0.0621 ||:  34%|###3      | 22/65 [04:10<08:09, 11.38s/it]
2022-04-09 16:33:05,450 - INFO - tqdm - accuracy: 0.9728, batch_loss: 0.0410, loss: 0.0612 ||:  35%|###5      | 23/65 [04:22<07:58, 11.39s/it]
2022-04-09 16:33:16,855 - INFO - tqdm - accuracy: 0.9740, batch_loss: 0.0095, loss: 0.0591 ||:  37%|###6      | 24/65 [04:33<07:47, 11.40s/it]
2022-04-09 16:33:28,257 - INFO - tqdm - accuracy: 0.9750, batch_loss: 0.0213, loss: 0.0575 ||:  38%|###8      | 25/65 [04:44<07:35, 11.40s/it]
2022-04-09 16:33:39,656 - INFO - tqdm - accuracy: 0.9748, batch_loss: 0.0822, loss: 0.0585 ||:  40%|####      | 26/65 [04:56<07:24, 11.40s/it]
2022-04-09 16:33:51,068 - INFO - tqdm - accuracy: 0.9757, batch_loss: 0.0130, loss: 0.0568 ||:  42%|####1     | 27/65 [05:07<07:13, 11.40s/it]
2022-04-09 16:34:02,487 - INFO - tqdm - accuracy: 0.9766, batch_loss: 0.0044, loss: 0.0549 ||:  43%|####3     | 28/65 [05:19<07:02, 11.41s/it]
2022-04-09 16:34:13,887 - INFO - tqdm - accuracy: 0.9763, batch_loss: 0.0892, loss: 0.0561 ||:  45%|####4     | 29/65 [05:30<06:50, 11.41s/it]
2022-04-09 16:34:25,301 - INFO - tqdm - accuracy: 0.9760, batch_loss: 0.1556, loss: 0.0594 ||:  46%|####6     | 30/65 [05:41<06:39, 11.41s/it]
2022-04-09 16:34:36,705 - INFO - tqdm - accuracy: 0.9758, batch_loss: 0.0404, loss: 0.0588 ||:  48%|####7     | 31/65 [05:53<06:27, 11.41s/it]
2022-04-09 16:34:48,096 - INFO - tqdm - accuracy: 0.9766, batch_loss: 0.0316, loss: 0.0580 ||:  49%|####9     | 32/65 [06:04<06:16, 11.40s/it]
2022-04-09 16:34:59,501 - INFO - tqdm - accuracy: 0.9773, batch_loss: 0.0076, loss: 0.0564 ||:  51%|#####     | 33/65 [06:16<06:04, 11.40s/it]
2022-04-09 16:35:10,908 - INFO - tqdm - accuracy: 0.9752, batch_loss: 0.1571, loss: 0.0594 ||:  52%|#####2    | 34/65 [06:27<05:53, 11.40s/it]
2022-04-09 16:35:22,333 - INFO - tqdm - accuracy: 0.9759, batch_loss: 0.0441, loss: 0.0590 ||:  54%|#####3    | 35/65 [06:38<05:42, 11.41s/it]
2022-04-09 16:35:33,752 - INFO - tqdm - accuracy: 0.9757, batch_loss: 0.0434, loss: 0.0585 ||:  55%|#####5    | 36/65 [06:50<05:30, 11.41s/it]
2022-04-09 16:35:45,174 - INFO - tqdm - accuracy: 0.9755, batch_loss: 0.1793, loss: 0.0618 ||:  57%|#####6    | 37/65 [07:01<05:19, 11.42s/it]
2022-04-09 16:35:56,606 - INFO - tqdm - accuracy: 0.9762, batch_loss: 0.0119, loss: 0.0605 ||:  58%|#####8    | 38/65 [07:13<05:08, 11.42s/it]
2022-04-09 16:36:07,999 - INFO - tqdm - accuracy: 0.9768, batch_loss: 0.0174, loss: 0.0594 ||:  60%|######    | 39/65 [07:24<04:56, 11.41s/it]
2022-04-09 16:36:19,395 - INFO - tqdm - accuracy: 0.9766, batch_loss: 0.0458, loss: 0.0590 ||:  62%|######1   | 40/65 [07:36<04:45, 11.41s/it]
2022-04-09 16:36:30,795 - INFO - tqdm - accuracy: 0.9771, batch_loss: 0.0337, loss: 0.0584 ||:  63%|######3   | 41/65 [07:47<04:33, 11.41s/it]
2022-04-09 16:36:42,221 - INFO - tqdm - accuracy: 0.9777, batch_loss: 0.0294, loss: 0.0577 ||:  65%|######4   | 42/65 [07:58<04:22, 11.41s/it]
2022-04-09 16:36:53,620 - INFO - tqdm - accuracy: 0.9782, batch_loss: 0.0086, loss: 0.0566 ||:  66%|######6   | 43/65 [08:10<04:10, 11.41s/it]
2022-04-09 16:37:05,025 - INFO - tqdm - accuracy: 0.9780, batch_loss: 0.0677, loss: 0.0568 ||:  68%|######7   | 44/65 [08:21<03:59, 11.41s/it]
2022-04-09 16:37:16,983 - INFO - tqdm - accuracy: 0.9785, batch_loss: 0.0207, loss: 0.0560 ||:  69%|######9   | 45/65 [08:33<03:51, 11.57s/it]
2022-04-09 16:37:28,348 - INFO - tqdm - accuracy: 0.9789, batch_loss: 0.0093, loss: 0.0550 ||:  71%|#######   | 46/65 [08:45<03:38, 11.51s/it]
2022-04-09 16:37:39,768 - INFO - tqdm - accuracy: 0.9794, batch_loss: 0.0091, loss: 0.0540 ||:  72%|#######2  | 47/65 [08:56<03:26, 11.48s/it]
2022-04-09 16:37:51,207 - INFO - tqdm - accuracy: 0.9785, batch_loss: 0.1218, loss: 0.0555 ||:  74%|#######3  | 48/65 [09:07<03:14, 11.47s/it]
2022-04-09 16:38:02,638 - INFO - tqdm - accuracy: 0.9783, batch_loss: 0.0342, loss: 0.0550 ||:  75%|#######5  | 49/65 [09:19<03:03, 11.46s/it]
2022-04-09 16:38:14,059 - INFO - tqdm - accuracy: 0.9781, batch_loss: 0.0719, loss: 0.0554 ||:  77%|#######6  | 50/65 [09:30<02:51, 11.45s/it]
2022-04-09 16:38:25,498 - INFO - tqdm - accuracy: 0.9786, batch_loss: 0.0074, loss: 0.0544 ||:  78%|#######8  | 51/65 [09:42<02:40, 11.44s/it]
2022-04-09 16:38:36,934 - INFO - tqdm - accuracy: 0.9790, batch_loss: 0.0303, loss: 0.0540 ||:  80%|########  | 52/65 [09:53<02:28, 11.44s/it]
2022-04-09 16:38:48,378 - INFO - tqdm - accuracy: 0.9782, batch_loss: 0.1079, loss: 0.0550 ||:  82%|########1 | 53/65 [10:05<02:17, 11.44s/it]
2022-04-09 16:38:59,830 - INFO - tqdm - accuracy: 0.9786, batch_loss: 0.0090, loss: 0.0541 ||:  83%|########3 | 54/65 [10:16<02:05, 11.45s/it]
2022-04-09 16:39:11,280 - INFO - tqdm - accuracy: 0.9784, batch_loss: 0.1508, loss: 0.0559 ||:  85%|########4 | 55/65 [10:27<01:54, 11.45s/it]
2022-04-09 16:39:22,708 - INFO - tqdm - accuracy: 0.9788, batch_loss: 0.0340, loss: 0.0555 ||:  86%|########6 | 56/65 [10:39<01:42, 11.44s/it]
2022-04-09 16:39:34,156 - INFO - tqdm - accuracy: 0.9792, batch_loss: 0.0067, loss: 0.0546 ||:  88%|########7 | 57/65 [10:50<01:31, 11.44s/it]
2022-04-09 16:39:45,615 - INFO - tqdm - accuracy: 0.9795, batch_loss: 0.0148, loss: 0.0540 ||:  89%|########9 | 58/65 [11:02<01:20, 11.45s/it]
2022-04-09 16:39:56,456 - INFO - tqdm - accuracy: 0.9799, batch_loss: 0.0071, loss: 0.0532 ||:  91%|######### | 59/65 [11:13<01:07, 11.27s/it]
2022-04-09 16:40:07,899 - INFO - tqdm - accuracy: 0.9802, batch_loss: 0.0159, loss: 0.0525 ||:  92%|#########2| 60/65 [11:24<00:56, 11.32s/it]
2022-04-09 16:40:19,328 - INFO - tqdm - accuracy: 0.9805, batch_loss: 0.0173, loss: 0.0520 ||:  94%|#########3| 61/65 [11:35<00:45, 11.35s/it]
2022-04-09 16:40:30,744 - INFO - tqdm - accuracy: 0.9803, batch_loss: 0.0762, loss: 0.0523 ||:  95%|#########5| 62/65 [11:47<00:34, 11.37s/it]
2022-04-09 16:40:42,172 - INFO - tqdm - accuracy: 0.9806, batch_loss: 0.0151, loss: 0.0518 ||:  97%|#########6| 63/65 [11:58<00:22, 11.39s/it]
2022-04-09 16:40:53,579 - INFO - tqdm - accuracy: 0.9805, batch_loss: 0.1040, loss: 0.0526 ||:  98%|#########8| 64/65 [12:10<00:11, 11.39s/it]
2022-04-09 16:40:58,656 - INFO - tqdm - accuracy: 0.9806, batch_loss: 0.0058, loss: 0.0519 ||: 100%|##########| 65/65 [12:15<00:00,  9.50s/it]
2022-04-09 16:40:58,657 - INFO - tqdm - accuracy: 0.9806, batch_loss: 0.0058, loss: 0.0519 ||: 100%|##########| 65/65 [12:15<00:00, 11.31s/it]
2022-04-09 16:41:01,606 - INFO - allennlp.training.trainer - Validating
2022-04-09 16:41:01,608 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-09 16:41:11,795 - INFO - tqdm - accuracy: 0.7195, batch_loss: 2.9612, loss: 1.4218 ||:  36%|###5      | 41/115 [00:10<00:18,  4.04it/s]
2022-04-09 16:41:22,042 - INFO - tqdm - accuracy: 0.6503, batch_loss: 0.0130, loss: 1.7291 ||:  71%|#######1  | 82/115 [00:20<00:08,  3.93it/s]
2022-04-09 16:41:30,452 - INFO - tqdm - accuracy: 0.6332, batch_loss: 3.3868, loss: 1.8239 ||: 100%|##########| 115/115 [00:28<00:00,  3.93it/s]
2022-04-09 16:41:30,452 - INFO - tqdm - accuracy: 0.6332, batch_loss: 3.3868, loss: 1.8239 ||: 100%|##########| 115/115 [00:28<00:00,  3.99it/s]
2022-04-09 16:41:30,452 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-09 16:41:30,453 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.981  |     0.633
2022-04-09 16:41:30,453 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-09 16:41:30,453 - INFO - allennlp.training.tensorboard_writer - loss               |     0.052  |     1.824
2022-04-09 16:41:30,455 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5804.656  |       N/A
2022-04-09 16:41:36,815 - INFO - allennlp.training.trainer - Epoch duration: 0:12:53.473579
2022-04-09 16:41:36,816 - INFO - allennlp.training.trainer - Estimated training time remaining: 0:25:52
2022-04-09 16:41:36,816 - INFO - allennlp.training.trainer - Epoch 8/9
2022-04-09 16:41:36,816 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-09 16:41:36,816 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-09 16:41:36,817 - INFO - allennlp.training.trainer - Training
2022-04-09 16:41:36,818 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-09 16:41:48,084 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.0356, loss: 0.0356 ||:   2%|1         | 1/65 [00:11<12:00, 11.27s/it]
2022-04-09 16:41:59,595 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0081, loss: 0.0218 ||:   3%|3         | 2/65 [00:22<11:58, 11.41s/it]
2022-04-09 16:42:11,249 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.0071, loss: 0.0169 ||:   5%|4         | 3/65 [00:34<11:54, 11.52s/it]
2022-04-09 16:42:22,682 - INFO - tqdm - accuracy: 0.9922, batch_loss: 0.0105, loss: 0.0153 ||:   6%|6         | 4/65 [00:45<11:40, 11.49s/it]
2022-04-09 16:42:34,052 - INFO - tqdm - accuracy: 0.9938, batch_loss: 0.0116, loss: 0.0146 ||:   8%|7         | 5/65 [00:57<11:26, 11.44s/it]
2022-04-09 16:42:45,407 - INFO - tqdm - accuracy: 0.9948, batch_loss: 0.0144, loss: 0.0146 ||:   9%|9         | 6/65 [01:08<11:13, 11.41s/it]
2022-04-09 16:42:56,761 - INFO - tqdm - accuracy: 0.9955, batch_loss: 0.0142, loss: 0.0145 ||:  11%|#         | 7/65 [01:19<11:00, 11.39s/it]
2022-04-09 16:43:08,200 - INFO - tqdm - accuracy: 0.9961, batch_loss: 0.0127, loss: 0.0143 ||:  12%|#2        | 8/65 [01:31<10:50, 11.41s/it]
2022-04-09 16:43:19,638 - INFO - tqdm - accuracy: 0.9931, batch_loss: 0.0508, loss: 0.0183 ||:  14%|#3        | 9/65 [01:42<10:39, 11.42s/it]
2022-04-09 16:43:31,054 - INFO - tqdm - accuracy: 0.9906, batch_loss: 0.0466, loss: 0.0212 ||:  15%|#5        | 10/65 [01:54<10:27, 11.42s/it]
2022-04-09 16:43:42,422 - INFO - tqdm - accuracy: 0.9915, batch_loss: 0.0258, loss: 0.0216 ||:  17%|#6        | 11/65 [02:05<10:15, 11.40s/it]
2022-04-09 16:43:53,822 - INFO - tqdm - accuracy: 0.9870, batch_loss: 0.0979, loss: 0.0279 ||:  18%|#8        | 12/65 [02:17<10:04, 11.40s/it]
2022-04-09 16:44:05,177 - INFO - tqdm - accuracy: 0.9832, batch_loss: 0.0707, loss: 0.0312 ||:  20%|##        | 13/65 [02:28<09:52, 11.39s/it]
2022-04-09 16:44:16,544 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0089, loss: 0.0296 ||:  22%|##1       | 14/65 [02:39<09:40, 11.38s/it]
2022-04-09 16:44:27,915 - INFO - tqdm - accuracy: 0.9854, batch_loss: 0.0089, loss: 0.0283 ||:  23%|##3       | 15/65 [02:51<09:28, 11.38s/it]
2022-04-09 16:44:39,297 - INFO - tqdm - accuracy: 0.9863, batch_loss: 0.0112, loss: 0.0272 ||:  25%|##4       | 16/65 [03:02<09:17, 11.38s/it]
2022-04-09 16:44:50,682 - INFO - tqdm - accuracy: 0.9871, batch_loss: 0.0091, loss: 0.0261 ||:  26%|##6       | 17/65 [03:13<09:06, 11.38s/it]
2022-04-09 16:45:02,076 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.1002, loss: 0.0302 ||:  28%|##7       | 18/65 [03:25<08:55, 11.38s/it]
2022-04-09 16:45:13,516 - INFO - tqdm - accuracy: 0.9852, batch_loss: 0.0080, loss: 0.0291 ||:  29%|##9       | 19/65 [03:36<08:44, 11.40s/it]
2022-04-09 16:45:24,920 - INFO - tqdm - accuracy: 0.9828, batch_loss: 0.0723, loss: 0.0312 ||:  31%|###       | 20/65 [03:48<08:33, 11.40s/it]
2022-04-09 16:45:36,363 - INFO - tqdm - accuracy: 0.9836, batch_loss: 0.0146, loss: 0.0304 ||:  32%|###2      | 21/65 [03:59<08:22, 11.41s/it]
2022-04-09 16:45:47,807 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0077, loss: 0.0294 ||:  34%|###3      | 22/65 [04:10<08:11, 11.42s/it]
2022-04-09 16:45:59,234 - INFO - tqdm - accuracy: 0.9851, batch_loss: 0.0081, loss: 0.0285 ||:  35%|###5      | 23/65 [04:22<07:59, 11.42s/it]
2022-04-09 16:46:10,667 - INFO - tqdm - accuracy: 0.9857, batch_loss: 0.0066, loss: 0.0276 ||:  37%|###6      | 24/65 [04:33<07:48, 11.43s/it]
2022-04-09 16:46:22,097 - INFO - tqdm - accuracy: 0.9862, batch_loss: 0.0360, loss: 0.0279 ||:  38%|###8      | 25/65 [04:45<07:37, 11.43s/it]
2022-04-09 16:46:33,543 - INFO - tqdm - accuracy: 0.9868, batch_loss: 0.0041, loss: 0.0270 ||:  40%|####      | 26/65 [04:56<07:25, 11.43s/it]
2022-04-09 16:46:44,742 - INFO - tqdm - accuracy: 0.9873, batch_loss: 0.0297, loss: 0.0271 ||:  42%|####1     | 27/65 [05:07<07:11, 11.36s/it]
2022-04-09 16:46:56,180 - INFO - tqdm - accuracy: 0.9877, batch_loss: 0.0123, loss: 0.0266 ||:  43%|####3     | 28/65 [05:19<07:01, 11.39s/it]
2022-04-09 16:47:07,638 - INFO - tqdm - accuracy: 0.9881, batch_loss: 0.0052, loss: 0.0258 ||:  45%|####4     | 29/65 [05:30<06:50, 11.41s/it]
2022-04-09 16:47:19,095 - INFO - tqdm - accuracy: 0.9885, batch_loss: 0.0123, loss: 0.0254 ||:  46%|####6     | 30/65 [05:42<06:39, 11.42s/it]
2022-04-09 16:47:30,542 - INFO - tqdm - accuracy: 0.9889, batch_loss: 0.0056, loss: 0.0247 ||:  48%|####7     | 31/65 [05:53<06:28, 11.43s/it]
2022-04-09 16:47:41,966 - INFO - tqdm - accuracy: 0.9893, batch_loss: 0.0046, loss: 0.0241 ||:  49%|####9     | 32/65 [06:05<06:17, 11.43s/it]
2022-04-09 16:47:53,406 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.0239, loss: 0.0241 ||:  51%|#####     | 33/65 [06:16<06:05, 11.43s/it]
2022-04-09 16:48:04,854 - INFO - tqdm - accuracy: 0.9899, batch_loss: 0.0343, loss: 0.0244 ||:  52%|#####2    | 34/65 [06:28<05:54, 11.44s/it]
2022-04-09 16:48:16,314 - INFO - tqdm - accuracy: 0.9902, batch_loss: 0.0033, loss: 0.0238 ||:  54%|#####3    | 35/65 [06:39<05:43, 11.44s/it]
2022-04-09 16:48:27,771 - INFO - tqdm - accuracy: 0.9905, batch_loss: 0.0061, loss: 0.0233 ||:  55%|#####5    | 36/65 [06:50<05:31, 11.45s/it]
2022-04-09 16:48:39,221 - INFO - tqdm - accuracy: 0.9907, batch_loss: 0.0340, loss: 0.0236 ||:  57%|#####6    | 37/65 [07:02<05:20, 11.45s/it]
2022-04-09 16:48:50,680 - INFO - tqdm - accuracy: 0.9910, batch_loss: 0.0064, loss: 0.0231 ||:  58%|#####8    | 38/65 [07:13<05:09, 11.45s/it]
2022-04-09 16:49:01,875 - INFO - tqdm - accuracy: 0.9912, batch_loss: 0.0121, loss: 0.0229 ||:  60%|######    | 39/65 [07:25<04:55, 11.37s/it]
2022-04-09 16:49:13,336 - INFO - tqdm - accuracy: 0.9906, batch_loss: 0.0390, loss: 0.0233 ||:  62%|######1   | 40/65 [07:36<04:45, 11.40s/it]
2022-04-09 16:49:24,801 - INFO - tqdm - accuracy: 0.9901, batch_loss: 0.0716, loss: 0.0244 ||:  63%|######3   | 41/65 [07:47<04:34, 11.42s/it]
2022-04-09 16:49:36,254 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.0596, loss: 0.0253 ||:  65%|######4   | 42/65 [07:59<04:22, 11.43s/it]
2022-04-09 16:49:47,713 - INFO - tqdm - accuracy: 0.9898, batch_loss: 0.0108, loss: 0.0249 ||:  66%|######6   | 43/65 [08:10<04:11, 11.44s/it]
2022-04-09 16:49:59,157 - INFO - tqdm - accuracy: 0.9893, batch_loss: 0.0444, loss: 0.0254 ||:  68%|######7   | 44/65 [08:22<04:00, 11.44s/it]
2022-04-09 16:50:10,605 - INFO - tqdm - accuracy: 0.9889, batch_loss: 0.0440, loss: 0.0258 ||:  69%|######9   | 45/65 [08:33<03:48, 11.44s/it]
2022-04-09 16:50:22,049 - INFO - tqdm - accuracy: 0.9891, batch_loss: 0.0096, loss: 0.0255 ||:  71%|#######   | 46/65 [08:45<03:37, 11.44s/it]
2022-04-09 16:50:33,507 - INFO - tqdm - accuracy: 0.9894, batch_loss: 0.0053, loss: 0.0250 ||:  72%|#######2  | 47/65 [08:56<03:26, 11.45s/it]
2022-04-09 16:50:44,958 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.0232, loss: 0.0250 ||:  74%|#######3  | 48/65 [09:08<03:14, 11.45s/it]
2022-04-09 16:50:56,406 - INFO - tqdm - accuracy: 0.9898, batch_loss: 0.0196, loss: 0.0249 ||:  75%|#######5  | 49/65 [09:19<03:03, 11.45s/it]
2022-04-09 16:51:07,851 - INFO - tqdm - accuracy: 0.9894, batch_loss: 0.0337, loss: 0.0251 ||:  77%|#######6  | 50/65 [09:31<02:51, 11.45s/it]
2022-04-09 16:51:19,296 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.0040, loss: 0.0246 ||:  78%|#######8  | 51/65 [09:42<02:40, 11.45s/it]
2022-04-09 16:51:30,739 - INFO - tqdm - accuracy: 0.9898, batch_loss: 0.0041, loss: 0.0242 ||:  80%|########  | 52/65 [09:53<02:28, 11.45s/it]
2022-04-09 16:51:42,171 - INFO - tqdm - accuracy: 0.9900, batch_loss: 0.0133, loss: 0.0240 ||:  82%|########1 | 53/65 [10:05<02:17, 11.44s/it]
2022-04-09 16:51:53,602 - INFO - tqdm - accuracy: 0.9902, batch_loss: 0.0136, loss: 0.0238 ||:  83%|########3 | 54/65 [10:16<02:05, 11.44s/it]
2022-04-09 16:52:05,028 - INFO - tqdm - accuracy: 0.9903, batch_loss: 0.0024, loss: 0.0235 ||:  85%|########4 | 55/65 [10:28<01:54, 11.43s/it]
2022-04-09 16:52:16,456 - INFO - tqdm - accuracy: 0.9905, batch_loss: 0.0495, loss: 0.0239 ||:  86%|########6 | 56/65 [10:39<01:42, 11.43s/it]
2022-04-09 16:52:27,871 - INFO - tqdm - accuracy: 0.9907, batch_loss: 0.0310, loss: 0.0240 ||:  88%|########7 | 57/65 [10:51<01:31, 11.43s/it]
2022-04-09 16:52:39,298 - INFO - tqdm - accuracy: 0.9908, batch_loss: 0.0043, loss: 0.0237 ||:  89%|########9 | 58/65 [11:02<01:19, 11.43s/it]
2022-04-09 16:52:50,711 - INFO - tqdm - accuracy: 0.9910, batch_loss: 0.0072, loss: 0.0234 ||:  91%|######### | 59/65 [11:13<01:08, 11.42s/it]
2022-04-09 16:53:01,783 - INFO - tqdm - accuracy: 0.9911, batch_loss: 0.0083, loss: 0.0232 ||:  92%|#########2| 60/65 [11:24<00:56, 11.32s/it]
2022-04-09 16:53:13,194 - INFO - tqdm - accuracy: 0.9913, batch_loss: 0.0084, loss: 0.0229 ||:  94%|#########3| 61/65 [11:36<00:45, 11.35s/it]
2022-04-09 16:53:24,595 - INFO - tqdm - accuracy: 0.9909, batch_loss: 0.1058, loss: 0.0243 ||:  95%|#########5| 62/65 [11:47<00:34, 11.36s/it]
2022-04-09 16:53:36,017 - INFO - tqdm - accuracy: 0.9906, batch_loss: 0.0530, loss: 0.0247 ||:  97%|#########6| 63/65 [11:59<00:22, 11.38s/it]
2022-04-09 16:53:47,446 - INFO - tqdm - accuracy: 0.9902, batch_loss: 0.0401, loss: 0.0250 ||:  98%|#########8| 64/65 [12:10<00:11, 11.39s/it]
2022-04-09 16:53:52,391 - INFO - tqdm - accuracy: 0.9903, batch_loss: 0.0053, loss: 0.0247 ||: 100%|##########| 65/65 [12:15<00:00,  9.46s/it]
2022-04-09 16:53:52,392 - INFO - tqdm - accuracy: 0.9903, batch_loss: 0.0053, loss: 0.0247 ||: 100%|##########| 65/65 [12:15<00:00, 11.32s/it]
2022-04-09 16:53:55,342 - INFO - allennlp.training.trainer - Validating
2022-04-09 16:53:55,344 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-09 16:54:05,386 - INFO - tqdm - accuracy: 0.6375, batch_loss: 0.2763, loss: 2.0304 ||:  35%|###4      | 40/115 [00:10<00:18,  3.98it/s]
2022-04-09 16:54:15,511 - INFO - tqdm - accuracy: 0.6250, batch_loss: 1.6437, loss: 2.1064 ||:  70%|######9   | 80/115 [00:20<00:08,  3.93it/s]
2022-04-09 16:54:24,211 - INFO - tqdm - accuracy: 0.6245, batch_loss: 0.0022, loss: 1.9835 ||: 100%|##########| 115/115 [00:28<00:00,  4.02it/s]
2022-04-09 16:54:24,212 - INFO - tqdm - accuracy: 0.6245, batch_loss: 0.0022, loss: 1.9835 ||: 100%|##########| 115/115 [00:28<00:00,  3.98it/s]
2022-04-09 16:54:24,212 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-09 16:54:24,212 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.990  |     0.624
2022-04-09 16:54:24,213 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-09 16:54:24,213 - INFO - allennlp.training.tensorboard_writer - loss               |     0.025  |     1.983
2022-04-09 16:54:24,213 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5804.656  |       N/A
2022-04-09 16:54:30,384 - INFO - allennlp.training.trainer - Epoch duration: 0:12:53.568400
2022-04-09 16:54:30,384 - INFO - allennlp.training.trainer - Estimated training time remaining: 0:12:55
2022-04-09 16:54:30,384 - INFO - allennlp.training.trainer - Epoch 9/9
2022-04-09 16:54:30,385 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-09 16:54:30,385 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-09 16:54:30,386 - INFO - allennlp.training.trainer - Training
2022-04-09 16:54:30,386 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-09 16:54:41,641 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0180, loss: 0.0180 ||:   2%|1         | 1/65 [00:11<12:00, 11.25s/it]
2022-04-09 16:54:53,123 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0127, loss: 0.0153 ||:   3%|3         | 2/65 [00:22<11:57, 11.39s/it]
2022-04-09 16:55:04,827 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0252, loss: 0.0186 ||:   5%|4         | 3/65 [00:34<11:54, 11.53s/it]
2022-04-09 16:55:16,177 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0125, loss: 0.0171 ||:   6%|6         | 4/65 [00:45<11:39, 11.46s/it]
2022-04-09 16:55:27,538 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0092, loss: 0.0155 ||:   8%|7         | 5/65 [00:57<11:25, 11.42s/it]
2022-04-09 16:55:38,910 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0154, loss: 0.0155 ||:   9%|9         | 6/65 [01:08<11:13, 11.41s/it]
2022-04-09 16:55:50,356 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0049, loss: 0.0140 ||:  11%|#         | 7/65 [01:19<11:02, 11.42s/it]
2022-04-09 16:56:01,826 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0299, loss: 0.0160 ||:  12%|#2        | 8/65 [01:31<10:51, 11.44s/it]
2022-04-09 16:56:13,292 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0018, loss: 0.0144 ||:  14%|#3        | 9/65 [01:42<10:40, 11.45s/it]
2022-04-09 16:56:24,691 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0205, loss: 0.0150 ||:  15%|#5        | 10/65 [01:54<10:28, 11.43s/it]
2022-04-09 16:56:36,053 - INFO - tqdm - accuracy: 0.9943, batch_loss: 0.1044, loss: 0.0231 ||:  17%|#6        | 11/65 [02:05<10:16, 11.41s/it]
2022-04-09 16:56:47,473 - INFO - tqdm - accuracy: 0.9948, batch_loss: 0.0030, loss: 0.0215 ||:  18%|#8        | 12/65 [02:17<10:04, 11.41s/it]
2022-04-09 16:56:58,893 - INFO - tqdm - accuracy: 0.9952, batch_loss: 0.0088, loss: 0.0205 ||:  20%|##        | 13/65 [02:28<09:53, 11.42s/it]
2022-04-09 16:57:10,325 - INFO - tqdm - accuracy: 0.9955, batch_loss: 0.0149, loss: 0.0201 ||:  22%|##1       | 14/65 [02:39<09:42, 11.42s/it]
2022-04-09 16:57:22,209 - INFO - tqdm - accuracy: 0.9958, batch_loss: 0.0056, loss: 0.0191 ||:  23%|##3       | 15/65 [02:51<09:37, 11.56s/it]
2022-04-09 16:57:33,639 - INFO - tqdm - accuracy: 0.9961, batch_loss: 0.0040, loss: 0.0182 ||:  25%|##4       | 16/65 [03:03<09:24, 11.52s/it]
2022-04-09 16:57:45,100 - INFO - tqdm - accuracy: 0.9963, batch_loss: 0.0228, loss: 0.0185 ||:  26%|##6       | 17/65 [03:14<09:12, 11.50s/it]
2022-04-09 16:57:56,542 - INFO - tqdm - accuracy: 0.9965, batch_loss: 0.0164, loss: 0.0183 ||:  28%|##7       | 18/65 [03:26<08:59, 11.48s/it]
2022-04-09 16:58:08,001 - INFO - tqdm - accuracy: 0.9967, batch_loss: 0.0052, loss: 0.0176 ||:  29%|##9       | 19/65 [03:37<08:47, 11.48s/it]
2022-04-09 16:58:19,420 - INFO - tqdm - accuracy: 0.9938, batch_loss: 0.2147, loss: 0.0275 ||:  31%|###       | 20/65 [03:49<08:35, 11.46s/it]
2022-04-09 16:58:30,806 - INFO - tqdm - accuracy: 0.9940, batch_loss: 0.0037, loss: 0.0264 ||:  32%|###2      | 21/65 [04:00<08:23, 11.44s/it]
2022-04-09 16:58:42,195 - INFO - tqdm - accuracy: 0.9943, batch_loss: 0.0084, loss: 0.0255 ||:  34%|###3      | 22/65 [04:11<08:11, 11.42s/it]
2022-04-09 16:58:53,592 - INFO - tqdm - accuracy: 0.9946, batch_loss: 0.0061, loss: 0.0247 ||:  35%|###5      | 23/65 [04:23<07:59, 11.42s/it]
2022-04-09 16:59:05,035 - INFO - tqdm - accuracy: 0.9922, batch_loss: 0.1244, loss: 0.0289 ||:  37%|###6      | 24/65 [04:34<07:48, 11.42s/it]
2022-04-09 16:59:16,461 - INFO - tqdm - accuracy: 0.9925, batch_loss: 0.0123, loss: 0.0282 ||:  38%|###8      | 25/65 [04:46<07:36, 11.42s/it]
2022-04-09 16:59:27,897 - INFO - tqdm - accuracy: 0.9928, batch_loss: 0.0034, loss: 0.0272 ||:  40%|####      | 26/65 [04:57<07:25, 11.43s/it]
2022-04-09 16:59:39,346 - INFO - tqdm - accuracy: 0.9919, batch_loss: 0.1013, loss: 0.0300 ||:  42%|####1     | 27/65 [05:08<07:14, 11.43s/it]
2022-04-09 16:59:50,791 - INFO - tqdm - accuracy: 0.9911, batch_loss: 0.1704, loss: 0.0350 ||:  43%|####3     | 28/65 [05:20<07:03, 11.44s/it]
2022-04-09 17:00:02,257 - INFO - tqdm - accuracy: 0.9914, batch_loss: 0.0056, loss: 0.0340 ||:  45%|####4     | 29/65 [05:31<06:52, 11.45s/it]
2022-04-09 17:00:13,590 - INFO - tqdm - accuracy: 0.9917, batch_loss: 0.0102, loss: 0.0332 ||:  46%|####6     | 30/65 [05:43<06:39, 11.41s/it]
2022-04-09 17:00:25,024 - INFO - tqdm - accuracy: 0.9919, batch_loss: 0.0179, loss: 0.0327 ||:  48%|####7     | 31/65 [05:54<06:28, 11.42s/it]
2022-04-09 17:00:36,494 - INFO - tqdm - accuracy: 0.9912, batch_loss: 0.0682, loss: 0.0338 ||:  49%|####9     | 32/65 [06:06<06:17, 11.43s/it]
2022-04-09 17:00:47,950 - INFO - tqdm - accuracy: 0.9915, batch_loss: 0.0070, loss: 0.0330 ||:  51%|#####     | 33/65 [06:17<06:06, 11.44s/it]
2022-04-09 17:00:59,396 - INFO - tqdm - accuracy: 0.9917, batch_loss: 0.0244, loss: 0.0327 ||:  52%|#####2    | 34/65 [06:29<05:54, 11.44s/it]
2022-04-09 17:01:10,520 - INFO - tqdm - accuracy: 0.9920, batch_loss: 0.0027, loss: 0.0319 ||:  54%|#####3    | 35/65 [06:40<05:40, 11.35s/it]
2022-04-09 17:01:21,971 - INFO - tqdm - accuracy: 0.9922, batch_loss: 0.0098, loss: 0.0313 ||:  55%|#####5    | 36/65 [06:51<05:29, 11.38s/it]
2022-04-09 17:01:33,427 - INFO - tqdm - accuracy: 0.9915, batch_loss: 0.1901, loss: 0.0356 ||:  57%|#####6    | 37/65 [07:03<05:19, 11.40s/it]
2022-04-09 17:01:44,859 - INFO - tqdm - accuracy: 0.9918, batch_loss: 0.0096, loss: 0.0349 ||:  58%|#####8    | 38/65 [07:14<05:08, 11.41s/it]
2022-04-09 17:01:56,276 - INFO - tqdm - accuracy: 0.9920, batch_loss: 0.0040, loss: 0.0341 ||:  60%|######    | 39/65 [07:25<04:56, 11.41s/it]
2022-04-09 17:02:07,699 - INFO - tqdm - accuracy: 0.9922, batch_loss: 0.0062, loss: 0.0334 ||:  62%|######1   | 40/65 [07:37<04:45, 11.42s/it]
2022-04-09 17:02:19,147 - INFO - tqdm - accuracy: 0.9924, batch_loss: 0.0060, loss: 0.0327 ||:  63%|######3   | 41/65 [07:48<04:34, 11.43s/it]
2022-04-09 17:02:30,583 - INFO - tqdm - accuracy: 0.9926, batch_loss: 0.0052, loss: 0.0321 ||:  65%|######4   | 42/65 [08:00<04:22, 11.43s/it]
2022-04-09 17:02:41,995 - INFO - tqdm - accuracy: 0.9927, batch_loss: 0.0032, loss: 0.0314 ||:  66%|######6   | 43/65 [08:11<04:11, 11.42s/it]
2022-04-09 17:02:53,409 - INFO - tqdm - accuracy: 0.9929, batch_loss: 0.0036, loss: 0.0308 ||:  68%|######7   | 44/65 [08:23<03:59, 11.42s/it]
2022-04-09 17:03:04,796 - INFO - tqdm - accuracy: 0.9924, batch_loss: 0.0874, loss: 0.0320 ||:  69%|######9   | 45/65 [08:34<03:48, 11.41s/it]
2022-04-09 17:03:16,223 - INFO - tqdm - accuracy: 0.9918, batch_loss: 0.0968, loss: 0.0334 ||:  71%|#######   | 46/65 [08:45<03:36, 11.42s/it]
2022-04-09 17:03:27,646 - INFO - tqdm - accuracy: 0.9914, batch_loss: 0.1131, loss: 0.0351 ||:  72%|#######2  | 47/65 [08:57<03:25, 11.42s/it]
2022-04-09 17:03:39,074 - INFO - tqdm - accuracy: 0.9915, batch_loss: 0.0102, loss: 0.0346 ||:  74%|#######3  | 48/65 [09:08<03:14, 11.42s/it]
2022-04-09 17:03:50,495 - INFO - tqdm - accuracy: 0.9917, batch_loss: 0.0140, loss: 0.0342 ||:  75%|#######5  | 49/65 [09:20<03:02, 11.42s/it]
2022-04-09 17:04:01,665 - INFO - tqdm - accuracy: 0.9919, batch_loss: 0.0087, loss: 0.0337 ||:  77%|#######6  | 50/65 [09:31<02:50, 11.35s/it]
2022-04-09 17:04:13,076 - INFO - tqdm - accuracy: 0.9920, batch_loss: 0.0214, loss: 0.0334 ||:  78%|#######8  | 51/65 [09:42<02:39, 11.37s/it]
2022-04-09 17:04:24,500 - INFO - tqdm - accuracy: 0.9922, batch_loss: 0.0109, loss: 0.0330 ||:  80%|########  | 52/65 [09:54<02:27, 11.38s/it]
2022-04-09 17:04:35,905 - INFO - tqdm - accuracy: 0.9923, batch_loss: 0.0154, loss: 0.0327 ||:  82%|########1 | 53/65 [10:05<02:16, 11.39s/it]
2022-04-09 17:04:47,315 - INFO - tqdm - accuracy: 0.9925, batch_loss: 0.0044, loss: 0.0322 ||:  83%|########3 | 54/65 [10:16<02:05, 11.40s/it]
2022-04-09 17:04:58,726 - INFO - tqdm - accuracy: 0.9926, batch_loss: 0.0021, loss: 0.0316 ||:  85%|########4 | 55/65 [10:28<01:54, 11.40s/it]
2022-04-09 17:05:10,117 - INFO - tqdm - accuracy: 0.9922, batch_loss: 0.1066, loss: 0.0329 ||:  86%|########6 | 56/65 [10:39<01:42, 11.40s/it]
2022-04-09 17:05:21,535 - INFO - tqdm - accuracy: 0.9923, batch_loss: 0.0163, loss: 0.0327 ||:  88%|########7 | 57/65 [10:51<01:31, 11.40s/it]
2022-04-09 17:05:32,938 - INFO - tqdm - accuracy: 0.9919, batch_loss: 0.0802, loss: 0.0335 ||:  89%|########9 | 58/65 [11:02<01:19, 11.40s/it]
2022-04-09 17:05:44,331 - INFO - tqdm - accuracy: 0.9915, batch_loss: 0.0476, loss: 0.0337 ||:  91%|######### | 59/65 [11:13<01:08, 11.40s/it]
2022-04-09 17:05:55,745 - INFO - tqdm - accuracy: 0.9917, batch_loss: 0.0055, loss: 0.0332 ||:  92%|#########2| 60/65 [11:25<00:57, 11.40s/it]
2022-04-09 17:06:07,149 - INFO - tqdm - accuracy: 0.9918, batch_loss: 0.0105, loss: 0.0329 ||:  94%|#########3| 61/65 [11:36<00:45, 11.40s/it]
2022-04-09 17:06:18,546 - INFO - tqdm - accuracy: 0.9914, batch_loss: 0.0373, loss: 0.0329 ||:  95%|#########5| 62/65 [11:48<00:34, 11.40s/it]
2022-04-09 17:06:29,945 - INFO - tqdm - accuracy: 0.9916, batch_loss: 0.0036, loss: 0.0325 ||:  97%|#########6| 63/65 [11:59<00:22, 11.40s/it]
2022-04-09 17:06:41,352 - INFO - tqdm - accuracy: 0.9917, batch_loss: 0.0073, loss: 0.0321 ||:  98%|#########8| 64/65 [12:10<00:11, 11.40s/it]
2022-04-09 17:06:46,397 - INFO - tqdm - accuracy: 0.9918, batch_loss: 0.0037, loss: 0.0316 ||: 100%|##########| 65/65 [12:16<00:00,  9.50s/it]
2022-04-09 17:06:46,398 - INFO - tqdm - accuracy: 0.9918, batch_loss: 0.0037, loss: 0.0316 ||: 100%|##########| 65/65 [12:16<00:00, 11.32s/it]
2022-04-09 17:06:49,324 - INFO - allennlp.training.trainer - Validating
2022-04-09 17:06:49,325 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-09 17:06:59,394 - INFO - tqdm - accuracy: 0.6173, batch_loss: 3.6350, loss: 2.1771 ||:  36%|###5      | 41/115 [00:10<00:18,  3.99it/s]
2022-04-09 17:07:09,503 - INFO - tqdm - accuracy: 0.6211, batch_loss: 6.6205, loss: 2.2235 ||:  70%|#######   | 81/115 [00:20<00:08,  3.93it/s]
2022-04-09 17:07:18,198 - INFO - tqdm - accuracy: 0.6288, batch_loss: 0.6097, loss: 1.9866 ||: 100%|##########| 115/115 [00:28<00:00,  3.92it/s]
2022-04-09 17:07:18,198 - INFO - tqdm - accuracy: 0.6288, batch_loss: 0.6097, loss: 1.9866 ||: 100%|##########| 115/115 [00:28<00:00,  3.98it/s]
2022-04-09 17:07:18,198 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-09 17:07:18,199 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.992  |     0.629
2022-04-09 17:07:18,200 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-09 17:07:18,200 - INFO - allennlp.training.tensorboard_writer - loss               |     0.032  |     1.987
2022-04-09 17:07:18,200 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5804.656  |       N/A
2022-04-09 17:07:24,423 - INFO - allennlp.training.trainer - Epoch duration: 0:12:54.038173
2022-04-09 17:07:24,423 - INFO - allennlp.training.checkpointer - loading best weights
2022-04-09 17:07:25,420 - INFO - allennlp.common.util - Metrics: {
  "best_epoch": 3,
  "peak_worker_0_memory_MB": 5804.65625,
  "peak_gpu_0_memory_MB": 9580.5712890625,
  "training_duration": "2:09:09.953638",
  "training_start_epoch": 0,
  "training_epochs": 9,
  "epoch": 9,
  "training_accuracy": 0.9917515769044153,
  "training_loss": 0.03164274851799116,
  "training_worker_0_memory_MB": 5804.65625,
  "training_gpu_0_memory_MB": 9580.5712890625,
  "validation_accuracy": 0.62882096069869,
  "validation_loss": 1.9865986391680528,
  "best_validation_accuracy": 0.6375545851528385,
  "best_validation_loss": 0.8721293103597735
}
2022-04-09 17:07:25,421 - INFO - allennlp.models.archival - archiving weights and vocabulary to ./output_ir-ora-d_hyper0/model.tar.gz
