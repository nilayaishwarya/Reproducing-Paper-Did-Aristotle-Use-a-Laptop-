2022-04-10 11:29:29,251 - INFO - allennlp.common.params - random_seed = 42
2022-04-10 11:29:29,252 - INFO - allennlp.common.params - numpy_seed = 42
2022-04-10 11:29:29,252 - INFO - allennlp.common.params - pytorch_seed = 42
2022-04-10 11:29:29,933 - INFO - allennlp.common.checks - Pytorch version: 1.7.1
2022-04-10 11:29:29,933 - INFO - allennlp.common.params - type = default
2022-04-10 11:29:29,934 - INFO - allennlp.common.params - dataset_reader.type = strategy_qa_reader
2022-04-10 11:29:29,934 - INFO - allennlp.common.params - dataset_reader.lazy = False
2022-04-10 11:29:29,934 - INFO - allennlp.common.params - dataset_reader.cache_directory = None
2022-04-10 11:29:29,934 - INFO - allennlp.common.params - dataset_reader.max_instances = None
2022-04-10 11:29:29,934 - INFO - allennlp.common.params - dataset_reader.manual_distributed_sharding = False
2022-04-10 11:29:29,934 - INFO - allennlp.common.params - dataset_reader.manual_multi_process_sharding = False
2022-04-10 11:29:29,934 - INFO - allennlp.common.params - dataset_reader.tokenizer_wrapper.type = default
2022-04-10 11:29:29,935 - INFO - allennlp.common.params - dataset_reader.tokenizer_wrapper.pretrained_model = roberta-large
2022-04-10 11:29:29,935 - INFO - allennlp.common.params - dataset_reader.tokenizer_wrapper.call_kwargs.return_offsets_mapping = True
2022-04-10 11:29:29,935 - INFO - allennlp.common.params - dataset_reader.tokenizer_wrapper.call_kwargs.return_special_tokens_mask = True
2022-04-10 11:29:29,935 - INFO - allennlp.common.params - dataset_reader.tokenizer_wrapper.call_kwargs.truncation = only_first
2022-04-10 11:29:31,125 - INFO - allennlp.common.params - dataset_reader.save_tokenizer = True
2022-04-10 11:29:31,126 - INFO - allennlp.common.params - dataset_reader.is_training = True
2022-04-10 11:29:31,126 - INFO - allennlp.common.params - dataset_reader.pickle.action = None
2022-04-10 11:29:31,126 - INFO - allennlp.common.params - dataset_reader.pickle.file_name = None
2022-04-10 11:29:31,126 - INFO - allennlp.common.params - dataset_reader.pickle.path = ../pickle
2022-04-10 11:29:31,126 - INFO - allennlp.common.params - dataset_reader.pickle.save_even_when_max_instances = False
2022-04-10 11:29:31,126 - INFO - allennlp.common.params - dataset_reader.paragraphs_source = IR-ORA-D
2022-04-10 11:29:31,126 - INFO - allennlp.common.params - dataset_reader.answer_last_decomposition_step = False
2022-04-10 11:29:31,126 - INFO - allennlp.common.params - dataset_reader.skip_if_context_missing = True
2022-04-10 11:29:31,126 - INFO - allennlp.common.params - dataset_reader.paragraphs_limit = 10
2022-04-10 11:29:31,126 - INFO - allennlp.common.params - dataset_reader.generated_decompositions_paths = None
2022-04-10 11:29:31,126 - INFO - allennlp.common.params - dataset_reader.save_elasticsearch_cache = True
2022-04-10 11:29:31,574 - INFO - filelock - Lock 139794021343120 acquired on cache.lock
2022-04-10 11:29:32,243 - INFO - filelock - Lock 139794021343120 released on cache.lock
2022-04-10 11:29:32,243 - INFO - allennlp.common.params - train_data_path = data/strategyqa/train.json
2022-04-10 11:29:32,243 - INFO - allennlp.common.params - vocabulary = <allennlp.common.lazy.Lazy object at 0x7f245fc001d0>
2022-04-10 11:29:32,243 - INFO - allennlp.common.params - datasets_for_vocab_creation = None
2022-04-10 11:29:32,244 - INFO - allennlp.common.params - validation_dataset_reader.type = strategy_qa_reader
2022-04-10 11:29:32,244 - INFO - allennlp.common.params - validation_dataset_reader.lazy = False
2022-04-10 11:29:32,244 - INFO - allennlp.common.params - validation_dataset_reader.cache_directory = None
2022-04-10 11:29:32,244 - INFO - allennlp.common.params - validation_dataset_reader.max_instances = None
2022-04-10 11:29:32,244 - INFO - allennlp.common.params - validation_dataset_reader.manual_distributed_sharding = False
2022-04-10 11:29:32,244 - INFO - allennlp.common.params - validation_dataset_reader.manual_multi_process_sharding = False
2022-04-10 11:29:32,244 - INFO - allennlp.common.params - validation_dataset_reader.tokenizer_wrapper.type = default
2022-04-10 11:29:32,244 - INFO - allennlp.common.params - validation_dataset_reader.tokenizer_wrapper.pretrained_model = roberta-large
2022-04-10 11:29:32,245 - INFO - allennlp.common.params - validation_dataset_reader.tokenizer_wrapper.call_kwargs.return_offsets_mapping = True
2022-04-10 11:29:32,245 - INFO - allennlp.common.params - validation_dataset_reader.tokenizer_wrapper.call_kwargs.return_special_tokens_mask = True
2022-04-10 11:29:32,245 - INFO - allennlp.common.params - validation_dataset_reader.tokenizer_wrapper.call_kwargs.truncation = only_first
2022-04-10 11:29:33,430 - INFO - allennlp.common.params - validation_dataset_reader.save_tokenizer = False
2022-04-10 11:29:33,430 - INFO - allennlp.common.params - validation_dataset_reader.is_training = False
2022-04-10 11:29:33,430 - INFO - allennlp.common.params - validation_dataset_reader.pickle.action = None
2022-04-10 11:29:33,430 - INFO - allennlp.common.params - validation_dataset_reader.pickle.file_name = None
2022-04-10 11:29:33,431 - INFO - allennlp.common.params - validation_dataset_reader.pickle.path = ../pickle
2022-04-10 11:29:33,431 - INFO - allennlp.common.params - validation_dataset_reader.pickle.save_even_when_max_instances = False
2022-04-10 11:29:33,431 - INFO - allennlp.common.params - validation_dataset_reader.paragraphs_source = IR-ORA-D
2022-04-10 11:29:33,431 - INFO - allennlp.common.params - validation_dataset_reader.answer_last_decomposition_step = False
2022-04-10 11:29:33,431 - INFO - allennlp.common.params - validation_dataset_reader.skip_if_context_missing = True
2022-04-10 11:29:33,431 - INFO - allennlp.common.params - validation_dataset_reader.paragraphs_limit = 10
2022-04-10 11:29:33,431 - INFO - allennlp.common.params - validation_dataset_reader.generated_decompositions_paths = None
2022-04-10 11:29:33,431 - INFO - allennlp.common.params - validation_dataset_reader.save_elasticsearch_cache = True
2022-04-10 11:29:33,431 - INFO - filelock - Lock 139794021775632 acquired on cache.lock
2022-04-10 11:29:34,249 - INFO - filelock - Lock 139794021775632 released on cache.lock
2022-04-10 11:29:34,249 - INFO - allennlp.common.params - validation_data_path = data/strategyqa/dev.json
2022-04-10 11:29:34,249 - INFO - allennlp.common.params - validation_data_loader = None
2022-04-10 11:29:34,249 - INFO - allennlp.common.params - test_data_path = None
2022-04-10 11:29:34,249 - INFO - allennlp.common.params - evaluate_on_test = True
2022-04-10 11:29:34,249 - INFO - allennlp.common.params - batch_weight_key = 
2022-04-10 11:29:34,249 - INFO - allennlp.training.util - Reading training data from data/strategyqa/train.json
2022-04-10 11:29:34,250 - INFO - tqdm - reading instances: 0it [00:00, ?it/s]
2022-04-10 11:29:34,250 - INFO - src.data.dataset_readers.base.base_dataset_reader - Reading the dataset from scratch
2022-04-10 11:29:34,250 - INFO - src.data.dataset_readers.strategy_qa_reader - Reading the dataset:
2022-04-10 11:29:34,250 - INFO - src.data.dataset_readers.strategy_qa_reader - Reading file at data/strategyqa/train.json
2022-04-10 11:29:41,529 - INFO - filelock - Lock 139793362807056 acquired on cache.lock
2022-04-10 11:29:42,301 - INFO - filelock - Lock 139793362807056 released on cache.lock
2022-04-10 11:29:42,338 - INFO - allennlp.training.util - Reading validation data from data/strategyqa/dev.json
2022-04-10 11:29:42,338 - INFO - tqdm - reading instances: 0it [00:00, ?it/s]
2022-04-10 11:29:42,338 - INFO - src.data.dataset_readers.base.base_dataset_reader - Reading the dataset from scratch
2022-04-10 11:29:42,338 - INFO - src.data.dataset_readers.strategy_qa_reader - Reading the dataset:
2022-04-10 11:29:42,338 - INFO - src.data.dataset_readers.strategy_qa_reader - Reading file at data/strategyqa/dev.json
2022-04-10 11:29:42,783 - INFO - src.data.dataset_readers.utils.elasticsearch_utils - Empty results from Elasticsearch for: actions commercial pilots take concerning engine noise arriving departing Orange County, California?
2022-04-10 11:29:43,107 - INFO - filelock - Lock 139793370220688 acquired on cache.lock
2022-04-10 11:29:43,873 - INFO - filelock - Lock 139793370220688 released on cache.lock
2022-04-10 11:29:43,908 - INFO - allennlp.common.params - type = from_instances
2022-04-10 11:29:43,908 - INFO - allennlp.common.params - min_count = None
2022-04-10 11:29:43,908 - INFO - allennlp.common.params - max_vocab_size = None
2022-04-10 11:29:43,909 - INFO - allennlp.common.params - non_padded_namespaces = ('*tags', '*labels')
2022-04-10 11:29:43,909 - INFO - allennlp.common.params - pretrained_files = None
2022-04-10 11:29:43,909 - INFO - allennlp.common.params - only_include_pretrained_words = False
2022-04-10 11:29:43,909 - INFO - allennlp.common.params - tokens_to_add = None
2022-04-10 11:29:43,909 - INFO - allennlp.common.params - min_pretrained_embeddings = None
2022-04-10 11:29:43,909 - INFO - allennlp.common.params - padding_token = @@PADDING@@
2022-04-10 11:29:43,909 - INFO - allennlp.common.params - oov_token = @@UNKNOWN@@
2022-04-10 11:29:43,909 - INFO - allennlp.data.vocabulary - Fitting token dictionary from dataset.
2022-04-10 11:29:43,909 - INFO - tqdm - building vocab: 0it [00:00, ?it/s]
2022-04-10 11:29:43,915 - INFO - allennlp.common.params - model.type = hf_classifier
2022-04-10 11:29:43,915 - INFO - allennlp.common.params - model.regularizer = None
2022-04-10 11:29:43,915 - INFO - allennlp.common.params - model.pretrained_model = roberta-large
2022-04-10 11:29:43,915 - INFO - allennlp.common.params - model.tokenizer_wrapper.type = default
2022-04-10 11:29:43,916 - INFO - allennlp.common.params - model.tokenizer_wrapper.pretrained_model = roberta-large
2022-04-10 11:29:43,916 - INFO - allennlp.common.params - model.tokenizer_wrapper.call_kwargs.return_offsets_mapping = True
2022-04-10 11:29:43,916 - INFO - allennlp.common.params - model.tokenizer_wrapper.call_kwargs.return_special_tokens_mask = True
2022-04-10 11:29:43,916 - INFO - allennlp.common.params - model.tokenizer_wrapper.call_kwargs.truncation = only_first
2022-04-10 11:29:45,091 - INFO - allennlp.common.params - model.num_labels = 2
2022-04-10 11:29:45,092 - INFO - allennlp.common.params - model.label_namespace = labels
2022-04-10 11:29:45,092 - INFO - allennlp.common.params - model.transformer_weights_path = None
2022-04-10 11:29:45,092 - INFO - allennlp.common.params - model.initializer.regexes.0.1.type = pretrained
2022-04-10 11:29:45,092 - INFO - allennlp.common.params - model.initializer.regexes.0.1.weights_file_path = output_roberta-star/model.tar.gz
2022-04-10 11:29:45,096 - INFO - allennlp.models.archival - extracting archive file output_roberta-star/model.tar.gz to temp dir /tmp/tmptdz4mrmm
2022-04-10 11:29:57,160 - INFO - allennlp.models.archival - removing temporary unarchived model dir at /tmp/tmptdz4mrmm
2022-04-10 11:29:57,334 - INFO - allennlp.common.params - model.initializer.prevent_regexes = None
2022-04-10 11:30:21,146 - INFO - allennlp.nn.initializers - Initializing parameters
2022-04-10 11:30:21,146 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.embeddings.word_embeddings.weight using .* initializer
2022-04-10 11:30:21,155 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.embeddings.position_embeddings.weight using .* initializer
2022-04-10 11:30:21,156 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.embeddings.token_type_embeddings.weight using .* initializer
2022-04-10 11:30:21,156 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.embeddings.LayerNorm.weight using .* initializer
2022-04-10 11:30:21,156 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.embeddings.LayerNorm.bias using .* initializer
2022-04-10 11:30:21,156 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.self.query.weight using .* initializer
2022-04-10 11:30:21,157 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.self.query.bias using .* initializer
2022-04-10 11:30:21,157 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.self.key.weight using .* initializer
2022-04-10 11:30:21,157 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.self.key.bias using .* initializer
2022-04-10 11:30:21,158 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.self.value.weight using .* initializer
2022-04-10 11:30:21,158 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.self.value.bias using .* initializer
2022-04-10 11:30:21,158 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.output.dense.weight using .* initializer
2022-04-10 11:30:21,158 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.output.dense.bias using .* initializer
2022-04-10 11:30:21,159 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.output.LayerNorm.weight using .* initializer
2022-04-10 11:30:21,159 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.output.LayerNorm.bias using .* initializer
2022-04-10 11:30:21,159 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.intermediate.dense.weight using .* initializer
2022-04-10 11:30:21,160 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.intermediate.dense.bias using .* initializer
2022-04-10 11:30:21,160 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.output.dense.weight using .* initializer
2022-04-10 11:30:21,161 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.output.dense.bias using .* initializer
2022-04-10 11:30:21,161 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.output.LayerNorm.weight using .* initializer
2022-04-10 11:30:21,161 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.output.LayerNorm.bias using .* initializer
2022-04-10 11:30:21,161 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.self.query.weight using .* initializer
2022-04-10 11:30:21,162 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.self.query.bias using .* initializer
2022-04-10 11:30:21,162 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.self.key.weight using .* initializer
2022-04-10 11:30:21,162 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.self.key.bias using .* initializer
2022-04-10 11:30:21,162 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.self.value.weight using .* initializer
2022-04-10 11:30:21,163 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.self.value.bias using .* initializer
2022-04-10 11:30:21,163 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.output.dense.weight using .* initializer
2022-04-10 11:30:21,163 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.output.dense.bias using .* initializer
2022-04-10 11:30:21,163 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.output.LayerNorm.weight using .* initializer
2022-04-10 11:30:21,163 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.output.LayerNorm.bias using .* initializer
2022-04-10 11:30:21,163 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.intermediate.dense.weight using .* initializer
2022-04-10 11:30:21,164 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.intermediate.dense.bias using .* initializer
2022-04-10 11:30:21,165 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.output.dense.weight using .* initializer
2022-04-10 11:30:21,165 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.output.dense.bias using .* initializer
2022-04-10 11:30:21,166 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.output.LayerNorm.weight using .* initializer
2022-04-10 11:30:21,166 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.output.LayerNorm.bias using .* initializer
2022-04-10 11:30:21,166 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.self.query.weight using .* initializer
2022-04-10 11:30:21,166 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.self.query.bias using .* initializer
2022-04-10 11:30:21,166 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.self.key.weight using .* initializer
2022-04-10 11:30:21,167 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.self.key.bias using .* initializer
2022-04-10 11:30:21,167 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.self.value.weight using .* initializer
2022-04-10 11:30:21,167 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.self.value.bias using .* initializer
2022-04-10 11:30:21,167 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.output.dense.weight using .* initializer
2022-04-10 11:30:21,168 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.output.dense.bias using .* initializer
2022-04-10 11:30:21,168 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.output.LayerNorm.weight using .* initializer
2022-04-10 11:30:21,168 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.output.LayerNorm.bias using .* initializer
2022-04-10 11:30:21,168 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.intermediate.dense.weight using .* initializer
2022-04-10 11:30:21,169 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.intermediate.dense.bias using .* initializer
2022-04-10 11:30:21,169 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.output.dense.weight using .* initializer
2022-04-10 11:30:21,170 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.output.dense.bias using .* initializer
2022-04-10 11:30:21,170 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.output.LayerNorm.weight using .* initializer
2022-04-10 11:30:21,171 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.output.LayerNorm.bias using .* initializer
2022-04-10 11:30:21,171 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.self.query.weight using .* initializer
2022-04-10 11:30:21,171 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.self.query.bias using .* initializer
2022-04-10 11:30:21,171 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.self.key.weight using .* initializer
2022-04-10 11:30:21,172 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.self.key.bias using .* initializer
2022-04-10 11:30:21,172 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.self.value.weight using .* initializer
2022-04-10 11:30:21,172 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.self.value.bias using .* initializer
2022-04-10 11:30:21,173 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.output.dense.weight using .* initializer
2022-04-10 11:30:21,173 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.output.dense.bias using .* initializer
2022-04-10 11:30:21,173 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.output.LayerNorm.weight using .* initializer
2022-04-10 11:30:21,173 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.output.LayerNorm.bias using .* initializer
2022-04-10 11:30:21,173 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.intermediate.dense.weight using .* initializer
2022-04-10 11:30:21,174 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.intermediate.dense.bias using .* initializer
2022-04-10 11:30:21,174 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.output.dense.weight using .* initializer
2022-04-10 11:30:21,175 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.output.dense.bias using .* initializer
2022-04-10 11:30:21,175 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.output.LayerNorm.weight using .* initializer
2022-04-10 11:30:21,176 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.output.LayerNorm.bias using .* initializer
2022-04-10 11:30:21,176 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.self.query.weight using .* initializer
2022-04-10 11:30:21,176 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.self.query.bias using .* initializer
2022-04-10 11:30:21,176 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.self.key.weight using .* initializer
2022-04-10 11:30:21,177 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.self.key.bias using .* initializer
2022-04-10 11:30:21,177 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.self.value.weight using .* initializer
2022-04-10 11:30:21,177 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.self.value.bias using .* initializer
2022-04-10 11:30:21,177 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.output.dense.weight using .* initializer
2022-04-10 11:30:21,178 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.output.dense.bias using .* initializer
2022-04-10 11:30:21,178 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.output.LayerNorm.weight using .* initializer
2022-04-10 11:30:21,178 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.output.LayerNorm.bias using .* initializer
2022-04-10 11:30:21,178 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.intermediate.dense.weight using .* initializer
2022-04-10 11:30:21,179 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.intermediate.dense.bias using .* initializer
2022-04-10 11:30:21,179 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.output.dense.weight using .* initializer
2022-04-10 11:30:21,180 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.output.dense.bias using .* initializer
2022-04-10 11:30:21,180 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.output.LayerNorm.weight using .* initializer
2022-04-10 11:30:21,180 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.output.LayerNorm.bias using .* initializer
2022-04-10 11:30:21,181 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.self.query.weight using .* initializer
2022-04-10 11:30:21,181 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.self.query.bias using .* initializer
2022-04-10 11:30:21,181 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.self.key.weight using .* initializer
2022-04-10 11:30:21,181 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.self.key.bias using .* initializer
2022-04-10 11:30:21,182 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.self.value.weight using .* initializer
2022-04-10 11:30:21,182 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.self.value.bias using .* initializer
2022-04-10 11:30:21,182 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.output.dense.weight using .* initializer
2022-04-10 11:30:21,182 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.output.dense.bias using .* initializer
2022-04-10 11:30:21,182 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.output.LayerNorm.weight using .* initializer
2022-04-10 11:30:21,183 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.output.LayerNorm.bias using .* initializer
2022-04-10 11:30:21,183 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.intermediate.dense.weight using .* initializer
2022-04-10 11:30:21,184 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.intermediate.dense.bias using .* initializer
2022-04-10 11:30:21,184 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.output.dense.weight using .* initializer
2022-04-10 11:30:21,185 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.output.dense.bias using .* initializer
2022-04-10 11:30:21,185 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.output.LayerNorm.weight using .* initializer
2022-04-10 11:30:21,185 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.output.LayerNorm.bias using .* initializer
2022-04-10 11:30:21,185 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.self.query.weight using .* initializer
2022-04-10 11:30:21,186 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.self.query.bias using .* initializer
2022-04-10 11:30:21,186 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.self.key.weight using .* initializer
2022-04-10 11:30:21,186 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.self.key.bias using .* initializer
2022-04-10 11:30:21,186 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.self.value.weight using .* initializer
2022-04-10 11:30:21,187 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.self.value.bias using .* initializer
2022-04-10 11:30:21,187 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.output.dense.weight using .* initializer
2022-04-10 11:30:21,187 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.output.dense.bias using .* initializer
2022-04-10 11:30:21,188 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.output.LayerNorm.weight using .* initializer
2022-04-10 11:30:21,188 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.output.LayerNorm.bias using .* initializer
2022-04-10 11:30:21,188 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.intermediate.dense.weight using .* initializer
2022-04-10 11:30:21,189 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.intermediate.dense.bias using .* initializer
2022-04-10 11:30:21,189 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.output.dense.weight using .* initializer
2022-04-10 11:30:21,190 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.output.dense.bias using .* initializer
2022-04-10 11:30:21,190 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.output.LayerNorm.weight using .* initializer
2022-04-10 11:30:21,191 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.output.LayerNorm.bias using .* initializer
2022-04-10 11:30:21,191 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.self.query.weight using .* initializer
2022-04-10 11:30:21,191 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.self.query.bias using .* initializer
2022-04-10 11:30:21,191 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.self.key.weight using .* initializer
2022-04-10 11:30:21,192 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.self.key.bias using .* initializer
2022-04-10 11:30:21,192 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.self.value.weight using .* initializer
2022-04-10 11:30:21,192 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.self.value.bias using .* initializer
2022-04-10 11:30:21,193 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.output.dense.weight using .* initializer
2022-04-10 11:30:21,193 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.output.dense.bias using .* initializer
2022-04-10 11:30:21,193 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.output.LayerNorm.weight using .* initializer
2022-04-10 11:30:21,193 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.output.LayerNorm.bias using .* initializer
2022-04-10 11:30:21,193 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.intermediate.dense.weight using .* initializer
2022-04-10 11:30:21,194 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.intermediate.dense.bias using .* initializer
2022-04-10 11:30:21,195 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.output.dense.weight using .* initializer
2022-04-10 11:30:21,195 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.output.dense.bias using .* initializer
2022-04-10 11:30:21,196 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.output.LayerNorm.weight using .* initializer
2022-04-10 11:30:21,196 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.output.LayerNorm.bias using .* initializer
2022-04-10 11:30:21,196 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.self.query.weight using .* initializer
2022-04-10 11:30:21,196 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.self.query.bias using .* initializer
2022-04-10 11:30:21,196 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.self.key.weight using .* initializer
2022-04-10 11:30:21,197 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.self.key.bias using .* initializer
2022-04-10 11:30:21,197 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.self.value.weight using .* initializer
2022-04-10 11:30:21,197 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.self.value.bias using .* initializer
2022-04-10 11:30:21,197 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.output.dense.weight using .* initializer
2022-04-10 11:30:21,198 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.output.dense.bias using .* initializer
2022-04-10 11:30:21,198 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.output.LayerNorm.weight using .* initializer
2022-04-10 11:30:21,198 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.output.LayerNorm.bias using .* initializer
2022-04-10 11:30:21,198 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.intermediate.dense.weight using .* initializer
2022-04-10 11:30:21,199 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.intermediate.dense.bias using .* initializer
2022-04-10 11:30:21,199 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.output.dense.weight using .* initializer
2022-04-10 11:30:21,200 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.output.dense.bias using .* initializer
2022-04-10 11:30:21,200 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.output.LayerNorm.weight using .* initializer
2022-04-10 11:30:21,200 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.output.LayerNorm.bias using .* initializer
2022-04-10 11:30:21,200 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.self.query.weight using .* initializer
2022-04-10 11:30:21,201 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.self.query.bias using .* initializer
2022-04-10 11:30:21,201 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.self.key.weight using .* initializer
2022-04-10 11:30:21,201 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.self.key.bias using .* initializer
2022-04-10 11:30:21,201 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.self.value.weight using .* initializer
2022-04-10 11:30:21,202 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.self.value.bias using .* initializer
2022-04-10 11:30:21,202 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.output.dense.weight using .* initializer
2022-04-10 11:30:21,202 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.output.dense.bias using .* initializer
2022-04-10 11:30:21,202 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.output.LayerNorm.weight using .* initializer
2022-04-10 11:30:21,202 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.output.LayerNorm.bias using .* initializer
2022-04-10 11:30:21,203 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.intermediate.dense.weight using .* initializer
2022-04-10 11:30:21,203 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.intermediate.dense.bias using .* initializer
2022-04-10 11:30:21,204 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.output.dense.weight using .* initializer
2022-04-10 11:30:21,205 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.output.dense.bias using .* initializer
2022-04-10 11:30:21,205 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.output.LayerNorm.weight using .* initializer
2022-04-10 11:30:21,205 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.output.LayerNorm.bias using .* initializer
2022-04-10 11:30:21,205 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.self.query.weight using .* initializer
2022-04-10 11:30:21,205 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.self.query.bias using .* initializer
2022-04-10 11:30:21,205 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.self.key.weight using .* initializer
2022-04-10 11:30:21,206 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.self.key.bias using .* initializer
2022-04-10 11:30:21,206 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.self.value.weight using .* initializer
2022-04-10 11:30:21,206 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.self.value.bias using .* initializer
2022-04-10 11:30:21,206 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.output.dense.weight using .* initializer
2022-04-10 11:30:21,207 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.output.dense.bias using .* initializer
2022-04-10 11:30:21,207 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.output.LayerNorm.weight using .* initializer
2022-04-10 11:30:21,207 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.output.LayerNorm.bias using .* initializer
2022-04-10 11:30:21,207 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.intermediate.dense.weight using .* initializer
2022-04-10 11:30:21,208 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.intermediate.dense.bias using .* initializer
2022-04-10 11:30:21,208 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.output.dense.weight using .* initializer
2022-04-10 11:30:21,209 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.output.dense.bias using .* initializer
2022-04-10 11:30:21,209 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.output.LayerNorm.weight using .* initializer
2022-04-10 11:30:21,209 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.output.LayerNorm.bias using .* initializer
2022-04-10 11:30:21,210 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.self.query.weight using .* initializer
2022-04-10 11:30:21,210 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.self.query.bias using .* initializer
2022-04-10 11:30:21,210 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.self.key.weight using .* initializer
2022-04-10 11:30:21,210 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.self.key.bias using .* initializer
2022-04-10 11:30:21,211 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.self.value.weight using .* initializer
2022-04-10 11:30:21,211 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.self.value.bias using .* initializer
2022-04-10 11:30:21,211 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.output.dense.weight using .* initializer
2022-04-10 11:30:21,212 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.output.dense.bias using .* initializer
2022-04-10 11:30:21,212 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.output.LayerNorm.weight using .* initializer
2022-04-10 11:30:21,212 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.output.LayerNorm.bias using .* initializer
2022-04-10 11:30:21,212 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.intermediate.dense.weight using .* initializer
2022-04-10 11:30:21,213 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.intermediate.dense.bias using .* initializer
2022-04-10 11:30:21,213 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.output.dense.weight using .* initializer
2022-04-10 11:30:21,214 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.output.dense.bias using .* initializer
2022-04-10 11:30:21,214 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.output.LayerNorm.weight using .* initializer
2022-04-10 11:30:21,214 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.output.LayerNorm.bias using .* initializer
2022-04-10 11:30:21,214 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.self.query.weight using .* initializer
2022-04-10 11:30:21,215 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.self.query.bias using .* initializer
2022-04-10 11:30:21,215 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.self.key.weight using .* initializer
2022-04-10 11:30:21,215 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.self.key.bias using .* initializer
2022-04-10 11:30:21,215 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.self.value.weight using .* initializer
2022-04-10 11:30:21,216 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.self.value.bias using .* initializer
2022-04-10 11:30:21,216 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.output.dense.weight using .* initializer
2022-04-10 11:30:21,216 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.output.dense.bias using .* initializer
2022-04-10 11:30:21,216 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.output.LayerNorm.weight using .* initializer
2022-04-10 11:30:21,217 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.output.LayerNorm.bias using .* initializer
2022-04-10 11:30:21,217 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.intermediate.dense.weight using .* initializer
2022-04-10 11:30:21,218 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.intermediate.dense.bias using .* initializer
2022-04-10 11:30:21,218 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.output.dense.weight using .* initializer
2022-04-10 11:30:21,219 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.output.dense.bias using .* initializer
2022-04-10 11:30:21,219 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.output.LayerNorm.weight using .* initializer
2022-04-10 11:30:21,219 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.output.LayerNorm.bias using .* initializer
2022-04-10 11:30:21,219 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.self.query.weight using .* initializer
2022-04-10 11:30:21,220 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.self.query.bias using .* initializer
2022-04-10 11:30:21,220 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.self.key.weight using .* initializer
2022-04-10 11:30:21,220 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.self.key.bias using .* initializer
2022-04-10 11:30:21,220 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.self.value.weight using .* initializer
2022-04-10 11:30:21,221 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.self.value.bias using .* initializer
2022-04-10 11:30:21,221 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.output.dense.weight using .* initializer
2022-04-10 11:30:21,221 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.output.dense.bias using .* initializer
2022-04-10 11:30:21,221 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.output.LayerNorm.weight using .* initializer
2022-04-10 11:30:21,221 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.output.LayerNorm.bias using .* initializer
2022-04-10 11:30:21,222 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.intermediate.dense.weight using .* initializer
2022-04-10 11:30:21,222 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.intermediate.dense.bias using .* initializer
2022-04-10 11:30:21,223 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.output.dense.weight using .* initializer
2022-04-10 11:30:21,224 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.output.dense.bias using .* initializer
2022-04-10 11:30:21,224 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.output.LayerNorm.weight using .* initializer
2022-04-10 11:30:21,224 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.output.LayerNorm.bias using .* initializer
2022-04-10 11:30:21,224 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.self.query.weight using .* initializer
2022-04-10 11:30:21,224 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.self.query.bias using .* initializer
2022-04-10 11:30:21,224 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.self.key.weight using .* initializer
2022-04-10 11:30:21,225 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.self.key.bias using .* initializer
2022-04-10 11:30:21,225 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.self.value.weight using .* initializer
2022-04-10 11:30:21,225 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.self.value.bias using .* initializer
2022-04-10 11:30:21,225 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.output.dense.weight using .* initializer
2022-04-10 11:30:21,226 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.output.dense.bias using .* initializer
2022-04-10 11:30:21,226 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.output.LayerNorm.weight using .* initializer
2022-04-10 11:30:21,226 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.output.LayerNorm.bias using .* initializer
2022-04-10 11:30:21,226 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.intermediate.dense.weight using .* initializer
2022-04-10 11:30:21,227 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.intermediate.dense.bias using .* initializer
2022-04-10 11:30:21,227 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.output.dense.weight using .* initializer
2022-04-10 11:30:21,228 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.output.dense.bias using .* initializer
2022-04-10 11:30:21,229 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.output.LayerNorm.weight using .* initializer
2022-04-10 11:30:21,229 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.output.LayerNorm.bias using .* initializer
2022-04-10 11:30:21,229 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.self.query.weight using .* initializer
2022-04-10 11:30:21,229 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.self.query.bias using .* initializer
2022-04-10 11:30:21,229 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.self.key.weight using .* initializer
2022-04-10 11:30:21,230 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.self.key.bias using .* initializer
2022-04-10 11:30:21,230 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.self.value.weight using .* initializer
2022-04-10 11:30:21,230 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.self.value.bias using .* initializer
2022-04-10 11:30:21,230 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.output.dense.weight using .* initializer
2022-04-10 11:30:21,231 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.output.dense.bias using .* initializer
2022-04-10 11:30:21,231 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.output.LayerNorm.weight using .* initializer
2022-04-10 11:30:21,231 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.output.LayerNorm.bias using .* initializer
2022-04-10 11:30:21,231 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.intermediate.dense.weight using .* initializer
2022-04-10 11:30:21,232 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.intermediate.dense.bias using .* initializer
2022-04-10 11:30:21,232 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.output.dense.weight using .* initializer
2022-04-10 11:30:21,233 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.output.dense.bias using .* initializer
2022-04-10 11:30:21,233 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.output.LayerNorm.weight using .* initializer
2022-04-10 11:30:21,233 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.output.LayerNorm.bias using .* initializer
2022-04-10 11:30:21,234 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.self.query.weight using .* initializer
2022-04-10 11:30:21,234 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.self.query.bias using .* initializer
2022-04-10 11:30:21,234 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.self.key.weight using .* initializer
2022-04-10 11:30:21,234 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.self.key.bias using .* initializer
2022-04-10 11:30:21,234 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.self.value.weight using .* initializer
2022-04-10 11:30:21,235 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.self.value.bias using .* initializer
2022-04-10 11:30:21,235 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.output.dense.weight using .* initializer
2022-04-10 11:30:21,235 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.output.dense.bias using .* initializer
2022-04-10 11:30:21,235 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.output.LayerNorm.weight using .* initializer
2022-04-10 11:30:21,236 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.output.LayerNorm.bias using .* initializer
2022-04-10 11:30:21,236 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.intermediate.dense.weight using .* initializer
2022-04-10 11:30:21,237 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.intermediate.dense.bias using .* initializer
2022-04-10 11:30:21,237 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.output.dense.weight using .* initializer
2022-04-10 11:30:21,238 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.output.dense.bias using .* initializer
2022-04-10 11:30:21,238 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.output.LayerNorm.weight using .* initializer
2022-04-10 11:30:21,238 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.output.LayerNorm.bias using .* initializer
2022-04-10 11:30:21,238 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.self.query.weight using .* initializer
2022-04-10 11:30:21,239 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.self.query.bias using .* initializer
2022-04-10 11:30:21,239 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.self.key.weight using .* initializer
2022-04-10 11:30:21,239 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.self.key.bias using .* initializer
2022-04-10 11:30:21,239 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.self.value.weight using .* initializer
2022-04-10 11:30:21,240 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.self.value.bias using .* initializer
2022-04-10 11:30:21,240 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.output.dense.weight using .* initializer
2022-04-10 11:30:21,240 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.output.dense.bias using .* initializer
2022-04-10 11:30:21,240 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.output.LayerNorm.weight using .* initializer
2022-04-10 11:30:21,240 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.output.LayerNorm.bias using .* initializer
2022-04-10 11:30:21,241 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.intermediate.dense.weight using .* initializer
2022-04-10 11:30:21,241 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.intermediate.dense.bias using .* initializer
2022-04-10 11:30:21,242 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.output.dense.weight using .* initializer
2022-04-10 11:30:21,243 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.output.dense.bias using .* initializer
2022-04-10 11:30:21,243 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.output.LayerNorm.weight using .* initializer
2022-04-10 11:30:21,243 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.output.LayerNorm.bias using .* initializer
2022-04-10 11:30:21,243 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.self.query.weight using .* initializer
2022-04-10 11:30:21,243 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.self.query.bias using .* initializer
2022-04-10 11:30:21,243 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.self.key.weight using .* initializer
2022-04-10 11:30:21,244 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.self.key.bias using .* initializer
2022-04-10 11:30:21,244 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.self.value.weight using .* initializer
2022-04-10 11:30:21,244 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.self.value.bias using .* initializer
2022-04-10 11:30:21,244 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.output.dense.weight using .* initializer
2022-04-10 11:30:21,245 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.output.dense.bias using .* initializer
2022-04-10 11:30:21,245 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.output.LayerNorm.weight using .* initializer
2022-04-10 11:30:21,245 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.output.LayerNorm.bias using .* initializer
2022-04-10 11:30:21,245 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.intermediate.dense.weight using .* initializer
2022-04-10 11:30:21,246 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.intermediate.dense.bias using .* initializer
2022-04-10 11:30:21,246 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.output.dense.weight using .* initializer
2022-04-10 11:30:21,247 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.output.dense.bias using .* initializer
2022-04-10 11:30:21,248 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.output.LayerNorm.weight using .* initializer
2022-04-10 11:30:21,248 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.output.LayerNorm.bias using .* initializer
2022-04-10 11:30:21,248 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.self.query.weight using .* initializer
2022-04-10 11:30:21,248 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.self.query.bias using .* initializer
2022-04-10 11:30:21,248 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.self.key.weight using .* initializer
2022-04-10 11:30:21,249 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.self.key.bias using .* initializer
2022-04-10 11:30:21,249 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.self.value.weight using .* initializer
2022-04-10 11:30:21,249 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.self.value.bias using .* initializer
2022-04-10 11:30:21,249 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.output.dense.weight using .* initializer
2022-04-10 11:30:21,250 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.output.dense.bias using .* initializer
2022-04-10 11:30:21,250 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.output.LayerNorm.weight using .* initializer
2022-04-10 11:30:21,250 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.output.LayerNorm.bias using .* initializer
2022-04-10 11:30:21,250 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.intermediate.dense.weight using .* initializer
2022-04-10 11:30:21,251 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.intermediate.dense.bias using .* initializer
2022-04-10 11:30:21,251 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.output.dense.weight using .* initializer
2022-04-10 11:30:21,252 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.output.dense.bias using .* initializer
2022-04-10 11:30:21,252 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.output.LayerNorm.weight using .* initializer
2022-04-10 11:30:21,253 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.output.LayerNorm.bias using .* initializer
2022-04-10 11:30:21,253 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.self.query.weight using .* initializer
2022-04-10 11:30:21,253 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.self.query.bias using .* initializer
2022-04-10 11:30:21,253 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.self.key.weight using .* initializer
2022-04-10 11:30:21,254 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.self.key.bias using .* initializer
2022-04-10 11:30:21,254 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.self.value.weight using .* initializer
2022-04-10 11:30:21,254 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.self.value.bias using .* initializer
2022-04-10 11:30:21,254 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.output.dense.weight using .* initializer
2022-04-10 11:30:21,255 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.output.dense.bias using .* initializer
2022-04-10 11:30:21,255 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.output.LayerNorm.weight using .* initializer
2022-04-10 11:30:21,255 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.output.LayerNorm.bias using .* initializer
2022-04-10 11:30:21,255 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.intermediate.dense.weight using .* initializer
2022-04-10 11:30:21,256 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.intermediate.dense.bias using .* initializer
2022-04-10 11:30:21,256 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.output.dense.weight using .* initializer
2022-04-10 11:30:21,257 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.output.dense.bias using .* initializer
2022-04-10 11:30:21,257 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.output.LayerNorm.weight using .* initializer
2022-04-10 11:30:21,257 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.output.LayerNorm.bias using .* initializer
2022-04-10 11:30:21,257 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.self.query.weight using .* initializer
2022-04-10 11:30:21,258 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.self.query.bias using .* initializer
2022-04-10 11:30:21,258 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.self.key.weight using .* initializer
2022-04-10 11:30:21,258 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.self.key.bias using .* initializer
2022-04-10 11:30:21,259 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.self.value.weight using .* initializer
2022-04-10 11:30:21,259 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.self.value.bias using .* initializer
2022-04-10 11:30:21,259 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.output.dense.weight using .* initializer
2022-04-10 11:30:21,259 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.output.dense.bias using .* initializer
2022-04-10 11:30:21,260 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.output.LayerNorm.weight using .* initializer
2022-04-10 11:30:21,260 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.output.LayerNorm.bias using .* initializer
2022-04-10 11:30:21,260 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.intermediate.dense.weight using .* initializer
2022-04-10 11:30:21,261 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.intermediate.dense.bias using .* initializer
2022-04-10 11:30:21,261 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.output.dense.weight using .* initializer
2022-04-10 11:30:21,262 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.output.dense.bias using .* initializer
2022-04-10 11:30:21,262 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.output.LayerNorm.weight using .* initializer
2022-04-10 11:30:21,262 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.output.LayerNorm.bias using .* initializer
2022-04-10 11:30:21,262 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.self.query.weight using .* initializer
2022-04-10 11:30:21,263 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.self.query.bias using .* initializer
2022-04-10 11:30:21,263 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.self.key.weight using .* initializer
2022-04-10 11:30:21,263 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.self.key.bias using .* initializer
2022-04-10 11:30:21,263 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.self.value.weight using .* initializer
2022-04-10 11:30:21,264 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.self.value.bias using .* initializer
2022-04-10 11:30:21,264 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.output.dense.weight using .* initializer
2022-04-10 11:30:21,264 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.output.dense.bias using .* initializer
2022-04-10 11:30:21,264 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.output.LayerNorm.weight using .* initializer
2022-04-10 11:30:21,264 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.output.LayerNorm.bias using .* initializer
2022-04-10 11:30:21,265 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.intermediate.dense.weight using .* initializer
2022-04-10 11:30:21,265 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.intermediate.dense.bias using .* initializer
2022-04-10 11:30:21,266 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.output.dense.weight using .* initializer
2022-04-10 11:30:21,267 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.output.dense.bias using .* initializer
2022-04-10 11:30:21,267 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.output.LayerNorm.weight using .* initializer
2022-04-10 11:30:21,267 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.output.LayerNorm.bias using .* initializer
2022-04-10 11:30:21,267 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.self.query.weight using .* initializer
2022-04-10 11:30:21,267 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.self.query.bias using .* initializer
2022-04-10 11:30:21,268 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.self.key.weight using .* initializer
2022-04-10 11:30:21,268 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.self.key.bias using .* initializer
2022-04-10 11:30:21,268 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.self.value.weight using .* initializer
2022-04-10 11:30:21,268 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.self.value.bias using .* initializer
2022-04-10 11:30:21,269 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.output.dense.weight using .* initializer
2022-04-10 11:30:21,269 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.output.dense.bias using .* initializer
2022-04-10 11:30:21,269 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.output.LayerNorm.weight using .* initializer
2022-04-10 11:30:21,269 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.output.LayerNorm.bias using .* initializer
2022-04-10 11:30:21,269 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.intermediate.dense.weight using .* initializer
2022-04-10 11:30:21,270 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.intermediate.dense.bias using .* initializer
2022-04-10 11:30:21,271 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.output.dense.weight using .* initializer
2022-04-10 11:30:21,271 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.output.dense.bias using .* initializer
2022-04-10 11:30:21,272 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.output.LayerNorm.weight using .* initializer
2022-04-10 11:30:21,272 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.output.LayerNorm.bias using .* initializer
2022-04-10 11:30:21,272 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.pooler.dense.weight using .* initializer
2022-04-10 11:30:21,272 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.pooler.dense.bias using .* initializer
2022-04-10 11:30:21,272 - INFO - allennlp.nn.initializers - Initializing _classifier.classifier.dense.weight using .* initializer
2022-04-10 11:30:21,273 - INFO - allennlp.nn.initializers - Initializing _classifier.classifier.dense.bias using .* initializer
2022-04-10 11:30:21,273 - INFO - allennlp.nn.initializers - Initializing _classifier.classifier.out_proj.weight using .* initializer
2022-04-10 11:30:21,273 - INFO - allennlp.nn.initializers - Initializing _classifier.classifier.out_proj.bias using .* initializer
2022-04-10 11:30:21,273 - INFO - allennlp.nn.initializers - Done initializing parameters; the following parameters are using their default initialization from their code
2022-04-10 11:30:21,827 - INFO - filelock - Lock 139793362805584 acquired on ./output_ir-ora-d_hyper3/vocabulary/.lock
2022-04-10 11:30:21,828 - INFO - filelock - Lock 139793362805584 released on ./output_ir-ora-d_hyper3/vocabulary/.lock
2022-04-10 11:30:21,828 - INFO - allennlp.common.params - data_loader.type = pytorch_dataloader
2022-04-10 11:30:21,828 - INFO - allennlp.common.params - data_loader.batch_size = 1
2022-04-10 11:30:21,829 - INFO - allennlp.common.params - data_loader.shuffle = False
2022-04-10 11:30:21,829 - INFO - allennlp.common.params - data_loader.sampler = None
2022-04-10 11:30:21,829 - INFO - allennlp.common.params - data_loader.num_workers = 0
2022-04-10 11:30:21,829 - INFO - allennlp.common.params - data_loader.pin_memory = False
2022-04-10 11:30:21,829 - INFO - allennlp.common.params - data_loader.drop_last = False
2022-04-10 11:30:21,829 - INFO - allennlp.common.params - data_loader.timeout = 0
2022-04-10 11:30:21,829 - INFO - allennlp.common.params - data_loader.worker_init_fn = None
2022-04-10 11:30:21,829 - INFO - allennlp.common.params - data_loader.multiprocessing_context = None
2022-04-10 11:30:21,830 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None
2022-04-10 11:30:21,830 - INFO - allennlp.common.params - data_loader.batch_sampler.type = bucket
2022-04-10 11:30:21,830 - INFO - allennlp.common.params - data_loader.batch_sampler.batch_size = 2
2022-04-10 11:30:21,830 - INFO - allennlp.common.params - data_loader.batch_sampler.sorting_keys = None
2022-04-10 11:30:21,830 - INFO - allennlp.common.params - data_loader.batch_sampler.padding_noise = 0.1
2022-04-10 11:30:21,830 - INFO - allennlp.common.params - data_loader.batch_sampler.drop_last = False
2022-04-10 11:30:21,834 - INFO - allennlp.common.params - data_loader.type = pytorch_dataloader
2022-04-10 11:30:21,835 - INFO - allennlp.common.params - data_loader.batch_size = 1
2022-04-10 11:30:21,835 - INFO - allennlp.common.params - data_loader.shuffle = False
2022-04-10 11:30:21,835 - INFO - allennlp.common.params - data_loader.sampler = None
2022-04-10 11:30:21,835 - INFO - allennlp.common.params - data_loader.num_workers = 0
2022-04-10 11:30:21,835 - INFO - allennlp.common.params - data_loader.pin_memory = False
2022-04-10 11:30:21,835 - INFO - allennlp.common.params - data_loader.drop_last = False
2022-04-10 11:30:21,836 - INFO - allennlp.common.params - data_loader.timeout = 0
2022-04-10 11:30:21,836 - INFO - allennlp.common.params - data_loader.worker_init_fn = None
2022-04-10 11:30:21,836 - INFO - allennlp.common.params - data_loader.multiprocessing_context = None
2022-04-10 11:30:21,836 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None
2022-04-10 11:30:21,836 - INFO - allennlp.common.params - data_loader.batch_sampler.type = bucket
2022-04-10 11:30:21,836 - INFO - allennlp.common.params - data_loader.batch_sampler.batch_size = 2
2022-04-10 11:30:21,836 - INFO - allennlp.common.params - data_loader.batch_sampler.sorting_keys = None
2022-04-10 11:30:21,836 - INFO - allennlp.common.params - data_loader.batch_sampler.padding_noise = 0.1
2022-04-10 11:30:21,837 - INFO - allennlp.common.params - data_loader.batch_sampler.drop_last = False
2022-04-10 11:30:21,837 - INFO - allennlp.common.params - trainer.type = gradient_descent
2022-04-10 11:30:21,837 - INFO - allennlp.common.params - trainer.patience = None
2022-04-10 11:30:21,837 - INFO - allennlp.common.params - trainer.validation_metric = +accuracy
2022-04-10 11:30:21,838 - INFO - allennlp.common.params - trainer.num_epochs = 15
2022-04-10 11:30:21,838 - INFO - allennlp.common.params - trainer.cuda_device = 0
2022-04-10 11:30:21,838 - INFO - allennlp.common.params - trainer.grad_norm = None
2022-04-10 11:30:21,838 - INFO - allennlp.common.params - trainer.grad_clipping = None
2022-04-10 11:30:21,838 - INFO - allennlp.common.params - trainer.distributed = False
2022-04-10 11:30:21,838 - INFO - allennlp.common.params - trainer.world_size = 1
2022-04-10 11:30:21,838 - INFO - allennlp.common.params - trainer.num_gradient_accumulation_steps = 16
2022-04-10 11:30:21,838 - INFO - allennlp.common.params - trainer.use_amp = False
2022-04-10 11:30:21,838 - INFO - allennlp.common.params - trainer.no_grad = None
2022-04-10 11:30:21,839 - INFO - allennlp.common.params - trainer.momentum_scheduler = None
2022-04-10 11:30:21,839 - INFO - allennlp.common.params - trainer.tensorboard_writer = <allennlp.common.lazy.Lazy object at 0x7f24557ab490>
2022-04-10 11:30:21,839 - INFO - allennlp.common.params - trainer.moving_average = None
2022-04-10 11:30:21,839 - INFO - allennlp.common.params - trainer.checkpointer = <allennlp.common.lazy.Lazy object at 0x7f24557ab510>
2022-04-10 11:30:21,839 - INFO - allennlp.common.params - trainer.batch_callbacks = None
2022-04-10 11:30:21,839 - INFO - allennlp.common.params - trainer.epoch_callbacks = None
2022-04-10 11:30:21,839 - INFO - allennlp.common.params - trainer.end_callbacks = None
2022-04-10 11:30:21,839 - INFO - allennlp.common.params - trainer.trainer_callbacks = None
2022-04-10 11:30:26,056 - INFO - allennlp.common.params - trainer.optimizer.type = huggingface_adamw
2022-04-10 11:30:26,056 - INFO - allennlp.common.params - trainer.optimizer.parameter_groups = None
2022-04-10 11:30:26,057 - INFO - allennlp.common.params - trainer.optimizer.lr = 2e-05
2022-04-10 11:30:26,057 - INFO - allennlp.common.params - trainer.optimizer.betas = [0.9, 0.98]
2022-04-10 11:30:26,057 - INFO - allennlp.common.params - trainer.optimizer.eps = 1e-06
2022-04-10 11:30:26,057 - INFO - allennlp.common.params - trainer.optimizer.weight_decay = 0.1
2022-04-10 11:30:26,057 - INFO - allennlp.common.params - trainer.optimizer.correct_bias = True
2022-04-10 11:30:26,058 - INFO - allennlp.training.optimizers - Number of trainable parameters: 356411394
2022-04-10 11:30:26,060 - INFO - allennlp.common.util - The following parameters are Frozen (without gradient):
2022-04-10 11:30:26,063 - INFO - allennlp.common.util - The following parameters are Tunable (with gradient):
2022-04-10 11:30:26,063 - INFO - allennlp.common.util - _classifier.roberta.embeddings.word_embeddings.weight
2022-04-10 11:30:26,063 - INFO - allennlp.common.util - _classifier.roberta.embeddings.position_embeddings.weight
2022-04-10 11:30:26,063 - INFO - allennlp.common.util - _classifier.roberta.embeddings.token_type_embeddings.weight
2022-04-10 11:30:26,063 - INFO - allennlp.common.util - _classifier.roberta.embeddings.LayerNorm.weight
2022-04-10 11:30:26,063 - INFO - allennlp.common.util - _classifier.roberta.embeddings.LayerNorm.bias
2022-04-10 11:30:26,063 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.self.query.weight
2022-04-10 11:30:26,063 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.self.query.bias
2022-04-10 11:30:26,064 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.self.key.weight
2022-04-10 11:30:26,064 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.self.key.bias
2022-04-10 11:30:26,064 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.self.value.weight
2022-04-10 11:30:26,064 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.self.value.bias
2022-04-10 11:30:26,064 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.output.dense.weight
2022-04-10 11:30:26,064 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.output.dense.bias
2022-04-10 11:30:26,064 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.output.LayerNorm.weight
2022-04-10 11:30:26,064 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.output.LayerNorm.bias
2022-04-10 11:30:26,065 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.intermediate.dense.weight
2022-04-10 11:30:26,065 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.intermediate.dense.bias
2022-04-10 11:30:26,065 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.output.dense.weight
2022-04-10 11:30:26,065 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.output.dense.bias
2022-04-10 11:30:26,065 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.output.LayerNorm.weight
2022-04-10 11:30:26,065 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.output.LayerNorm.bias
2022-04-10 11:30:26,065 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.self.query.weight
2022-04-10 11:30:26,065 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.self.query.bias
2022-04-10 11:30:26,065 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.self.key.weight
2022-04-10 11:30:26,066 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.self.key.bias
2022-04-10 11:30:26,066 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.self.value.weight
2022-04-10 11:30:26,066 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.self.value.bias
2022-04-10 11:30:26,066 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.output.dense.weight
2022-04-10 11:30:26,066 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.output.dense.bias
2022-04-10 11:30:26,066 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.output.LayerNorm.weight
2022-04-10 11:30:26,066 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.output.LayerNorm.bias
2022-04-10 11:30:26,066 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.intermediate.dense.weight
2022-04-10 11:30:26,067 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.intermediate.dense.bias
2022-04-10 11:30:26,067 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.output.dense.weight
2022-04-10 11:30:26,067 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.output.dense.bias
2022-04-10 11:30:26,067 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.output.LayerNorm.weight
2022-04-10 11:30:26,067 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.output.LayerNorm.bias
2022-04-10 11:30:26,068 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.self.query.weight
2022-04-10 11:30:26,068 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.self.query.bias
2022-04-10 11:30:26,068 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.self.key.weight
2022-04-10 11:30:26,068 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.self.key.bias
2022-04-10 11:30:26,068 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.self.value.weight
2022-04-10 11:30:26,068 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.self.value.bias
2022-04-10 11:30:26,069 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.output.dense.weight
2022-04-10 11:30:26,069 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.output.dense.bias
2022-04-10 11:30:26,069 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.output.LayerNorm.weight
2022-04-10 11:30:26,069 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.output.LayerNorm.bias
2022-04-10 11:30:26,069 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.intermediate.dense.weight
2022-04-10 11:30:26,069 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.intermediate.dense.bias
2022-04-10 11:30:26,069 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.output.dense.weight
2022-04-10 11:30:26,070 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.output.dense.bias
2022-04-10 11:30:26,070 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.output.LayerNorm.weight
2022-04-10 11:30:26,070 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.output.LayerNorm.bias
2022-04-10 11:30:26,070 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.self.query.weight
2022-04-10 11:30:26,070 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.self.query.bias
2022-04-10 11:30:26,070 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.self.key.weight
2022-04-10 11:30:26,070 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.self.key.bias
2022-04-10 11:30:26,071 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.self.value.weight
2022-04-10 11:30:26,071 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.self.value.bias
2022-04-10 11:30:26,071 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.output.dense.weight
2022-04-10 11:30:26,071 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.output.dense.bias
2022-04-10 11:30:26,071 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.output.LayerNorm.weight
2022-04-10 11:30:26,071 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.output.LayerNorm.bias
2022-04-10 11:30:26,071 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.intermediate.dense.weight
2022-04-10 11:30:26,072 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.intermediate.dense.bias
2022-04-10 11:30:26,072 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.output.dense.weight
2022-04-10 11:30:26,072 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.output.dense.bias
2022-04-10 11:30:26,072 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.output.LayerNorm.weight
2022-04-10 11:30:26,072 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.output.LayerNorm.bias
2022-04-10 11:30:26,072 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.self.query.weight
2022-04-10 11:30:26,073 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.self.query.bias
2022-04-10 11:30:26,073 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.self.key.weight
2022-04-10 11:30:26,073 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.self.key.bias
2022-04-10 11:30:26,073 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.self.value.weight
2022-04-10 11:30:26,073 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.self.value.bias
2022-04-10 11:30:26,073 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.output.dense.weight
2022-04-10 11:30:26,073 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.output.dense.bias
2022-04-10 11:30:26,073 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.output.LayerNorm.weight
2022-04-10 11:30:26,073 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.output.LayerNorm.bias
2022-04-10 11:30:26,074 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.intermediate.dense.weight
2022-04-10 11:30:26,074 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.intermediate.dense.bias
2022-04-10 11:30:26,074 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.output.dense.weight
2022-04-10 11:30:26,074 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.output.dense.bias
2022-04-10 11:30:26,074 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.output.LayerNorm.weight
2022-04-10 11:30:26,074 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.output.LayerNorm.bias
2022-04-10 11:30:26,074 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.self.query.weight
2022-04-10 11:30:26,074 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.self.query.bias
2022-04-10 11:30:26,075 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.self.key.weight
2022-04-10 11:30:26,075 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.self.key.bias
2022-04-10 11:30:26,075 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.self.value.weight
2022-04-10 11:30:26,075 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.self.value.bias
2022-04-10 11:30:26,075 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.output.dense.weight
2022-04-10 11:30:26,075 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.output.dense.bias
2022-04-10 11:30:26,075 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.output.LayerNorm.weight
2022-04-10 11:30:26,076 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.output.LayerNorm.bias
2022-04-10 11:30:26,076 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.intermediate.dense.weight
2022-04-10 11:30:26,076 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.intermediate.dense.bias
2022-04-10 11:30:26,076 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.output.dense.weight
2022-04-10 11:30:26,076 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.output.dense.bias
2022-04-10 11:30:26,076 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.output.LayerNorm.weight
2022-04-10 11:30:26,077 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.output.LayerNorm.bias
2022-04-10 11:30:26,077 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.self.query.weight
2022-04-10 11:30:26,077 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.self.query.bias
2022-04-10 11:30:26,077 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.self.key.weight
2022-04-10 11:30:26,077 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.self.key.bias
2022-04-10 11:30:26,077 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.self.value.weight
2022-04-10 11:30:26,077 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.self.value.bias
2022-04-10 11:30:26,077 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.output.dense.weight
2022-04-10 11:30:26,078 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.output.dense.bias
2022-04-10 11:30:26,078 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.output.LayerNorm.weight
2022-04-10 11:30:26,078 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.output.LayerNorm.bias
2022-04-10 11:30:26,078 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.intermediate.dense.weight
2022-04-10 11:30:26,078 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.intermediate.dense.bias
2022-04-10 11:30:26,078 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.output.dense.weight
2022-04-10 11:30:26,078 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.output.dense.bias
2022-04-10 11:30:26,078 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.output.LayerNorm.weight
2022-04-10 11:30:26,079 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.output.LayerNorm.bias
2022-04-10 11:30:26,079 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.self.query.weight
2022-04-10 11:30:26,079 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.self.query.bias
2022-04-10 11:30:26,079 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.self.key.weight
2022-04-10 11:30:26,079 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.self.key.bias
2022-04-10 11:30:26,079 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.self.value.weight
2022-04-10 11:30:26,079 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.self.value.bias
2022-04-10 11:30:26,079 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.output.dense.weight
2022-04-10 11:30:26,079 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.output.dense.bias
2022-04-10 11:30:26,080 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.output.LayerNorm.weight
2022-04-10 11:30:26,080 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.output.LayerNorm.bias
2022-04-10 11:30:26,080 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.intermediate.dense.weight
2022-04-10 11:30:26,080 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.intermediate.dense.bias
2022-04-10 11:30:26,080 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.output.dense.weight
2022-04-10 11:30:26,080 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.output.dense.bias
2022-04-10 11:30:26,081 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.output.LayerNorm.weight
2022-04-10 11:30:26,081 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.output.LayerNorm.bias
2022-04-10 11:30:26,081 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.self.query.weight
2022-04-10 11:30:26,081 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.self.query.bias
2022-04-10 11:30:26,081 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.self.key.weight
2022-04-10 11:30:26,081 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.self.key.bias
2022-04-10 11:30:26,081 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.self.value.weight
2022-04-10 11:30:26,081 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.self.value.bias
2022-04-10 11:30:26,082 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.output.dense.weight
2022-04-10 11:30:26,082 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.output.dense.bias
2022-04-10 11:30:26,082 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.output.LayerNorm.weight
2022-04-10 11:30:26,082 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.output.LayerNorm.bias
2022-04-10 11:30:26,082 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.intermediate.dense.weight
2022-04-10 11:30:26,082 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.intermediate.dense.bias
2022-04-10 11:30:26,082 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.output.dense.weight
2022-04-10 11:30:26,083 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.output.dense.bias
2022-04-10 11:30:26,083 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.output.LayerNorm.weight
2022-04-10 11:30:26,083 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.output.LayerNorm.bias
2022-04-10 11:30:26,083 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.self.query.weight
2022-04-10 11:30:26,083 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.self.query.bias
2022-04-10 11:30:26,083 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.self.key.weight
2022-04-10 11:30:26,083 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.self.key.bias
2022-04-10 11:30:26,084 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.self.value.weight
2022-04-10 11:30:26,084 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.self.value.bias
2022-04-10 11:30:26,084 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.output.dense.weight
2022-04-10 11:30:26,084 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.output.dense.bias
2022-04-10 11:30:26,084 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.output.LayerNorm.weight
2022-04-10 11:30:26,084 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.output.LayerNorm.bias
2022-04-10 11:30:26,084 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.intermediate.dense.weight
2022-04-10 11:30:26,084 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.intermediate.dense.bias
2022-04-10 11:30:26,085 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.output.dense.weight
2022-04-10 11:30:26,085 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.output.dense.bias
2022-04-10 11:30:26,085 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.output.LayerNorm.weight
2022-04-10 11:30:26,085 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.output.LayerNorm.bias
2022-04-10 11:30:26,085 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.self.query.weight
2022-04-10 11:30:26,085 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.self.query.bias
2022-04-10 11:30:26,085 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.self.key.weight
2022-04-10 11:30:26,085 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.self.key.bias
2022-04-10 11:30:26,085 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.self.value.weight
2022-04-10 11:30:26,086 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.self.value.bias
2022-04-10 11:30:26,086 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.output.dense.weight
2022-04-10 11:30:26,086 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.output.dense.bias
2022-04-10 11:30:26,086 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.output.LayerNorm.weight
2022-04-10 11:30:26,086 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.output.LayerNorm.bias
2022-04-10 11:30:26,086 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.intermediate.dense.weight
2022-04-10 11:30:26,086 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.intermediate.dense.bias
2022-04-10 11:30:26,086 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.output.dense.weight
2022-04-10 11:30:26,086 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.output.dense.bias
2022-04-10 11:30:26,086 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.output.LayerNorm.weight
2022-04-10 11:30:26,086 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.output.LayerNorm.bias
2022-04-10 11:30:26,086 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.self.query.weight
2022-04-10 11:30:26,086 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.self.query.bias
2022-04-10 11:30:26,087 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.self.key.weight
2022-04-10 11:30:26,087 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.self.key.bias
2022-04-10 11:30:26,087 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.self.value.weight
2022-04-10 11:30:26,087 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.self.value.bias
2022-04-10 11:30:26,087 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.output.dense.weight
2022-04-10 11:30:26,087 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.output.dense.bias
2022-04-10 11:30:26,087 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.output.LayerNorm.weight
2022-04-10 11:30:26,087 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.output.LayerNorm.bias
2022-04-10 11:30:26,087 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.intermediate.dense.weight
2022-04-10 11:30:26,087 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.intermediate.dense.bias
2022-04-10 11:30:26,087 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.output.dense.weight
2022-04-10 11:30:26,087 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.output.dense.bias
2022-04-10 11:30:26,088 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.output.LayerNorm.weight
2022-04-10 11:30:26,088 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.output.LayerNorm.bias
2022-04-10 11:30:26,088 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.self.query.weight
2022-04-10 11:30:26,088 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.self.query.bias
2022-04-10 11:30:26,088 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.self.key.weight
2022-04-10 11:30:26,088 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.self.key.bias
2022-04-10 11:30:26,088 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.self.value.weight
2022-04-10 11:30:26,088 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.self.value.bias
2022-04-10 11:30:26,088 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.output.dense.weight
2022-04-10 11:30:26,088 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.output.dense.bias
2022-04-10 11:30:26,088 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.output.LayerNorm.weight
2022-04-10 11:30:26,088 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.output.LayerNorm.bias
2022-04-10 11:30:26,088 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.intermediate.dense.weight
2022-04-10 11:30:26,088 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.intermediate.dense.bias
2022-04-10 11:30:26,088 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.output.dense.weight
2022-04-10 11:30:26,088 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.output.dense.bias
2022-04-10 11:30:26,089 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.output.LayerNorm.weight
2022-04-10 11:30:26,089 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.output.LayerNorm.bias
2022-04-10 11:30:26,089 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.self.query.weight
2022-04-10 11:30:26,089 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.self.query.bias
2022-04-10 11:30:26,089 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.self.key.weight
2022-04-10 11:30:26,089 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.self.key.bias
2022-04-10 11:30:26,089 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.self.value.weight
2022-04-10 11:30:26,089 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.self.value.bias
2022-04-10 11:30:26,089 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.output.dense.weight
2022-04-10 11:30:26,089 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.output.dense.bias
2022-04-10 11:30:26,089 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.output.LayerNorm.weight
2022-04-10 11:30:26,089 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.output.LayerNorm.bias
2022-04-10 11:30:26,089 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.intermediate.dense.weight
2022-04-10 11:30:26,089 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.intermediate.dense.bias
2022-04-10 11:30:26,090 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.output.dense.weight
2022-04-10 11:30:26,090 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.output.dense.bias
2022-04-10 11:30:26,090 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.output.LayerNorm.weight
2022-04-10 11:30:26,090 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.output.LayerNorm.bias
2022-04-10 11:30:26,090 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.self.query.weight
2022-04-10 11:30:26,090 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.self.query.bias
2022-04-10 11:30:26,090 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.self.key.weight
2022-04-10 11:30:26,090 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.self.key.bias
2022-04-10 11:30:26,090 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.self.value.weight
2022-04-10 11:30:26,090 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.self.value.bias
2022-04-10 11:30:26,090 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.output.dense.weight
2022-04-10 11:30:26,090 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.output.dense.bias
2022-04-10 11:30:26,090 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.output.LayerNorm.weight
2022-04-10 11:30:26,090 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.output.LayerNorm.bias
2022-04-10 11:30:26,090 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.intermediate.dense.weight
2022-04-10 11:30:26,091 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.intermediate.dense.bias
2022-04-10 11:30:26,091 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.output.dense.weight
2022-04-10 11:30:26,091 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.output.dense.bias
2022-04-10 11:30:26,091 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.output.LayerNorm.weight
2022-04-10 11:30:26,091 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.output.LayerNorm.bias
2022-04-10 11:30:26,091 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.self.query.weight
2022-04-10 11:30:26,091 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.self.query.bias
2022-04-10 11:30:26,091 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.self.key.weight
2022-04-10 11:30:26,091 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.self.key.bias
2022-04-10 11:30:26,091 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.self.value.weight
2022-04-10 11:30:26,091 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.self.value.bias
2022-04-10 11:30:26,091 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.output.dense.weight
2022-04-10 11:30:26,091 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.output.dense.bias
2022-04-10 11:30:26,091 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.output.LayerNorm.weight
2022-04-10 11:30:26,091 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.output.LayerNorm.bias
2022-04-10 11:30:26,092 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.intermediate.dense.weight
2022-04-10 11:30:26,092 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.intermediate.dense.bias
2022-04-10 11:30:26,092 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.output.dense.weight
2022-04-10 11:30:26,092 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.output.dense.bias
2022-04-10 11:30:26,092 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.output.LayerNorm.weight
2022-04-10 11:30:26,092 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.output.LayerNorm.bias
2022-04-10 11:30:26,092 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.self.query.weight
2022-04-10 11:30:26,092 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.self.query.bias
2022-04-10 11:30:26,092 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.self.key.weight
2022-04-10 11:30:26,092 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.self.key.bias
2022-04-10 11:30:26,092 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.self.value.weight
2022-04-10 11:30:26,092 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.self.value.bias
2022-04-10 11:30:26,092 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.output.dense.weight
2022-04-10 11:30:26,092 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.output.dense.bias
2022-04-10 11:30:26,092 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.output.LayerNorm.weight
2022-04-10 11:30:26,092 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.output.LayerNorm.bias
2022-04-10 11:30:26,092 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.intermediate.dense.weight
2022-04-10 11:30:26,092 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.intermediate.dense.bias
2022-04-10 11:30:26,093 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.output.dense.weight
2022-04-10 11:30:26,093 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.output.dense.bias
2022-04-10 11:30:26,093 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.output.LayerNorm.weight
2022-04-10 11:30:26,093 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.output.LayerNorm.bias
2022-04-10 11:30:26,093 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.self.query.weight
2022-04-10 11:30:26,093 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.self.query.bias
2022-04-10 11:30:26,093 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.self.key.weight
2022-04-10 11:30:26,093 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.self.key.bias
2022-04-10 11:30:26,093 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.self.value.weight
2022-04-10 11:30:26,093 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.self.value.bias
2022-04-10 11:30:26,093 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.output.dense.weight
2022-04-10 11:30:26,093 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.output.dense.bias
2022-04-10 11:30:26,093 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.output.LayerNorm.weight
2022-04-10 11:30:26,093 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.output.LayerNorm.bias
2022-04-10 11:30:26,093 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.intermediate.dense.weight
2022-04-10 11:30:26,093 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.intermediate.dense.bias
2022-04-10 11:30:26,093 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.output.dense.weight
2022-04-10 11:30:26,093 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.output.dense.bias
2022-04-10 11:30:26,094 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.output.LayerNorm.weight
2022-04-10 11:30:26,094 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.output.LayerNorm.bias
2022-04-10 11:30:26,094 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.self.query.weight
2022-04-10 11:30:26,094 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.self.query.bias
2022-04-10 11:30:26,094 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.self.key.weight
2022-04-10 11:30:26,094 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.self.key.bias
2022-04-10 11:30:26,094 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.self.value.weight
2022-04-10 11:30:26,094 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.self.value.bias
2022-04-10 11:30:26,094 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.output.dense.weight
2022-04-10 11:30:26,094 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.output.dense.bias
2022-04-10 11:30:26,094 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.output.LayerNorm.weight
2022-04-10 11:30:26,094 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.output.LayerNorm.bias
2022-04-10 11:30:26,094 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.intermediate.dense.weight
2022-04-10 11:30:26,095 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.intermediate.dense.bias
2022-04-10 11:30:26,095 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.output.dense.weight
2022-04-10 11:30:26,095 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.output.dense.bias
2022-04-10 11:30:26,095 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.output.LayerNorm.weight
2022-04-10 11:30:26,095 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.output.LayerNorm.bias
2022-04-10 11:30:26,095 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.self.query.weight
2022-04-10 11:30:26,095 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.self.query.bias
2022-04-10 11:30:26,095 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.self.key.weight
2022-04-10 11:30:26,095 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.self.key.bias
2022-04-10 11:30:26,095 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.self.value.weight
2022-04-10 11:30:26,095 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.self.value.bias
2022-04-10 11:30:26,095 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.output.dense.weight
2022-04-10 11:30:26,095 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.output.dense.bias
2022-04-10 11:30:26,095 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.output.LayerNorm.weight
2022-04-10 11:30:26,095 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.output.LayerNorm.bias
2022-04-10 11:30:26,095 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.intermediate.dense.weight
2022-04-10 11:30:26,095 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.intermediate.dense.bias
2022-04-10 11:30:26,095 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.output.dense.weight
2022-04-10 11:30:26,096 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.output.dense.bias
2022-04-10 11:30:26,096 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.output.LayerNorm.weight
2022-04-10 11:30:26,096 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.output.LayerNorm.bias
2022-04-10 11:30:26,096 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.self.query.weight
2022-04-10 11:30:26,096 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.self.query.bias
2022-04-10 11:30:26,096 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.self.key.weight
2022-04-10 11:30:26,096 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.self.key.bias
2022-04-10 11:30:26,096 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.self.value.weight
2022-04-10 11:30:26,096 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.self.value.bias
2022-04-10 11:30:26,096 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.output.dense.weight
2022-04-10 11:30:26,096 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.output.dense.bias
2022-04-10 11:30:26,096 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.output.LayerNorm.weight
2022-04-10 11:30:26,096 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.output.LayerNorm.bias
2022-04-10 11:30:26,096 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.intermediate.dense.weight
2022-04-10 11:30:26,096 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.intermediate.dense.bias
2022-04-10 11:30:26,097 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.output.dense.weight
2022-04-10 11:30:26,097 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.output.dense.bias
2022-04-10 11:30:26,097 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.output.LayerNorm.weight
2022-04-10 11:30:26,097 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.output.LayerNorm.bias
2022-04-10 11:30:26,097 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.self.query.weight
2022-04-10 11:30:26,097 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.self.query.bias
2022-04-10 11:30:26,097 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.self.key.weight
2022-04-10 11:30:26,097 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.self.key.bias
2022-04-10 11:30:26,097 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.self.value.weight
2022-04-10 11:30:26,097 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.self.value.bias
2022-04-10 11:30:26,097 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.output.dense.weight
2022-04-10 11:30:26,097 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.output.dense.bias
2022-04-10 11:30:26,097 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.output.LayerNorm.weight
2022-04-10 11:30:26,097 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.output.LayerNorm.bias
2022-04-10 11:30:26,097 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.intermediate.dense.weight
2022-04-10 11:30:26,097 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.intermediate.dense.bias
2022-04-10 11:30:26,098 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.output.dense.weight
2022-04-10 11:30:26,098 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.output.dense.bias
2022-04-10 11:30:26,098 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.output.LayerNorm.weight
2022-04-10 11:30:26,098 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.output.LayerNorm.bias
2022-04-10 11:30:26,098 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.self.query.weight
2022-04-10 11:30:26,098 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.self.query.bias
2022-04-10 11:30:26,098 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.self.key.weight
2022-04-10 11:30:26,098 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.self.key.bias
2022-04-10 11:30:26,098 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.self.value.weight
2022-04-10 11:30:26,099 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.self.value.bias
2022-04-10 11:30:26,099 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.output.dense.weight
2022-04-10 11:30:26,099 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.output.dense.bias
2022-04-10 11:30:26,099 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.output.LayerNorm.weight
2022-04-10 11:30:26,099 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.output.LayerNorm.bias
2022-04-10 11:30:26,099 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.intermediate.dense.weight
2022-04-10 11:30:26,099 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.intermediate.dense.bias
2022-04-10 11:30:26,099 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.output.dense.weight
2022-04-10 11:30:26,100 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.output.dense.bias
2022-04-10 11:30:26,100 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.output.LayerNorm.weight
2022-04-10 11:30:26,100 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.output.LayerNorm.bias
2022-04-10 11:30:26,100 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.self.query.weight
2022-04-10 11:30:26,100 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.self.query.bias
2022-04-10 11:30:26,100 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.self.key.weight
2022-04-10 11:30:26,100 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.self.key.bias
2022-04-10 11:30:26,100 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.self.value.weight
2022-04-10 11:30:26,100 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.self.value.bias
2022-04-10 11:30:26,101 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.output.dense.weight
2022-04-10 11:30:26,101 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.output.dense.bias
2022-04-10 11:30:26,101 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.output.LayerNorm.weight
2022-04-10 11:30:26,101 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.output.LayerNorm.bias
2022-04-10 11:30:26,101 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.intermediate.dense.weight
2022-04-10 11:30:26,101 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.intermediate.dense.bias
2022-04-10 11:30:26,101 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.output.dense.weight
2022-04-10 11:30:26,101 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.output.dense.bias
2022-04-10 11:30:26,102 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.output.LayerNorm.weight
2022-04-10 11:30:26,102 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.output.LayerNorm.bias
2022-04-10 11:30:26,102 - INFO - allennlp.common.util - _classifier.roberta.pooler.dense.weight
2022-04-10 11:30:26,102 - INFO - allennlp.common.util - _classifier.roberta.pooler.dense.bias
2022-04-10 11:30:26,102 - INFO - allennlp.common.util - _classifier.classifier.dense.weight
2022-04-10 11:30:26,102 - INFO - allennlp.common.util - _classifier.classifier.dense.bias
2022-04-10 11:30:26,102 - INFO - allennlp.common.util - _classifier.classifier.out_proj.weight
2022-04-10 11:30:26,102 - INFO - allennlp.common.util - _classifier.classifier.out_proj.bias
2022-04-10 11:30:26,103 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.type = slanted_triangular
2022-04-10 11:30:26,103 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.cut_frac = 0.06
2022-04-10 11:30:26,103 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.ratio = 32
2022-04-10 11:30:26,103 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.last_epoch = -1
2022-04-10 11:30:26,103 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.gradual_unfreezing = False
2022-04-10 11:30:26,103 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.discriminative_fine_tuning = False
2022-04-10 11:30:26,104 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.decay_factor = 0.38
2022-04-10 11:30:26,104 - INFO - allennlp.common.params - type = default
2022-04-10 11:30:26,104 - INFO - allennlp.common.params - keep_serialized_model_every_num_seconds = None
2022-04-10 11:30:26,104 - INFO - allennlp.common.params - num_serialized_models_to_keep = 2
2022-04-10 11:30:26,104 - INFO - allennlp.common.params - model_save_interval = None
2022-04-10 11:30:26,105 - INFO - allennlp.common.params - summary_interval = 100
2022-04-10 11:30:26,105 - INFO - allennlp.common.params - histogram_interval = None
2022-04-10 11:30:26,105 - INFO - allennlp.common.params - batch_size_interval = None
2022-04-10 11:30:26,105 - INFO - allennlp.common.params - should_log_parameter_statistics = True
2022-04-10 11:30:26,105 - INFO - allennlp.common.params - should_log_learning_rate = False
2022-04-10 11:30:26,105 - INFO - allennlp.common.params - get_batch_num_total = None
2022-04-10 11:30:26,109 - WARNING - allennlp.training.trainer - You provided a validation dataset but patience was set to None, meaning that early stopping is disabled
2022-04-10 11:30:26,201 - INFO - allennlp.training.trainer - Beginning training.
2022-04-10 11:30:26,201 - INFO - allennlp.training.trainer - Epoch 0/14
2022-04-10 11:30:26,201 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-10 11:30:26,201 - INFO - allennlp.training.trainer - GPU 0 memory usage: 1.3G
2022-04-10 11:30:26,202 - INFO - allennlp.training.trainer - Training
2022-04-10 11:30:26,203 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-10 11:30:26,203 - INFO - allennlp.data.samplers.bucket_batch_sampler - No sorting keys given; trying to guess a good one
2022-04-10 11:30:26,203 - INFO - allennlp.data.samplers.bucket_batch_sampler - Using ['tokens'] as the sorting keys
2022-04-10 11:30:36,431 - INFO - tqdm - accuracy: 0.4688, batch_loss: 1.5490, loss: 1.5490 ||:   2%|1         | 1/65 [00:10<10:54, 10.23s/it]
2022-04-10 11:30:46,674 - INFO - tqdm - accuracy: 0.5156, batch_loss: 1.5258, loss: 1.5374 ||:   3%|3         | 2/65 [00:20<10:44, 10.24s/it]
2022-04-10 11:30:57,013 - INFO - tqdm - accuracy: 0.5000, batch_loss: 1.9956, loss: 1.6901 ||:   5%|4         | 3/65 [00:30<10:37, 10.28s/it]
2022-04-10 11:31:07,494 - INFO - tqdm - accuracy: 0.5312, batch_loss: 1.3072, loss: 1.5944 ||:   6%|6         | 4/65 [00:41<10:32, 10.36s/it]
2022-04-10 11:31:18,084 - INFO - tqdm - accuracy: 0.5188, batch_loss: 1.1312, loss: 1.5018 ||:   8%|7         | 5/65 [00:51<10:26, 10.44s/it]
2022-04-10 11:31:28,785 - INFO - tqdm - accuracy: 0.4948, batch_loss: 1.5799, loss: 1.5148 ||:   9%|9         | 6/65 [01:02<10:21, 10.53s/it]
2022-04-10 11:31:39,622 - INFO - tqdm - accuracy: 0.5089, batch_loss: 1.3322, loss: 1.4887 ||:  11%|#         | 7/65 [01:13<10:16, 10.63s/it]
2022-04-10 11:31:50,666 - INFO - tqdm - accuracy: 0.5039, batch_loss: 1.2781, loss: 1.4624 ||:  12%|#2        | 8/65 [01:24<10:13, 10.76s/it]
2022-04-10 11:32:01,879 - INFO - tqdm - accuracy: 0.5000, batch_loss: 1.3841, loss: 1.4537 ||:  14%|#3        | 9/65 [01:35<10:10, 10.90s/it]
2022-04-10 11:32:13,162 - INFO - tqdm - accuracy: 0.4844, batch_loss: 1.2134, loss: 1.4296 ||:  15%|#5        | 10/65 [01:46<10:06, 11.02s/it]
2022-04-10 11:32:24,807 - INFO - tqdm - accuracy: 0.4773, batch_loss: 1.1468, loss: 1.4039 ||:  17%|#6        | 11/65 [01:58<10:05, 11.21s/it]
2022-04-10 11:32:36,598 - INFO - tqdm - accuracy: 0.4948, batch_loss: 0.7146, loss: 1.3465 ||:  18%|#8        | 12/65 [02:10<10:03, 11.39s/it]
2022-04-10 11:32:48,173 - INFO - tqdm - accuracy: 0.5072, batch_loss: 0.6713, loss: 1.2946 ||:  20%|##        | 13/65 [02:21<09:55, 11.44s/it]
2022-04-10 11:32:59,414 - INFO - tqdm - accuracy: 0.5045, batch_loss: 0.8417, loss: 1.2622 ||:  22%|##1       | 14/65 [02:33<09:40, 11.38s/it]
2022-04-10 11:33:10,760 - INFO - tqdm - accuracy: 0.5104, batch_loss: 0.7302, loss: 1.2267 ||:  23%|##3       | 15/65 [02:44<09:28, 11.37s/it]
2022-04-10 11:33:22,233 - INFO - tqdm - accuracy: 0.5039, batch_loss: 0.7831, loss: 1.1990 ||:  25%|##4       | 16/65 [02:56<09:18, 11.40s/it]
2022-04-10 11:33:33,881 - INFO - tqdm - accuracy: 0.5074, batch_loss: 0.6471, loss: 1.1665 ||:  26%|##6       | 17/65 [03:07<09:10, 11.48s/it]
2022-04-10 11:33:45,503 - INFO - tqdm - accuracy: 0.5052, batch_loss: 0.7028, loss: 1.1408 ||:  28%|##7       | 18/65 [03:19<09:01, 11.52s/it]
2022-04-10 11:33:57,036 - INFO - tqdm - accuracy: 0.5033, batch_loss: 0.7236, loss: 1.1188 ||:  29%|##9       | 19/65 [03:30<08:50, 11.52s/it]
2022-04-10 11:34:08,277 - INFO - tqdm - accuracy: 0.5094, batch_loss: 0.6548, loss: 1.0956 ||:  31%|###       | 20/65 [03:42<08:34, 11.44s/it]
2022-04-10 11:34:19,765 - INFO - tqdm - accuracy: 0.5045, batch_loss: 0.7477, loss: 1.0791 ||:  32%|###2      | 21/65 [03:53<08:23, 11.45s/it]
2022-04-10 11:34:31,194 - INFO - tqdm - accuracy: 0.5014, batch_loss: 0.7129, loss: 1.0624 ||:  34%|###3      | 22/65 [04:04<08:12, 11.45s/it]
2022-04-10 11:34:42,768 - INFO - tqdm - accuracy: 0.5014, batch_loss: 0.7216, loss: 1.0476 ||:  35%|###5      | 23/65 [04:16<08:02, 11.48s/it]
2022-04-10 11:34:54,375 - INFO - tqdm - accuracy: 0.5000, batch_loss: 0.6891, loss: 1.0327 ||:  37%|###6      | 24/65 [04:28<07:52, 11.52s/it]
2022-04-10 11:35:05,939 - INFO - tqdm - accuracy: 0.5025, batch_loss: 0.6946, loss: 1.0191 ||:  38%|###8      | 25/65 [04:39<07:41, 11.53s/it]
2022-04-10 11:35:17,468 - INFO - tqdm - accuracy: 0.5012, batch_loss: 0.7344, loss: 1.0082 ||:  40%|####      | 26/65 [04:51<07:29, 11.53s/it]
2022-04-10 11:35:28,979 - INFO - tqdm - accuracy: 0.5000, batch_loss: 0.7231, loss: 0.9976 ||:  42%|####1     | 27/65 [05:02<07:17, 11.53s/it]
2022-04-10 11:35:40,461 - INFO - tqdm - accuracy: 0.5011, batch_loss: 0.7178, loss: 0.9876 ||:  43%|####3     | 28/65 [05:14<07:05, 11.51s/it]
2022-04-10 11:35:51,953 - INFO - tqdm - accuracy: 0.4989, batch_loss: 0.7709, loss: 0.9802 ||:  45%|####4     | 29/65 [05:25<06:54, 11.51s/it]
2022-04-10 11:36:03,451 - INFO - tqdm - accuracy: 0.4969, batch_loss: 0.7074, loss: 0.9711 ||:  46%|####6     | 30/65 [05:37<06:42, 11.50s/it]
2022-04-10 11:36:14,963 - INFO - tqdm - accuracy: 0.4950, batch_loss: 0.7092, loss: 0.9626 ||:  48%|####7     | 31/65 [05:48<06:31, 11.51s/it]
2022-04-10 11:36:26,489 - INFO - tqdm - accuracy: 0.4932, batch_loss: 0.7158, loss: 0.9549 ||:  49%|####9     | 32/65 [06:00<06:19, 11.51s/it]
2022-04-10 11:36:38,032 - INFO - tqdm - accuracy: 0.4915, batch_loss: 0.7103, loss: 0.9475 ||:  51%|#####     | 33/65 [06:11<06:08, 11.52s/it]
2022-04-10 11:36:49,580 - INFO - tqdm - accuracy: 0.4917, batch_loss: 0.7040, loss: 0.9403 ||:  52%|#####2    | 34/65 [06:23<05:57, 11.53s/it]
2022-04-10 11:37:01,140 - INFO - tqdm - accuracy: 0.4884, batch_loss: 0.7346, loss: 0.9344 ||:  54%|#####3    | 35/65 [06:34<05:46, 11.54s/it]
2022-04-10 11:37:12,696 - INFO - tqdm - accuracy: 0.4905, batch_loss: 0.6871, loss: 0.9276 ||:  55%|#####5    | 36/65 [06:46<05:34, 11.54s/it]
2022-04-10 11:37:24,276 - INFO - tqdm - accuracy: 0.4882, batch_loss: 0.7750, loss: 0.9235 ||:  57%|#####6    | 37/65 [06:58<05:23, 11.55s/it]
2022-04-10 11:37:35,842 - INFO - tqdm - accuracy: 0.4844, batch_loss: 0.7285, loss: 0.9183 ||:  58%|#####8    | 38/65 [07:09<05:12, 11.56s/it]
2022-04-10 11:37:47,426 - INFO - tqdm - accuracy: 0.4848, batch_loss: 0.7043, loss: 0.9128 ||:  60%|######    | 39/65 [07:21<05:00, 11.57s/it]
2022-04-10 11:37:58,991 - INFO - tqdm - accuracy: 0.4836, batch_loss: 0.7036, loss: 0.9076 ||:  62%|######1   | 40/65 [07:32<04:49, 11.57s/it]
2022-04-10 11:38:10,554 - INFO - tqdm - accuracy: 0.4809, batch_loss: 0.7210, loss: 0.9031 ||:  63%|######3   | 41/65 [07:44<04:37, 11.57s/it]
2022-04-10 11:38:22,102 - INFO - tqdm - accuracy: 0.4836, batch_loss: 0.6694, loss: 0.8975 ||:  65%|######4   | 42/65 [07:55<04:25, 11.56s/it]
2022-04-10 11:38:33,638 - INFO - tqdm - accuracy: 0.4818, batch_loss: 0.7175, loss: 0.8933 ||:  66%|######6   | 43/65 [08:07<04:14, 11.55s/it]
2022-04-10 11:38:45,183 - INFO - tqdm - accuracy: 0.4858, batch_loss: 0.6467, loss: 0.8877 ||:  68%|######7   | 44/65 [08:18<04:02, 11.55s/it]
2022-04-10 11:38:56,732 - INFO - tqdm - accuracy: 0.4861, batch_loss: 0.7233, loss: 0.8840 ||:  69%|######9   | 45/65 [08:30<03:50, 11.55s/it]
2022-04-10 11:39:08,266 - INFO - tqdm - accuracy: 0.4885, batch_loss: 0.6858, loss: 0.8797 ||:  71%|#######   | 46/65 [08:42<03:39, 11.55s/it]
2022-04-10 11:39:19,800 - INFO - tqdm - accuracy: 0.4900, batch_loss: 0.6858, loss: 0.8756 ||:  72%|#######2  | 47/65 [08:53<03:27, 11.54s/it]
2022-04-10 11:39:31,333 - INFO - tqdm - accuracy: 0.4909, batch_loss: 0.7351, loss: 0.8727 ||:  74%|#######3  | 48/65 [09:05<03:16, 11.54s/it]
2022-04-10 11:39:42,861 - INFO - tqdm - accuracy: 0.4936, batch_loss: 0.6762, loss: 0.8687 ||:  75%|#######5  | 49/65 [09:16<03:04, 11.54s/it]
2022-04-10 11:39:54,381 - INFO - tqdm - accuracy: 0.4944, batch_loss: 0.6996, loss: 0.8653 ||:  77%|#######6  | 50/65 [09:28<02:52, 11.53s/it]
2022-04-10 11:40:05,911 - INFO - tqdm - accuracy: 0.4945, batch_loss: 0.7207, loss: 0.8625 ||:  78%|#######8  | 51/65 [09:39<02:41, 11.53s/it]
2022-04-10 11:40:17,435 - INFO - tqdm - accuracy: 0.4988, batch_loss: 0.6418, loss: 0.8582 ||:  80%|########  | 52/65 [09:51<02:29, 11.53s/it]
2022-04-10 11:40:28,961 - INFO - tqdm - accuracy: 0.4994, batch_loss: 0.6854, loss: 0.8550 ||:  82%|########1 | 53/65 [10:02<02:18, 11.53s/it]
2022-04-10 11:40:40,144 - INFO - tqdm - accuracy: 0.5049, batch_loss: 0.6240, loss: 0.8507 ||:  83%|########3 | 54/65 [10:13<02:05, 11.42s/it]
2022-04-10 11:40:51,670 - INFO - tqdm - accuracy: 0.5031, batch_loss: 0.7259, loss: 0.8484 ||:  85%|########4 | 55/65 [10:25<01:54, 11.45s/it]
2022-04-10 11:41:03,189 - INFO - tqdm - accuracy: 0.5036, batch_loss: 0.6839, loss: 0.8455 ||:  86%|########6 | 56/65 [10:36<01:43, 11.47s/it]
2022-04-10 11:41:14,723 - INFO - tqdm - accuracy: 0.4992, batch_loss: 0.7937, loss: 0.8446 ||:  88%|########7 | 57/65 [10:48<01:31, 11.49s/it]
2022-04-10 11:41:26,271 - INFO - tqdm - accuracy: 0.4997, batch_loss: 0.6798, loss: 0.8417 ||:  89%|########9 | 58/65 [11:00<01:20, 11.51s/it]
2022-04-10 11:41:37,832 - INFO - tqdm - accuracy: 0.4997, batch_loss: 0.7015, loss: 0.8393 ||:  91%|######### | 59/65 [11:11<01:09, 11.52s/it]
2022-04-10 11:41:49,398 - INFO - tqdm - accuracy: 0.4992, batch_loss: 0.6884, loss: 0.8368 ||:  92%|#########2| 60/65 [11:23<00:57, 11.54s/it]
2022-04-10 11:42:00,975 - INFO - tqdm - accuracy: 0.4992, batch_loss: 0.6989, loss: 0.8346 ||:  94%|#########3| 61/65 [11:34<00:46, 11.55s/it]
2022-04-10 11:42:12,551 - INFO - tqdm - accuracy: 0.5003, batch_loss: 0.6943, loss: 0.8323 ||:  95%|#########5| 62/65 [11:46<00:34, 11.56s/it]
2022-04-10 11:42:24,122 - INFO - tqdm - accuracy: 0.4993, batch_loss: 0.7177, loss: 0.8305 ||:  97%|#########6| 63/65 [11:57<00:23, 11.56s/it]
2022-04-10 11:42:35,694 - INFO - tqdm - accuracy: 0.5017, batch_loss: 0.6724, loss: 0.8280 ||:  98%|#########8| 64/65 [12:09<00:11, 11.56s/it]
2022-04-10 11:42:40,838 - INFO - tqdm - accuracy: 0.5007, batch_loss: 0.7154, loss: 0.8263 ||: 100%|##########| 65/65 [12:14<00:00,  9.64s/it]
2022-04-10 11:42:40,838 - INFO - tqdm - accuracy: 0.5007, batch_loss: 0.7154, loss: 0.8263 ||: 100%|##########| 65/65 [12:14<00:00, 11.30s/it]
2022-04-10 11:42:43,809 - INFO - allennlp.training.trainer - Validating
2022-04-10 11:42:43,811 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-10 11:42:43,811 - INFO - allennlp.data.samplers.bucket_batch_sampler - No sorting keys given; trying to guess a good one
2022-04-10 11:42:43,811 - INFO - allennlp.data.samplers.bucket_batch_sampler - Using ['tokens'] as the sorting keys
2022-04-10 11:42:53,941 - INFO - tqdm - accuracy: 0.5125, batch_loss: 0.6899, loss: 0.6933 ||:  35%|###4      | 40/115 [00:10<00:19,  3.91it/s]
2022-04-10 11:43:04,063 - INFO - tqdm - accuracy: 0.4750, batch_loss: 0.6788, loss: 0.6980 ||:  70%|######9   | 80/115 [00:20<00:08,  3.97it/s]
2022-04-10 11:43:12,989 - INFO - tqdm - accuracy: 0.4672, batch_loss: 0.7473, loss: 0.6977 ||: 100%|##########| 115/115 [00:29<00:00,  3.87it/s]
2022-04-10 11:43:12,989 - INFO - tqdm - accuracy: 0.4672, batch_loss: 0.7473, loss: 0.6977 ||: 100%|##########| 115/115 [00:29<00:00,  3.94it/s]
2022-04-10 11:43:12,989 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-10 11:43:12,992 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.501  |     0.467
2022-04-10 11:43:12,992 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  1359.606  |       N/A
2022-04-10 11:43:12,992 - INFO - allennlp.training.tensorboard_writer - loss               |     0.826  |     0.698
2022-04-10 11:43:12,993 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5797.879  |       N/A
2022-04-10 11:43:18,754 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to './output_ir-ora-d_hyper3/best.th'.
2022-04-10 11:43:20,055 - INFO - allennlp.training.trainer - Epoch duration: 0:12:53.853994
2022-04-10 11:43:20,055 - INFO - allennlp.training.trainer - Estimated training time remaining: 3:00:33
2022-04-10 11:43:20,055 - INFO - allennlp.training.trainer - Epoch 1/14
2022-04-10 11:43:20,055 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-10 11:43:20,056 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-10 11:43:20,057 - INFO - allennlp.training.trainer - Training
2022-04-10 11:43:20,057 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-10 11:43:31,393 - INFO - tqdm - accuracy: 0.3750, batch_loss: 0.7110, loss: 0.7110 ||:   2%|1         | 1/65 [00:11<12:05, 11.34s/it]
2022-04-10 11:43:42,971 - INFO - tqdm - accuracy: 0.4375, batch_loss: 0.6878, loss: 0.6994 ||:   3%|3         | 2/65 [00:22<12:03, 11.48s/it]
2022-04-10 11:43:54,767 - INFO - tqdm - accuracy: 0.4167, batch_loss: 0.7121, loss: 0.7036 ||:   5%|4         | 3/65 [00:34<12:00, 11.62s/it]
2022-04-10 11:44:06,396 - INFO - tqdm - accuracy: 0.4531, batch_loss: 0.6877, loss: 0.6996 ||:   6%|6         | 4/65 [00:46<11:49, 11.63s/it]
2022-04-10 11:44:17,835 - INFO - tqdm - accuracy: 0.4437, batch_loss: 0.6918, loss: 0.6981 ||:   8%|7         | 5/65 [00:57<11:33, 11.56s/it]
2022-04-10 11:44:29,286 - INFO - tqdm - accuracy: 0.4635, batch_loss: 0.6885, loss: 0.6965 ||:   9%|9         | 6/65 [01:09<11:19, 11.52s/it]
2022-04-10 11:44:40,775 - INFO - tqdm - accuracy: 0.4688, batch_loss: 0.7021, loss: 0.6973 ||:  11%|#         | 7/65 [01:20<11:07, 11.51s/it]
2022-04-10 11:44:52,393 - INFO - tqdm - accuracy: 0.4883, batch_loss: 0.6890, loss: 0.6962 ||:  12%|#2        | 8/65 [01:32<10:58, 11.55s/it]
2022-04-10 11:45:03,949 - INFO - tqdm - accuracy: 0.5000, batch_loss: 0.6852, loss: 0.6950 ||:  14%|#3        | 9/65 [01:43<10:46, 11.55s/it]
2022-04-10 11:45:15,487 - INFO - tqdm - accuracy: 0.5000, batch_loss: 0.6842, loss: 0.6939 ||:  15%|#5        | 10/65 [01:55<10:34, 11.55s/it]
2022-04-10 11:45:26,996 - INFO - tqdm - accuracy: 0.4972, batch_loss: 0.7430, loss: 0.6984 ||:  17%|#6        | 11/65 [02:06<10:22, 11.53s/it]
2022-04-10 11:45:38,508 - INFO - tqdm - accuracy: 0.5026, batch_loss: 0.6985, loss: 0.6984 ||:  18%|#8        | 12/65 [02:18<10:10, 11.53s/it]
2022-04-10 11:45:50,011 - INFO - tqdm - accuracy: 0.5144, batch_loss: 0.6581, loss: 0.6953 ||:  20%|##        | 13/65 [02:29<09:59, 11.52s/it]
2022-04-10 11:46:01,393 - INFO - tqdm - accuracy: 0.5112, batch_loss: 0.7216, loss: 0.6972 ||:  22%|##1       | 14/65 [02:41<09:45, 11.48s/it]
2022-04-10 11:46:12,890 - INFO - tqdm - accuracy: 0.5104, batch_loss: 0.6986, loss: 0.6973 ||:  23%|##3       | 15/65 [02:52<09:34, 11.48s/it]
2022-04-10 11:46:24,389 - INFO - tqdm - accuracy: 0.5176, batch_loss: 0.6744, loss: 0.6958 ||:  25%|##4       | 16/65 [03:04<09:22, 11.49s/it]
2022-04-10 11:46:35,887 - INFO - tqdm - accuracy: 0.5165, batch_loss: 0.6868, loss: 0.6953 ||:  26%|##6       | 17/65 [03:15<09:11, 11.49s/it]
2022-04-10 11:46:47,369 - INFO - tqdm - accuracy: 0.5208, batch_loss: 0.6378, loss: 0.6921 ||:  28%|##7       | 18/65 [03:27<08:59, 11.49s/it]
2022-04-10 11:46:58,887 - INFO - tqdm - accuracy: 0.5197, batch_loss: 0.6993, loss: 0.6925 ||:  29%|##9       | 19/65 [03:38<08:48, 11.50s/it]
2022-04-10 11:47:10,390 - INFO - tqdm - accuracy: 0.5219, batch_loss: 0.6950, loss: 0.6926 ||:  31%|###       | 20/65 [03:50<08:37, 11.50s/it]
2022-04-10 11:47:21,894 - INFO - tqdm - accuracy: 0.5208, batch_loss: 0.6885, loss: 0.6924 ||:  32%|###2      | 21/65 [04:01<08:26, 11.50s/it]
2022-04-10 11:47:33,280 - INFO - tqdm - accuracy: 0.5199, batch_loss: 0.7024, loss: 0.6929 ||:  34%|###3      | 22/65 [04:13<08:13, 11.47s/it]
2022-04-10 11:47:44,783 - INFO - tqdm - accuracy: 0.5258, batch_loss: 0.6567, loss: 0.6913 ||:  35%|###5      | 23/65 [04:24<08:02, 11.48s/it]
2022-04-10 11:47:56,296 - INFO - tqdm - accuracy: 0.5195, batch_loss: 0.7396, loss: 0.6933 ||:  37%|###6      | 24/65 [04:36<07:51, 11.49s/it]
2022-04-10 11:48:07,795 - INFO - tqdm - accuracy: 0.5225, batch_loss: 0.6701, loss: 0.6924 ||:  38%|###8      | 25/65 [04:47<07:39, 11.49s/it]
2022-04-10 11:48:19,320 - INFO - tqdm - accuracy: 0.5252, batch_loss: 0.6860, loss: 0.6921 ||:  40%|####      | 26/65 [04:59<07:28, 11.50s/it]
2022-04-10 11:48:30,812 - INFO - tqdm - accuracy: 0.5255, batch_loss: 0.6894, loss: 0.6920 ||:  42%|####1     | 27/65 [05:10<07:16, 11.50s/it]
2022-04-10 11:48:42,324 - INFO - tqdm - accuracy: 0.5279, batch_loss: 0.6719, loss: 0.6913 ||:  43%|####3     | 28/65 [05:22<07:05, 11.50s/it]
2022-04-10 11:48:53,826 - INFO - tqdm - accuracy: 0.5248, batch_loss: 0.7014, loss: 0.6917 ||:  45%|####4     | 29/65 [05:33<06:54, 11.50s/it]
2022-04-10 11:49:05,332 - INFO - tqdm - accuracy: 0.5188, batch_loss: 0.7211, loss: 0.6927 ||:  46%|####6     | 30/65 [05:45<06:42, 11.50s/it]
2022-04-10 11:49:16,854 - INFO - tqdm - accuracy: 0.5181, batch_loss: 0.6981, loss: 0.6928 ||:  48%|####7     | 31/65 [05:56<06:31, 11.51s/it]
2022-04-10 11:49:28,328 - INFO - tqdm - accuracy: 0.5146, batch_loss: 0.6812, loss: 0.6925 ||:  49%|####9     | 32/65 [06:08<06:19, 11.50s/it]
2022-04-10 11:49:39,815 - INFO - tqdm - accuracy: 0.5142, batch_loss: 0.6893, loss: 0.6924 ||:  51%|#####     | 33/65 [06:19<06:07, 11.50s/it]
2022-04-10 11:49:51,300 - INFO - tqdm - accuracy: 0.5129, batch_loss: 0.7087, loss: 0.6929 ||:  52%|#####2    | 34/65 [06:31<05:56, 11.49s/it]
2022-04-10 11:50:03,090 - INFO - tqdm - accuracy: 0.5152, batch_loss: 0.6736, loss: 0.6923 ||:  54%|#####3    | 35/65 [06:43<05:47, 11.58s/it]
2022-04-10 11:50:14,548 - INFO - tqdm - accuracy: 0.5182, batch_loss: 0.6586, loss: 0.6914 ||:  55%|#####5    | 36/65 [06:54<05:34, 11.54s/it]
2022-04-10 11:50:26,061 - INFO - tqdm - accuracy: 0.5177, batch_loss: 0.6803, loss: 0.6911 ||:  57%|#####6    | 37/65 [07:06<05:22, 11.53s/it]
2022-04-10 11:50:37,562 - INFO - tqdm - accuracy: 0.5222, batch_loss: 0.6678, loss: 0.6905 ||:  58%|#####8    | 38/65 [07:17<05:11, 11.52s/it]
2022-04-10 11:50:49,090 - INFO - tqdm - accuracy: 0.5216, batch_loss: 0.7023, loss: 0.6908 ||:  60%|######    | 39/65 [07:29<04:59, 11.53s/it]
2022-04-10 11:51:00,629 - INFO - tqdm - accuracy: 0.5219, batch_loss: 0.6823, loss: 0.6905 ||:  62%|######1   | 40/65 [07:40<04:48, 11.53s/it]
2022-04-10 11:51:12,157 - INFO - tqdm - accuracy: 0.5206, batch_loss: 0.6835, loss: 0.6904 ||:  63%|######3   | 41/65 [07:52<04:36, 11.53s/it]
2022-04-10 11:51:23,714 - INFO - tqdm - accuracy: 0.5193, batch_loss: 0.6843, loss: 0.6902 ||:  65%|######4   | 42/65 [08:03<04:25, 11.54s/it]
2022-04-10 11:51:35,268 - INFO - tqdm - accuracy: 0.5233, batch_loss: 0.6869, loss: 0.6902 ||:  66%|######6   | 43/65 [08:15<04:13, 11.54s/it]
2022-04-10 11:51:46,819 - INFO - tqdm - accuracy: 0.5234, batch_loss: 0.6855, loss: 0.6900 ||:  68%|######7   | 44/65 [08:26<04:02, 11.54s/it]
2022-04-10 11:51:58,369 - INFO - tqdm - accuracy: 0.5236, batch_loss: 0.6629, loss: 0.6894 ||:  69%|######9   | 45/65 [08:38<03:50, 11.55s/it]
2022-04-10 11:52:09,935 - INFO - tqdm - accuracy: 0.5204, batch_loss: 0.7294, loss: 0.6903 ||:  71%|#######   | 46/65 [08:49<03:39, 11.55s/it]
2022-04-10 11:52:21,487 - INFO - tqdm - accuracy: 0.5206, batch_loss: 0.7087, loss: 0.6907 ||:  72%|#######2  | 47/65 [09:01<03:27, 11.55s/it]
2022-04-10 11:52:33,053 - INFO - tqdm - accuracy: 0.5202, batch_loss: 0.6875, loss: 0.6906 ||:  74%|#######3  | 48/65 [09:12<03:16, 11.56s/it]
2022-04-10 11:52:44,641 - INFO - tqdm - accuracy: 0.5198, batch_loss: 0.7208, loss: 0.6913 ||:  75%|#######5  | 49/65 [09:24<03:05, 11.57s/it]
2022-04-10 11:52:56,214 - INFO - tqdm - accuracy: 0.5219, batch_loss: 0.6645, loss: 0.6907 ||:  77%|#######6  | 50/65 [09:36<02:53, 11.57s/it]
2022-04-10 11:53:07,804 - INFO - tqdm - accuracy: 0.5196, batch_loss: 0.7427, loss: 0.6917 ||:  78%|#######8  | 51/65 [09:47<02:42, 11.57s/it]
2022-04-10 11:53:19,378 - INFO - tqdm - accuracy: 0.5192, batch_loss: 0.6888, loss: 0.6917 ||:  80%|########  | 52/65 [09:59<02:30, 11.57s/it]
2022-04-10 11:53:30,945 - INFO - tqdm - accuracy: 0.5189, batch_loss: 0.6769, loss: 0.6914 ||:  82%|########1 | 53/65 [10:10<02:18, 11.57s/it]
2022-04-10 11:53:42,541 - INFO - tqdm - accuracy: 0.5191, batch_loss: 0.7083, loss: 0.6917 ||:  83%|########3 | 54/65 [10:22<02:07, 11.58s/it]
2022-04-10 11:53:54,118 - INFO - tqdm - accuracy: 0.5216, batch_loss: 0.6452, loss: 0.6909 ||:  85%|########4 | 55/65 [10:34<01:55, 11.58s/it]
2022-04-10 11:54:05,702 - INFO - tqdm - accuracy: 0.5201, batch_loss: 0.6986, loss: 0.6910 ||:  86%|########6 | 56/65 [10:45<01:44, 11.58s/it]
2022-04-10 11:54:17,031 - INFO - tqdm - accuracy: 0.5219, batch_loss: 0.6254, loss: 0.6899 ||:  88%|########7 | 57/65 [10:56<01:32, 11.50s/it]
2022-04-10 11:54:28,613 - INFO - tqdm - accuracy: 0.5232, batch_loss: 0.7240, loss: 0.6904 ||:  89%|########9 | 58/65 [11:08<01:20, 11.53s/it]
2022-04-10 11:54:40,169 - INFO - tqdm - accuracy: 0.5254, batch_loss: 0.6425, loss: 0.6896 ||:  91%|######### | 59/65 [11:20<01:09, 11.54s/it]
2022-04-10 11:54:51,723 - INFO - tqdm - accuracy: 0.5255, batch_loss: 0.7771, loss: 0.6911 ||:  92%|#########2| 60/65 [11:31<00:57, 11.54s/it]
2022-04-10 11:55:03,283 - INFO - tqdm - accuracy: 0.5292, batch_loss: 0.5536, loss: 0.6888 ||:  94%|#########3| 61/65 [11:43<00:46, 11.55s/it]
2022-04-10 11:55:14,838 - INFO - tqdm - accuracy: 0.5318, batch_loss: 0.5352, loss: 0.6864 ||:  95%|#########5| 62/65 [11:54<00:34, 11.55s/it]
2022-04-10 11:55:26,365 - INFO - tqdm - accuracy: 0.5337, batch_loss: 0.6214, loss: 0.6853 ||:  97%|#########6| 63/65 [12:06<00:23, 11.54s/it]
2022-04-10 11:55:37,559 - INFO - tqdm - accuracy: 0.5335, batch_loss: 0.7310, loss: 0.6860 ||:  98%|#########8| 64/65 [12:17<00:11, 11.44s/it]
2022-04-10 11:55:42,673 - INFO - tqdm - accuracy: 0.5342, batch_loss: 0.6930, loss: 0.6861 ||: 100%|##########| 65/65 [12:22<00:00,  9.54s/it]
2022-04-10 11:55:42,673 - INFO - tqdm - accuracy: 0.5342, batch_loss: 0.6930, loss: 0.6861 ||: 100%|##########| 65/65 [12:22<00:00, 11.42s/it]
2022-04-10 11:55:45,624 - INFO - allennlp.training.trainer - Validating
2022-04-10 11:55:45,626 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-10 11:55:55,653 - INFO - tqdm - accuracy: 0.6625, batch_loss: 0.7573, loss: 0.6245 ||:  35%|###4      | 40/115 [00:10<00:19,  3.94it/s]
2022-04-10 11:56:05,888 - INFO - tqdm - accuracy: 0.6125, batch_loss: 0.7691, loss: 0.6533 ||:  70%|######9   | 80/115 [00:20<00:08,  3.89it/s]
2022-04-10 11:56:14,805 - INFO - tqdm - accuracy: 0.6070, batch_loss: 0.5283, loss: 0.6606 ||: 100%|##########| 115/115 [00:29<00:00,  3.86it/s]
2022-04-10 11:56:14,805 - INFO - tqdm - accuracy: 0.6070, batch_loss: 0.5283, loss: 0.6606 ||: 100%|##########| 115/115 [00:29<00:00,  3.94it/s]
2022-04-10 11:56:14,805 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-10 11:56:14,806 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.534  |     0.607
2022-04-10 11:56:14,807 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-10 11:56:14,807 - INFO - allennlp.training.tensorboard_writer - loss               |     0.686  |     0.661
2022-04-10 11:56:14,808 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5797.879  |       N/A
2022-04-10 11:56:20,553 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to './output_ir-ora-d_hyper3/best.th'.
2022-04-10 11:56:33,878 - INFO - allennlp.training.trainer - Epoch duration: 0:13:13.823116
2022-04-10 11:56:33,879 - INFO - allennlp.training.trainer - Estimated training time remaining: 2:49:49
2022-04-10 11:56:33,879 - INFO - allennlp.training.trainer - Epoch 2/14
2022-04-10 11:56:33,879 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-10 11:56:33,879 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-10 11:56:33,881 - INFO - allennlp.training.trainer - Training
2022-04-10 11:56:33,881 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-10 11:56:45,097 - INFO - tqdm - accuracy: 0.5938, batch_loss: 0.6656, loss: 0.6656 ||:   2%|1         | 1/65 [00:11<11:57, 11.22s/it]
2022-04-10 11:56:56,648 - INFO - tqdm - accuracy: 0.7031, batch_loss: 0.5535, loss: 0.6095 ||:   3%|3         | 2/65 [00:22<11:59, 11.41s/it]
2022-04-10 11:57:08,484 - INFO - tqdm - accuracy: 0.7188, batch_loss: 0.5974, loss: 0.6055 ||:   5%|4         | 3/65 [00:34<11:59, 11.61s/it]
2022-04-10 11:57:20,387 - INFO - tqdm - accuracy: 0.7266, batch_loss: 0.5698, loss: 0.5966 ||:   6%|6         | 4/65 [00:46<11:55, 11.72s/it]
2022-04-10 11:57:31,884 - INFO - tqdm - accuracy: 0.7250, batch_loss: 0.5991, loss: 0.5971 ||:   8%|7         | 5/65 [00:58<11:38, 11.64s/it]
2022-04-10 11:57:43,098 - INFO - tqdm - accuracy: 0.7292, batch_loss: 0.5813, loss: 0.5944 ||:   9%|9         | 6/65 [01:09<11:18, 11.50s/it]
2022-04-10 11:57:54,482 - INFO - tqdm - accuracy: 0.7277, batch_loss: 0.6019, loss: 0.5955 ||:  11%|#         | 7/65 [01:20<11:04, 11.46s/it]
2022-04-10 11:58:05,995 - INFO - tqdm - accuracy: 0.7266, batch_loss: 0.6416, loss: 0.6013 ||:  12%|#2        | 8/65 [01:32<10:54, 11.48s/it]
2022-04-10 11:58:17,624 - INFO - tqdm - accuracy: 0.7396, batch_loss: 0.5011, loss: 0.5901 ||:  14%|#3        | 9/65 [01:43<10:45, 11.52s/it]
2022-04-10 11:58:29,239 - INFO - tqdm - accuracy: 0.7375, batch_loss: 0.5307, loss: 0.5842 ||:  15%|#5        | 10/65 [01:55<10:35, 11.55s/it]
2022-04-10 11:58:40,775 - INFO - tqdm - accuracy: 0.7301, batch_loss: 0.6009, loss: 0.5857 ||:  17%|#6        | 11/65 [02:06<10:23, 11.55s/it]
2022-04-10 11:58:52,261 - INFO - tqdm - accuracy: 0.7188, batch_loss: 0.6394, loss: 0.5902 ||:  18%|#8        | 12/65 [02:18<10:11, 11.53s/it]
2022-04-10 11:59:03,392 - INFO - tqdm - accuracy: 0.7036, batch_loss: 0.7501, loss: 0.6025 ||:  20%|##        | 13/65 [02:29<09:53, 11.41s/it]
2022-04-10 11:59:14,928 - INFO - tqdm - accuracy: 0.7025, batch_loss: 0.5825, loss: 0.6011 ||:  22%|##1       | 14/65 [02:41<09:43, 11.45s/it]
2022-04-10 11:59:26,487 - INFO - tqdm - accuracy: 0.6973, batch_loss: 0.6719, loss: 0.6058 ||:  23%|##3       | 15/65 [02:52<09:34, 11.48s/it]
2022-04-10 11:59:38,084 - INFO - tqdm - accuracy: 0.7045, batch_loss: 0.4731, loss: 0.5975 ||:  25%|##4       | 16/65 [03:04<09:24, 11.52s/it]
2022-04-10 11:59:49,652 - INFO - tqdm - accuracy: 0.7017, batch_loss: 0.5950, loss: 0.5973 ||:  26%|##6       | 17/65 [03:15<09:13, 11.53s/it]
2022-04-10 12:00:01,213 - INFO - tqdm - accuracy: 0.6939, batch_loss: 0.8142, loss: 0.6094 ||:  28%|##7       | 18/65 [03:27<09:02, 11.54s/it]
2022-04-10 12:00:12,723 - INFO - tqdm - accuracy: 0.6985, batch_loss: 0.6031, loss: 0.6091 ||:  29%|##9       | 19/65 [03:38<08:50, 11.53s/it]
2022-04-10 12:00:24,185 - INFO - tqdm - accuracy: 0.6980, batch_loss: 0.6456, loss: 0.6109 ||:  31%|###       | 20/65 [03:50<08:37, 11.51s/it]
2022-04-10 12:00:35,694 - INFO - tqdm - accuracy: 0.6945, batch_loss: 0.6960, loss: 0.6149 ||:  32%|###2      | 21/65 [04:01<08:26, 11.51s/it]
2022-04-10 12:00:47,217 - INFO - tqdm - accuracy: 0.6927, batch_loss: 0.5465, loss: 0.6118 ||:  34%|###3      | 22/65 [04:13<08:15, 11.51s/it]
2022-04-10 12:00:58,790 - INFO - tqdm - accuracy: 0.6925, batch_loss: 0.5484, loss: 0.6091 ||:  35%|###5      | 23/65 [04:24<08:04, 11.53s/it]
2022-04-10 12:01:10,245 - INFO - tqdm - accuracy: 0.6871, batch_loss: 0.6801, loss: 0.6120 ||:  37%|###6      | 24/65 [04:36<07:51, 11.51s/it]
2022-04-10 12:01:21,826 - INFO - tqdm - accuracy: 0.6859, batch_loss: 0.6437, loss: 0.6133 ||:  38%|###8      | 25/65 [04:47<07:41, 11.53s/it]
2022-04-10 12:01:33,401 - INFO - tqdm - accuracy: 0.6871, batch_loss: 0.5882, loss: 0.6123 ||:  40%|####      | 26/65 [04:59<07:30, 11.54s/it]
2022-04-10 12:01:44,997 - INFO - tqdm - accuracy: 0.6906, batch_loss: 0.5863, loss: 0.6114 ||:  42%|####1     | 27/65 [05:11<07:19, 11.56s/it]
2022-04-10 12:01:56,561 - INFO - tqdm - accuracy: 0.6939, batch_loss: 0.5611, loss: 0.6096 ||:  43%|####3     | 28/65 [05:22<07:07, 11.56s/it]
2022-04-10 12:02:08,115 - INFO - tqdm - accuracy: 0.6904, batch_loss: 0.6792, loss: 0.6120 ||:  45%|####4     | 29/65 [05:34<06:56, 11.56s/it]
2022-04-10 12:02:19,660 - INFO - tqdm - accuracy: 0.6913, batch_loss: 0.5554, loss: 0.6101 ||:  46%|####6     | 30/65 [05:45<06:44, 11.55s/it]
2022-04-10 12:02:31,201 - INFO - tqdm - accuracy: 0.6852, batch_loss: 0.6647, loss: 0.6118 ||:  48%|####7     | 31/65 [05:57<06:32, 11.55s/it]
2022-04-10 12:02:42,720 - INFO - tqdm - accuracy: 0.6872, batch_loss: 0.5734, loss: 0.6106 ||:  49%|####9     | 32/65 [06:08<06:20, 11.54s/it]
2022-04-10 12:02:54,254 - INFO - tqdm - accuracy: 0.6853, batch_loss: 0.6422, loss: 0.6116 ||:  51%|#####     | 33/65 [06:20<06:09, 11.54s/it]
2022-04-10 12:03:05,781 - INFO - tqdm - accuracy: 0.6835, batch_loss: 0.6600, loss: 0.6130 ||:  52%|#####2    | 34/65 [06:31<05:57, 11.54s/it]
2022-04-10 12:03:17,321 - INFO - tqdm - accuracy: 0.6854, batch_loss: 0.5217, loss: 0.6104 ||:  54%|#####3    | 35/65 [06:43<05:46, 11.54s/it]
2022-04-10 12:03:28,842 - INFO - tqdm - accuracy: 0.6864, batch_loss: 0.5950, loss: 0.6100 ||:  55%|#####5    | 36/65 [06:54<05:34, 11.53s/it]
2022-04-10 12:03:40,367 - INFO - tqdm - accuracy: 0.6822, batch_loss: 0.7415, loss: 0.6135 ||:  57%|#####6    | 37/65 [07:06<05:22, 11.53s/it]
2022-04-10 12:03:51,889 - INFO - tqdm - accuracy: 0.6872, batch_loss: 0.4003, loss: 0.6079 ||:  58%|#####8    | 38/65 [07:18<05:11, 11.53s/it]
2022-04-10 12:04:03,395 - INFO - tqdm - accuracy: 0.6889, batch_loss: 0.5265, loss: 0.6058 ||:  60%|######    | 39/65 [07:29<04:59, 11.52s/it]
2022-04-10 12:04:14,883 - INFO - tqdm - accuracy: 0.6880, batch_loss: 0.6187, loss: 0.6062 ||:  62%|######1   | 40/65 [07:41<04:47, 11.51s/it]
2022-04-10 12:04:26,423 - INFO - tqdm - accuracy: 0.6857, batch_loss: 0.6068, loss: 0.6062 ||:  63%|######3   | 41/65 [07:52<04:36, 11.52s/it]
2022-04-10 12:04:37,789 - INFO - tqdm - accuracy: 0.6880, batch_loss: 0.4722, loss: 0.6030 ||:  65%|######4   | 42/65 [08:03<04:23, 11.47s/it]
2022-04-10 12:04:49,311 - INFO - tqdm - accuracy: 0.6895, batch_loss: 0.5928, loss: 0.6028 ||:  66%|######6   | 43/65 [08:15<04:12, 11.49s/it]
2022-04-10 12:05:00,845 - INFO - tqdm - accuracy: 0.6887, batch_loss: 0.5721, loss: 0.6021 ||:  68%|######7   | 44/65 [08:26<04:01, 11.50s/it]
2022-04-10 12:05:12,367 - INFO - tqdm - accuracy: 0.6866, batch_loss: 0.6796, loss: 0.6038 ||:  69%|######9   | 45/65 [08:38<03:50, 11.51s/it]
2022-04-10 12:05:23,879 - INFO - tqdm - accuracy: 0.6886, batch_loss: 0.4698, loss: 0.6009 ||:  71%|#######   | 46/65 [08:49<03:38, 11.51s/it]
2022-04-10 12:05:35,406 - INFO - tqdm - accuracy: 0.6873, batch_loss: 0.6306, loss: 0.6015 ||:  72%|#######2  | 47/65 [09:01<03:27, 11.51s/it]
2022-04-10 12:05:46,960 - INFO - tqdm - accuracy: 0.6847, batch_loss: 0.7957, loss: 0.6056 ||:  74%|#######3  | 48/65 [09:13<03:15, 11.53s/it]
2022-04-10 12:05:58,511 - INFO - tqdm - accuracy: 0.6841, batch_loss: 0.6208, loss: 0.6059 ||:  75%|#######5  | 49/65 [09:24<03:04, 11.53s/it]
2022-04-10 12:06:10,033 - INFO - tqdm - accuracy: 0.6823, batch_loss: 0.6716, loss: 0.6072 ||:  77%|#######6  | 50/65 [09:36<02:52, 11.53s/it]
2022-04-10 12:06:21,575 - INFO - tqdm - accuracy: 0.6800, batch_loss: 0.7156, loss: 0.6093 ||:  78%|#######8  | 51/65 [09:47<02:41, 11.53s/it]
2022-04-10 12:06:33,103 - INFO - tqdm - accuracy: 0.6807, batch_loss: 0.5325, loss: 0.6078 ||:  80%|########  | 52/65 [09:59<02:29, 11.53s/it]
2022-04-10 12:06:44,659 - INFO - tqdm - accuracy: 0.6791, batch_loss: 0.6026, loss: 0.6077 ||:  82%|########1 | 53/65 [10:10<02:18, 11.54s/it]
2022-04-10 12:06:56,202 - INFO - tqdm - accuracy: 0.6781, batch_loss: 0.5673, loss: 0.6070 ||:  83%|########3 | 54/65 [10:22<02:06, 11.54s/it]
2022-04-10 12:07:07,725 - INFO - tqdm - accuracy: 0.6771, batch_loss: 0.6810, loss: 0.6083 ||:  85%|########4 | 55/65 [10:33<01:55, 11.54s/it]
2022-04-10 12:07:19,239 - INFO - tqdm - accuracy: 0.6750, batch_loss: 0.6606, loss: 0.6093 ||:  86%|########6 | 56/65 [10:45<01:43, 11.53s/it]
2022-04-10 12:07:30,764 - INFO - tqdm - accuracy: 0.6736, batch_loss: 0.6721, loss: 0.6104 ||:  88%|########7 | 57/65 [10:56<01:32, 11.53s/it]
2022-04-10 12:07:42,029 - INFO - tqdm - accuracy: 0.6744, batch_loss: 0.5904, loss: 0.6100 ||:  89%|########9 | 58/65 [11:08<01:20, 11.45s/it]
2022-04-10 12:07:53,517 - INFO - tqdm - accuracy: 0.6746, batch_loss: 0.5848, loss: 0.6096 ||:  91%|######### | 59/65 [11:19<01:08, 11.46s/it]
2022-04-10 12:08:05,022 - INFO - tqdm - accuracy: 0.6738, batch_loss: 0.6048, loss: 0.6095 ||:  92%|#########2| 60/65 [11:31<00:57, 11.47s/it]
2022-04-10 12:08:16,524 - INFO - tqdm - accuracy: 0.6756, batch_loss: 0.5239, loss: 0.6081 ||:  94%|#########3| 61/65 [11:42<00:45, 11.48s/it]
2022-04-10 12:08:28,027 - INFO - tqdm - accuracy: 0.6737, batch_loss: 0.6836, loss: 0.6093 ||:  95%|#########5| 62/65 [11:54<00:34, 11.49s/it]
2022-04-10 12:08:39,519 - INFO - tqdm - accuracy: 0.6725, batch_loss: 0.6243, loss: 0.6096 ||:  97%|#########6| 63/65 [12:05<00:22, 11.49s/it]
2022-04-10 12:08:50,995 - INFO - tqdm - accuracy: 0.6707, batch_loss: 0.6922, loss: 0.6109 ||:  98%|#########8| 64/65 [12:17<00:11, 11.49s/it]
2022-04-10 12:08:56,100 - INFO - tqdm - accuracy: 0.6710, batch_loss: 0.6722, loss: 0.6118 ||: 100%|##########| 65/65 [12:22<00:00,  9.57s/it]
2022-04-10 12:08:56,100 - INFO - tqdm - accuracy: 0.6710, batch_loss: 0.6722, loss: 0.6118 ||: 100%|##########| 65/65 [12:22<00:00, 11.42s/it]
2022-04-10 12:08:59,060 - INFO - allennlp.training.trainer - Validating
2022-04-10 12:08:59,061 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-10 12:09:09,177 - INFO - tqdm - accuracy: 0.5875, batch_loss: 0.6574, loss: 0.7131 ||:  35%|###4      | 40/115 [00:10<00:18,  3.95it/s]
2022-04-10 12:09:19,431 - INFO - tqdm - accuracy: 0.6335, batch_loss: 0.3503, loss: 0.6646 ||:  70%|#######   | 81/115 [00:20<00:08,  3.94it/s]
2022-04-10 12:09:28,246 - INFO - tqdm - accuracy: 0.6463, batch_loss: 1.0274, loss: 0.6533 ||: 100%|##########| 115/115 [00:29<00:00,  3.85it/s]
2022-04-10 12:09:28,246 - INFO - tqdm - accuracy: 0.6463, batch_loss: 1.0274, loss: 0.6533 ||: 100%|##########| 115/115 [00:29<00:00,  3.94it/s]
2022-04-10 12:09:28,246 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-10 12:09:28,247 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.671  |     0.646
2022-04-10 12:09:28,247 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-10 12:09:28,248 - INFO - allennlp.training.tensorboard_writer - loss               |     0.612  |     0.653
2022-04-10 12:09:28,249 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5797.879  |       N/A
2022-04-10 12:09:34,003 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to './output_ir-ora-d_hyper3/best.th'.
2022-04-10 12:09:48,121 - INFO - allennlp.training.trainer - Epoch duration: 0:13:14.242666
2022-04-10 12:09:48,122 - INFO - allennlp.training.trainer - Estimated training time remaining: 2:37:27
2022-04-10 12:09:48,122 - INFO - allennlp.training.trainer - Epoch 3/14
2022-04-10 12:09:48,122 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-10 12:09:48,122 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-10 12:09:48,123 - INFO - allennlp.training.trainer - Training
2022-04-10 12:09:48,124 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-10 12:09:59,249 - INFO - tqdm - accuracy: 0.8125, batch_loss: 0.4959, loss: 0.4959 ||:   2%|1         | 1/65 [00:11<11:51, 11.12s/it]
2022-04-10 12:10:10,729 - INFO - tqdm - accuracy: 0.7188, batch_loss: 0.6049, loss: 0.5504 ||:   3%|3         | 2/65 [00:22<11:54, 11.33s/it]
2022-04-10 12:10:22,492 - INFO - tqdm - accuracy: 0.7604, batch_loss: 0.4804, loss: 0.5271 ||:   5%|4         | 3/65 [00:34<11:54, 11.53s/it]
2022-04-10 12:10:34,390 - INFO - tqdm - accuracy: 0.7812, batch_loss: 0.4227, loss: 0.5010 ||:   6%|6         | 4/65 [00:46<11:52, 11.68s/it]
2022-04-10 12:10:46,373 - INFO - tqdm - accuracy: 0.7937, batch_loss: 0.4436, loss: 0.4895 ||:   8%|7         | 5/65 [00:58<11:47, 11.79s/it]
2022-04-10 12:10:57,732 - INFO - tqdm - accuracy: 0.8021, batch_loss: 0.4345, loss: 0.4803 ||:   9%|9         | 6/65 [01:09<11:26, 11.64s/it]
2022-04-10 12:11:09,122 - INFO - tqdm - accuracy: 0.8214, batch_loss: 0.3743, loss: 0.4652 ||:  11%|#         | 7/65 [01:20<11:10, 11.56s/it]
2022-04-10 12:11:20,582 - INFO - tqdm - accuracy: 0.8125, batch_loss: 0.3820, loss: 0.4548 ||:  12%|#2        | 8/65 [01:32<10:57, 11.53s/it]
2022-04-10 12:11:32,338 - INFO - tqdm - accuracy: 0.8021, batch_loss: 0.4693, loss: 0.4564 ||:  14%|#3        | 9/65 [01:44<10:49, 11.60s/it]
2022-04-10 12:11:43,848 - INFO - tqdm - accuracy: 0.8094, batch_loss: 0.2807, loss: 0.4388 ||:  15%|#5        | 10/65 [01:55<10:36, 11.57s/it]
2022-04-10 12:11:55,161 - INFO - tqdm - accuracy: 0.7926, batch_loss: 0.6131, loss: 0.4547 ||:  17%|#6        | 11/65 [02:07<10:20, 11.49s/it]
2022-04-10 12:12:06,665 - INFO - tqdm - accuracy: 0.8047, batch_loss: 0.1619, loss: 0.4303 ||:  18%|#8        | 12/65 [02:18<10:09, 11.50s/it]
2022-04-10 12:12:18,214 - INFO - tqdm - accuracy: 0.8149, batch_loss: 0.1798, loss: 0.4110 ||:  20%|##        | 13/65 [02:30<09:58, 11.51s/it]
2022-04-10 12:12:29,803 - INFO - tqdm - accuracy: 0.8192, batch_loss: 0.3383, loss: 0.4058 ||:  22%|##1       | 14/65 [02:41<09:48, 11.54s/it]
2022-04-10 12:12:41,409 - INFO - tqdm - accuracy: 0.8208, batch_loss: 0.4049, loss: 0.4058 ||:  23%|##3       | 15/65 [02:53<09:37, 11.56s/it]
2022-04-10 12:12:52,955 - INFO - tqdm - accuracy: 0.8242, batch_loss: 0.2728, loss: 0.3974 ||:  25%|##4       | 16/65 [03:04<09:26, 11.55s/it]
2022-04-10 12:13:04,456 - INFO - tqdm - accuracy: 0.8235, batch_loss: 0.4305, loss: 0.3994 ||:  26%|##6       | 17/65 [03:16<09:13, 11.54s/it]
2022-04-10 12:13:15,931 - INFO - tqdm - accuracy: 0.8333, batch_loss: 0.1016, loss: 0.3828 ||:  28%|##7       | 18/65 [03:27<09:01, 11.52s/it]
2022-04-10 12:13:27,415 - INFO - tqdm - accuracy: 0.8355, batch_loss: 0.3341, loss: 0.3803 ||:  29%|##9       | 19/65 [03:39<08:49, 11.51s/it]
2022-04-10 12:13:38,983 - INFO - tqdm - accuracy: 0.8313, batch_loss: 0.7559, loss: 0.3991 ||:  31%|###       | 20/65 [03:50<08:38, 11.53s/it]
2022-04-10 12:13:50,581 - INFO - tqdm - accuracy: 0.8363, batch_loss: 0.2998, loss: 0.3943 ||:  32%|###2      | 21/65 [04:02<08:28, 11.55s/it]
2022-04-10 12:14:02,173 - INFO - tqdm - accuracy: 0.8381, batch_loss: 0.3129, loss: 0.3906 ||:  34%|###3      | 22/65 [04:14<08:17, 11.56s/it]
2022-04-10 12:14:13,727 - INFO - tqdm - accuracy: 0.8397, batch_loss: 0.2804, loss: 0.3858 ||:  35%|###5      | 23/65 [04:25<08:05, 11.56s/it]
2022-04-10 12:14:25,258 - INFO - tqdm - accuracy: 0.8411, batch_loss: 0.3177, loss: 0.3830 ||:  37%|###6      | 24/65 [04:37<07:53, 11.55s/it]
2022-04-10 12:14:36,796 - INFO - tqdm - accuracy: 0.8425, batch_loss: 0.3152, loss: 0.3803 ||:  38%|###8      | 25/65 [04:48<07:41, 11.55s/it]
2022-04-10 12:14:48,318 - INFO - tqdm - accuracy: 0.8450, batch_loss: 0.2369, loss: 0.3748 ||:  40%|####      | 26/65 [05:00<07:30, 11.54s/it]
2022-04-10 12:14:59,824 - INFO - tqdm - accuracy: 0.8461, batch_loss: 0.3620, loss: 0.3743 ||:  42%|####1     | 27/65 [05:11<07:18, 11.53s/it]
2022-04-10 12:15:11,324 - INFO - tqdm - accuracy: 0.8449, batch_loss: 0.4357, loss: 0.3765 ||:  43%|####3     | 28/65 [05:23<07:06, 11.52s/it]
2022-04-10 12:15:22,808 - INFO - tqdm - accuracy: 0.8448, batch_loss: 0.2763, loss: 0.3730 ||:  45%|####4     | 29/65 [05:34<06:54, 11.51s/it]
2022-04-10 12:15:34,314 - INFO - tqdm - accuracy: 0.8438, batch_loss: 0.3326, loss: 0.3717 ||:  46%|####6     | 30/65 [05:46<06:42, 11.51s/it]
2022-04-10 12:15:45,827 - INFO - tqdm - accuracy: 0.8458, batch_loss: 0.3808, loss: 0.3720 ||:  48%|####7     | 31/65 [05:57<06:31, 11.51s/it]
2022-04-10 12:15:57,214 - INFO - tqdm - accuracy: 0.8447, batch_loss: 0.3086, loss: 0.3700 ||:  49%|####9     | 32/65 [06:09<06:18, 11.47s/it]
2022-04-10 12:16:08,747 - INFO - tqdm - accuracy: 0.8419, batch_loss: 0.4267, loss: 0.3717 ||:  51%|#####     | 33/65 [06:20<06:07, 11.49s/it]
2022-04-10 12:16:20,295 - INFO - tqdm - accuracy: 0.8392, batch_loss: 0.6042, loss: 0.3786 ||:  52%|#####2    | 34/65 [06:32<05:56, 11.51s/it]
2022-04-10 12:16:31,839 - INFO - tqdm - accuracy: 0.8384, batch_loss: 0.4093, loss: 0.3794 ||:  54%|#####3    | 35/65 [06:43<05:45, 11.52s/it]
2022-04-10 12:16:43,383 - INFO - tqdm - accuracy: 0.8377, batch_loss: 0.4353, loss: 0.3810 ||:  55%|#####5    | 36/65 [06:55<05:34, 11.53s/it]
2022-04-10 12:16:54,924 - INFO - tqdm - accuracy: 0.8395, batch_loss: 0.2451, loss: 0.3773 ||:  57%|#####6    | 37/65 [07:06<05:22, 11.53s/it]
2022-04-10 12:17:06,463 - INFO - tqdm - accuracy: 0.8363, batch_loss: 0.4957, loss: 0.3804 ||:  58%|#####8    | 38/65 [07:18<05:11, 11.53s/it]
2022-04-10 12:17:18,016 - INFO - tqdm - accuracy: 0.8341, batch_loss: 0.4595, loss: 0.3825 ||:  60%|######    | 39/65 [07:29<05:00, 11.54s/it]
2022-04-10 12:17:29,573 - INFO - tqdm - accuracy: 0.8336, batch_loss: 0.4130, loss: 0.3832 ||:  62%|######1   | 40/65 [07:41<04:48, 11.54s/it]
2022-04-10 12:17:41,121 - INFO - tqdm - accuracy: 0.8361, batch_loss: 0.2720, loss: 0.3805 ||:  63%|######3   | 41/65 [07:52<04:37, 11.55s/it]
2022-04-10 12:17:52,674 - INFO - tqdm - accuracy: 0.8371, batch_loss: 0.3675, loss: 0.3802 ||:  65%|######4   | 42/65 [08:04<04:25, 11.55s/it]
2022-04-10 12:18:04,240 - INFO - tqdm - accuracy: 0.8350, batch_loss: 0.4580, loss: 0.3820 ||:  66%|######6   | 43/65 [08:16<04:14, 11.55s/it]
2022-04-10 12:18:15,801 - INFO - tqdm - accuracy: 0.8338, batch_loss: 0.5513, loss: 0.3859 ||:  68%|######7   | 44/65 [08:27<04:02, 11.56s/it]
2022-04-10 12:18:27,367 - INFO - tqdm - accuracy: 0.8313, batch_loss: 0.5308, loss: 0.3891 ||:  69%|######9   | 45/65 [08:39<03:51, 11.56s/it]
2022-04-10 12:18:38,932 - INFO - tqdm - accuracy: 0.8295, batch_loss: 0.4351, loss: 0.3901 ||:  71%|#######   | 46/65 [08:50<03:39, 11.56s/it]
2022-04-10 12:18:50,495 - INFO - tqdm - accuracy: 0.8285, batch_loss: 0.4272, loss: 0.3909 ||:  72%|#######2  | 47/65 [09:02<03:28, 11.56s/it]
2022-04-10 12:19:02,060 - INFO - tqdm - accuracy: 0.8301, batch_loss: 0.3008, loss: 0.3890 ||:  74%|#######3  | 48/65 [09:13<03:16, 11.56s/it]
2022-04-10 12:19:13,600 - INFO - tqdm - accuracy: 0.8278, batch_loss: 0.4281, loss: 0.3898 ||:  75%|#######5  | 49/65 [09:25<03:04, 11.56s/it]
2022-04-10 12:19:25,163 - INFO - tqdm - accuracy: 0.8263, batch_loss: 0.5662, loss: 0.3933 ||:  77%|#######6  | 50/65 [09:37<02:53, 11.56s/it]
2022-04-10 12:19:36,749 - INFO - tqdm - accuracy: 0.8266, batch_loss: 0.5206, loss: 0.3958 ||:  78%|#######8  | 51/65 [09:48<02:41, 11.57s/it]
2022-04-10 12:19:48,038 - INFO - tqdm - accuracy: 0.8251, batch_loss: 0.5364, loss: 0.3985 ||:  80%|########  | 52/65 [09:59<02:29, 11.48s/it]
2022-04-10 12:19:59,593 - INFO - tqdm - accuracy: 0.8243, batch_loss: 0.4402, loss: 0.3993 ||:  82%|########1 | 53/65 [10:11<02:18, 11.50s/it]
2022-04-10 12:20:11,141 - INFO - tqdm - accuracy: 0.8235, batch_loss: 0.5082, loss: 0.4013 ||:  83%|########3 | 54/65 [10:23<02:06, 11.52s/it]
2022-04-10 12:20:22,687 - INFO - tqdm - accuracy: 0.8239, batch_loss: 0.4231, loss: 0.4017 ||:  85%|########4 | 55/65 [10:34<01:55, 11.53s/it]
2022-04-10 12:20:34,207 - INFO - tqdm - accuracy: 0.8231, batch_loss: 0.4638, loss: 0.4028 ||:  86%|########6 | 56/65 [10:46<01:43, 11.52s/it]
2022-04-10 12:20:45,728 - INFO - tqdm - accuracy: 0.8235, batch_loss: 0.3371, loss: 0.4017 ||:  88%|########7 | 57/65 [10:57<01:32, 11.52s/it]
2022-04-10 12:20:57,218 - INFO - tqdm - accuracy: 0.8249, batch_loss: 0.2996, loss: 0.3999 ||:  89%|########9 | 58/65 [11:09<01:20, 11.51s/it]
2022-04-10 12:21:08,364 - INFO - tqdm - accuracy: 0.8241, batch_loss: 0.4070, loss: 0.4000 ||:  91%|######### | 59/65 [11:20<01:08, 11.40s/it]
2022-04-10 12:21:19,874 - INFO - tqdm - accuracy: 0.8260, batch_loss: 0.2724, loss: 0.3979 ||:  92%|#########2| 60/65 [11:31<00:57, 11.44s/it]
2022-04-10 12:21:31,386 - INFO - tqdm - accuracy: 0.8247, batch_loss: 0.4079, loss: 0.3981 ||:  94%|#########3| 61/65 [11:43<00:45, 11.46s/it]
2022-04-10 12:21:42,888 - INFO - tqdm - accuracy: 0.8250, batch_loss: 0.3537, loss: 0.3974 ||:  95%|#########5| 62/65 [11:54<00:34, 11.47s/it]
2022-04-10 12:21:54,397 - INFO - tqdm - accuracy: 0.8243, batch_loss: 0.4363, loss: 0.3980 ||:  97%|#########6| 63/65 [12:06<00:22, 11.48s/it]
2022-04-10 12:22:05,891 - INFO - tqdm - accuracy: 0.8246, batch_loss: 0.3274, loss: 0.3969 ||:  98%|#########8| 64/65 [12:17<00:11, 11.49s/it]
2022-04-10 12:22:10,997 - INFO - tqdm - accuracy: 0.8239, batch_loss: 0.4945, loss: 0.3984 ||: 100%|##########| 65/65 [12:22<00:00,  9.57s/it]
2022-04-10 12:22:10,998 - INFO - tqdm - accuracy: 0.8239, batch_loss: 0.4945, loss: 0.3984 ||: 100%|##########| 65/65 [12:22<00:00, 11.43s/it]
2022-04-10 12:22:13,979 - INFO - allennlp.training.trainer - Validating
2022-04-10 12:22:13,980 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-10 12:22:24,085 - INFO - tqdm - accuracy: 0.6500, batch_loss: 0.0372, loss: 1.1723 ||:  35%|###4      | 40/115 [00:10<00:19,  3.93it/s]
2022-04-10 12:22:34,211 - INFO - tqdm - accuracy: 0.6438, batch_loss: 0.6733, loss: 1.0545 ||:  70%|######9   | 80/115 [00:20<00:09,  3.87it/s]
2022-04-10 12:22:43,187 - INFO - tqdm - accuracy: 0.6376, batch_loss: 0.3402, loss: 1.0260 ||: 100%|##########| 115/115 [00:29<00:00,  3.89it/s]
2022-04-10 12:22:43,187 - INFO - tqdm - accuracy: 0.6376, batch_loss: 0.3402, loss: 1.0260 ||: 100%|##########| 115/115 [00:29<00:00,  3.94it/s]
2022-04-10 12:22:43,187 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-10 12:22:43,188 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.824  |     0.638
2022-04-10 12:22:43,190 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-10 12:22:43,190 - INFO - allennlp.training.tensorboard_writer - loss               |     0.398  |     1.026
2022-04-10 12:22:43,191 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5797.879  |       N/A
2022-04-10 12:22:49,591 - INFO - allennlp.training.trainer - Epoch duration: 0:13:01.469174
2022-04-10 12:22:49,591 - INFO - allennlp.training.trainer - Estimated training time remaining: 2:24:04
2022-04-10 12:22:49,591 - INFO - allennlp.training.trainer - Epoch 4/14
2022-04-10 12:22:49,592 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-10 12:22:49,592 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-10 12:22:49,593 - INFO - allennlp.training.trainer - Training
2022-04-10 12:22:49,593 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-10 12:23:01,015 - INFO - tqdm - accuracy: 0.9375, batch_loss: 0.2312, loss: 0.2312 ||:   2%|1         | 1/65 [00:11<12:10, 11.42s/it]
2022-04-10 12:23:12,572 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.1009, loss: 0.1660 ||:   3%|3         | 2/65 [00:22<12:04, 11.50s/it]
2022-04-10 12:23:24,358 - INFO - tqdm - accuracy: 0.9375, batch_loss: 0.2919, loss: 0.2080 ||:   5%|4         | 3/65 [00:34<12:01, 11.63s/it]
2022-04-10 12:23:35,958 - INFO - tqdm - accuracy: 0.9141, batch_loss: 0.2875, loss: 0.2279 ||:   6%|6         | 4/65 [00:46<11:48, 11.62s/it]
2022-04-10 12:23:47,412 - INFO - tqdm - accuracy: 0.9125, batch_loss: 0.1727, loss: 0.2169 ||:   8%|7         | 5/65 [00:57<11:33, 11.56s/it]
2022-04-10 12:23:58,923 - INFO - tqdm - accuracy: 0.9219, batch_loss: 0.1284, loss: 0.2021 ||:   9%|9         | 6/65 [01:09<11:21, 11.54s/it]
2022-04-10 12:24:10,430 - INFO - tqdm - accuracy: 0.9241, batch_loss: 0.1260, loss: 0.1912 ||:  11%|#         | 7/65 [01:20<11:08, 11.53s/it]
2022-04-10 12:24:21,967 - INFO - tqdm - accuracy: 0.9141, batch_loss: 0.2040, loss: 0.1928 ||:  12%|#2        | 8/65 [01:32<10:57, 11.53s/it]
2022-04-10 12:24:33,489 - INFO - tqdm - accuracy: 0.9201, batch_loss: 0.1076, loss: 0.1834 ||:  14%|#3        | 9/65 [01:43<10:45, 11.53s/it]
2022-04-10 12:24:45,049 - INFO - tqdm - accuracy: 0.9125, batch_loss: 0.2227, loss: 0.1873 ||:  15%|#5        | 10/65 [01:55<10:34, 11.54s/it]
2022-04-10 12:24:56,611 - INFO - tqdm - accuracy: 0.9119, batch_loss: 0.1825, loss: 0.1869 ||:  17%|#6        | 11/65 [02:07<10:23, 11.55s/it]
2022-04-10 12:25:08,129 - INFO - tqdm - accuracy: 0.9062, batch_loss: 0.2429, loss: 0.1915 ||:  18%|#8        | 12/65 [02:18<10:11, 11.54s/it]
2022-04-10 12:25:19,707 - INFO - tqdm - accuracy: 0.9038, batch_loss: 0.2733, loss: 0.1978 ||:  20%|##        | 13/65 [02:30<10:00, 11.55s/it]
2022-04-10 12:25:31,299 - INFO - tqdm - accuracy: 0.9107, batch_loss: 0.0517, loss: 0.1874 ||:  22%|##1       | 14/65 [02:41<09:49, 11.56s/it]
2022-04-10 12:25:42,886 - INFO - tqdm - accuracy: 0.9125, batch_loss: 0.2067, loss: 0.1887 ||:  23%|##3       | 15/65 [02:53<09:38, 11.57s/it]
2022-04-10 12:25:54,494 - INFO - tqdm - accuracy: 0.9180, batch_loss: 0.0477, loss: 0.1799 ||:  25%|##4       | 16/65 [03:04<09:27, 11.58s/it]
2022-04-10 12:26:06,067 - INFO - tqdm - accuracy: 0.9173, batch_loss: 0.1754, loss: 0.1796 ||:  26%|##6       | 17/65 [03:16<09:15, 11.58s/it]
2022-04-10 12:26:17,637 - INFO - tqdm - accuracy: 0.9201, batch_loss: 0.0785, loss: 0.1740 ||:  28%|##7       | 18/65 [03:28<09:04, 11.58s/it]
2022-04-10 12:26:29,197 - INFO - tqdm - accuracy: 0.9211, batch_loss: 0.2003, loss: 0.1754 ||:  29%|##9       | 19/65 [03:39<08:52, 11.57s/it]
2022-04-10 12:26:40,735 - INFO - tqdm - accuracy: 0.9219, batch_loss: 0.1786, loss: 0.1755 ||:  31%|###       | 20/65 [03:51<08:40, 11.56s/it]
2022-04-10 12:26:52,280 - INFO - tqdm - accuracy: 0.9226, batch_loss: 0.1308, loss: 0.1734 ||:  32%|###2      | 21/65 [04:02<08:28, 11.56s/it]
2022-04-10 12:27:03,800 - INFO - tqdm - accuracy: 0.9219, batch_loss: 0.2238, loss: 0.1757 ||:  34%|###3      | 22/65 [04:14<08:16, 11.55s/it]
2022-04-10 12:27:15,318 - INFO - tqdm - accuracy: 0.9171, batch_loss: 0.5431, loss: 0.1917 ||:  35%|###5      | 23/65 [04:25<08:04, 11.54s/it]
2022-04-10 12:27:26,826 - INFO - tqdm - accuracy: 0.9180, batch_loss: 0.1678, loss: 0.1907 ||:  37%|###6      | 24/65 [04:37<07:52, 11.53s/it]
2022-04-10 12:27:38,336 - INFO - tqdm - accuracy: 0.9175, batch_loss: 0.2336, loss: 0.1924 ||:  38%|###8      | 25/65 [04:48<07:40, 11.52s/it]
2022-04-10 12:27:49,839 - INFO - tqdm - accuracy: 0.9159, batch_loss: 0.3305, loss: 0.1977 ||:  40%|####      | 26/65 [05:00<07:29, 11.52s/it]
2022-04-10 12:28:01,343 - INFO - tqdm - accuracy: 0.9155, batch_loss: 0.2268, loss: 0.1988 ||:  42%|####1     | 27/65 [05:11<07:17, 11.51s/it]
2022-04-10 12:28:12,828 - INFO - tqdm - accuracy: 0.9174, batch_loss: 0.1400, loss: 0.1967 ||:  43%|####3     | 28/65 [05:23<07:05, 11.50s/it]
2022-04-10 12:28:24,324 - INFO - tqdm - accuracy: 0.9192, batch_loss: 0.0899, loss: 0.1930 ||:  45%|####4     | 29/65 [05:34<06:54, 11.50s/it]
2022-04-10 12:28:35,679 - INFO - tqdm - accuracy: 0.9187, batch_loss: 0.1581, loss: 0.1918 ||:  46%|####6     | 30/65 [05:46<06:41, 11.46s/it]
2022-04-10 12:28:47,147 - INFO - tqdm - accuracy: 0.9183, batch_loss: 0.1528, loss: 0.1906 ||:  48%|####7     | 31/65 [05:57<06:29, 11.46s/it]
2022-04-10 12:28:58,637 - INFO - tqdm - accuracy: 0.9160, batch_loss: 0.2694, loss: 0.1930 ||:  49%|####9     | 32/65 [06:09<06:18, 11.47s/it]
2022-04-10 12:29:10,126 - INFO - tqdm - accuracy: 0.9176, batch_loss: 0.1582, loss: 0.1920 ||:  51%|#####     | 33/65 [06:20<06:07, 11.48s/it]
2022-04-10 12:29:21,611 - INFO - tqdm - accuracy: 0.9200, batch_loss: 0.0533, loss: 0.1879 ||:  52%|#####2    | 34/65 [06:32<05:55, 11.48s/it]
2022-04-10 12:29:33,114 - INFO - tqdm - accuracy: 0.9205, batch_loss: 0.2116, loss: 0.1886 ||:  54%|#####3    | 35/65 [06:43<05:44, 11.49s/it]
2022-04-10 12:29:44,657 - INFO - tqdm - accuracy: 0.9193, batch_loss: 0.3405, loss: 0.1928 ||:  55%|#####5    | 36/65 [06:55<05:33, 11.50s/it]
2022-04-10 12:29:55,863 - INFO - tqdm - accuracy: 0.9189, batch_loss: 0.2889, loss: 0.1954 ||:  57%|#####6    | 37/65 [07:06<05:19, 11.41s/it]
2022-04-10 12:30:07,424 - INFO - tqdm - accuracy: 0.9193, batch_loss: 0.1603, loss: 0.1945 ||:  58%|#####8    | 38/65 [07:17<05:09, 11.46s/it]
2022-04-10 12:30:18,962 - INFO - tqdm - accuracy: 0.9182, batch_loss: 0.1736, loss: 0.1939 ||:  60%|######    | 39/65 [07:29<04:58, 11.48s/it]
2022-04-10 12:30:30,928 - INFO - tqdm - accuracy: 0.9179, batch_loss: 0.3989, loss: 0.1991 ||:  62%|######1   | 40/65 [07:41<04:50, 11.63s/it]
2022-04-10 12:30:42,445 - INFO - tqdm - accuracy: 0.9191, batch_loss: 0.1182, loss: 0.1971 ||:  63%|######3   | 41/65 [07:52<04:38, 11.59s/it]
2022-04-10 12:30:54,024 - INFO - tqdm - accuracy: 0.9173, batch_loss: 0.2879, loss: 0.1993 ||:  65%|######4   | 42/65 [08:04<04:26, 11.59s/it]
2022-04-10 12:31:05,584 - INFO - tqdm - accuracy: 0.9185, batch_loss: 0.1384, loss: 0.1978 ||:  66%|######6   | 43/65 [08:15<04:14, 11.58s/it]
2022-04-10 12:31:17,153 - INFO - tqdm - accuracy: 0.9183, batch_loss: 0.1806, loss: 0.1974 ||:  68%|######7   | 44/65 [08:27<04:03, 11.58s/it]
2022-04-10 12:31:28,564 - INFO - tqdm - accuracy: 0.9180, batch_loss: 0.2044, loss: 0.1976 ||:  69%|######9   | 45/65 [08:38<03:50, 11.53s/it]
2022-04-10 12:31:40,122 - INFO - tqdm - accuracy: 0.9177, batch_loss: 0.1625, loss: 0.1968 ||:  71%|#######   | 46/65 [08:50<03:39, 11.54s/it]
2022-04-10 12:31:51,665 - INFO - tqdm - accuracy: 0.9162, batch_loss: 0.2680, loss: 0.1984 ||:  72%|#######2  | 47/65 [09:02<03:27, 11.54s/it]
2022-04-10 12:32:03,201 - INFO - tqdm - accuracy: 0.9140, batch_loss: 0.5061, loss: 0.2048 ||:  74%|#######3  | 48/65 [09:13<03:16, 11.54s/it]
2022-04-10 12:32:14,744 - INFO - tqdm - accuracy: 0.9132, batch_loss: 0.1999, loss: 0.2047 ||:  75%|#######5  | 49/65 [09:25<03:04, 11.54s/it]
2022-04-10 12:32:26,288 - INFO - tqdm - accuracy: 0.9131, batch_loss: 0.1759, loss: 0.2041 ||:  77%|#######6  | 50/65 [09:36<02:53, 11.54s/it]
2022-04-10 12:32:37,827 - INFO - tqdm - accuracy: 0.9142, batch_loss: 0.1548, loss: 0.2031 ||:  78%|#######8  | 51/65 [09:48<02:41, 11.54s/it]
2022-04-10 12:32:49,338 - INFO - tqdm - accuracy: 0.9128, batch_loss: 0.2398, loss: 0.2038 ||:  80%|########  | 52/65 [09:59<02:29, 11.53s/it]
2022-04-10 12:33:00,871 - INFO - tqdm - accuracy: 0.9127, batch_loss: 0.2376, loss: 0.2045 ||:  82%|########1 | 53/65 [10:11<02:18, 11.53s/it]
2022-04-10 12:33:12,399 - INFO - tqdm - accuracy: 0.9126, batch_loss: 0.3386, loss: 0.2069 ||:  83%|########3 | 54/65 [10:22<02:06, 11.53s/it]
2022-04-10 12:33:23,919 - INFO - tqdm - accuracy: 0.9113, batch_loss: 0.4445, loss: 0.2113 ||:  85%|########4 | 55/65 [10:34<01:55, 11.53s/it]
2022-04-10 12:33:35,453 - INFO - tqdm - accuracy: 0.9112, batch_loss: 0.2610, loss: 0.2122 ||:  86%|########6 | 56/65 [10:45<01:43, 11.53s/it]
2022-04-10 12:33:46,986 - INFO - tqdm - accuracy: 0.9100, batch_loss: 0.3489, loss: 0.2146 ||:  88%|########7 | 57/65 [10:57<01:32, 11.53s/it]
2022-04-10 12:33:58,541 - INFO - tqdm - accuracy: 0.9111, batch_loss: 0.1437, loss: 0.2133 ||:  89%|########9 | 58/65 [11:08<01:20, 11.54s/it]
2022-04-10 12:34:10,069 - INFO - tqdm - accuracy: 0.9115, batch_loss: 0.1754, loss: 0.2127 ||:  91%|######### | 59/65 [11:20<01:09, 11.53s/it]
2022-04-10 12:34:21,578 - INFO - tqdm - accuracy: 0.9119, batch_loss: 0.1603, loss: 0.2118 ||:  92%|#########2| 60/65 [11:31<00:57, 11.53s/it]
2022-04-10 12:34:33,103 - INFO - tqdm - accuracy: 0.9118, batch_loss: 0.2192, loss: 0.2119 ||:  94%|#########3| 61/65 [11:43<00:46, 11.53s/it]
2022-04-10 12:34:44,389 - INFO - tqdm - accuracy: 0.9128, batch_loss: 0.1372, loss: 0.2107 ||:  95%|#########5| 62/65 [11:54<00:34, 11.45s/it]
2022-04-10 12:34:55,908 - INFO - tqdm - accuracy: 0.9132, batch_loss: 0.1734, loss: 0.2101 ||:  97%|#########6| 63/65 [12:06<00:22, 11.47s/it]
2022-04-10 12:35:07,411 - INFO - tqdm - accuracy: 0.9130, batch_loss: 0.2060, loss: 0.2101 ||:  98%|#########8| 64/65 [12:17<00:11, 11.48s/it]
2022-04-10 12:35:12,531 - INFO - tqdm - accuracy: 0.9131, batch_loss: 0.2685, loss: 0.2110 ||: 100%|##########| 65/65 [12:22<00:00,  9.57s/it]
2022-04-10 12:35:12,531 - INFO - tqdm - accuracy: 0.9131, batch_loss: 0.2685, loss: 0.2110 ||: 100%|##########| 65/65 [12:22<00:00, 11.43s/it]
2022-04-10 12:35:15,482 - INFO - allennlp.training.trainer - Validating
2022-04-10 12:35:15,484 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-10 12:35:25,625 - INFO - tqdm - accuracy: 0.6500, batch_loss: 0.0027, loss: 1.3649 ||:  35%|###4      | 40/115 [00:10<00:19,  3.93it/s]
2022-04-10 12:35:35,843 - INFO - tqdm - accuracy: 0.6062, batch_loss: 0.0025, loss: 1.3043 ||:  70%|######9   | 80/115 [00:20<00:08,  3.90it/s]
2022-04-10 12:35:44,616 - INFO - tqdm - accuracy: 0.6157, batch_loss: 0.0086, loss: 1.2490 ||: 100%|##########| 115/115 [00:29<00:00,  3.92it/s]
2022-04-10 12:35:44,616 - INFO - tqdm - accuracy: 0.6157, batch_loss: 0.0086, loss: 1.2490 ||: 100%|##########| 115/115 [00:29<00:00,  3.95it/s]
2022-04-10 12:35:44,616 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-10 12:35:44,617 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.913  |     0.616
2022-04-10 12:35:44,618 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-10 12:35:44,618 - INFO - allennlp.training.tensorboard_writer - loss               |     0.211  |     1.249
2022-04-10 12:35:44,619 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5797.879  |       N/A
2022-04-10 12:35:50,781 - INFO - allennlp.training.trainer - Epoch duration: 0:13:01.189995
2022-04-10 12:35:50,782 - INFO - allennlp.training.trainer - Estimated training time remaining: 2:10:49
2022-04-10 12:35:50,782 - INFO - allennlp.training.trainer - Epoch 5/14
2022-04-10 12:35:50,782 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-10 12:35:50,782 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-10 12:35:50,784 - INFO - allennlp.training.trainer - Training
2022-04-10 12:35:50,784 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-10 12:36:02,048 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.1577, loss: 0.1577 ||:   2%|1         | 1/65 [00:11<12:00, 11.26s/it]
2022-04-10 12:36:13,674 - INFO - tqdm - accuracy: 0.9375, batch_loss: 0.1598, loss: 0.1587 ||:   3%|3         | 2/65 [00:22<12:03, 11.48s/it]
2022-04-10 12:36:25,545 - INFO - tqdm - accuracy: 0.9479, batch_loss: 0.1120, loss: 0.1432 ||:   5%|4         | 3/65 [00:34<12:02, 11.66s/it]
2022-04-10 12:36:37,131 - INFO - tqdm - accuracy: 0.9531, batch_loss: 0.1580, loss: 0.1469 ||:   6%|6         | 4/65 [00:46<11:49, 11.63s/it]
2022-04-10 12:36:48,565 - INFO - tqdm - accuracy: 0.9500, batch_loss: 0.1129, loss: 0.1401 ||:   8%|7         | 5/65 [00:57<11:33, 11.56s/it]
2022-04-10 12:36:59,992 - INFO - tqdm - accuracy: 0.9583, batch_loss: 0.0566, loss: 0.1262 ||:   9%|9         | 6/65 [01:09<11:19, 11.51s/it]
2022-04-10 12:37:11,362 - INFO - tqdm - accuracy: 0.9598, batch_loss: 0.0974, loss: 0.1221 ||:  11%|#         | 7/65 [01:20<11:05, 11.47s/it]
2022-04-10 12:37:22,925 - INFO - tqdm - accuracy: 0.9570, batch_loss: 0.1991, loss: 0.1317 ||:  12%|#2        | 8/65 [01:32<10:55, 11.50s/it]
2022-04-10 12:37:34,529 - INFO - tqdm - accuracy: 0.9583, batch_loss: 0.1591, loss: 0.1347 ||:  14%|#3        | 9/65 [01:43<10:45, 11.53s/it]
2022-04-10 12:37:46,098 - INFO - tqdm - accuracy: 0.9625, batch_loss: 0.0365, loss: 0.1249 ||:  15%|#5        | 10/65 [01:55<10:34, 11.54s/it]
2022-04-10 12:37:57,641 - INFO - tqdm - accuracy: 0.9602, batch_loss: 0.2149, loss: 0.1331 ||:  17%|#6        | 11/65 [02:06<10:23, 11.54s/it]
2022-04-10 12:38:09,130 - INFO - tqdm - accuracy: 0.9609, batch_loss: 0.0977, loss: 0.1301 ||:  18%|#8        | 12/65 [02:18<10:10, 11.53s/it]
2022-04-10 12:38:20,607 - INFO - tqdm - accuracy: 0.9591, batch_loss: 0.1125, loss: 0.1288 ||:  20%|##        | 13/65 [02:29<09:58, 11.51s/it]
2022-04-10 12:38:32,079 - INFO - tqdm - accuracy: 0.9576, batch_loss: 0.1449, loss: 0.1299 ||:  22%|##1       | 14/65 [02:41<09:46, 11.50s/it]
2022-04-10 12:38:43,558 - INFO - tqdm - accuracy: 0.9604, batch_loss: 0.0409, loss: 0.1240 ||:  23%|##3       | 15/65 [02:52<09:34, 11.49s/it]
2022-04-10 12:38:54,673 - INFO - tqdm - accuracy: 0.9628, batch_loss: 0.0558, loss: 0.1197 ||:  25%|##4       | 16/65 [03:03<09:17, 11.38s/it]
2022-04-10 12:39:06,177 - INFO - tqdm - accuracy: 0.9632, batch_loss: 0.1157, loss: 0.1195 ||:  26%|##6       | 17/65 [03:15<09:08, 11.42s/it]
2022-04-10 12:39:17,655 - INFO - tqdm - accuracy: 0.9617, batch_loss: 0.1302, loss: 0.1201 ||:  28%|##7       | 18/65 [03:26<08:57, 11.44s/it]
2022-04-10 12:39:29,146 - INFO - tqdm - accuracy: 0.9638, batch_loss: 0.0425, loss: 0.1160 ||:  29%|##9       | 19/65 [03:38<08:46, 11.45s/it]
2022-04-10 12:39:40,641 - INFO - tqdm - accuracy: 0.9640, batch_loss: 0.1083, loss: 0.1156 ||:  31%|###       | 20/65 [03:49<08:35, 11.47s/it]
2022-04-10 12:39:52,136 - INFO - tqdm - accuracy: 0.9613, batch_loss: 0.2126, loss: 0.1203 ||:  32%|###2      | 21/65 [04:01<08:24, 11.47s/it]
2022-04-10 12:40:03,633 - INFO - tqdm - accuracy: 0.9630, batch_loss: 0.0338, loss: 0.1163 ||:  34%|###3      | 22/65 [04:12<08:13, 11.48s/it]
2022-04-10 12:40:15,157 - INFO - tqdm - accuracy: 0.9633, batch_loss: 0.0423, loss: 0.1131 ||:  35%|###5      | 23/65 [04:24<08:02, 11.49s/it]
2022-04-10 12:40:26,686 - INFO - tqdm - accuracy: 0.9635, batch_loss: 0.0978, loss: 0.1125 ||:  37%|###6      | 24/65 [04:35<07:51, 11.50s/it]
2022-04-10 12:40:38,216 - INFO - tqdm - accuracy: 0.9637, batch_loss: 0.0609, loss: 0.1104 ||:  38%|###8      | 25/65 [04:47<07:40, 11.51s/it]
2022-04-10 12:40:49,749 - INFO - tqdm - accuracy: 0.9639, batch_loss: 0.0533, loss: 0.1082 ||:  40%|####      | 26/65 [04:58<07:29, 11.52s/it]
2022-04-10 12:41:01,277 - INFO - tqdm - accuracy: 0.9641, batch_loss: 0.1135, loss: 0.1084 ||:  42%|####1     | 27/65 [05:10<07:17, 11.52s/it]
2022-04-10 12:41:12,829 - INFO - tqdm - accuracy: 0.9654, batch_loss: 0.0152, loss: 0.1051 ||:  43%|####3     | 28/65 [05:22<07:06, 11.53s/it]
2022-04-10 12:41:24,386 - INFO - tqdm - accuracy: 0.9655, batch_loss: 0.0769, loss: 0.1041 ||:  45%|####4     | 29/65 [05:33<06:55, 11.54s/it]
2022-04-10 12:41:35,919 - INFO - tqdm - accuracy: 0.9635, batch_loss: 0.2803, loss: 0.1100 ||:  46%|####6     | 30/65 [05:45<06:43, 11.54s/it]
2022-04-10 12:41:47,462 - INFO - tqdm - accuracy: 0.9647, batch_loss: 0.0192, loss: 0.1071 ||:  48%|####7     | 31/65 [05:56<06:32, 11.54s/it]
2022-04-10 12:41:59,010 - INFO - tqdm - accuracy: 0.9629, batch_loss: 0.1993, loss: 0.1099 ||:  49%|####9     | 32/65 [06:08<06:20, 11.54s/it]
2022-04-10 12:42:10,544 - INFO - tqdm - accuracy: 0.9621, batch_loss: 0.2092, loss: 0.1129 ||:  51%|#####     | 33/65 [06:19<06:09, 11.54s/it]
2022-04-10 12:42:22,084 - INFO - tqdm - accuracy: 0.9614, batch_loss: 0.1909, loss: 0.1152 ||:  52%|#####2    | 34/65 [06:31<05:57, 11.54s/it]
2022-04-10 12:42:33,617 - INFO - tqdm - accuracy: 0.9607, batch_loss: 0.0811, loss: 0.1143 ||:  54%|#####3    | 35/65 [06:42<05:46, 11.54s/it]
2022-04-10 12:42:45,138 - INFO - tqdm - accuracy: 0.9618, batch_loss: 0.0215, loss: 0.1117 ||:  55%|#####5    | 36/65 [06:54<05:34, 11.53s/it]
2022-04-10 12:42:56,681 - INFO - tqdm - accuracy: 0.9620, batch_loss: 0.0639, loss: 0.1104 ||:  57%|#####6    | 37/65 [07:05<05:22, 11.54s/it]
2022-04-10 12:43:08,216 - INFO - tqdm - accuracy: 0.9621, batch_loss: 0.1263, loss: 0.1108 ||:  58%|#####8    | 38/65 [07:17<05:11, 11.54s/it]
2022-04-10 12:43:19,736 - INFO - tqdm - accuracy: 0.9623, batch_loss: 0.0893, loss: 0.1103 ||:  60%|######    | 39/65 [07:28<04:59, 11.53s/it]
2022-04-10 12:43:31,267 - INFO - tqdm - accuracy: 0.9625, batch_loss: 0.0558, loss: 0.1089 ||:  62%|######1   | 40/65 [07:40<04:48, 11.53s/it]
2022-04-10 12:43:42,810 - INFO - tqdm - accuracy: 0.9626, batch_loss: 0.0514, loss: 0.1075 ||:  63%|######3   | 41/65 [07:52<04:36, 11.53s/it]
2022-04-10 12:43:54,351 - INFO - tqdm - accuracy: 0.9620, batch_loss: 0.0849, loss: 0.1070 ||:  65%|######4   | 42/65 [08:03<04:25, 11.54s/it]
2022-04-10 12:44:05,875 - INFO - tqdm - accuracy: 0.9615, batch_loss: 0.2210, loss: 0.1096 ||:  66%|######6   | 43/65 [08:15<04:13, 11.53s/it]
2022-04-10 12:44:17,385 - INFO - tqdm - accuracy: 0.9595, batch_loss: 0.2093, loss: 0.1119 ||:  68%|######7   | 44/65 [08:26<04:02, 11.53s/it]
2022-04-10 12:44:28,652 - INFO - tqdm - accuracy: 0.9597, batch_loss: 0.0601, loss: 0.1107 ||:  69%|######9   | 45/65 [08:37<03:48, 11.45s/it]
2022-04-10 12:44:40,166 - INFO - tqdm - accuracy: 0.9592, batch_loss: 0.1323, loss: 0.1112 ||:  71%|#######   | 46/65 [08:49<03:37, 11.47s/it]
2022-04-10 12:44:51,697 - INFO - tqdm - accuracy: 0.9594, batch_loss: 0.1118, loss: 0.1112 ||:  72%|#######2  | 47/65 [09:00<03:26, 11.49s/it]
2022-04-10 12:45:03,196 - INFO - tqdm - accuracy: 0.9603, batch_loss: 0.0293, loss: 0.1095 ||:  74%|#######3  | 48/65 [09:12<03:15, 11.49s/it]
2022-04-10 12:45:14,725 - INFO - tqdm - accuracy: 0.9598, batch_loss: 0.1823, loss: 0.1110 ||:  75%|#######5  | 49/65 [09:23<03:04, 11.50s/it]
2022-04-10 12:45:26,229 - INFO - tqdm - accuracy: 0.9606, batch_loss: 0.0186, loss: 0.1091 ||:  77%|#######6  | 50/65 [09:35<02:52, 11.50s/it]
2022-04-10 12:45:37,756 - INFO - tqdm - accuracy: 0.9608, batch_loss: 0.0706, loss: 0.1084 ||:  78%|#######8  | 51/65 [09:46<02:41, 11.51s/it]
2022-04-10 12:45:49,291 - INFO - tqdm - accuracy: 0.9603, batch_loss: 0.0971, loss: 0.1082 ||:  80%|########  | 52/65 [09:58<02:29, 11.52s/it]
2022-04-10 12:46:00,814 - INFO - tqdm - accuracy: 0.9605, batch_loss: 0.0697, loss: 0.1074 ||:  82%|########1 | 53/65 [10:10<02:18, 11.52s/it]
2022-04-10 12:46:12,363 - INFO - tqdm - accuracy: 0.9600, batch_loss: 0.1298, loss: 0.1079 ||:  83%|########3 | 54/65 [10:21<02:06, 11.53s/it]
2022-04-10 12:46:23,900 - INFO - tqdm - accuracy: 0.9585, batch_loss: 0.4733, loss: 0.1145 ||:  85%|########4 | 55/65 [10:33<01:55, 11.53s/it]
2022-04-10 12:46:35,423 - INFO - tqdm - accuracy: 0.9581, batch_loss: 0.1864, loss: 0.1158 ||:  86%|########6 | 56/65 [10:44<01:43, 11.53s/it]
2022-04-10 12:46:46,932 - INFO - tqdm - accuracy: 0.9583, batch_loss: 0.0495, loss: 0.1146 ||:  88%|########7 | 57/65 [10:56<01:32, 11.52s/it]
2022-04-10 12:46:58,453 - INFO - tqdm - accuracy: 0.9580, batch_loss: 0.1944, loss: 0.1160 ||:  89%|########9 | 58/65 [11:07<01:20, 11.52s/it]
2022-04-10 12:47:09,857 - INFO - tqdm - accuracy: 0.9576, batch_loss: 0.2246, loss: 0.1178 ||:  91%|######### | 59/65 [11:19<01:08, 11.49s/it]
2022-04-10 12:47:21,368 - INFO - tqdm - accuracy: 0.9567, batch_loss: 0.1638, loss: 0.1186 ||:  92%|#########2| 60/65 [11:30<00:57, 11.49s/it]
2022-04-10 12:47:32,904 - INFO - tqdm - accuracy: 0.9569, batch_loss: 0.0373, loss: 0.1173 ||:  94%|#########3| 61/65 [11:42<00:46, 11.51s/it]
2022-04-10 12:47:44,411 - INFO - tqdm - accuracy: 0.9566, batch_loss: 0.0900, loss: 0.1168 ||:  95%|#########5| 62/65 [11:53<00:34, 11.51s/it]
2022-04-10 12:47:55,929 - INFO - tqdm - accuracy: 0.9563, batch_loss: 0.1965, loss: 0.1181 ||:  97%|#########6| 63/65 [12:05<00:23, 11.51s/it]
2022-04-10 12:48:07,444 - INFO - tqdm - accuracy: 0.9570, batch_loss: 0.0550, loss: 0.1171 ||:  98%|#########8| 64/65 [12:16<00:11, 11.51s/it]
2022-04-10 12:48:12,564 - INFO - tqdm - accuracy: 0.9568, batch_loss: 0.1186, loss: 0.1171 ||: 100%|##########| 65/65 [12:21<00:00,  9.59s/it]
2022-04-10 12:48:12,564 - INFO - tqdm - accuracy: 0.9568, batch_loss: 0.1186, loss: 0.1171 ||: 100%|##########| 65/65 [12:21<00:00, 11.41s/it]
2022-04-10 12:48:15,531 - INFO - allennlp.training.trainer - Validating
2022-04-10 12:48:15,533 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-10 12:48:25,656 - INFO - tqdm - accuracy: 0.5625, batch_loss: 5.3929, loss: 1.7265 ||:  35%|###4      | 40/115 [00:10<00:19,  3.94it/s]
2022-04-10 12:48:35,901 - INFO - tqdm - accuracy: 0.6211, batch_loss: 0.4438, loss: 1.3867 ||:  70%|#######   | 81/115 [00:20<00:08,  4.05it/s]
2022-04-10 12:48:44,646 - INFO - tqdm - accuracy: 0.6114, batch_loss: 0.0088, loss: 1.4766 ||: 100%|##########| 115/115 [00:29<00:00,  3.90it/s]
2022-04-10 12:48:44,646 - INFO - tqdm - accuracy: 0.6114, batch_loss: 0.0088, loss: 1.4766 ||: 100%|##########| 115/115 [00:29<00:00,  3.95it/s]
2022-04-10 12:48:44,646 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-10 12:48:44,647 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.957  |     0.611
2022-04-10 12:48:44,648 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-10 12:48:44,648 - INFO - allennlp.training.tensorboard_writer - loss               |     0.117  |     1.477
2022-04-10 12:48:44,648 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5797.879  |       N/A
2022-04-10 12:48:50,932 - INFO - allennlp.training.trainer - Epoch duration: 0:13:00.149580
2022-04-10 12:48:50,932 - INFO - allennlp.training.trainer - Estimated training time remaining: 1:57:37
2022-04-10 12:48:50,932 - INFO - allennlp.training.trainer - Epoch 6/14
2022-04-10 12:48:50,932 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-10 12:48:50,932 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-10 12:48:50,934 - INFO - allennlp.training.trainer - Training
2022-04-10 12:48:50,934 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-10 12:49:02,358 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.0787, loss: 0.0787 ||:   2%|1         | 1/65 [00:11<12:11, 11.42s/it]
2022-04-10 12:49:14,009 - INFO - tqdm - accuracy: 0.9375, batch_loss: 0.2055, loss: 0.1421 ||:   3%|3         | 2/65 [00:23<12:08, 11.56s/it]
2022-04-10 12:49:25,875 - INFO - tqdm - accuracy: 0.9479, batch_loss: 0.0961, loss: 0.1267 ||:   5%|4         | 3/65 [00:34<12:05, 11.70s/it]
2022-04-10 12:49:37,456 - INFO - tqdm - accuracy: 0.9609, batch_loss: 0.0553, loss: 0.1089 ||:   6%|6         | 4/65 [00:46<11:50, 11.65s/it]
2022-04-10 12:49:48,887 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.0233, loss: 0.0918 ||:   8%|7         | 5/65 [00:57<11:34, 11.57s/it]
2022-04-10 12:50:00,329 - INFO - tqdm - accuracy: 0.9740, batch_loss: 0.0720, loss: 0.0885 ||:   9%|9         | 6/65 [01:09<11:20, 11.53s/it]
2022-04-10 12:50:11,862 - INFO - tqdm - accuracy: 0.9777, batch_loss: 0.0612, loss: 0.0846 ||:  11%|#         | 7/65 [01:20<11:08, 11.53s/it]
2022-04-10 12:50:23,411 - INFO - tqdm - accuracy: 0.9805, batch_loss: 0.0306, loss: 0.0778 ||:  12%|#2        | 8/65 [01:32<10:57, 11.54s/it]
2022-04-10 12:50:35,018 - INFO - tqdm - accuracy: 0.9826, batch_loss: 0.0286, loss: 0.0724 ||:  14%|#3        | 9/65 [01:44<10:47, 11.56s/it]
2022-04-10 12:50:46,987 - INFO - tqdm - accuracy: 0.9781, batch_loss: 0.1016, loss: 0.0753 ||:  15%|#5        | 10/65 [01:56<10:42, 11.68s/it]
2022-04-10 12:50:58,478 - INFO - tqdm - accuracy: 0.9801, batch_loss: 0.0273, loss: 0.0709 ||:  17%|#6        | 11/65 [02:07<10:27, 11.63s/it]
2022-04-10 12:51:09,990 - INFO - tqdm - accuracy: 0.9792, batch_loss: 0.1562, loss: 0.0780 ||:  18%|#8        | 12/65 [02:19<10:14, 11.59s/it]
2022-04-10 12:51:21,496 - INFO - tqdm - accuracy: 0.9808, batch_loss: 0.0271, loss: 0.0741 ||:  20%|##        | 13/65 [02:30<10:01, 11.57s/it]
2022-04-10 12:51:32,970 - INFO - tqdm - accuracy: 0.9821, batch_loss: 0.0157, loss: 0.0699 ||:  22%|##1       | 14/65 [02:42<09:48, 11.54s/it]
2022-04-10 12:51:44,513 - INFO - tqdm - accuracy: 0.9833, batch_loss: 0.0034, loss: 0.0655 ||:  23%|##3       | 15/65 [02:53<09:36, 11.54s/it]
2022-04-10 12:51:56,072 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0256, loss: 0.0630 ||:  25%|##4       | 16/65 [03:05<09:25, 11.55s/it]
2022-04-10 12:52:07,649 - INFO - tqdm - accuracy: 0.9853, batch_loss: 0.0104, loss: 0.0599 ||:  26%|##6       | 17/65 [03:16<09:14, 11.55s/it]
2022-04-10 12:52:19,250 - INFO - tqdm - accuracy: 0.9861, batch_loss: 0.0396, loss: 0.0588 ||:  28%|##7       | 18/65 [03:28<09:03, 11.57s/it]
2022-04-10 12:52:30,833 - INFO - tqdm - accuracy: 0.9852, batch_loss: 0.0795, loss: 0.0599 ||:  29%|##9       | 19/65 [03:39<08:52, 11.57s/it]
2022-04-10 12:52:42,407 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0781, loss: 0.0608 ||:  31%|###       | 20/65 [03:51<08:40, 11.57s/it]
2022-04-10 12:52:53,960 - INFO - tqdm - accuracy: 0.9821, batch_loss: 0.1408, loss: 0.0646 ||:  32%|###2      | 21/65 [04:03<08:28, 11.57s/it]
2022-04-10 12:53:05,511 - INFO - tqdm - accuracy: 0.9830, batch_loss: 0.0087, loss: 0.0621 ||:  34%|###3      | 22/65 [04:14<08:17, 11.56s/it]
2022-04-10 12:53:17,076 - INFO - tqdm - accuracy: 0.9837, batch_loss: 0.0279, loss: 0.0606 ||:  35%|###5      | 23/65 [04:26<08:05, 11.56s/it]
2022-04-10 12:53:28,611 - INFO - tqdm - accuracy: 0.9831, batch_loss: 0.0674, loss: 0.0609 ||:  37%|###6      | 24/65 [04:37<07:53, 11.55s/it]
2022-04-10 12:53:40,131 - INFO - tqdm - accuracy: 0.9825, batch_loss: 0.0602, loss: 0.0608 ||:  38%|###8      | 25/65 [04:49<07:41, 11.54s/it]
2022-04-10 12:53:51,649 - INFO - tqdm - accuracy: 0.9820, batch_loss: 0.2005, loss: 0.0662 ||:  40%|####      | 26/65 [05:00<07:29, 11.54s/it]
2022-04-10 12:54:03,188 - INFO - tqdm - accuracy: 0.9826, batch_loss: 0.0159, loss: 0.0643 ||:  42%|####1     | 27/65 [05:12<07:18, 11.54s/it]
2022-04-10 12:54:14,729 - INFO - tqdm - accuracy: 0.9833, batch_loss: 0.0132, loss: 0.0625 ||:  43%|####3     | 28/65 [05:23<07:06, 11.54s/it]
2022-04-10 12:54:26,239 - INFO - tqdm - accuracy: 0.9838, batch_loss: 0.0017, loss: 0.0604 ||:  45%|####4     | 29/65 [05:35<06:55, 11.53s/it]
2022-04-10 12:54:37,624 - INFO - tqdm - accuracy: 0.9833, batch_loss: 0.0695, loss: 0.0607 ||:  46%|####6     | 30/65 [05:46<06:42, 11.49s/it]
2022-04-10 12:54:49,151 - INFO - tqdm - accuracy: 0.9839, batch_loss: 0.0055, loss: 0.0589 ||:  48%|####7     | 31/65 [05:58<06:30, 11.50s/it]
2022-04-10 12:55:00,668 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0072, loss: 0.0573 ||:  49%|####9     | 32/65 [06:09<06:19, 11.50s/it]
2022-04-10 12:55:12,177 - INFO - tqdm - accuracy: 0.9839, batch_loss: 0.1750, loss: 0.0609 ||:  51%|#####     | 33/65 [06:21<06:08, 11.51s/it]
2022-04-10 12:55:23,683 - INFO - tqdm - accuracy: 0.9835, batch_loss: 0.0452, loss: 0.0604 ||:  52%|#####2    | 34/65 [06:32<05:56, 11.51s/it]
2022-04-10 12:55:35,188 - INFO - tqdm - accuracy: 0.9830, batch_loss: 0.0560, loss: 0.0603 ||:  54%|#####3    | 35/65 [06:44<05:45, 11.51s/it]
2022-04-10 12:55:46,713 - INFO - tqdm - accuracy: 0.9835, batch_loss: 0.0034, loss: 0.0587 ||:  55%|#####5    | 36/65 [06:55<05:33, 11.51s/it]
2022-04-10 12:55:58,227 - INFO - tqdm - accuracy: 0.9831, batch_loss: 0.0604, loss: 0.0588 ||:  57%|#####6    | 37/65 [07:07<05:22, 11.51s/it]
2022-04-10 12:56:09,725 - INFO - tqdm - accuracy: 0.9836, batch_loss: 0.0239, loss: 0.0578 ||:  58%|#####8    | 38/65 [07:18<05:10, 11.51s/it]
2022-04-10 12:56:21,232 - INFO - tqdm - accuracy: 0.9832, batch_loss: 0.0635, loss: 0.0580 ||:  60%|######    | 39/65 [07:30<04:59, 11.51s/it]
2022-04-10 12:56:32,754 - INFO - tqdm - accuracy: 0.9836, batch_loss: 0.0583, loss: 0.0580 ||:  62%|######1   | 40/65 [07:41<04:47, 11.51s/it]
2022-04-10 12:56:43,919 - INFO - tqdm - accuracy: 0.9840, batch_loss: 0.0073, loss: 0.0568 ||:  63%|######3   | 41/65 [07:52<04:33, 11.41s/it]
2022-04-10 12:56:55,395 - INFO - tqdm - accuracy: 0.9836, batch_loss: 0.0745, loss: 0.0572 ||:  65%|######4   | 42/65 [08:04<04:22, 11.43s/it]
2022-04-10 12:57:06,639 - INFO - tqdm - accuracy: 0.9818, batch_loss: 0.1663, loss: 0.0597 ||:  66%|######6   | 43/65 [08:15<04:10, 11.37s/it]
2022-04-10 12:57:18,155 - INFO - tqdm - accuracy: 0.9822, batch_loss: 0.0175, loss: 0.0588 ||:  68%|######7   | 44/65 [08:27<03:59, 11.42s/it]
2022-04-10 12:57:29,678 - INFO - tqdm - accuracy: 0.9826, batch_loss: 0.0073, loss: 0.0576 ||:  69%|######9   | 45/65 [08:38<03:48, 11.45s/it]
2022-04-10 12:57:41,188 - INFO - tqdm - accuracy: 0.9823, batch_loss: 0.0913, loss: 0.0584 ||:  71%|#######   | 46/65 [08:50<03:37, 11.47s/it]
2022-04-10 12:57:52,679 - INFO - tqdm - accuracy: 0.9827, batch_loss: 0.0129, loss: 0.0574 ||:  72%|#######2  | 47/65 [09:01<03:26, 11.47s/it]
2022-04-10 12:58:04,190 - INFO - tqdm - accuracy: 0.9831, batch_loss: 0.0048, loss: 0.0563 ||:  74%|#######3  | 48/65 [09:13<03:15, 11.49s/it]
2022-04-10 12:58:15,705 - INFO - tqdm - accuracy: 0.9828, batch_loss: 0.1250, loss: 0.0577 ||:  75%|#######5  | 49/65 [09:24<03:03, 11.49s/it]
2022-04-10 12:58:27,212 - INFO - tqdm - accuracy: 0.9831, batch_loss: 0.0081, loss: 0.0567 ||:  77%|#######6  | 50/65 [09:36<02:52, 11.50s/it]
2022-04-10 12:58:38,744 - INFO - tqdm - accuracy: 0.9822, batch_loss: 0.2209, loss: 0.0599 ||:  78%|#######8  | 51/65 [09:47<02:41, 11.51s/it]
2022-04-10 12:58:50,149 - INFO - tqdm - accuracy: 0.9826, batch_loss: 0.0027, loss: 0.0588 ||:  80%|########  | 52/65 [09:59<02:29, 11.48s/it]
2022-04-10 12:59:01,700 - INFO - tqdm - accuracy: 0.9823, batch_loss: 0.1629, loss: 0.0608 ||:  82%|########1 | 53/65 [10:10<02:17, 11.50s/it]
2022-04-10 12:59:13,221 - INFO - tqdm - accuracy: 0.9820, batch_loss: 0.0288, loss: 0.0602 ||:  83%|########3 | 54/65 [10:22<02:06, 11.51s/it]
2022-04-10 12:59:24,747 - INFO - tqdm - accuracy: 0.9818, batch_loss: 0.0607, loss: 0.0602 ||:  85%|########4 | 55/65 [10:33<01:55, 11.51s/it]
2022-04-10 12:59:36,165 - INFO - tqdm - accuracy: 0.9821, batch_loss: 0.0043, loss: 0.0592 ||:  86%|########6 | 56/65 [10:45<01:43, 11.48s/it]
2022-04-10 12:59:47,704 - INFO - tqdm - accuracy: 0.9813, batch_loss: 0.1419, loss: 0.0607 ||:  88%|########7 | 57/65 [10:56<01:32, 11.50s/it]
2022-04-10 12:59:59,256 - INFO - tqdm - accuracy: 0.9801, batch_loss: 0.2024, loss: 0.0631 ||:  89%|########9 | 58/65 [11:08<01:20, 11.52s/it]
2022-04-10 13:00:10,787 - INFO - tqdm - accuracy: 0.9804, batch_loss: 0.0147, loss: 0.0623 ||:  91%|######### | 59/65 [11:19<01:09, 11.52s/it]
2022-04-10 13:00:22,317 - INFO - tqdm - accuracy: 0.9802, batch_loss: 0.1081, loss: 0.0630 ||:  92%|#########2| 60/65 [11:31<00:57, 11.52s/it]
2022-04-10 13:00:33,850 - INFO - tqdm - accuracy: 0.9800, batch_loss: 0.0591, loss: 0.0630 ||:  94%|#########3| 61/65 [11:42<00:46, 11.53s/it]
2022-04-10 13:00:45,404 - INFO - tqdm - accuracy: 0.9798, batch_loss: 0.0972, loss: 0.0635 ||:  95%|#########5| 62/65 [11:54<00:34, 11.53s/it]
2022-04-10 13:00:56,930 - INFO - tqdm - accuracy: 0.9801, batch_loss: 0.0049, loss: 0.0626 ||:  97%|#########6| 63/65 [12:05<00:23, 11.53s/it]
2022-04-10 13:01:08,460 - INFO - tqdm - accuracy: 0.9800, batch_loss: 0.0547, loss: 0.0625 ||:  98%|#########8| 64/65 [12:17<00:11, 11.53s/it]
2022-04-10 13:01:13,578 - INFO - tqdm - accuracy: 0.9801, batch_loss: 0.0060, loss: 0.0616 ||: 100%|##########| 65/65 [12:22<00:00,  9.61s/it]
2022-04-10 13:01:13,578 - INFO - tqdm - accuracy: 0.9801, batch_loss: 0.0060, loss: 0.0616 ||: 100%|##########| 65/65 [12:22<00:00, 11.43s/it]
2022-04-10 13:01:16,555 - INFO - allennlp.training.trainer - Validating
2022-04-10 13:01:16,556 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-10 13:01:26,797 - INFO - tqdm - accuracy: 0.6296, batch_loss: 4.2834, loss: 1.5288 ||:  36%|###5      | 41/115 [00:10<00:18,  3.94it/s]
2022-04-10 13:01:36,932 - INFO - tqdm - accuracy: 0.6522, batch_loss: 0.5853, loss: 1.4813 ||:  70%|#######   | 81/115 [00:20<00:07,  4.31it/s]
2022-04-10 13:01:45,705 - INFO - tqdm - accuracy: 0.6376, batch_loss: 3.4962, loss: 1.5516 ||: 100%|##########| 115/115 [00:29<00:00,  3.88it/s]
2022-04-10 13:01:45,706 - INFO - tqdm - accuracy: 0.6376, batch_loss: 3.4962, loss: 1.5516 ||: 100%|##########| 115/115 [00:29<00:00,  3.95it/s]
2022-04-10 13:01:45,706 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-10 13:01:45,706 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.980  |     0.638
2022-04-10 13:01:45,707 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-10 13:01:45,708 - INFO - allennlp.training.tensorboard_writer - loss               |     0.062  |     1.552
2022-04-10 13:01:45,709 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5797.879  |       N/A
2022-04-10 13:01:51,852 - INFO - allennlp.training.trainer - Epoch duration: 0:13:00.919907
2022-04-10 13:01:51,852 - INFO - allennlp.training.trainer - Estimated training time remaining: 1:44:29
2022-04-10 13:01:51,852 - INFO - allennlp.training.trainer - Epoch 7/14
2022-04-10 13:01:51,852 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-10 13:01:51,853 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-10 13:01:51,854 - INFO - allennlp.training.trainer - Training
2022-04-10 13:01:51,855 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-10 13:02:03,255 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.1392, loss: 0.1392 ||:   2%|1         | 1/65 [00:11<12:09, 11.40s/it]
2022-04-10 13:02:14,918 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0371, loss: 0.0881 ||:   3%|3         | 2/65 [00:23<12:07, 11.55s/it]
2022-04-10 13:02:26,727 - INFO - tqdm - accuracy: 0.9792, batch_loss: 0.0460, loss: 0.0741 ||:   5%|4         | 3/65 [00:34<12:03, 11.67s/it]
2022-04-10 13:02:38,263 - INFO - tqdm - accuracy: 0.9766, batch_loss: 0.0587, loss: 0.0703 ||:   6%|6         | 4/65 [00:46<11:48, 11.62s/it]
2022-04-10 13:02:49,719 - INFO - tqdm - accuracy: 0.9750, batch_loss: 0.0682, loss: 0.0699 ||:   8%|7         | 5/65 [00:57<11:33, 11.56s/it]
2022-04-10 13:03:01,221 - INFO - tqdm - accuracy: 0.9740, batch_loss: 0.0357, loss: 0.0642 ||:   9%|9         | 6/65 [01:09<11:20, 11.54s/it]
2022-04-10 13:03:12,757 - INFO - tqdm - accuracy: 0.9777, batch_loss: 0.0257, loss: 0.0587 ||:  11%|#         | 7/65 [01:20<11:09, 11.54s/it]
2022-04-10 13:03:24,317 - INFO - tqdm - accuracy: 0.9805, batch_loss: 0.0390, loss: 0.0562 ||:  12%|#2        | 8/65 [01:32<10:58, 11.55s/it]
2022-04-10 13:03:35,911 - INFO - tqdm - accuracy: 0.9757, batch_loss: 0.1281, loss: 0.0642 ||:  14%|#3        | 9/65 [01:44<10:47, 11.56s/it]
2022-04-10 13:03:47,502 - INFO - tqdm - accuracy: 0.9750, batch_loss: 0.0975, loss: 0.0675 ||:  15%|#5        | 10/65 [01:55<10:36, 11.57s/it]
2022-04-10 13:03:59,077 - INFO - tqdm - accuracy: 0.9716, batch_loss: 0.0688, loss: 0.0676 ||:  17%|#6        | 11/65 [02:07<10:24, 11.57s/it]
2022-04-10 13:04:10,635 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.1489, loss: 0.0744 ||:  18%|#8        | 12/65 [02:18<10:13, 11.57s/it]
2022-04-10 13:04:22,187 - INFO - tqdm - accuracy: 0.9712, batch_loss: 0.0173, loss: 0.0700 ||:  20%|##        | 13/65 [02:30<10:01, 11.56s/it]
2022-04-10 13:04:33,739 - INFO - tqdm - accuracy: 0.9710, batch_loss: 0.0721, loss: 0.0702 ||:  22%|##1       | 14/65 [02:41<09:49, 11.56s/it]
2022-04-10 13:04:45,152 - INFO - tqdm - accuracy: 0.9729, batch_loss: 0.0099, loss: 0.0661 ||:  23%|##3       | 15/65 [02:53<09:35, 11.52s/it]
2022-04-10 13:04:56,698 - INFO - tqdm - accuracy: 0.9707, batch_loss: 0.0766, loss: 0.0668 ||:  25%|##4       | 16/65 [03:04<09:24, 11.52s/it]
2022-04-10 13:05:08,093 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.0756, loss: 0.0673 ||:  26%|##6       | 17/65 [03:16<09:11, 11.49s/it]
2022-04-10 13:05:19,629 - INFO - tqdm - accuracy: 0.9635, batch_loss: 0.2384, loss: 0.0768 ||:  28%|##7       | 18/65 [03:27<09:00, 11.50s/it]
2022-04-10 13:05:31,167 - INFO - tqdm - accuracy: 0.9638, batch_loss: 0.0438, loss: 0.0751 ||:  29%|##9       | 19/65 [03:39<08:49, 11.51s/it]
2022-04-10 13:05:42,600 - INFO - tqdm - accuracy: 0.9609, batch_loss: 0.1629, loss: 0.0795 ||:  31%|###       | 20/65 [03:50<08:36, 11.49s/it]
2022-04-10 13:05:54,118 - INFO - tqdm - accuracy: 0.9628, batch_loss: 0.0143, loss: 0.0764 ||:  32%|###2      | 21/65 [04:02<08:25, 11.50s/it]
2022-04-10 13:06:05,648 - INFO - tqdm - accuracy: 0.9616, batch_loss: 0.1566, loss: 0.0800 ||:  34%|###3      | 22/65 [04:13<08:14, 11.51s/it]
2022-04-10 13:06:17,167 - INFO - tqdm - accuracy: 0.9633, batch_loss: 0.0273, loss: 0.0777 ||:  35%|###5      | 23/65 [04:25<08:03, 11.51s/it]
2022-04-10 13:06:28,687 - INFO - tqdm - accuracy: 0.9635, batch_loss: 0.1289, loss: 0.0799 ||:  37%|###6      | 24/65 [04:36<07:52, 11.51s/it]
2022-04-10 13:06:40,195 - INFO - tqdm - accuracy: 0.9650, batch_loss: 0.0363, loss: 0.0781 ||:  38%|###8      | 25/65 [04:48<07:40, 11.51s/it]
2022-04-10 13:06:51,718 - INFO - tqdm - accuracy: 0.9627, batch_loss: 0.1558, loss: 0.0811 ||:  40%|####      | 26/65 [04:59<07:29, 11.52s/it]
2022-04-10 13:07:03,227 - INFO - tqdm - accuracy: 0.9641, batch_loss: 0.0238, loss: 0.0790 ||:  42%|####1     | 27/65 [05:11<07:17, 11.51s/it]
2022-04-10 13:07:14,746 - INFO - tqdm - accuracy: 0.9654, batch_loss: 0.0093, loss: 0.0765 ||:  43%|####3     | 28/65 [05:22<07:06, 11.51s/it]
2022-04-10 13:07:26,247 - INFO - tqdm - accuracy: 0.9666, batch_loss: 0.0177, loss: 0.0745 ||:  45%|####4     | 29/65 [05:34<06:54, 11.51s/it]
2022-04-10 13:07:37,752 - INFO - tqdm - accuracy: 0.9667, batch_loss: 0.0745, loss: 0.0745 ||:  46%|####6     | 30/65 [05:45<06:42, 11.51s/it]
2022-04-10 13:07:49,254 - INFO - tqdm - accuracy: 0.9677, batch_loss: 0.0201, loss: 0.0727 ||:  48%|####7     | 31/65 [05:57<06:31, 11.51s/it]
2022-04-10 13:08:00,758 - INFO - tqdm - accuracy: 0.9678, batch_loss: 0.0454, loss: 0.0719 ||:  49%|####9     | 32/65 [06:08<06:19, 11.51s/it]
2022-04-10 13:08:12,262 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.0164, loss: 0.0702 ||:  51%|#####     | 33/65 [06:20<06:08, 11.51s/it]
2022-04-10 13:08:23,749 - INFO - tqdm - accuracy: 0.9697, batch_loss: 0.0331, loss: 0.0691 ||:  52%|#####2    | 34/65 [06:31<05:56, 11.50s/it]
2022-04-10 13:08:35,236 - INFO - tqdm - accuracy: 0.9696, batch_loss: 0.0446, loss: 0.0684 ||:  54%|#####3    | 35/65 [06:43<05:44, 11.50s/it]
2022-04-10 13:08:46,737 - INFO - tqdm - accuracy: 0.9705, batch_loss: 0.0164, loss: 0.0669 ||:  55%|#####5    | 36/65 [06:54<05:33, 11.50s/it]
2022-04-10 13:08:58,259 - INFO - tqdm - accuracy: 0.9713, batch_loss: 0.0058, loss: 0.0653 ||:  57%|#####6    | 37/65 [07:06<05:22, 11.50s/it]
2022-04-10 13:09:09,764 - INFO - tqdm - accuracy: 0.9720, batch_loss: 0.0176, loss: 0.0640 ||:  58%|#####8    | 38/65 [07:17<05:10, 11.51s/it]
2022-04-10 13:09:21,268 - INFO - tqdm - accuracy: 0.9720, batch_loss: 0.0749, loss: 0.0643 ||:  60%|######    | 39/65 [07:29<04:59, 11.50s/it]
2022-04-10 13:09:32,793 - INFO - tqdm - accuracy: 0.9727, batch_loss: 0.0271, loss: 0.0634 ||:  62%|######1   | 40/65 [07:40<04:47, 11.51s/it]
2022-04-10 13:09:44,317 - INFO - tqdm - accuracy: 0.9726, batch_loss: 0.0388, loss: 0.0628 ||:  63%|######3   | 41/65 [07:52<04:36, 11.51s/it]
2022-04-10 13:09:55,835 - INFO - tqdm - accuracy: 0.9732, batch_loss: 0.0299, loss: 0.0620 ||:  65%|######4   | 42/65 [08:03<04:24, 11.52s/it]
2022-04-10 13:10:07,376 - INFO - tqdm - accuracy: 0.9738, batch_loss: 0.0326, loss: 0.0613 ||:  66%|######6   | 43/65 [08:15<04:13, 11.52s/it]
2022-04-10 13:10:18,885 - INFO - tqdm - accuracy: 0.9744, batch_loss: 0.0097, loss: 0.0601 ||:  68%|######7   | 44/65 [08:27<04:01, 11.52s/it]
2022-04-10 13:10:30,815 - INFO - tqdm - accuracy: 0.9743, batch_loss: 0.0901, loss: 0.0608 ||:  69%|######9   | 45/65 [08:38<03:52, 11.64s/it]
2022-04-10 13:10:42,309 - INFO - tqdm - accuracy: 0.9742, batch_loss: 0.0571, loss: 0.0607 ||:  71%|#######   | 46/65 [08:50<03:40, 11.60s/it]
2022-04-10 13:10:53,859 - INFO - tqdm - accuracy: 0.9747, batch_loss: 0.0232, loss: 0.0599 ||:  72%|#######2  | 47/65 [09:02<03:28, 11.58s/it]
2022-04-10 13:11:05,393 - INFO - tqdm - accuracy: 0.9740, batch_loss: 0.1044, loss: 0.0609 ||:  74%|#######3  | 48/65 [09:13<03:16, 11.57s/it]
2022-04-10 13:11:16,932 - INFO - tqdm - accuracy: 0.9745, batch_loss: 0.0111, loss: 0.0598 ||:  75%|#######5  | 49/65 [09:25<03:04, 11.56s/it]
2022-04-10 13:11:28,458 - INFO - tqdm - accuracy: 0.9750, batch_loss: 0.0104, loss: 0.0589 ||:  77%|#######6  | 50/65 [09:36<02:53, 11.55s/it]
2022-04-10 13:11:39,978 - INFO - tqdm - accuracy: 0.9755, batch_loss: 0.0034, loss: 0.0578 ||:  78%|#######8  | 51/65 [09:48<02:41, 11.54s/it]
2022-04-10 13:11:51,503 - INFO - tqdm - accuracy: 0.9760, batch_loss: 0.0117, loss: 0.0569 ||:  80%|########  | 52/65 [09:59<02:29, 11.54s/it]
2022-04-10 13:12:03,038 - INFO - tqdm - accuracy: 0.9758, batch_loss: 0.0589, loss: 0.0569 ||:  82%|########1 | 53/65 [10:11<02:18, 11.54s/it]
2022-04-10 13:12:14,552 - INFO - tqdm - accuracy: 0.9763, batch_loss: 0.0096, loss: 0.0560 ||:  83%|########3 | 54/65 [10:22<02:06, 11.53s/it]
2022-04-10 13:12:26,084 - INFO - tqdm - accuracy: 0.9767, batch_loss: 0.0070, loss: 0.0551 ||:  85%|########4 | 55/65 [10:34<01:55, 11.53s/it]
2022-04-10 13:12:37,618 - INFO - tqdm - accuracy: 0.9766, batch_loss: 0.0522, loss: 0.0551 ||:  86%|########6 | 56/65 [10:45<01:43, 11.53s/it]
2022-04-10 13:12:49,138 - INFO - tqdm - accuracy: 0.9770, batch_loss: 0.0059, loss: 0.0542 ||:  88%|########7 | 57/65 [10:57<01:32, 11.53s/it]
2022-04-10 13:13:00,641 - INFO - tqdm - accuracy: 0.9774, batch_loss: 0.0099, loss: 0.0535 ||:  89%|########9 | 58/65 [11:08<01:20, 11.52s/it]
2022-04-10 13:13:11,559 - INFO - tqdm - accuracy: 0.9777, batch_loss: 0.0109, loss: 0.0527 ||:  91%|######### | 59/65 [11:19<01:08, 11.34s/it]
2022-04-10 13:13:23,067 - INFO - tqdm - accuracy: 0.9781, batch_loss: 0.0024, loss: 0.0519 ||:  92%|#########2| 60/65 [11:31<00:56, 11.39s/it]
2022-04-10 13:13:34,573 - INFO - tqdm - accuracy: 0.9785, batch_loss: 0.0065, loss: 0.0512 ||:  94%|#########3| 61/65 [11:42<00:45, 11.42s/it]
2022-04-10 13:13:46,089 - INFO - tqdm - accuracy: 0.9788, batch_loss: 0.0078, loss: 0.0505 ||:  95%|#########5| 62/65 [11:54<00:34, 11.45s/it]
2022-04-10 13:13:57,613 - INFO - tqdm - accuracy: 0.9792, batch_loss: 0.0010, loss: 0.0497 ||:  97%|#########6| 63/65 [12:05<00:22, 11.47s/it]
2022-04-10 13:14:09,156 - INFO - tqdm - accuracy: 0.9795, batch_loss: 0.0026, loss: 0.0489 ||:  98%|#########8| 64/65 [12:17<00:11, 11.49s/it]
2022-04-10 13:14:14,264 - INFO - tqdm - accuracy: 0.9796, batch_loss: 0.0012, loss: 0.0482 ||: 100%|##########| 65/65 [12:22<00:00,  9.58s/it]
2022-04-10 13:14:14,265 - INFO - tqdm - accuracy: 0.9796, batch_loss: 0.0012, loss: 0.0482 ||: 100%|##########| 65/65 [12:22<00:00, 11.42s/it]
2022-04-10 13:14:17,234 - INFO - allennlp.training.trainer - Validating
2022-04-10 13:14:17,235 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-10 13:14:27,263 - INFO - tqdm - accuracy: 0.7250, batch_loss: 0.4916, loss: 1.5933 ||:  35%|###4      | 40/115 [00:10<00:18,  4.02it/s]
2022-04-10 13:14:37,380 - INFO - tqdm - accuracy: 0.6667, batch_loss: 1.1857, loss: 2.0569 ||:  70%|######9   | 80/115 [00:20<00:08,  3.91it/s]
2022-04-10 13:14:46,367 - INFO - tqdm - accuracy: 0.6594, batch_loss: 0.2966, loss: 2.0602 ||: 100%|##########| 115/115 [00:29<00:00,  3.90it/s]
2022-04-10 13:14:46,367 - INFO - tqdm - accuracy: 0.6594, batch_loss: 0.2966, loss: 2.0602 ||: 100%|##########| 115/115 [00:29<00:00,  3.95it/s]
2022-04-10 13:14:46,367 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-10 13:14:46,368 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.980  |     0.659
2022-04-10 13:14:46,368 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-10 13:14:46,369 - INFO - allennlp.training.tensorboard_writer - loss               |     0.048  |     2.060
2022-04-10 13:14:46,370 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5797.879  |       N/A
2022-04-10 13:14:52,116 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to './output_ir-ora-d_hyper3/best.th'.
2022-04-10 13:15:03,376 - INFO - allennlp.training.trainer - Epoch duration: 0:13:11.523792
2022-04-10 13:15:03,376 - INFO - allennlp.training.trainer - Estimated training time remaining: 1:31:32
2022-04-10 13:15:03,377 - INFO - allennlp.training.trainer - Epoch 8/14
2022-04-10 13:15:03,377 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-10 13:15:03,377 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-10 13:15:03,378 - INFO - allennlp.training.trainer - Training
2022-04-10 13:15:03,379 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-10 13:15:14,675 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0012, loss: 0.0012 ||:   2%|1         | 1/65 [00:11<12:02, 11.30s/it]
2022-04-10 13:15:26,326 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0268, loss: 0.0140 ||:   3%|3         | 2/65 [00:22<12:04, 11.50s/it]
2022-04-10 13:15:38,273 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0011, loss: 0.0097 ||:   5%|4         | 3/65 [00:34<12:05, 11.71s/it]
2022-04-10 13:15:50,022 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0070, loss: 0.0090 ||:   6%|6         | 4/65 [00:46<11:55, 11.72s/it]
2022-04-10 13:16:01,473 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0010, loss: 0.0074 ||:   8%|7         | 5/65 [00:58<11:37, 11.63s/it]
2022-04-10 13:16:12,845 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0017, loss: 0.0065 ||:   9%|9         | 6/65 [01:09<11:20, 11.54s/it]
2022-04-10 13:16:24,276 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0011, loss: 0.0057 ||:  11%|#         | 7/65 [01:20<11:07, 11.50s/it]
2022-04-10 13:16:35,831 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0021, loss: 0.0052 ||:  12%|#2        | 8/65 [01:32<10:56, 11.52s/it]
2022-04-10 13:16:47,469 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0010, loss: 0.0048 ||:  14%|#3        | 9/65 [01:44<10:47, 11.56s/it]
2022-04-10 13:16:59,076 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0008, loss: 0.0044 ||:  15%|#5        | 10/65 [01:55<10:36, 11.57s/it]
2022-04-10 13:17:10,589 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0010, loss: 0.0041 ||:  17%|#6        | 11/65 [02:07<10:23, 11.55s/it]
2022-04-10 13:17:22,078 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0079, loss: 0.0044 ||:  18%|#8        | 12/65 [02:18<10:11, 11.53s/it]
2022-04-10 13:17:33,562 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0008, loss: 0.0041 ||:  20%|##        | 13/65 [02:30<09:59, 11.52s/it]
2022-04-10 13:17:45,066 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0032, loss: 0.0040 ||:  22%|##1       | 14/65 [02:41<09:47, 11.51s/it]
2022-04-10 13:17:56,571 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0048, loss: 0.0041 ||:  23%|##3       | 15/65 [02:53<09:35, 11.51s/it]
2022-04-10 13:18:08,093 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0010, loss: 0.0039 ||:  25%|##4       | 16/65 [03:04<09:24, 11.51s/it]
2022-04-10 13:18:19,619 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0007, loss: 0.0037 ||:  26%|##6       | 17/65 [03:16<09:12, 11.52s/it]
2022-04-10 13:18:31,146 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0075, loss: 0.0039 ||:  28%|##7       | 18/65 [03:27<09:01, 11.52s/it]
2022-04-10 13:18:42,682 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0010, loss: 0.0038 ||:  29%|##9       | 19/65 [03:39<08:50, 11.53s/it]
2022-04-10 13:18:54,214 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0045, loss: 0.0038 ||:  31%|###       | 20/65 [03:50<08:38, 11.53s/it]
2022-04-10 13:19:05,748 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0079, loss: 0.0040 ||:  32%|###2      | 21/65 [04:02<08:27, 11.53s/it]
2022-04-10 13:19:17,281 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0008, loss: 0.0039 ||:  34%|###3      | 22/65 [04:13<08:15, 11.53s/it]
2022-04-10 13:19:28,826 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0060, loss: 0.0039 ||:  35%|###5      | 23/65 [04:25<08:04, 11.53s/it]
2022-04-10 13:19:40,368 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0017, loss: 0.0039 ||:  37%|###6      | 24/65 [04:36<07:53, 11.54s/it]
2022-04-10 13:19:51,906 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0037, loss: 0.0038 ||:  38%|###8      | 25/65 [04:48<07:41, 11.54s/it]
2022-04-10 13:20:03,450 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0008, loss: 0.0037 ||:  40%|####      | 26/65 [05:00<07:30, 11.54s/it]
2022-04-10 13:20:14,741 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0019, loss: 0.0037 ||:  42%|####1     | 27/65 [05:11<07:15, 11.46s/it]
2022-04-10 13:20:26,311 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0009, loss: 0.0036 ||:  43%|####3     | 28/65 [05:22<07:05, 11.50s/it]
2022-04-10 13:20:37,879 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0097, loss: 0.0038 ||:  45%|####4     | 29/65 [05:34<06:54, 11.52s/it]
2022-04-10 13:20:49,440 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0093, loss: 0.0040 ||:  46%|####6     | 30/65 [05:46<06:43, 11.53s/it]
2022-04-10 13:21:01,004 - INFO - tqdm - accuracy: 0.9990, batch_loss: 0.0694, loss: 0.0061 ||:  48%|####7     | 31/65 [05:57<06:32, 11.54s/it]
2022-04-10 13:21:12,572 - INFO - tqdm - accuracy: 0.9990, batch_loss: 0.0008, loss: 0.0059 ||:  49%|####9     | 32/65 [06:09<06:21, 11.55s/it]
2022-04-10 13:21:24,131 - INFO - tqdm - accuracy: 0.9991, batch_loss: 0.0006, loss: 0.0057 ||:  51%|#####     | 33/65 [06:20<06:09, 11.55s/it]
2022-04-10 13:21:35,692 - INFO - tqdm - accuracy: 0.9991, batch_loss: 0.0220, loss: 0.0062 ||:  52%|#####2    | 34/65 [06:32<05:58, 11.55s/it]
2022-04-10 13:21:47,243 - INFO - tqdm - accuracy: 0.9982, batch_loss: 0.0709, loss: 0.0081 ||:  54%|#####3    | 35/65 [06:43<05:46, 11.55s/it]
2022-04-10 13:21:58,802 - INFO - tqdm - accuracy: 0.9983, batch_loss: 0.0011, loss: 0.0079 ||:  55%|#####5    | 36/65 [06:55<05:35, 11.56s/it]
2022-04-10 13:22:10,364 - INFO - tqdm - accuracy: 0.9983, batch_loss: 0.0011, loss: 0.0077 ||:  57%|#####6    | 37/65 [07:06<05:23, 11.56s/it]
2022-04-10 13:22:21,921 - INFO - tqdm - accuracy: 0.9984, batch_loss: 0.0007, loss: 0.0075 ||:  58%|#####8    | 38/65 [07:18<05:12, 11.56s/it]
2022-04-10 13:22:33,245 - INFO - tqdm - accuracy: 0.9984, batch_loss: 0.0009, loss: 0.0073 ||:  60%|######    | 39/65 [07:29<04:58, 11.49s/it]
2022-04-10 13:22:44,794 - INFO - tqdm - accuracy: 0.9977, batch_loss: 0.0379, loss: 0.0081 ||:  62%|######1   | 40/65 [07:41<04:47, 11.51s/it]
2022-04-10 13:22:56,348 - INFO - tqdm - accuracy: 0.9970, batch_loss: 0.0647, loss: 0.0095 ||:  63%|######3   | 41/65 [07:52<04:36, 11.52s/it]
2022-04-10 13:23:07,914 - INFO - tqdm - accuracy: 0.9970, batch_loss: 0.0013, loss: 0.0093 ||:  65%|######4   | 42/65 [08:04<04:25, 11.53s/it]
2022-04-10 13:23:19,465 - INFO - tqdm - accuracy: 0.9964, batch_loss: 0.0318, loss: 0.0098 ||:  66%|######6   | 43/65 [08:16<04:13, 11.54s/it]
2022-04-10 13:23:31,021 - INFO - tqdm - accuracy: 0.9957, batch_loss: 0.0392, loss: 0.0105 ||:  68%|######7   | 44/65 [08:27<04:02, 11.54s/it]
2022-04-10 13:23:42,583 - INFO - tqdm - accuracy: 0.9951, batch_loss: 0.0627, loss: 0.0116 ||:  69%|######9   | 45/65 [08:39<03:50, 11.55s/it]
2022-04-10 13:23:54,114 - INFO - tqdm - accuracy: 0.9952, batch_loss: 0.0125, loss: 0.0117 ||:  71%|#######   | 46/65 [08:50<03:39, 11.54s/it]
2022-04-10 13:24:05,649 - INFO - tqdm - accuracy: 0.9953, batch_loss: 0.0005, loss: 0.0114 ||:  72%|#######2  | 47/65 [09:02<03:27, 11.54s/it]
2022-04-10 13:24:17,178 - INFO - tqdm - accuracy: 0.9954, batch_loss: 0.0115, loss: 0.0114 ||:  74%|#######3  | 48/65 [09:13<03:16, 11.54s/it]
2022-04-10 13:24:28,715 - INFO - tqdm - accuracy: 0.9955, batch_loss: 0.0009, loss: 0.0112 ||:  75%|#######5  | 49/65 [09:25<03:04, 11.54s/it]
2022-04-10 13:24:40,247 - INFO - tqdm - accuracy: 0.9950, batch_loss: 0.0244, loss: 0.0115 ||:  77%|#######6  | 50/65 [09:36<02:53, 11.54s/it]
2022-04-10 13:24:51,770 - INFO - tqdm - accuracy: 0.9951, batch_loss: 0.0006, loss: 0.0113 ||:  78%|#######8  | 51/65 [09:48<02:41, 11.53s/it]
2022-04-10 13:25:03,287 - INFO - tqdm - accuracy: 0.9952, batch_loss: 0.0011, loss: 0.0111 ||:  80%|########  | 52/65 [09:59<02:29, 11.53s/it]
2022-04-10 13:25:14,840 - INFO - tqdm - accuracy: 0.9947, batch_loss: 0.0827, loss: 0.0124 ||:  82%|########1 | 53/65 [10:11<02:18, 11.54s/it]
2022-04-10 13:25:26,368 - INFO - tqdm - accuracy: 0.9948, batch_loss: 0.0007, loss: 0.0122 ||:  83%|########3 | 54/65 [10:22<02:06, 11.53s/it]
2022-04-10 13:25:37,888 - INFO - tqdm - accuracy: 0.9949, batch_loss: 0.0023, loss: 0.0120 ||:  85%|########4 | 55/65 [10:34<01:55, 11.53s/it]
2022-04-10 13:25:49,405 - INFO - tqdm - accuracy: 0.9950, batch_loss: 0.0025, loss: 0.0118 ||:  86%|########6 | 56/65 [10:46<01:43, 11.53s/it]
2022-04-10 13:26:00,935 - INFO - tqdm - accuracy: 0.9951, batch_loss: 0.0229, loss: 0.0120 ||:  88%|########7 | 57/65 [10:57<01:32, 11.53s/it]
2022-04-10 13:26:12,475 - INFO - tqdm - accuracy: 0.9946, batch_loss: 0.0350, loss: 0.0124 ||:  89%|########9 | 58/65 [11:09<01:20, 11.53s/it]
2022-04-10 13:26:23,988 - INFO - tqdm - accuracy: 0.9947, batch_loss: 0.0067, loss: 0.0123 ||:  91%|######### | 59/65 [11:20<01:09, 11.53s/it]
2022-04-10 13:26:35,161 - INFO - tqdm - accuracy: 0.9943, batch_loss: 0.0234, loss: 0.0125 ||:  92%|#########2| 60/65 [11:31<00:57, 11.42s/it]
2022-04-10 13:26:46,680 - INFO - tqdm - accuracy: 0.9933, batch_loss: 0.3590, loss: 0.0182 ||:  94%|#########3| 61/65 [11:43<00:45, 11.45s/it]
2022-04-10 13:26:58,204 - INFO - tqdm - accuracy: 0.9919, batch_loss: 0.3222, loss: 0.0231 ||:  95%|#########5| 62/65 [11:54<00:34, 11.47s/it]
2022-04-10 13:27:09,713 - INFO - tqdm - accuracy: 0.9916, batch_loss: 0.0583, loss: 0.0237 ||:  97%|#########6| 63/65 [12:06<00:22, 11.48s/it]
2022-04-10 13:27:21,233 - INFO - tqdm - accuracy: 0.9917, batch_loss: 0.0006, loss: 0.0233 ||:  98%|#########8| 64/65 [12:17<00:11, 11.49s/it]
2022-04-10 13:27:26,212 - INFO - tqdm - accuracy: 0.9918, batch_loss: 0.0005, loss: 0.0230 ||: 100%|##########| 65/65 [12:22<00:00,  9.54s/it]
2022-04-10 13:27:26,212 - INFO - tqdm - accuracy: 0.9918, batch_loss: 0.0005, loss: 0.0230 ||: 100%|##########| 65/65 [12:22<00:00, 11.43s/it]
2022-04-10 13:27:29,188 - INFO - allennlp.training.trainer - Validating
2022-04-10 13:27:29,190 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-10 13:27:39,303 - INFO - tqdm - accuracy: 0.6875, batch_loss: 0.0012, loss: 2.0582 ||:  35%|###4      | 40/115 [00:10<00:18,  3.95it/s]
2022-04-10 13:27:49,489 - INFO - tqdm - accuracy: 0.6375, batch_loss: 4.0726, loss: 2.4320 ||:  70%|######9   | 80/115 [00:20<00:08,  3.89it/s]
2022-04-10 13:27:58,279 - INFO - tqdm - accuracy: 0.6288, batch_loss: 0.0078, loss: 2.3236 ||: 100%|##########| 115/115 [00:29<00:00,  3.95it/s]
2022-04-10 13:27:58,280 - INFO - tqdm - accuracy: 0.6288, batch_loss: 0.0078, loss: 2.3236 ||: 100%|##########| 115/115 [00:29<00:00,  3.95it/s]
2022-04-10 13:27:58,280 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-10 13:27:58,280 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.992  |     0.629
2022-04-10 13:27:58,281 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-10 13:27:58,282 - INFO - allennlp.training.tensorboard_writer - loss               |     0.023  |     2.324
2022-04-10 13:27:58,282 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5797.879  |       N/A
2022-04-10 13:28:04,759 - INFO - allennlp.training.trainer - Epoch duration: 0:13:01.382819
2022-04-10 13:28:04,760 - INFO - allennlp.training.trainer - Estimated training time remaining: 1:18:25
2022-04-10 13:28:04,760 - INFO - allennlp.training.trainer - Epoch 9/14
2022-04-10 13:28:04,760 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-10 13:28:04,760 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-10 13:28:04,762 - INFO - allennlp.training.trainer - Training
2022-04-10 13:28:04,762 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-10 13:28:16,176 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0017, loss: 0.0017 ||:   2%|1         | 1/65 [00:11<12:10, 11.41s/it]
2022-04-10 13:28:27,816 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0471, loss: 0.0244 ||:   3%|3         | 2/65 [00:23<12:07, 11.55s/it]
2022-04-10 13:28:39,622 - INFO - tqdm - accuracy: 0.9792, batch_loss: 0.2710, loss: 0.1066 ||:   5%|4         | 3/65 [00:34<12:03, 11.67s/it]
2022-04-10 13:28:51,038 - INFO - tqdm - accuracy: 0.9766, batch_loss: 0.1838, loss: 0.1259 ||:   6%|6         | 4/65 [00:46<11:45, 11.57s/it]
2022-04-10 13:29:02,481 - INFO - tqdm - accuracy: 0.9625, batch_loss: 0.1791, loss: 0.1365 ||:   8%|7         | 5/65 [00:57<11:31, 11.52s/it]
2022-04-10 13:29:13,951 - INFO - tqdm - accuracy: 0.9635, batch_loss: 0.0461, loss: 0.1214 ||:   9%|9         | 6/65 [01:09<11:18, 11.50s/it]
2022-04-10 13:29:25,490 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.0015, loss: 0.1043 ||:  11%|#         | 7/65 [01:20<11:07, 11.52s/it]
2022-04-10 13:29:37,086 - INFO - tqdm - accuracy: 0.9727, batch_loss: 0.0177, loss: 0.0935 ||:  12%|#2        | 8/65 [01:32<10:57, 11.54s/it]
2022-04-10 13:29:48,671 - INFO - tqdm - accuracy: 0.9757, batch_loss: 0.0136, loss: 0.0846 ||:  14%|#3        | 9/65 [01:43<10:47, 11.55s/it]
2022-04-10 13:30:00,228 - INFO - tqdm - accuracy: 0.9781, batch_loss: 0.0007, loss: 0.0762 ||:  15%|#5        | 10/65 [01:55<10:35, 11.56s/it]
2022-04-10 13:30:11,764 - INFO - tqdm - accuracy: 0.9801, batch_loss: 0.0013, loss: 0.0694 ||:  17%|#6        | 11/65 [02:07<10:23, 11.55s/it]
2022-04-10 13:30:23,262 - INFO - tqdm - accuracy: 0.9818, batch_loss: 0.0071, loss: 0.0642 ||:  18%|#8        | 12/65 [02:18<10:11, 11.53s/it]
2022-04-10 13:30:34,739 - INFO - tqdm - accuracy: 0.9832, batch_loss: 0.0045, loss: 0.0596 ||:  20%|##        | 13/65 [02:29<09:58, 11.52s/it]
2022-04-10 13:30:46,234 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0233, loss: 0.0570 ||:  22%|##1       | 14/65 [02:41<09:47, 11.51s/it]
2022-04-10 13:30:58,116 - INFO - tqdm - accuracy: 0.9833, batch_loss: 0.0440, loss: 0.0562 ||:  23%|##3       | 15/65 [02:53<09:41, 11.62s/it]
2022-04-10 13:31:09,570 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0020, loss: 0.0528 ||:  25%|##4       | 16/65 [03:04<09:27, 11.57s/it]
2022-04-10 13:31:21,083 - INFO - tqdm - accuracy: 0.9853, batch_loss: 0.0022, loss: 0.0498 ||:  26%|##6       | 17/65 [03:16<09:14, 11.55s/it]
2022-04-10 13:31:32,592 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0654, loss: 0.0507 ||:  28%|##7       | 18/65 [03:27<09:02, 11.54s/it]
2022-04-10 13:31:44,106 - INFO - tqdm - accuracy: 0.9803, batch_loss: 0.2817, loss: 0.0628 ||:  29%|##9       | 19/65 [03:39<08:50, 11.53s/it]
2022-04-10 13:31:55,630 - INFO - tqdm - accuracy: 0.9797, batch_loss: 0.1286, loss: 0.0661 ||:  31%|###       | 20/65 [03:50<08:38, 11.53s/it]
2022-04-10 13:32:07,133 - INFO - tqdm - accuracy: 0.9792, batch_loss: 0.0653, loss: 0.0661 ||:  32%|###2      | 21/65 [04:02<08:26, 11.52s/it]
2022-04-10 13:32:18,673 - INFO - tqdm - accuracy: 0.9801, batch_loss: 0.0033, loss: 0.0632 ||:  34%|###3      | 22/65 [04:13<08:15, 11.53s/it]
2022-04-10 13:32:30,197 - INFO - tqdm - accuracy: 0.9810, batch_loss: 0.0017, loss: 0.0605 ||:  35%|###5      | 23/65 [04:25<08:04, 11.53s/it]
2022-04-10 13:32:41,746 - INFO - tqdm - accuracy: 0.9818, batch_loss: 0.0054, loss: 0.0582 ||:  37%|###6      | 24/65 [04:36<07:52, 11.53s/it]
2022-04-10 13:32:53,289 - INFO - tqdm - accuracy: 0.9825, batch_loss: 0.0291, loss: 0.0571 ||:  38%|###8      | 25/65 [04:48<07:41, 11.54s/it]
2022-04-10 13:33:04,817 - INFO - tqdm - accuracy: 0.9820, batch_loss: 0.2568, loss: 0.0648 ||:  40%|####      | 26/65 [05:00<07:29, 11.53s/it]
2022-04-10 13:33:16,367 - INFO - tqdm - accuracy: 0.9826, batch_loss: 0.0136, loss: 0.0629 ||:  42%|####1     | 27/65 [05:11<07:18, 11.54s/it]
2022-04-10 13:33:27,943 - INFO - tqdm - accuracy: 0.9810, batch_loss: 0.4146, loss: 0.0754 ||:  43%|####3     | 28/65 [05:23<07:07, 11.55s/it]
2022-04-10 13:33:39,483 - INFO - tqdm - accuracy: 0.9806, batch_loss: 0.0590, loss: 0.0749 ||:  45%|####4     | 29/65 [05:34<06:55, 11.55s/it]
2022-04-10 13:33:50,896 - INFO - tqdm - accuracy: 0.9812, batch_loss: 0.0240, loss: 0.0732 ||:  46%|####6     | 30/65 [05:46<06:42, 11.51s/it]
2022-04-10 13:34:02,444 - INFO - tqdm - accuracy: 0.9819, batch_loss: 0.0050, loss: 0.0710 ||:  48%|####7     | 31/65 [05:57<06:31, 11.52s/it]
2022-04-10 13:34:14,007 - INFO - tqdm - accuracy: 0.9824, batch_loss: 0.0174, loss: 0.0693 ||:  49%|####9     | 32/65 [06:09<06:20, 11.53s/it]
2022-04-10 13:34:25,583 - INFO - tqdm - accuracy: 0.9820, batch_loss: 0.0584, loss: 0.0690 ||:  51%|#####     | 33/65 [06:20<06:09, 11.55s/it]
2022-04-10 13:34:37,149 - INFO - tqdm - accuracy: 0.9825, batch_loss: 0.0375, loss: 0.0680 ||:  52%|#####2    | 34/65 [06:32<05:58, 11.55s/it]
2022-04-10 13:34:48,359 - INFO - tqdm - accuracy: 0.9812, batch_loss: 0.2322, loss: 0.0727 ||:  54%|#####3    | 35/65 [06:43<05:43, 11.45s/it]
2022-04-10 13:34:59,920 - INFO - tqdm - accuracy: 0.9818, batch_loss: 0.0097, loss: 0.0710 ||:  55%|#####5    | 36/65 [06:55<05:33, 11.48s/it]
2022-04-10 13:35:11,488 - INFO - tqdm - accuracy: 0.9814, batch_loss: 0.1251, loss: 0.0724 ||:  57%|#####6    | 37/65 [07:06<05:22, 11.51s/it]
2022-04-10 13:35:23,055 - INFO - tqdm - accuracy: 0.9811, batch_loss: 0.0506, loss: 0.0719 ||:  58%|#####8    | 38/65 [07:18<05:11, 11.53s/it]
2022-04-10 13:35:34,628 - INFO - tqdm - accuracy: 0.9816, batch_loss: 0.0033, loss: 0.0701 ||:  60%|######    | 39/65 [07:29<05:00, 11.54s/it]
2022-04-10 13:35:46,181 - INFO - tqdm - accuracy: 0.9820, batch_loss: 0.0483, loss: 0.0696 ||:  62%|######1   | 40/65 [07:41<04:48, 11.54s/it]
2022-04-10 13:35:57,749 - INFO - tqdm - accuracy: 0.9817, batch_loss: 0.0851, loss: 0.0699 ||:  63%|######3   | 41/65 [07:52<04:37, 11.55s/it]
2022-04-10 13:36:09,331 - INFO - tqdm - accuracy: 0.9821, batch_loss: 0.0096, loss: 0.0685 ||:  65%|######4   | 42/65 [08:04<04:25, 11.56s/it]
2022-04-10 13:36:20,879 - INFO - tqdm - accuracy: 0.9818, batch_loss: 0.0561, loss: 0.0682 ||:  66%|######6   | 43/65 [08:16<04:14, 11.56s/it]
2022-04-10 13:36:32,424 - INFO - tqdm - accuracy: 0.9815, batch_loss: 0.2070, loss: 0.0714 ||:  68%|######7   | 44/65 [08:27<04:02, 11.55s/it]
2022-04-10 13:36:43,930 - INFO - tqdm - accuracy: 0.9819, batch_loss: 0.0278, loss: 0.0704 ||:  69%|######9   | 45/65 [08:39<03:50, 11.54s/it]
2022-04-10 13:36:55,434 - INFO - tqdm - accuracy: 0.9823, batch_loss: 0.0056, loss: 0.0690 ||:  71%|#######   | 46/65 [08:50<03:39, 11.53s/it]
2022-04-10 13:37:06,916 - INFO - tqdm - accuracy: 0.9827, batch_loss: 0.0384, loss: 0.0683 ||:  72%|#######2  | 47/65 [09:02<03:27, 11.51s/it]
2022-04-10 13:37:18,398 - INFO - tqdm - accuracy: 0.9831, batch_loss: 0.0056, loss: 0.0670 ||:  74%|#######3  | 48/65 [09:13<03:15, 11.50s/it]
2022-04-10 13:37:29,890 - INFO - tqdm - accuracy: 0.9834, batch_loss: 0.0204, loss: 0.0661 ||:  75%|#######5  | 49/65 [09:25<03:04, 11.50s/it]
2022-04-10 13:37:41,148 - INFO - tqdm - accuracy: 0.9831, batch_loss: 0.0512, loss: 0.0658 ||:  77%|#######6  | 50/65 [09:36<02:51, 11.43s/it]
2022-04-10 13:37:52,679 - INFO - tqdm - accuracy: 0.9834, batch_loss: 0.0069, loss: 0.0646 ||:  78%|#######8  | 51/65 [09:47<02:40, 11.46s/it]
2022-04-10 13:38:04,190 - INFO - tqdm - accuracy: 0.9838, batch_loss: 0.0142, loss: 0.0637 ||:  80%|########  | 52/65 [09:59<02:29, 11.47s/it]
2022-04-10 13:38:15,724 - INFO - tqdm - accuracy: 0.9835, batch_loss: 0.0481, loss: 0.0634 ||:  82%|########1 | 53/65 [10:10<02:17, 11.49s/it]
2022-04-10 13:38:27,249 - INFO - tqdm - accuracy: 0.9838, batch_loss: 0.0132, loss: 0.0624 ||:  83%|########3 | 54/65 [10:22<02:06, 11.50s/it]
2022-04-10 13:38:38,791 - INFO - tqdm - accuracy: 0.9841, batch_loss: 0.0050, loss: 0.0614 ||:  85%|########4 | 55/65 [10:34<01:55, 11.51s/it]
2022-04-10 13:38:50,324 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0058, loss: 0.0604 ||:  86%|########6 | 56/65 [10:45<01:43, 11.52s/it]
2022-04-10 13:39:01,864 - INFO - tqdm - accuracy: 0.9841, batch_loss: 0.0781, loss: 0.0607 ||:  88%|########7 | 57/65 [10:57<01:32, 11.53s/it]
2022-04-10 13:39:13,424 - INFO - tqdm - accuracy: 0.9838, batch_loss: 0.1000, loss: 0.0614 ||:  89%|########9 | 58/65 [11:08<01:20, 11.54s/it]
2022-04-10 13:39:24,972 - INFO - tqdm - accuracy: 0.9841, batch_loss: 0.0128, loss: 0.0606 ||:  91%|######### | 59/65 [11:20<01:09, 11.54s/it]
2022-04-10 13:39:36,518 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0170, loss: 0.0598 ||:  92%|#########2| 60/65 [11:31<00:57, 11.54s/it]
2022-04-10 13:39:48,077 - INFO - tqdm - accuracy: 0.9846, batch_loss: 0.0064, loss: 0.0590 ||:  94%|#########3| 61/65 [11:43<00:46, 11.55s/it]
2022-04-10 13:39:59,658 - INFO - tqdm - accuracy: 0.9849, batch_loss: 0.0059, loss: 0.0581 ||:  95%|#########5| 62/65 [11:54<00:34, 11.56s/it]
2022-04-10 13:40:11,237 - INFO - tqdm - accuracy: 0.9851, batch_loss: 0.0039, loss: 0.0572 ||:  97%|#########6| 63/65 [12:06<00:23, 11.56s/it]
2022-04-10 13:40:22,822 - INFO - tqdm - accuracy: 0.9849, batch_loss: 0.0271, loss: 0.0568 ||:  98%|#########8| 64/65 [12:18<00:11, 11.57s/it]
2022-04-10 13:40:27,955 - INFO - tqdm - accuracy: 0.9850, batch_loss: 0.0120, loss: 0.0561 ||: 100%|##########| 65/65 [12:23<00:00,  9.64s/it]
2022-04-10 13:40:27,956 - INFO - tqdm - accuracy: 0.9850, batch_loss: 0.0120, loss: 0.0561 ||: 100%|##########| 65/65 [12:23<00:00, 11.43s/it]
2022-04-10 13:40:30,922 - INFO - allennlp.training.trainer - Validating
2022-04-10 13:40:30,923 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-10 13:40:41,097 - INFO - tqdm - accuracy: 0.5802, batch_loss: 2.7890, loss: 2.1589 ||:  36%|###5      | 41/115 [00:10<00:18,  3.93it/s]
2022-04-10 13:40:51,317 - INFO - tqdm - accuracy: 0.6211, batch_loss: 6.6715, loss: 1.9525 ||:  70%|#######   | 81/115 [00:20<00:08,  3.91it/s]
2022-04-10 13:41:00,025 - INFO - tqdm - accuracy: 0.6245, batch_loss: 0.1027, loss: 1.7577 ||: 100%|##########| 115/115 [00:29<00:00,  3.91it/s]
2022-04-10 13:41:00,025 - INFO - tqdm - accuracy: 0.6245, batch_loss: 0.1027, loss: 1.7577 ||: 100%|##########| 115/115 [00:29<00:00,  3.95it/s]
2022-04-10 13:41:00,025 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-10 13:41:00,026 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.985  |     0.624
2022-04-10 13:41:00,027 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-10 13:41:00,028 - INFO - allennlp.training.tensorboard_writer - loss               |     0.056  |     1.758
2022-04-10 13:41:00,029 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5797.879  |       N/A
2022-04-10 13:41:06,438 - INFO - allennlp.training.trainer - Epoch duration: 0:13:01.677839
2022-04-10 13:41:06,438 - INFO - allennlp.training.trainer - Estimated training time remaining: 1:05:20
2022-04-10 13:41:06,438 - INFO - allennlp.training.trainer - Epoch 10/14
2022-04-10 13:41:06,438 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-10 13:41:06,439 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-10 13:41:06,440 - INFO - allennlp.training.trainer - Training
2022-04-10 13:41:06,440 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-10 13:41:17,846 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0049, loss: 0.0049 ||:   2%|1         | 1/65 [00:11<12:09, 11.41s/it]
2022-04-10 13:41:29,259 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.1588, loss: 0.0819 ||:   3%|3         | 2/65 [00:22<11:58, 11.41s/it]
2022-04-10 13:41:41,100 - INFO - tqdm - accuracy: 0.9792, batch_loss: 0.0187, loss: 0.0608 ||:   5%|4         | 3/65 [00:34<11:59, 11.61s/it]
2022-04-10 13:41:52,658 - INFO - tqdm - accuracy: 0.9766, batch_loss: 0.0574, loss: 0.0600 ||:   6%|6         | 4/65 [00:46<11:46, 11.59s/it]
2022-04-10 13:42:03,965 - INFO - tqdm - accuracy: 0.9812, batch_loss: 0.0027, loss: 0.0485 ||:   8%|7         | 5/65 [00:57<11:29, 11.49s/it]
2022-04-10 13:42:15,402 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0205, loss: 0.0438 ||:   9%|9         | 6/65 [01:08<11:16, 11.47s/it]
2022-04-10 13:42:26,899 - INFO - tqdm - accuracy: 0.9866, batch_loss: 0.0130, loss: 0.0394 ||:  11%|#         | 7/65 [01:20<11:05, 11.48s/it]
2022-04-10 13:42:38,461 - INFO - tqdm - accuracy: 0.9883, batch_loss: 0.0061, loss: 0.0353 ||:  12%|#2        | 8/65 [01:32<10:55, 11.50s/it]
2022-04-10 13:42:50,066 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.0041, loss: 0.0318 ||:  14%|#3        | 9/65 [01:43<10:46, 11.54s/it]
2022-04-10 13:43:01,630 - INFO - tqdm - accuracy: 0.9906, batch_loss: 0.0028, loss: 0.0289 ||:  15%|#5        | 10/65 [01:55<10:34, 11.54s/it]
2022-04-10 13:43:13,151 - INFO - tqdm - accuracy: 0.9915, batch_loss: 0.0031, loss: 0.0266 ||:  17%|#6        | 11/65 [02:06<10:23, 11.54s/it]
2022-04-10 13:43:24,640 - INFO - tqdm - accuracy: 0.9922, batch_loss: 0.0258, loss: 0.0265 ||:  18%|#8        | 12/65 [02:18<10:10, 11.52s/it]
2022-04-10 13:43:36,141 - INFO - tqdm - accuracy: 0.9928, batch_loss: 0.0073, loss: 0.0250 ||:  20%|##        | 13/65 [02:29<09:58, 11.52s/it]
2022-04-10 13:43:47,639 - INFO - tqdm - accuracy: 0.9888, batch_loss: 0.0708, loss: 0.0283 ||:  22%|##1       | 14/65 [02:41<09:47, 11.51s/it]
2022-04-10 13:43:59,150 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.0013, loss: 0.0265 ||:  23%|##3       | 15/65 [02:52<09:35, 11.51s/it]
2022-04-10 13:44:10,674 - INFO - tqdm - accuracy: 0.9902, batch_loss: 0.0054, loss: 0.0252 ||:  25%|##4       | 16/65 [03:04<09:24, 11.51s/it]
2022-04-10 13:44:22,201 - INFO - tqdm - accuracy: 0.9908, batch_loss: 0.0012, loss: 0.0238 ||:  26%|##6       | 17/65 [03:15<09:12, 11.52s/it]
2022-04-10 13:44:33,727 - INFO - tqdm - accuracy: 0.9913, batch_loss: 0.0112, loss: 0.0231 ||:  28%|##7       | 18/65 [03:27<09:01, 11.52s/it]
2022-04-10 13:44:45,255 - INFO - tqdm - accuracy: 0.9918, batch_loss: 0.0054, loss: 0.0221 ||:  29%|##9       | 19/65 [03:38<08:50, 11.52s/it]
2022-04-10 13:44:56,785 - INFO - tqdm - accuracy: 0.9922, batch_loss: 0.0032, loss: 0.0212 ||:  31%|###       | 20/65 [03:50<08:38, 11.52s/it]
2022-04-10 13:45:08,312 - INFO - tqdm - accuracy: 0.9926, batch_loss: 0.0027, loss: 0.0203 ||:  32%|###2      | 21/65 [04:01<08:27, 11.53s/it]
2022-04-10 13:45:19,849 - INFO - tqdm - accuracy: 0.9929, batch_loss: 0.0042, loss: 0.0196 ||:  34%|###3      | 22/65 [04:13<08:15, 11.53s/it]
2022-04-10 13:45:31,036 - INFO - tqdm - accuracy: 0.9918, batch_loss: 0.0951, loss: 0.0229 ||:  35%|###5      | 23/65 [04:24<07:59, 11.43s/it]
2022-04-10 13:45:42,541 - INFO - tqdm - accuracy: 0.9922, batch_loss: 0.0042, loss: 0.0221 ||:  37%|###6      | 24/65 [04:36<07:49, 11.45s/it]
2022-04-10 13:45:54,101 - INFO - tqdm - accuracy: 0.9925, batch_loss: 0.0073, loss: 0.0215 ||:  38%|###8      | 25/65 [04:47<07:39, 11.48s/it]
2022-04-10 13:46:05,520 - INFO - tqdm - accuracy: 0.9928, batch_loss: 0.0010, loss: 0.0207 ||:  40%|####      | 26/65 [04:59<07:27, 11.46s/it]
2022-04-10 13:46:17,070 - INFO - tqdm - accuracy: 0.9930, batch_loss: 0.0033, loss: 0.0201 ||:  42%|####1     | 27/65 [05:10<07:16, 11.49s/it]
2022-04-10 13:46:28,634 - INFO - tqdm - accuracy: 0.9933, batch_loss: 0.0041, loss: 0.0195 ||:  43%|####3     | 28/65 [05:22<07:05, 11.51s/it]
2022-04-10 13:46:40,190 - INFO - tqdm - accuracy: 0.9935, batch_loss: 0.0060, loss: 0.0190 ||:  45%|####4     | 29/65 [05:33<06:54, 11.53s/it]
2022-04-10 13:46:51,759 - INFO - tqdm - accuracy: 0.9937, batch_loss: 0.0118, loss: 0.0188 ||:  46%|####6     | 30/65 [05:45<06:43, 11.54s/it]
2022-04-10 13:47:03,330 - INFO - tqdm - accuracy: 0.9939, batch_loss: 0.0303, loss: 0.0192 ||:  48%|####7     | 31/65 [05:56<06:32, 11.55s/it]
2022-04-10 13:47:14,915 - INFO - tqdm - accuracy: 0.9941, batch_loss: 0.0025, loss: 0.0186 ||:  49%|####9     | 32/65 [06:08<06:21, 11.56s/it]
2022-04-10 13:47:26,487 - INFO - tqdm - accuracy: 0.9943, batch_loss: 0.0089, loss: 0.0183 ||:  51%|#####     | 33/65 [06:20<06:10, 11.56s/it]
2022-04-10 13:47:38,069 - INFO - tqdm - accuracy: 0.9945, batch_loss: 0.0144, loss: 0.0182 ||:  52%|#####2    | 34/65 [06:31<05:58, 11.57s/it]
2022-04-10 13:47:49,655 - INFO - tqdm - accuracy: 0.9946, batch_loss: 0.0061, loss: 0.0179 ||:  54%|#####3    | 35/65 [06:43<05:47, 11.57s/it]
2022-04-10 13:48:01,271 - INFO - tqdm - accuracy: 0.9948, batch_loss: 0.0012, loss: 0.0174 ||:  55%|#####5    | 36/65 [06:54<05:36, 11.59s/it]
2022-04-10 13:48:12,848 - INFO - tqdm - accuracy: 0.9949, batch_loss: 0.0010, loss: 0.0170 ||:  57%|#####6    | 37/65 [07:06<05:24, 11.58s/it]
2022-04-10 13:48:24,441 - INFO - tqdm - accuracy: 0.9951, batch_loss: 0.0020, loss: 0.0166 ||:  58%|#####8    | 38/65 [07:18<05:12, 11.59s/it]
2022-04-10 13:48:36,037 - INFO - tqdm - accuracy: 0.9952, batch_loss: 0.0020, loss: 0.0162 ||:  60%|######    | 39/65 [07:29<05:01, 11.59s/it]
2022-04-10 13:48:47,626 - INFO - tqdm - accuracy: 0.9953, batch_loss: 0.0015, loss: 0.0158 ||:  62%|######1   | 40/65 [07:41<04:49, 11.59s/it]
2022-04-10 13:48:59,195 - INFO - tqdm - accuracy: 0.9954, batch_loss: 0.0054, loss: 0.0156 ||:  63%|######3   | 41/65 [07:52<04:37, 11.58s/it]
2022-04-10 13:49:10,783 - INFO - tqdm - accuracy: 0.9955, batch_loss: 0.0227, loss: 0.0158 ||:  65%|######4   | 42/65 [08:04<04:26, 11.58s/it]
2022-04-10 13:49:22,375 - INFO - tqdm - accuracy: 0.9956, batch_loss: 0.0062, loss: 0.0155 ||:  66%|######6   | 43/65 [08:15<04:14, 11.59s/it]
2022-04-10 13:49:33,959 - INFO - tqdm - accuracy: 0.9957, batch_loss: 0.0008, loss: 0.0152 ||:  68%|######7   | 44/65 [08:27<04:03, 11.59s/it]
2022-04-10 13:49:45,522 - INFO - tqdm - accuracy: 0.9958, batch_loss: 0.0115, loss: 0.0151 ||:  69%|######9   | 45/65 [08:39<03:51, 11.58s/it]
2022-04-10 13:49:57,097 - INFO - tqdm - accuracy: 0.9959, batch_loss: 0.0016, loss: 0.0148 ||:  71%|#######   | 46/65 [08:50<03:39, 11.58s/it]
2022-04-10 13:50:08,670 - INFO - tqdm - accuracy: 0.9960, batch_loss: 0.0096, loss: 0.0147 ||:  72%|#######2  | 47/65 [09:02<03:28, 11.58s/it]
2022-04-10 13:50:20,255 - INFO - tqdm - accuracy: 0.9961, batch_loss: 0.0016, loss: 0.0144 ||:  74%|#######3  | 48/65 [09:13<03:16, 11.58s/it]
2022-04-10 13:50:31,857 - INFO - tqdm - accuracy: 0.9962, batch_loss: 0.0015, loss: 0.0142 ||:  75%|#######5  | 49/65 [09:25<03:05, 11.59s/it]
2022-04-10 13:50:43,831 - INFO - tqdm - accuracy: 0.9962, batch_loss: 0.0008, loss: 0.0139 ||:  77%|#######6  | 50/65 [09:37<02:55, 11.70s/it]
2022-04-10 13:50:55,376 - INFO - tqdm - accuracy: 0.9963, batch_loss: 0.0011, loss: 0.0137 ||:  78%|#######8  | 51/65 [09:48<02:43, 11.66s/it]
2022-04-10 13:51:06,950 - INFO - tqdm - accuracy: 0.9964, batch_loss: 0.0132, loss: 0.0136 ||:  80%|########  | 52/65 [10:00<02:31, 11.63s/it]
2022-04-10 13:51:18,494 - INFO - tqdm - accuracy: 0.9965, batch_loss: 0.0111, loss: 0.0136 ||:  82%|########1 | 53/65 [10:12<02:19, 11.60s/it]
2022-04-10 13:51:30,067 - INFO - tqdm - accuracy: 0.9965, batch_loss: 0.0032, loss: 0.0134 ||:  83%|########3 | 54/65 [10:23<02:07, 11.60s/it]
2022-04-10 13:51:41,614 - INFO - tqdm - accuracy: 0.9966, batch_loss: 0.0058, loss: 0.0133 ||:  85%|########4 | 55/65 [10:35<01:55, 11.58s/it]
2022-04-10 13:51:53,161 - INFO - tqdm - accuracy: 0.9966, batch_loss: 0.0017, loss: 0.0131 ||:  86%|########6 | 56/65 [10:46<01:44, 11.57s/it]
2022-04-10 13:52:04,704 - INFO - tqdm - accuracy: 0.9962, batch_loss: 0.0606, loss: 0.0139 ||:  88%|########7 | 57/65 [10:58<01:32, 11.56s/it]
2022-04-10 13:52:16,246 - INFO - tqdm - accuracy: 0.9962, batch_loss: 0.0045, loss: 0.0137 ||:  89%|########9 | 58/65 [11:09<01:20, 11.56s/it]
2022-04-10 13:52:27,791 - INFO - tqdm - accuracy: 0.9963, batch_loss: 0.0040, loss: 0.0136 ||:  91%|######### | 59/65 [11:21<01:09, 11.55s/it]
2022-04-10 13:52:39,334 - INFO - tqdm - accuracy: 0.9964, batch_loss: 0.0012, loss: 0.0134 ||:  92%|#########2| 60/65 [11:32<00:57, 11.55s/it]
2022-04-10 13:52:50,875 - INFO - tqdm - accuracy: 0.9964, batch_loss: 0.0011, loss: 0.0132 ||:  94%|#########3| 61/65 [11:44<00:46, 11.55s/it]
2022-04-10 13:53:02,413 - INFO - tqdm - accuracy: 0.9965, batch_loss: 0.0020, loss: 0.0130 ||:  95%|#########5| 62/65 [11:55<00:34, 11.54s/it]
2022-04-10 13:53:13,972 - INFO - tqdm - accuracy: 0.9965, batch_loss: 0.0009, loss: 0.0128 ||:  97%|#########6| 63/65 [12:07<00:23, 11.55s/it]
2022-04-10 13:53:25,505 - INFO - tqdm - accuracy: 0.9966, batch_loss: 0.0012, loss: 0.0126 ||:  98%|#########8| 64/65 [12:19<00:11, 11.54s/it]
2022-04-10 13:53:30,635 - INFO - tqdm - accuracy: 0.9966, batch_loss: 0.0130, loss: 0.0126 ||: 100%|##########| 65/65 [12:24<00:00,  9.62s/it]
2022-04-10 13:53:30,635 - INFO - tqdm - accuracy: 0.9966, batch_loss: 0.0130, loss: 0.0126 ||: 100%|##########| 65/65 [12:24<00:00, 11.45s/it]
2022-04-10 13:53:33,595 - INFO - allennlp.training.trainer - Validating
2022-04-10 13:53:33,597 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-10 13:53:43,734 - INFO - tqdm - accuracy: 0.7000, batch_loss: 0.0002, loss: 1.8210 ||:  35%|###4      | 40/115 [00:10<00:19,  3.93it/s]
2022-04-10 13:53:53,980 - INFO - tqdm - accuracy: 0.6087, batch_loss: 1.7281, loss: 2.2389 ||:  70%|#######   | 81/115 [00:20<00:08,  3.93it/s]
2022-04-10 13:54:02,688 - INFO - tqdm - accuracy: 0.6201, batch_loss: 4.1409, loss: 2.2136 ||: 100%|##########| 115/115 [00:29<00:00,  3.90it/s]
2022-04-10 13:54:02,688 - INFO - tqdm - accuracy: 0.6201, batch_loss: 4.1409, loss: 2.2136 ||: 100%|##########| 115/115 [00:29<00:00,  3.95it/s]
2022-04-10 13:54:02,688 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-10 13:54:02,689 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.997  |     0.620
2022-04-10 13:54:02,690 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-10 13:54:02,690 - INFO - allennlp.training.tensorboard_writer - loss               |     0.013  |     2.214
2022-04-10 13:54:02,690 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5797.879  |       N/A
2022-04-10 13:54:08,993 - INFO - allennlp.training.trainer - Epoch duration: 0:13:02.554828
2022-04-10 13:54:08,993 - INFO - allennlp.training.trainer - Estimated training time remaining: 0:52:15
2022-04-10 13:54:08,993 - INFO - allennlp.training.trainer - Epoch 11/14
2022-04-10 13:54:08,994 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-10 13:54:08,994 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-10 13:54:08,996 - INFO - allennlp.training.trainer - Training
2022-04-10 13:54:08,996 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-10 13:54:20,454 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0022, loss: 0.0022 ||:   2%|1         | 1/65 [00:11<12:13, 11.46s/it]
2022-04-10 13:54:32,150 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0006, loss: 0.0014 ||:   3%|3         | 2/65 [00:23<12:10, 11.60s/it]
2022-04-10 13:54:43,956 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0014, loss: 0.0014 ||:   5%|4         | 3/65 [00:34<12:04, 11.69s/it]
2022-04-10 13:54:55,474 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0009, loss: 0.0013 ||:   6%|6         | 4/65 [00:46<11:49, 11.62s/it]
2022-04-10 13:55:06,893 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0020, loss: 0.0014 ||:   8%|7         | 5/65 [00:57<11:33, 11.55s/it]
2022-04-10 13:55:18,351 - INFO - tqdm - accuracy: 0.9948, batch_loss: 0.0374, loss: 0.0074 ||:   9%|9         | 6/65 [01:09<11:19, 11.52s/it]
2022-04-10 13:55:29,873 - INFO - tqdm - accuracy: 0.9955, batch_loss: 0.0118, loss: 0.0081 ||:  11%|#         | 7/65 [01:20<11:08, 11.52s/it]
2022-04-10 13:55:41,466 - INFO - tqdm - accuracy: 0.9961, batch_loss: 0.0006, loss: 0.0071 ||:  12%|#2        | 8/65 [01:32<10:57, 11.54s/it]
2022-04-10 13:55:53,049 - INFO - tqdm - accuracy: 0.9965, batch_loss: 0.0012, loss: 0.0065 ||:  14%|#3        | 9/65 [01:44<10:47, 11.56s/it]
2022-04-10 13:56:04,625 - INFO - tqdm - accuracy: 0.9969, batch_loss: 0.0007, loss: 0.0059 ||:  15%|#5        | 10/65 [01:55<10:35, 11.56s/it]
2022-04-10 13:56:16,167 - INFO - tqdm - accuracy: 0.9972, batch_loss: 0.0008, loss: 0.0054 ||:  17%|#6        | 11/65 [02:07<10:24, 11.56s/it]
2022-04-10 13:56:27,692 - INFO - tqdm - accuracy: 0.9974, batch_loss: 0.0006, loss: 0.0050 ||:  18%|#8        | 12/65 [02:18<10:11, 11.55s/it]
2022-04-10 13:56:39,171 - INFO - tqdm - accuracy: 0.9976, batch_loss: 0.0010, loss: 0.0047 ||:  20%|##        | 13/65 [02:30<09:59, 11.53s/it]
2022-04-10 13:56:50,655 - INFO - tqdm - accuracy: 0.9955, batch_loss: 0.0249, loss: 0.0062 ||:  22%|##1       | 14/65 [02:41<09:47, 11.51s/it]
2022-04-10 13:57:02,185 - INFO - tqdm - accuracy: 0.9958, batch_loss: 0.0007, loss: 0.0058 ||:  23%|##3       | 15/65 [02:53<09:35, 11.52s/it]
2022-04-10 13:57:13,582 - INFO - tqdm - accuracy: 0.9961, batch_loss: 0.0095, loss: 0.0060 ||:  25%|##4       | 16/65 [03:04<09:22, 11.48s/it]
2022-04-10 13:57:25,109 - INFO - tqdm - accuracy: 0.9963, batch_loss: 0.0009, loss: 0.0057 ||:  26%|##6       | 17/65 [03:16<09:11, 11.50s/it]
2022-04-10 13:57:36,638 - INFO - tqdm - accuracy: 0.9965, batch_loss: 0.0009, loss: 0.0055 ||:  28%|##7       | 18/65 [03:27<09:00, 11.51s/it]
2022-04-10 13:57:48,162 - INFO - tqdm - accuracy: 0.9967, batch_loss: 0.0008, loss: 0.0052 ||:  29%|##9       | 19/65 [03:39<08:49, 11.51s/it]
2022-04-10 13:57:59,690 - INFO - tqdm - accuracy: 0.9969, batch_loss: 0.0013, loss: 0.0050 ||:  31%|###       | 20/65 [03:50<08:38, 11.52s/it]
2022-04-10 13:58:11,247 - INFO - tqdm - accuracy: 0.9970, batch_loss: 0.0013, loss: 0.0048 ||:  32%|###2      | 21/65 [04:02<08:27, 11.53s/it]
2022-04-10 13:58:22,799 - INFO - tqdm - accuracy: 0.9972, batch_loss: 0.0105, loss: 0.0051 ||:  34%|###3      | 22/65 [04:13<08:16, 11.54s/it]
2022-04-10 13:58:34,353 - INFO - tqdm - accuracy: 0.9973, batch_loss: 0.0045, loss: 0.0051 ||:  35%|###5      | 23/65 [04:25<08:04, 11.54s/it]
2022-04-10 13:58:45,893 - INFO - tqdm - accuracy: 0.9961, batch_loss: 0.0825, loss: 0.0083 ||:  37%|###6      | 24/65 [04:36<07:53, 11.54s/it]
2022-04-10 13:58:57,443 - INFO - tqdm - accuracy: 0.9962, batch_loss: 0.0007, loss: 0.0080 ||:  38%|###8      | 25/65 [04:48<07:41, 11.54s/it]
2022-04-10 13:59:09,013 - INFO - tqdm - accuracy: 0.9964, batch_loss: 0.0013, loss: 0.0077 ||:  40%|####      | 26/65 [05:00<07:30, 11.55s/it]
2022-04-10 13:59:20,580 - INFO - tqdm - accuracy: 0.9965, batch_loss: 0.0086, loss: 0.0078 ||:  42%|####1     | 27/65 [05:11<07:19, 11.56s/it]
2022-04-10 13:59:32,157 - INFO - tqdm - accuracy: 0.9967, batch_loss: 0.0006, loss: 0.0075 ||:  43%|####3     | 28/65 [05:23<07:07, 11.56s/it]
2022-04-10 13:59:43,712 - INFO - tqdm - accuracy: 0.9968, batch_loss: 0.0006, loss: 0.0073 ||:  45%|####4     | 29/65 [05:34<06:56, 11.56s/it]
2022-04-10 13:59:55,270 - INFO - tqdm - accuracy: 0.9969, batch_loss: 0.0065, loss: 0.0072 ||:  46%|####6     | 30/65 [05:46<06:44, 11.56s/it]
2022-04-10 14:00:06,838 - INFO - tqdm - accuracy: 0.9970, batch_loss: 0.0023, loss: 0.0071 ||:  48%|####7     | 31/65 [05:57<06:33, 11.56s/it]
2022-04-10 14:00:18,412 - INFO - tqdm - accuracy: 0.9971, batch_loss: 0.0007, loss: 0.0069 ||:  49%|####9     | 32/65 [06:09<06:21, 11.57s/it]
2022-04-10 14:00:29,972 - INFO - tqdm - accuracy: 0.9972, batch_loss: 0.0023, loss: 0.0067 ||:  51%|#####     | 33/65 [06:20<06:10, 11.56s/it]
2022-04-10 14:00:41,548 - INFO - tqdm - accuracy: 0.9972, batch_loss: 0.0006, loss: 0.0066 ||:  52%|#####2    | 34/65 [06:32<05:58, 11.57s/it]
2022-04-10 14:00:53,156 - INFO - tqdm - accuracy: 0.9973, batch_loss: 0.0009, loss: 0.0064 ||:  54%|#####3    | 35/65 [06:44<05:47, 11.58s/it]
2022-04-10 14:01:04,728 - INFO - tqdm - accuracy: 0.9974, batch_loss: 0.0121, loss: 0.0066 ||:  55%|#####5    | 36/65 [06:55<05:35, 11.58s/it]
2022-04-10 14:01:16,311 - INFO - tqdm - accuracy: 0.9975, batch_loss: 0.0134, loss: 0.0068 ||:  57%|#####6    | 37/65 [07:07<05:24, 11.58s/it]
2022-04-10 14:01:27,894 - INFO - tqdm - accuracy: 0.9975, batch_loss: 0.0008, loss: 0.0066 ||:  58%|#####8    | 38/65 [07:18<05:12, 11.58s/it]
2022-04-10 14:01:39,457 - INFO - tqdm - accuracy: 0.9960, batch_loss: 0.2010, loss: 0.0116 ||:  60%|######    | 39/65 [07:30<05:00, 11.57s/it]
2022-04-10 14:01:51,036 - INFO - tqdm - accuracy: 0.9961, batch_loss: 0.0008, loss: 0.0113 ||:  62%|######1   | 40/65 [07:42<04:49, 11.58s/it]
2022-04-10 14:02:02,585 - INFO - tqdm - accuracy: 0.9962, batch_loss: 0.0019, loss: 0.0111 ||:  63%|######3   | 41/65 [07:53<04:37, 11.57s/it]
2022-04-10 14:02:14,135 - INFO - tqdm - accuracy: 0.9963, batch_loss: 0.0008, loss: 0.0108 ||:  65%|######4   | 42/65 [08:05<04:25, 11.56s/it]
2022-04-10 14:02:25,560 - INFO - tqdm - accuracy: 0.9956, batch_loss: 0.0518, loss: 0.0118 ||:  66%|######6   | 43/65 [08:16<04:13, 11.52s/it]
2022-04-10 14:02:37,124 - INFO - tqdm - accuracy: 0.9957, batch_loss: 0.0008, loss: 0.0115 ||:  68%|######7   | 44/65 [08:28<04:02, 11.53s/it]
2022-04-10 14:02:48,335 - INFO - tqdm - accuracy: 0.9951, batch_loss: 0.0249, loss: 0.0118 ||:  69%|######9   | 45/65 [08:39<03:48, 11.44s/it]
2022-04-10 14:02:59,876 - INFO - tqdm - accuracy: 0.9952, batch_loss: 0.0056, loss: 0.0117 ||:  71%|#######   | 46/65 [08:50<03:37, 11.47s/it]
2022-04-10 14:03:11,433 - INFO - tqdm - accuracy: 0.9953, batch_loss: 0.0006, loss: 0.0115 ||:  72%|#######2  | 47/65 [09:02<03:26, 11.49s/it]
2022-04-10 14:03:22,978 - INFO - tqdm - accuracy: 0.9954, batch_loss: 0.0083, loss: 0.0114 ||:  74%|#######3  | 48/65 [09:13<03:15, 11.51s/it]
2022-04-10 14:03:34,517 - INFO - tqdm - accuracy: 0.9955, batch_loss: 0.0068, loss: 0.0113 ||:  75%|#######5  | 49/65 [09:25<03:04, 11.52s/it]
2022-04-10 14:03:46,048 - INFO - tqdm - accuracy: 0.9956, batch_loss: 0.0074, loss: 0.0112 ||:  77%|#######6  | 50/65 [09:37<02:52, 11.52s/it]
2022-04-10 14:03:57,607 - INFO - tqdm - accuracy: 0.9957, batch_loss: 0.0032, loss: 0.0111 ||:  78%|#######8  | 51/65 [09:48<02:41, 11.53s/it]
2022-04-10 14:04:09,144 - INFO - tqdm - accuracy: 0.9958, batch_loss: 0.0038, loss: 0.0109 ||:  80%|########  | 52/65 [10:00<02:29, 11.53s/it]
2022-04-10 14:04:20,672 - INFO - tqdm - accuracy: 0.9959, batch_loss: 0.0036, loss: 0.0108 ||:  82%|########1 | 53/65 [10:11<02:18, 11.53s/it]
2022-04-10 14:04:32,198 - INFO - tqdm - accuracy: 0.9959, batch_loss: 0.0009, loss: 0.0106 ||:  83%|########3 | 54/65 [10:23<02:06, 11.53s/it]
2022-04-10 14:04:43,723 - INFO - tqdm - accuracy: 0.9960, batch_loss: 0.0034, loss: 0.0105 ||:  85%|########4 | 55/65 [10:34<01:55, 11.53s/it]
2022-04-10 14:04:55,257 - INFO - tqdm - accuracy: 0.9961, batch_loss: 0.0013, loss: 0.0103 ||:  86%|########6 | 56/65 [10:46<01:43, 11.53s/it]
2022-04-10 14:05:06,515 - INFO - tqdm - accuracy: 0.9962, batch_loss: 0.0011, loss: 0.0102 ||:  88%|########7 | 57/65 [10:57<01:31, 11.45s/it]
2022-04-10 14:05:18,057 - INFO - tqdm - accuracy: 0.9957, batch_loss: 0.0293, loss: 0.0105 ||:  89%|########9 | 58/65 [11:09<01:20, 11.48s/it]
2022-04-10 14:05:29,588 - INFO - tqdm - accuracy: 0.9952, batch_loss: 0.1547, loss: 0.0129 ||:  91%|######### | 59/65 [11:20<01:08, 11.49s/it]
2022-04-10 14:05:41,115 - INFO - tqdm - accuracy: 0.9953, batch_loss: 0.0004, loss: 0.0127 ||:  92%|#########2| 60/65 [11:32<00:57, 11.50s/it]
2022-04-10 14:05:52,629 - INFO - tqdm - accuracy: 0.9954, batch_loss: 0.0037, loss: 0.0126 ||:  94%|#########3| 61/65 [11:43<00:46, 11.51s/it]
2022-04-10 14:06:04,155 - INFO - tqdm - accuracy: 0.9955, batch_loss: 0.0026, loss: 0.0124 ||:  95%|#########5| 62/65 [11:55<00:34, 11.51s/it]
2022-04-10 14:06:15,683 - INFO - tqdm - accuracy: 0.9955, batch_loss: 0.0011, loss: 0.0122 ||:  97%|#########6| 63/65 [12:06<00:23, 11.52s/it]
2022-04-10 14:06:27,221 - INFO - tqdm - accuracy: 0.9956, batch_loss: 0.0078, loss: 0.0122 ||:  98%|#########8| 64/65 [12:18<00:11, 11.52s/it]
2022-04-10 14:06:32,336 - INFO - tqdm - accuracy: 0.9956, batch_loss: 0.0004, loss: 0.0120 ||: 100%|##########| 65/65 [12:23<00:00,  9.60s/it]
2022-04-10 14:06:32,336 - INFO - tqdm - accuracy: 0.9956, batch_loss: 0.0004, loss: 0.0120 ||: 100%|##########| 65/65 [12:23<00:00, 11.44s/it]
2022-04-10 14:06:35,284 - INFO - allennlp.training.trainer - Validating
2022-04-10 14:06:35,285 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-10 14:06:45,401 - INFO - tqdm - accuracy: 0.5625, batch_loss: 0.0003, loss: 2.8469 ||:  35%|###4      | 40/115 [00:10<00:19,  3.93it/s]
2022-04-10 14:06:55,480 - INFO - tqdm - accuracy: 0.6164, batch_loss: 0.0210, loss: 2.4912 ||:  70%|######9   | 80/115 [00:20<00:07,  4.54it/s]
2022-04-10 14:07:04,459 - INFO - tqdm - accuracy: 0.6288, batch_loss: 2.2216, loss: 2.3550 ||: 100%|##########| 115/115 [00:29<00:00,  3.86it/s]
2022-04-10 14:07:04,460 - INFO - tqdm - accuracy: 0.6288, batch_loss: 2.2216, loss: 2.3550 ||: 100%|##########| 115/115 [00:29<00:00,  3.94it/s]
2022-04-10 14:07:04,460 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-10 14:07:04,461 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.996  |     0.629
2022-04-10 14:07:04,462 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-10 14:07:04,462 - INFO - allennlp.training.tensorboard_writer - loss               |     0.012  |     2.355
2022-04-10 14:07:04,462 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5797.879  |       N/A
2022-04-10 14:07:10,638 - INFO - allennlp.training.trainer - Epoch duration: 0:13:01.644905
2022-04-10 14:07:10,639 - INFO - allennlp.training.trainer - Estimated training time remaining: 0:39:11
2022-04-10 14:07:10,639 - INFO - allennlp.training.trainer - Epoch 12/14
2022-04-10 14:07:10,639 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-10 14:07:10,639 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-10 14:07:10,640 - INFO - allennlp.training.trainer - Training
2022-04-10 14:07:10,641 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-10 14:07:22,017 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0017, loss: 0.0017 ||:   2%|1         | 1/65 [00:11<12:08, 11.38s/it]
2022-04-10 14:07:33,604 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0408, loss: 0.0212 ||:   3%|3         | 2/65 [00:22<12:04, 11.50s/it]
2022-04-10 14:07:45,287 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.0013, loss: 0.0146 ||:   5%|4         | 3/65 [00:34<11:58, 11.58s/it]
2022-04-10 14:07:56,899 - INFO - tqdm - accuracy: 0.9922, batch_loss: 0.0010, loss: 0.0112 ||:   6%|6         | 4/65 [00:46<11:47, 11.59s/it]
2022-04-10 14:08:08,342 - INFO - tqdm - accuracy: 0.9938, batch_loss: 0.0009, loss: 0.0091 ||:   8%|7         | 5/65 [00:57<11:32, 11.54s/it]
2022-04-10 14:08:19,797 - INFO - tqdm - accuracy: 0.9948, batch_loss: 0.0033, loss: 0.0082 ||:   9%|9         | 6/65 [01:09<11:19, 11.51s/it]
2022-04-10 14:08:31,318 - INFO - tqdm - accuracy: 0.9955, batch_loss: 0.0028, loss: 0.0074 ||:  11%|#         | 7/65 [01:20<11:07, 11.51s/it]
2022-04-10 14:08:42,889 - INFO - tqdm - accuracy: 0.9961, batch_loss: 0.0003, loss: 0.0065 ||:  12%|#2        | 8/65 [01:32<10:57, 11.53s/it]
2022-04-10 14:08:54,499 - INFO - tqdm - accuracy: 0.9965, batch_loss: 0.0056, loss: 0.0064 ||:  14%|#3        | 9/65 [01:43<10:47, 11.56s/it]
2022-04-10 14:09:06,091 - INFO - tqdm - accuracy: 0.9969, batch_loss: 0.0005, loss: 0.0058 ||:  15%|#5        | 10/65 [01:55<10:36, 11.57s/it]
2022-04-10 14:09:17,624 - INFO - tqdm - accuracy: 0.9972, batch_loss: 0.0004, loss: 0.0053 ||:  17%|#6        | 11/65 [02:06<10:24, 11.56s/it]
2022-04-10 14:09:29,144 - INFO - tqdm - accuracy: 0.9974, batch_loss: 0.0011, loss: 0.0050 ||:  18%|#8        | 12/65 [02:18<10:11, 11.55s/it]
2022-04-10 14:09:40,311 - INFO - tqdm - accuracy: 0.9976, batch_loss: 0.0122, loss: 0.0055 ||:  20%|##        | 13/65 [02:29<09:54, 11.43s/it]
2022-04-10 14:09:51,802 - INFO - tqdm - accuracy: 0.9955, batch_loss: 0.1478, loss: 0.0157 ||:  22%|##1       | 14/65 [02:41<09:43, 11.45s/it]
2022-04-10 14:10:03,281 - INFO - tqdm - accuracy: 0.9958, batch_loss: 0.0017, loss: 0.0148 ||:  23%|##3       | 15/65 [02:52<09:32, 11.46s/it]
2022-04-10 14:10:14,783 - INFO - tqdm - accuracy: 0.9961, batch_loss: 0.0011, loss: 0.0139 ||:  25%|##4       | 16/65 [03:04<09:22, 11.47s/it]
2022-04-10 14:10:26,292 - INFO - tqdm - accuracy: 0.9963, batch_loss: 0.0007, loss: 0.0131 ||:  26%|##6       | 17/65 [03:15<09:11, 11.48s/it]
2022-04-10 14:10:37,802 - INFO - tqdm - accuracy: 0.9965, batch_loss: 0.0008, loss: 0.0125 ||:  28%|##7       | 18/65 [03:27<09:00, 11.49s/it]
2022-04-10 14:10:49,317 - INFO - tqdm - accuracy: 0.9967, batch_loss: 0.0004, loss: 0.0118 ||:  29%|##9       | 19/65 [03:38<08:48, 11.50s/it]
2022-04-10 14:11:01,418 - INFO - tqdm - accuracy: 0.9969, batch_loss: 0.0010, loss: 0.0113 ||:  31%|###       | 20/65 [03:50<08:45, 11.68s/it]
2022-04-10 14:11:12,906 - INFO - tqdm - accuracy: 0.9970, batch_loss: 0.0139, loss: 0.0114 ||:  32%|###2      | 21/65 [04:02<08:31, 11.62s/it]
2022-04-10 14:11:24,418 - INFO - tqdm - accuracy: 0.9972, batch_loss: 0.0005, loss: 0.0109 ||:  34%|###3      | 22/65 [04:13<08:18, 11.59s/it]
2022-04-10 14:11:35,933 - INFO - tqdm - accuracy: 0.9973, batch_loss: 0.0006, loss: 0.0105 ||:  35%|###5      | 23/65 [04:25<08:05, 11.57s/it]
2022-04-10 14:11:47,447 - INFO - tqdm - accuracy: 0.9974, batch_loss: 0.0119, loss: 0.0105 ||:  37%|###6      | 24/65 [04:36<07:53, 11.55s/it]
2022-04-10 14:11:58,734 - INFO - tqdm - accuracy: 0.9975, batch_loss: 0.0015, loss: 0.0102 ||:  38%|###8      | 25/65 [04:48<07:38, 11.47s/it]
2022-04-10 14:12:10,277 - INFO - tqdm - accuracy: 0.9976, batch_loss: 0.0008, loss: 0.0098 ||:  40%|####      | 26/65 [04:59<07:28, 11.49s/it]
2022-04-10 14:12:21,819 - INFO - tqdm - accuracy: 0.9977, batch_loss: 0.0004, loss: 0.0095 ||:  42%|####1     | 27/65 [05:11<07:17, 11.51s/it]
2022-04-10 14:12:33,368 - INFO - tqdm - accuracy: 0.9978, batch_loss: 0.0006, loss: 0.0091 ||:  43%|####3     | 28/65 [05:22<07:06, 11.52s/it]
2022-04-10 14:12:44,928 - INFO - tqdm - accuracy: 0.9978, batch_loss: 0.0004, loss: 0.0088 ||:  45%|####4     | 29/65 [05:34<06:55, 11.53s/it]
2022-04-10 14:12:56,512 - INFO - tqdm - accuracy: 0.9969, batch_loss: 0.0441, loss: 0.0100 ||:  46%|####6     | 30/65 [05:45<06:44, 11.55s/it]
2022-04-10 14:13:08,083 - INFO - tqdm - accuracy: 0.9970, batch_loss: 0.0006, loss: 0.0097 ||:  48%|####7     | 31/65 [05:57<06:32, 11.55s/it]
2022-04-10 14:13:19,538 - INFO - tqdm - accuracy: 0.9971, batch_loss: 0.0005, loss: 0.0094 ||:  49%|####9     | 32/65 [06:08<06:20, 11.52s/it]
2022-04-10 14:13:31,121 - INFO - tqdm - accuracy: 0.9972, batch_loss: 0.0021, loss: 0.0092 ||:  51%|#####     | 33/65 [06:20<06:09, 11.54s/it]
2022-04-10 14:13:42,686 - INFO - tqdm - accuracy: 0.9972, batch_loss: 0.0005, loss: 0.0089 ||:  52%|#####2    | 34/65 [06:32<05:58, 11.55s/it]
2022-04-10 14:13:54,258 - INFO - tqdm - accuracy: 0.9973, batch_loss: 0.0007, loss: 0.0087 ||:  54%|#####3    | 35/65 [06:43<05:46, 11.56s/it]
2022-04-10 14:14:05,857 - INFO - tqdm - accuracy: 0.9974, batch_loss: 0.0005, loss: 0.0085 ||:  55%|#####5    | 36/65 [06:55<05:35, 11.57s/it]
2022-04-10 14:14:17,444 - INFO - tqdm - accuracy: 0.9975, batch_loss: 0.0022, loss: 0.0083 ||:  57%|#####6    | 37/65 [07:06<05:24, 11.57s/it]
2022-04-10 14:14:29,034 - INFO - tqdm - accuracy: 0.9975, batch_loss: 0.0072, loss: 0.0083 ||:  58%|#####8    | 38/65 [07:18<05:12, 11.58s/it]
2022-04-10 14:14:40,614 - INFO - tqdm - accuracy: 0.9976, batch_loss: 0.0099, loss: 0.0083 ||:  60%|######    | 39/65 [07:29<05:01, 11.58s/it]
2022-04-10 14:14:52,190 - INFO - tqdm - accuracy: 0.9977, batch_loss: 0.0003, loss: 0.0081 ||:  62%|######1   | 40/65 [07:41<04:49, 11.58s/it]
2022-04-10 14:15:03,782 - INFO - tqdm - accuracy: 0.9977, batch_loss: 0.0006, loss: 0.0079 ||:  63%|######3   | 41/65 [07:53<04:37, 11.58s/it]
2022-04-10 14:15:15,367 - INFO - tqdm - accuracy: 0.9978, batch_loss: 0.0010, loss: 0.0078 ||:  65%|######4   | 42/65 [08:04<04:26, 11.58s/it]
2022-04-10 14:15:26,957 - INFO - tqdm - accuracy: 0.9978, batch_loss: 0.0004, loss: 0.0076 ||:  66%|######6   | 43/65 [08:16<04:14, 11.59s/it]
2022-04-10 14:15:38,551 - INFO - tqdm - accuracy: 0.9979, batch_loss: 0.0005, loss: 0.0074 ||:  68%|######7   | 44/65 [08:27<04:03, 11.59s/it]
2022-04-10 14:15:50,141 - INFO - tqdm - accuracy: 0.9979, batch_loss: 0.0004, loss: 0.0073 ||:  69%|######9   | 45/65 [08:39<03:51, 11.59s/it]
2022-04-10 14:16:01,710 - INFO - tqdm - accuracy: 0.9980, batch_loss: 0.0059, loss: 0.0073 ||:  71%|#######   | 46/65 [08:51<03:40, 11.58s/it]
2022-04-10 14:16:13,299 - INFO - tqdm - accuracy: 0.9980, batch_loss: 0.0003, loss: 0.0071 ||:  72%|#######2  | 47/65 [09:02<03:28, 11.58s/it]
2022-04-10 14:16:24,859 - INFO - tqdm - accuracy: 0.9980, batch_loss: 0.0005, loss: 0.0070 ||:  74%|#######3  | 48/65 [09:14<03:16, 11.58s/it]
2022-04-10 14:16:36,407 - INFO - tqdm - accuracy: 0.9981, batch_loss: 0.0004, loss: 0.0068 ||:  75%|#######5  | 49/65 [09:25<03:05, 11.57s/it]
2022-04-10 14:16:47,985 - INFO - tqdm - accuracy: 0.9981, batch_loss: 0.0012, loss: 0.0067 ||:  77%|#######6  | 50/65 [09:37<02:53, 11.57s/it]
2022-04-10 14:16:59,535 - INFO - tqdm - accuracy: 0.9982, batch_loss: 0.0038, loss: 0.0067 ||:  78%|#######8  | 51/65 [09:48<02:41, 11.56s/it]
2022-04-10 14:17:11,080 - INFO - tqdm - accuracy: 0.9982, batch_loss: 0.0008, loss: 0.0066 ||:  80%|########  | 52/65 [10:00<02:30, 11.56s/it]
2022-04-10 14:17:22,645 - INFO - tqdm - accuracy: 0.9982, batch_loss: 0.0005, loss: 0.0064 ||:  82%|########1 | 53/65 [10:12<02:18, 11.56s/it]
2022-04-10 14:17:34,173 - INFO - tqdm - accuracy: 0.9983, batch_loss: 0.0004, loss: 0.0063 ||:  83%|########3 | 54/65 [10:23<02:07, 11.55s/it]
2022-04-10 14:17:45,732 - INFO - tqdm - accuracy: 0.9983, batch_loss: 0.0005, loss: 0.0062 ||:  85%|########4 | 55/65 [10:35<01:55, 11.55s/it]
2022-04-10 14:17:57,278 - INFO - tqdm - accuracy: 0.9983, batch_loss: 0.0003, loss: 0.0061 ||:  86%|########6 | 56/65 [10:46<01:43, 11.55s/it]
2022-04-10 14:18:08,806 - INFO - tqdm - accuracy: 0.9984, batch_loss: 0.0005, loss: 0.0060 ||:  88%|########7 | 57/65 [10:58<01:32, 11.54s/it]
2022-04-10 14:18:20,351 - INFO - tqdm - accuracy: 0.9984, batch_loss: 0.0019, loss: 0.0059 ||:  89%|########9 | 58/65 [11:09<01:20, 11.54s/it]
2022-04-10 14:18:31,889 - INFO - tqdm - accuracy: 0.9984, batch_loss: 0.0050, loss: 0.0059 ||:  91%|######### | 59/65 [11:21<01:09, 11.54s/it]
2022-04-10 14:18:43,426 - INFO - tqdm - accuracy: 0.9984, batch_loss: 0.0033, loss: 0.0059 ||:  92%|#########2| 60/65 [11:32<00:57, 11.54s/it]
2022-04-10 14:18:54,964 - INFO - tqdm - accuracy: 0.9985, batch_loss: 0.0008, loss: 0.0058 ||:  94%|#########3| 61/65 [11:44<00:46, 11.54s/it]
2022-04-10 14:19:06,490 - INFO - tqdm - accuracy: 0.9985, batch_loss: 0.0011, loss: 0.0057 ||:  95%|#########5| 62/65 [11:55<00:34, 11.54s/it]
2022-04-10 14:19:18,029 - INFO - tqdm - accuracy: 0.9985, batch_loss: 0.0005, loss: 0.0056 ||:  97%|#########6| 63/65 [12:07<00:23, 11.54s/it]
2022-04-10 14:19:29,422 - INFO - tqdm - accuracy: 0.9985, batch_loss: 0.0007, loss: 0.0056 ||:  98%|#########8| 64/65 [12:18<00:11, 11.49s/it]
2022-04-10 14:19:34,540 - INFO - tqdm - accuracy: 0.9985, batch_loss: 0.0003, loss: 0.0055 ||: 100%|##########| 65/65 [12:23<00:00,  9.58s/it]
2022-04-10 14:19:34,540 - INFO - tqdm - accuracy: 0.9985, batch_loss: 0.0003, loss: 0.0055 ||: 100%|##########| 65/65 [12:23<00:00, 11.44s/it]
2022-04-10 14:19:37,509 - INFO - allennlp.training.trainer - Validating
2022-04-10 14:19:37,510 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-10 14:19:47,663 - INFO - tqdm - accuracy: 0.6296, batch_loss: 5.7812, loss: 2.0444 ||:  36%|###5      | 41/115 [00:10<00:18,  3.97it/s]
2022-04-10 14:19:57,870 - INFO - tqdm - accuracy: 0.6211, batch_loss: 0.2089, loss: 2.1746 ||:  70%|#######   | 81/115 [00:20<00:08,  3.91it/s]
2022-04-10 14:20:06,591 - INFO - tqdm - accuracy: 0.5983, batch_loss: 1.7984, loss: 2.3777 ||: 100%|##########| 115/115 [00:29<00:00,  3.90it/s]
2022-04-10 14:20:06,592 - INFO - tqdm - accuracy: 0.5983, batch_loss: 1.7984, loss: 2.3777 ||: 100%|##########| 115/115 [00:29<00:00,  3.95it/s]
2022-04-10 14:20:06,592 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-10 14:20:06,593 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.999  |     0.598
2022-04-10 14:20:06,593 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-10 14:20:06,594 - INFO - allennlp.training.tensorboard_writer - loss               |     0.005  |     2.378
2022-04-10 14:20:06,595 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5797.879  |       N/A
2022-04-10 14:20:12,768 - INFO - allennlp.training.trainer - Epoch duration: 0:13:02.129105
2022-04-10 14:20:12,768 - INFO - allennlp.training.trainer - Estimated training time remaining: 0:26:07
2022-04-10 14:20:12,768 - INFO - allennlp.training.trainer - Epoch 13/14
2022-04-10 14:20:12,768 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-10 14:20:12,769 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-10 14:20:12,770 - INFO - allennlp.training.trainer - Training
2022-04-10 14:20:12,770 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-10 14:20:24,210 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0009, loss: 0.0009 ||:   2%|1         | 1/65 [00:11<12:12, 11.44s/it]
2022-04-10 14:20:35,910 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0003, loss: 0.0006 ||:   3%|3         | 2/65 [00:23<12:10, 11.59s/it]
2022-04-10 14:20:47,699 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0165, loss: 0.0059 ||:   5%|4         | 3/65 [00:34<12:04, 11.68s/it]
2022-04-10 14:20:59,194 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0005, loss: 0.0046 ||:   6%|6         | 4/65 [00:46<11:48, 11.61s/it]
2022-04-10 14:21:10,650 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0004, loss: 0.0037 ||:   8%|7         | 5/65 [00:57<11:33, 11.55s/it]
2022-04-10 14:21:22,138 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0003, loss: 0.0032 ||:   9%|9         | 6/65 [01:09<11:20, 11.53s/it]
2022-04-10 14:21:33,692 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0007, loss: 0.0028 ||:  11%|#         | 7/65 [01:20<11:09, 11.54s/it]
2022-04-10 14:21:45,271 - INFO - tqdm - accuracy: 0.9961, batch_loss: 0.0248, loss: 0.0056 ||:  12%|#2        | 8/65 [01:32<10:58, 11.55s/it]
2022-04-10 14:21:56,893 - INFO - tqdm - accuracy: 0.9965, batch_loss: 0.0005, loss: 0.0050 ||:  14%|#3        | 9/65 [01:44<10:48, 11.57s/it]
2022-04-10 14:22:08,466 - INFO - tqdm - accuracy: 0.9969, batch_loss: 0.0063, loss: 0.0051 ||:  15%|#5        | 10/65 [01:55<10:36, 11.57s/it]
2022-04-10 14:22:20,034 - INFO - tqdm - accuracy: 0.9972, batch_loss: 0.0004, loss: 0.0047 ||:  17%|#6        | 11/65 [02:07<10:24, 11.57s/it]
2022-04-10 14:22:31,574 - INFO - tqdm - accuracy: 0.9974, batch_loss: 0.0132, loss: 0.0054 ||:  18%|#8        | 12/65 [02:18<10:12, 11.56s/it]
2022-04-10 14:22:43,077 - INFO - tqdm - accuracy: 0.9976, batch_loss: 0.0003, loss: 0.0050 ||:  20%|##        | 13/65 [02:30<10:00, 11.54s/it]
2022-04-10 14:22:54,567 - INFO - tqdm - accuracy: 0.9978, batch_loss: 0.0068, loss: 0.0051 ||:  22%|##1       | 14/65 [02:41<09:47, 11.53s/it]
2022-04-10 14:23:06,085 - INFO - tqdm - accuracy: 0.9979, batch_loss: 0.0004, loss: 0.0048 ||:  23%|##3       | 15/65 [02:53<09:36, 11.52s/it]
2022-04-10 14:23:17,605 - INFO - tqdm - accuracy: 0.9980, batch_loss: 0.0003, loss: 0.0046 ||:  25%|##4       | 16/65 [03:04<09:24, 11.52s/it]
2022-04-10 14:23:29,116 - INFO - tqdm - accuracy: 0.9982, batch_loss: 0.0004, loss: 0.0043 ||:  26%|##6       | 17/65 [03:16<09:12, 11.52s/it]
2022-04-10 14:23:40,320 - INFO - tqdm - accuracy: 0.9983, batch_loss: 0.0003, loss: 0.0041 ||:  28%|##7       | 18/65 [03:27<08:56, 11.42s/it]
2022-04-10 14:23:51,867 - INFO - tqdm - accuracy: 0.9984, batch_loss: 0.0004, loss: 0.0039 ||:  29%|##9       | 19/65 [03:39<08:47, 11.46s/it]
2022-04-10 14:24:03,428 - INFO - tqdm - accuracy: 0.9984, batch_loss: 0.0008, loss: 0.0037 ||:  31%|###       | 20/65 [03:50<08:37, 11.49s/it]
2022-04-10 14:24:15,006 - INFO - tqdm - accuracy: 0.9985, batch_loss: 0.0015, loss: 0.0036 ||:  32%|###2      | 21/65 [04:02<08:26, 11.52s/it]
2022-04-10 14:24:26,566 - INFO - tqdm - accuracy: 0.9986, batch_loss: 0.0003, loss: 0.0035 ||:  34%|###3      | 22/65 [04:13<08:15, 11.53s/it]
2022-04-10 14:24:38,112 - INFO - tqdm - accuracy: 0.9986, batch_loss: 0.0005, loss: 0.0034 ||:  35%|###5      | 23/65 [04:25<08:04, 11.54s/it]
2022-04-10 14:24:49,635 - INFO - tqdm - accuracy: 0.9987, batch_loss: 0.0004, loss: 0.0032 ||:  37%|###6      | 24/65 [04:36<07:52, 11.53s/it]
2022-04-10 14:25:01,199 - INFO - tqdm - accuracy: 0.9987, batch_loss: 0.0005, loss: 0.0031 ||:  38%|###8      | 25/65 [04:48<07:41, 11.54s/it]
2022-04-10 14:25:12,781 - INFO - tqdm - accuracy: 0.9988, batch_loss: 0.0012, loss: 0.0030 ||:  40%|####      | 26/65 [05:00<07:30, 11.55s/it]
2022-04-10 14:25:24,363 - INFO - tqdm - accuracy: 0.9988, batch_loss: 0.0015, loss: 0.0030 ||:  42%|####1     | 27/65 [05:11<07:19, 11.56s/it]
2022-04-10 14:25:35,945 - INFO - tqdm - accuracy: 0.9989, batch_loss: 0.0013, loss: 0.0029 ||:  43%|####3     | 28/65 [05:23<07:08, 11.57s/it]
2022-04-10 14:25:47,523 - INFO - tqdm - accuracy: 0.9989, batch_loss: 0.0046, loss: 0.0030 ||:  45%|####4     | 29/65 [05:34<06:56, 11.57s/it]
2022-04-10 14:25:59,100 - INFO - tqdm - accuracy: 0.9990, batch_loss: 0.0015, loss: 0.0029 ||:  46%|####6     | 30/65 [05:46<06:45, 11.57s/it]
2022-04-10 14:26:10,681 - INFO - tqdm - accuracy: 0.9990, batch_loss: 0.0018, loss: 0.0029 ||:  48%|####7     | 31/65 [05:57<06:33, 11.58s/it]
2022-04-10 14:26:22,276 - INFO - tqdm - accuracy: 0.9990, batch_loss: 0.0003, loss: 0.0028 ||:  49%|####9     | 32/65 [06:09<06:22, 11.58s/it]
2022-04-10 14:26:33,858 - INFO - tqdm - accuracy: 0.9991, batch_loss: 0.0003, loss: 0.0027 ||:  51%|#####     | 33/65 [06:21<06:10, 11.58s/it]
2022-04-10 14:26:45,297 - INFO - tqdm - accuracy: 0.9991, batch_loss: 0.0004, loss: 0.0027 ||:  52%|#####2    | 34/65 [06:32<05:57, 11.54s/it]
2022-04-10 14:26:56,839 - INFO - tqdm - accuracy: 0.9991, batch_loss: 0.0016, loss: 0.0026 ||:  54%|#####3    | 35/65 [06:44<05:46, 11.54s/it]
2022-04-10 14:27:08,382 - INFO - tqdm - accuracy: 0.9991, batch_loss: 0.0005, loss: 0.0026 ||:  55%|#####5    | 36/65 [06:55<05:34, 11.54s/it]
2022-04-10 14:27:19,925 - INFO - tqdm - accuracy: 0.9992, batch_loss: 0.0003, loss: 0.0025 ||:  57%|#####6    | 37/65 [07:07<05:23, 11.54s/it]
2022-04-10 14:27:31,495 - INFO - tqdm - accuracy: 0.9984, batch_loss: 0.0246, loss: 0.0031 ||:  58%|#####8    | 38/65 [07:18<05:11, 11.55s/it]
2022-04-10 14:27:43,054 - INFO - tqdm - accuracy: 0.9976, batch_loss: 0.0958, loss: 0.0055 ||:  60%|######    | 39/65 [07:30<05:00, 11.55s/it]
2022-04-10 14:27:54,573 - INFO - tqdm - accuracy: 0.9977, batch_loss: 0.0102, loss: 0.0056 ||:  62%|######1   | 40/65 [07:41<04:48, 11.54s/it]
2022-04-10 14:28:06,115 - INFO - tqdm - accuracy: 0.9977, batch_loss: 0.0003, loss: 0.0055 ||:  63%|######3   | 41/65 [07:53<04:37, 11.54s/it]
2022-04-10 14:28:17,651 - INFO - tqdm - accuracy: 0.9978, batch_loss: 0.0006, loss: 0.0054 ||:  65%|######4   | 42/65 [08:04<04:25, 11.54s/it]
2022-04-10 14:28:29,179 - INFO - tqdm - accuracy: 0.9978, batch_loss: 0.0003, loss: 0.0052 ||:  66%|######6   | 43/65 [08:16<04:13, 11.54s/it]
2022-04-10 14:28:40,700 - INFO - tqdm - accuracy: 0.9979, batch_loss: 0.0178, loss: 0.0055 ||:  68%|######7   | 44/65 [08:27<04:02, 11.53s/it]
2022-04-10 14:28:52,248 - INFO - tqdm - accuracy: 0.9972, batch_loss: 0.0754, loss: 0.0071 ||:  69%|######9   | 45/65 [08:39<03:50, 11.54s/it]
2022-04-10 14:29:03,528 - INFO - tqdm - accuracy: 0.9973, batch_loss: 0.0004, loss: 0.0069 ||:  71%|#######   | 46/65 [08:50<03:37, 11.46s/it]
2022-04-10 14:29:15,061 - INFO - tqdm - accuracy: 0.9973, batch_loss: 0.0005, loss: 0.0068 ||:  72%|#######2  | 47/65 [09:02<03:26, 11.48s/it]
2022-04-10 14:29:26,587 - INFO - tqdm - accuracy: 0.9974, batch_loss: 0.0002, loss: 0.0067 ||:  74%|#######3  | 48/65 [09:13<03:15, 11.49s/it]
2022-04-10 14:29:38,115 - INFO - tqdm - accuracy: 0.9974, batch_loss: 0.0004, loss: 0.0065 ||:  75%|#######5  | 49/65 [09:25<03:04, 11.50s/it]
2022-04-10 14:29:49,526 - INFO - tqdm - accuracy: 0.9975, batch_loss: 0.0004, loss: 0.0064 ||:  77%|#######6  | 50/65 [09:36<02:52, 11.48s/it]
2022-04-10 14:30:01,044 - INFO - tqdm - accuracy: 0.9975, batch_loss: 0.0082, loss: 0.0064 ||:  78%|#######8  | 51/65 [09:48<02:40, 11.49s/it]
2022-04-10 14:30:12,574 - INFO - tqdm - accuracy: 0.9976, batch_loss: 0.0003, loss: 0.0063 ||:  80%|########  | 52/65 [09:59<02:29, 11.50s/it]
2022-04-10 14:30:24,105 - INFO - tqdm - accuracy: 0.9976, batch_loss: 0.0003, loss: 0.0062 ||:  82%|########1 | 53/65 [10:11<02:18, 11.51s/it]
2022-04-10 14:30:35,641 - INFO - tqdm - accuracy: 0.9977, batch_loss: 0.0003, loss: 0.0061 ||:  83%|########3 | 54/65 [10:22<02:06, 11.52s/it]
2022-04-10 14:30:47,565 - INFO - tqdm - accuracy: 0.9977, batch_loss: 0.0008, loss: 0.0060 ||:  85%|########4 | 55/65 [10:34<01:56, 11.64s/it]
2022-04-10 14:30:59,037 - INFO - tqdm - accuracy: 0.9978, batch_loss: 0.0003, loss: 0.0059 ||:  86%|########6 | 56/65 [10:46<01:44, 11.59s/it]
2022-04-10 14:31:10,545 - INFO - tqdm - accuracy: 0.9978, batch_loss: 0.0013, loss: 0.0058 ||:  88%|########7 | 57/65 [10:57<01:32, 11.56s/it]
2022-04-10 14:31:22,088 - INFO - tqdm - accuracy: 0.9978, batch_loss: 0.0005, loss: 0.0057 ||:  89%|########9 | 58/65 [11:09<01:20, 11.56s/it]
2022-04-10 14:31:33,595 - INFO - tqdm - accuracy: 0.9979, batch_loss: 0.0003, loss: 0.0056 ||:  91%|######### | 59/65 [11:20<01:09, 11.54s/it]
2022-04-10 14:31:45,117 - INFO - tqdm - accuracy: 0.9979, batch_loss: 0.0002, loss: 0.0055 ||:  92%|#########2| 60/65 [11:32<00:57, 11.54s/it]
2022-04-10 14:31:56,653 - INFO - tqdm - accuracy: 0.9979, batch_loss: 0.0035, loss: 0.0055 ||:  94%|#########3| 61/65 [11:43<00:46, 11.54s/it]
2022-04-10 14:32:08,186 - INFO - tqdm - accuracy: 0.9980, batch_loss: 0.0004, loss: 0.0054 ||:  95%|#########5| 62/65 [11:55<00:34, 11.54s/it]
2022-04-10 14:32:19,705 - INFO - tqdm - accuracy: 0.9975, batch_loss: 0.0464, loss: 0.0061 ||:  97%|#########6| 63/65 [12:06<00:23, 11.53s/it]
2022-04-10 14:32:31,242 - INFO - tqdm - accuracy: 0.9976, batch_loss: 0.0002, loss: 0.0060 ||:  98%|#########8| 64/65 [12:18<00:11, 11.53s/it]
2022-04-10 14:32:36,360 - INFO - tqdm - accuracy: 0.9976, batch_loss: 0.0012, loss: 0.0059 ||: 100%|##########| 65/65 [12:23<00:00,  9.61s/it]
2022-04-10 14:32:36,360 - INFO - tqdm - accuracy: 0.9976, batch_loss: 0.0012, loss: 0.0059 ||: 100%|##########| 65/65 [12:23<00:00, 11.44s/it]
2022-04-10 14:32:39,320 - INFO - allennlp.training.trainer - Validating
2022-04-10 14:32:39,322 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-10 14:32:49,562 - INFO - tqdm - accuracy: 0.7160, batch_loss: 0.1674, loss: 1.9468 ||:  36%|###5      | 41/115 [00:10<00:18,  3.96it/s]
2022-04-10 14:32:59,777 - INFO - tqdm - accuracy: 0.6087, batch_loss: 0.0085, loss: 2.6578 ||:  70%|#######   | 81/115 [00:20<00:08,  3.89it/s]
2022-04-10 14:33:08,448 - INFO - tqdm - accuracy: 0.6201, batch_loss: 0.7513, loss: 2.5207 ||: 100%|##########| 115/115 [00:29<00:00,  3.87it/s]
2022-04-10 14:33:08,449 - INFO - tqdm - accuracy: 0.6201, batch_loss: 0.7513, loss: 2.5207 ||: 100%|##########| 115/115 [00:29<00:00,  3.95it/s]
2022-04-10 14:33:08,449 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-10 14:33:08,450 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.998  |     0.620
2022-04-10 14:33:08,451 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-10 14:33:08,451 - INFO - allennlp.training.tensorboard_writer - loss               |     0.006  |     2.521
2022-04-10 14:33:08,452 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5797.879  |       N/A
2022-04-10 14:33:14,670 - INFO - allennlp.training.trainer - Epoch duration: 0:13:01.901559
2022-04-10 14:33:14,670 - INFO - allennlp.training.trainer - Estimated training time remaining: 0:13:03
2022-04-10 14:33:14,670 - INFO - allennlp.training.trainer - Epoch 14/14
2022-04-10 14:33:14,670 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-10 14:33:14,671 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-10 14:33:14,672 - INFO - allennlp.training.trainer - Training
2022-04-10 14:33:14,672 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-10 14:33:26,052 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0004, loss: 0.0004 ||:   2%|1         | 1/65 [00:11<12:08, 11.38s/it]
2022-04-10 14:33:37,721 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0015, loss: 0.0010 ||:   3%|3         | 2/65 [00:23<12:07, 11.55s/it]
2022-04-10 14:33:49,609 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0005, loss: 0.0008 ||:   5%|4         | 3/65 [00:34<12:05, 11.70s/it]
2022-04-10 14:34:01,218 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0004, loss: 0.0007 ||:   6%|6         | 4/65 [00:46<11:51, 11.67s/it]
2022-04-10 14:34:12,663 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0004, loss: 0.0006 ||:   8%|7         | 5/65 [00:57<11:35, 11.59s/it]
2022-04-10 14:34:24,105 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0003, loss: 0.0006 ||:   9%|9         | 6/65 [01:09<11:20, 11.54s/it]
2022-04-10 14:34:35,555 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0004, loss: 0.0006 ||:  11%|#         | 7/65 [01:20<11:07, 11.51s/it]
2022-04-10 14:34:47,090 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0003, loss: 0.0005 ||:  12%|#2        | 8/65 [01:32<10:56, 11.52s/it]
2022-04-10 14:34:58,711 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0003, loss: 0.0005 ||:  14%|#3        | 9/65 [01:44<10:46, 11.55s/it]
2022-04-10 14:35:10,287 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0002, loss: 0.0005 ||:  15%|#5        | 10/65 [01:55<10:35, 11.56s/it]
2022-04-10 14:35:21,814 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0003, loss: 0.0005 ||:  17%|#6        | 11/65 [02:07<10:23, 11.55s/it]
2022-04-10 14:35:33,308 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0004, loss: 0.0005 ||:  18%|#8        | 12/65 [02:18<10:11, 11.53s/it]
2022-04-10 14:35:44,789 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0002, loss: 0.0004 ||:  20%|##        | 13/65 [02:30<09:58, 11.52s/it]
2022-04-10 14:35:56,295 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0005, loss: 0.0004 ||:  22%|##1       | 14/65 [02:41<09:47, 11.51s/it]
2022-04-10 14:36:07,839 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0012, loss: 0.0005 ||:  23%|##3       | 15/65 [02:53<09:36, 11.52s/it]
2022-04-10 14:36:19,412 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0003, loss: 0.0005 ||:  25%|##4       | 16/65 [03:04<09:25, 11.54s/it]
2022-04-10 14:36:31,002 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0032, loss: 0.0006 ||:  26%|##6       | 17/65 [03:16<09:14, 11.55s/it]
2022-04-10 14:36:42,593 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0019, loss: 0.0007 ||:  28%|##7       | 18/65 [03:27<09:03, 11.56s/it]
2022-04-10 14:36:54,164 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0003, loss: 0.0007 ||:  29%|##9       | 19/65 [03:39<08:52, 11.57s/it]
2022-04-10 14:37:05,724 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0002, loss: 0.0007 ||:  31%|###       | 20/65 [03:51<08:40, 11.56s/it]
2022-04-10 14:37:17,299 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0003, loss: 0.0006 ||:  32%|###2      | 21/65 [04:02<08:28, 11.57s/it]
2022-04-10 14:37:28,747 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0003, loss: 0.0006 ||:  34%|###3      | 22/65 [04:14<08:15, 11.53s/it]
2022-04-10 14:37:40,288 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0005, loss: 0.0006 ||:  35%|###5      | 23/65 [04:25<08:04, 11.53s/it]
2022-04-10 14:37:51,822 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0005, loss: 0.0006 ||:  37%|###6      | 24/65 [04:37<07:52, 11.53s/it]
2022-04-10 14:38:03,363 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0003, loss: 0.0006 ||:  38%|###8      | 25/65 [04:48<07:41, 11.54s/it]
2022-04-10 14:38:14,904 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0011, loss: 0.0006 ||:  40%|####      | 26/65 [05:00<07:29, 11.54s/it]
2022-04-10 14:38:26,448 - INFO - tqdm - accuracy: 0.9988, batch_loss: 0.0453, loss: 0.0023 ||:  42%|####1     | 27/65 [05:11<07:18, 11.54s/it]
2022-04-10 14:38:37,988 - INFO - tqdm - accuracy: 0.9989, batch_loss: 0.0004, loss: 0.0022 ||:  43%|####3     | 28/65 [05:23<07:06, 11.54s/it]
2022-04-10 14:38:49,529 - INFO - tqdm - accuracy: 0.9989, batch_loss: 0.0010, loss: 0.0022 ||:  45%|####4     | 29/65 [05:34<06:55, 11.54s/it]
2022-04-10 14:39:01,058 - INFO - tqdm - accuracy: 0.9990, batch_loss: 0.0060, loss: 0.0023 ||:  46%|####6     | 30/65 [05:46<06:43, 11.54s/it]
2022-04-10 14:39:12,597 - INFO - tqdm - accuracy: 0.9990, batch_loss: 0.0003, loss: 0.0022 ||:  48%|####7     | 31/65 [05:57<06:32, 11.54s/it]
2022-04-10 14:39:24,145 - INFO - tqdm - accuracy: 0.9990, batch_loss: 0.0002, loss: 0.0022 ||:  49%|####9     | 32/65 [06:09<06:20, 11.54s/it]
2022-04-10 14:39:35,652 - INFO - tqdm - accuracy: 0.9991, batch_loss: 0.0023, loss: 0.0022 ||:  51%|#####     | 33/65 [06:20<06:08, 11.53s/it]
2022-04-10 14:39:47,179 - INFO - tqdm - accuracy: 0.9991, batch_loss: 0.0006, loss: 0.0021 ||:  52%|#####2    | 34/65 [06:32<05:57, 11.53s/it]
2022-04-10 14:39:58,728 - INFO - tqdm - accuracy: 0.9991, batch_loss: 0.0003, loss: 0.0021 ||:  54%|#####3    | 35/65 [06:44<05:46, 11.54s/it]
2022-04-10 14:40:10,274 - INFO - tqdm - accuracy: 0.9991, batch_loss: 0.0005, loss: 0.0020 ||:  55%|#####5    | 36/65 [06:55<05:34, 11.54s/it]
2022-04-10 14:40:21,822 - INFO - tqdm - accuracy: 0.9992, batch_loss: 0.0004, loss: 0.0020 ||:  57%|#####6    | 37/65 [07:07<05:23, 11.54s/it]
2022-04-10 14:40:33,364 - INFO - tqdm - accuracy: 0.9992, batch_loss: 0.0005, loss: 0.0020 ||:  58%|#####8    | 38/65 [07:18<05:11, 11.54s/it]
2022-04-10 14:40:44,895 - INFO - tqdm - accuracy: 0.9992, batch_loss: 0.0006, loss: 0.0019 ||:  60%|######    | 39/65 [07:30<04:59, 11.54s/it]
2022-04-10 14:40:56,409 - INFO - tqdm - accuracy: 0.9992, batch_loss: 0.0003, loss: 0.0019 ||:  62%|######1   | 40/65 [07:41<04:48, 11.53s/it]
2022-04-10 14:41:07,941 - INFO - tqdm - accuracy: 0.9992, batch_loss: 0.0008, loss: 0.0019 ||:  63%|######3   | 41/65 [07:53<04:36, 11.53s/it]
2022-04-10 14:41:19,476 - INFO - tqdm - accuracy: 0.9993, batch_loss: 0.0002, loss: 0.0018 ||:  65%|######4   | 42/65 [08:04<04:25, 11.53s/it]
2022-04-10 14:41:31,007 - INFO - tqdm - accuracy: 0.9993, batch_loss: 0.0004, loss: 0.0018 ||:  66%|######6   | 43/65 [08:16<04:13, 11.53s/it]
2022-04-10 14:41:42,550 - INFO - tqdm - accuracy: 0.9993, batch_loss: 0.0003, loss: 0.0017 ||:  68%|######7   | 44/65 [08:27<04:02, 11.54s/it]
2022-04-10 14:41:54,081 - INFO - tqdm - accuracy: 0.9993, batch_loss: 0.0003, loss: 0.0017 ||:  69%|######9   | 45/65 [08:39<03:50, 11.53s/it]
2022-04-10 14:42:05,625 - INFO - tqdm - accuracy: 0.9993, batch_loss: 0.0004, loss: 0.0017 ||:  71%|#######   | 46/65 [08:50<03:39, 11.54s/it]
2022-04-10 14:42:17,144 - INFO - tqdm - accuracy: 0.9993, batch_loss: 0.0006, loss: 0.0017 ||:  72%|#######2  | 47/65 [09:02<03:27, 11.53s/it]
2022-04-10 14:42:28,694 - INFO - tqdm - accuracy: 0.9993, batch_loss: 0.0003, loss: 0.0016 ||:  74%|#######3  | 48/65 [09:14<03:16, 11.54s/it]
2022-04-10 14:42:40,219 - INFO - tqdm - accuracy: 0.9994, batch_loss: 0.0002, loss: 0.0016 ||:  75%|#######5  | 49/65 [09:25<03:04, 11.53s/it]
2022-04-10 14:42:51,745 - INFO - tqdm - accuracy: 0.9994, batch_loss: 0.0004, loss: 0.0016 ||:  77%|#######6  | 50/65 [09:37<02:52, 11.53s/it]
2022-04-10 14:43:03,276 - INFO - tqdm - accuracy: 0.9994, batch_loss: 0.0003, loss: 0.0016 ||:  78%|#######8  | 51/65 [09:48<02:41, 11.53s/it]
2022-04-10 14:43:14,344 - INFO - tqdm - accuracy: 0.9994, batch_loss: 0.0004, loss: 0.0015 ||:  80%|########  | 52/65 [09:59<02:28, 11.39s/it]
2022-04-10 14:43:25,867 - INFO - tqdm - accuracy: 0.9994, batch_loss: 0.0005, loss: 0.0015 ||:  82%|########1 | 53/65 [10:11<02:17, 11.43s/it]
2022-04-10 14:43:37,395 - INFO - tqdm - accuracy: 0.9994, batch_loss: 0.0003, loss: 0.0015 ||:  83%|########3 | 54/65 [10:22<02:06, 11.46s/it]
2022-04-10 14:43:48,922 - INFO - tqdm - accuracy: 0.9989, batch_loss: 0.2185, loss: 0.0054 ||:  85%|########4 | 55/65 [10:34<01:54, 11.48s/it]
2022-04-10 14:44:00,220 - INFO - tqdm - accuracy: 0.9989, batch_loss: 0.0005, loss: 0.0053 ||:  86%|########6 | 56/65 [10:45<01:42, 11.43s/it]
2022-04-10 14:44:11,707 - INFO - tqdm - accuracy: 0.9989, batch_loss: 0.0021, loss: 0.0053 ||:  88%|########7 | 57/65 [10:57<01:31, 11.44s/it]
2022-04-10 14:44:23,253 - INFO - tqdm - accuracy: 0.9989, batch_loss: 0.0003, loss: 0.0052 ||:  89%|########9 | 58/65 [11:08<01:20, 11.47s/it]
2022-04-10 14:44:34,800 - INFO - tqdm - accuracy: 0.9989, batch_loss: 0.0009, loss: 0.0051 ||:  91%|######### | 59/65 [11:20<01:08, 11.50s/it]
2022-04-10 14:44:46,335 - INFO - tqdm - accuracy: 0.9990, batch_loss: 0.0002, loss: 0.0051 ||:  92%|#########2| 60/65 [11:31<00:57, 11.51s/it]
2022-04-10 14:44:57,846 - INFO - tqdm - accuracy: 0.9990, batch_loss: 0.0003, loss: 0.0050 ||:  94%|#########3| 61/65 [11:43<00:46, 11.51s/it]
2022-04-10 14:45:09,364 - INFO - tqdm - accuracy: 0.9990, batch_loss: 0.0009, loss: 0.0049 ||:  95%|#########5| 62/65 [11:54<00:34, 11.51s/it]
2022-04-10 14:45:20,894 - INFO - tqdm - accuracy: 0.9985, batch_loss: 0.0352, loss: 0.0054 ||:  97%|#########6| 63/65 [12:06<00:23, 11.52s/it]
2022-04-10 14:45:32,414 - INFO - tqdm - accuracy: 0.9985, batch_loss: 0.0003, loss: 0.0053 ||:  98%|#########8| 64/65 [12:17<00:11, 11.52s/it]
2022-04-10 14:45:37,529 - INFO - tqdm - accuracy: 0.9985, batch_loss: 0.0003, loss: 0.0052 ||: 100%|##########| 65/65 [12:22<00:00,  9.60s/it]
2022-04-10 14:45:37,529 - INFO - tqdm - accuracy: 0.9985, batch_loss: 0.0003, loss: 0.0052 ||: 100%|##########| 65/65 [12:22<00:00, 11.43s/it]
2022-04-10 14:45:40,484 - INFO - allennlp.training.trainer - Validating
2022-04-10 14:45:40,486 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-10 14:45:50,588 - INFO - tqdm - accuracy: 0.6500, batch_loss: 0.0521, loss: 2.3252 ||:  35%|###4      | 40/115 [00:10<00:19,  3.93it/s]
2022-04-10 14:46:00,781 - INFO - tqdm - accuracy: 0.6312, batch_loss: 4.8946, loss: 2.3408 ||:  70%|######9   | 80/115 [00:20<00:08,  3.91it/s]
2022-04-10 14:46:09,528 - INFO - tqdm - accuracy: 0.5983, batch_loss: 3.3707, loss: 2.5628 ||: 100%|##########| 115/115 [00:29<00:00,  3.90it/s]
2022-04-10 14:46:09,529 - INFO - tqdm - accuracy: 0.5983, batch_loss: 3.3707, loss: 2.5628 ||: 100%|##########| 115/115 [00:29<00:00,  3.96it/s]
2022-04-10 14:46:09,529 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-10 14:46:09,529 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.999  |     0.598
2022-04-10 14:46:09,530 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-10 14:46:09,530 - INFO - allennlp.training.tensorboard_writer - loss               |     0.005  |     2.563
2022-04-10 14:46:09,530 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5797.879  |       N/A
2022-04-10 14:46:15,675 - INFO - allennlp.training.trainer - Epoch duration: 0:13:01.004493
2022-04-10 14:46:15,675 - INFO - allennlp.training.checkpointer - loading best weights
2022-04-10 14:46:16,682 - INFO - allennlp.common.util - Metrics: {
  "best_epoch": 7,
  "peak_worker_0_memory_MB": 5797.87890625,
  "peak_gpu_0_memory_MB": 9580.5712890625,
  "training_duration": "3:15:43.329233",
  "training_start_epoch": 0,
  "training_epochs": 14,
  "epoch": 14,
  "training_accuracy": 0.9985443959243085,
  "training_loss": 0.005228589716401845,
  "training_worker_0_memory_MB": 5797.87890625,
  "training_gpu_0_memory_MB": 9580.5712890625,
  "validation_accuracy": 0.5982532751091703,
  "validation_loss": 2.5627825923563923,
  "best_validation_accuracy": 0.6593886462882096,
  "best_validation_loss": 2.0602111723904155
}
2022-04-10 14:46:16,682 - INFO - allennlp.models.archival - archiving weights and vocabulary to ./output_ir-ora-d_hyper3/model.tar.gz
