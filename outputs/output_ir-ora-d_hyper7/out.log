2022-04-11 15:19:34,630 - INFO - allennlp.common.params - random_seed = 42
2022-04-11 15:19:34,630 - INFO - allennlp.common.params - numpy_seed = 42
2022-04-11 15:19:34,630 - INFO - allennlp.common.params - pytorch_seed = 42
2022-04-11 15:19:35,329 - INFO - allennlp.common.checks - Pytorch version: 1.7.1
2022-04-11 15:19:35,329 - INFO - allennlp.common.params - type = default
2022-04-11 15:19:35,330 - INFO - allennlp.common.params - dataset_reader.type = strategy_qa_reader
2022-04-11 15:19:35,330 - INFO - allennlp.common.params - dataset_reader.lazy = False
2022-04-11 15:19:35,330 - INFO - allennlp.common.params - dataset_reader.cache_directory = None
2022-04-11 15:19:35,330 - INFO - allennlp.common.params - dataset_reader.max_instances = None
2022-04-11 15:19:35,330 - INFO - allennlp.common.params - dataset_reader.manual_distributed_sharding = False
2022-04-11 15:19:35,330 - INFO - allennlp.common.params - dataset_reader.manual_multi_process_sharding = False
2022-04-11 15:19:35,330 - INFO - allennlp.common.params - dataset_reader.tokenizer_wrapper.type = default
2022-04-11 15:19:35,331 - INFO - allennlp.common.params - dataset_reader.tokenizer_wrapper.pretrained_model = roberta-large
2022-04-11 15:19:35,331 - INFO - allennlp.common.params - dataset_reader.tokenizer_wrapper.call_kwargs.return_offsets_mapping = True
2022-04-11 15:19:35,331 - INFO - allennlp.common.params - dataset_reader.tokenizer_wrapper.call_kwargs.return_special_tokens_mask = True
2022-04-11 15:19:35,331 - INFO - allennlp.common.params - dataset_reader.tokenizer_wrapper.call_kwargs.truncation = only_first
2022-04-11 15:19:36,750 - INFO - allennlp.common.params - dataset_reader.save_tokenizer = True
2022-04-11 15:19:36,750 - INFO - allennlp.common.params - dataset_reader.is_training = True
2022-04-11 15:19:36,750 - INFO - allennlp.common.params - dataset_reader.pickle.action = None
2022-04-11 15:19:36,751 - INFO - allennlp.common.params - dataset_reader.pickle.file_name = None
2022-04-11 15:19:36,751 - INFO - allennlp.common.params - dataset_reader.pickle.path = ../pickle
2022-04-11 15:19:36,751 - INFO - allennlp.common.params - dataset_reader.pickle.save_even_when_max_instances = False
2022-04-11 15:19:36,751 - INFO - allennlp.common.params - dataset_reader.paragraphs_source = IR-ORA-D
2022-04-11 15:19:36,751 - INFO - allennlp.common.params - dataset_reader.answer_last_decomposition_step = False
2022-04-11 15:19:36,751 - INFO - allennlp.common.params - dataset_reader.skip_if_context_missing = True
2022-04-11 15:19:36,751 - INFO - allennlp.common.params - dataset_reader.paragraphs_limit = 10
2022-04-11 15:19:36,751 - INFO - allennlp.common.params - dataset_reader.generated_decompositions_paths = None
2022-04-11 15:19:36,751 - INFO - allennlp.common.params - dataset_reader.save_elasticsearch_cache = True
2022-04-11 15:19:37,204 - INFO - filelock - Lock 139906695048656 acquired on cache.lock
2022-04-11 15:19:37,898 - INFO - filelock - Lock 139906695048656 released on cache.lock
2022-04-11 15:19:37,899 - INFO - allennlp.common.params - train_data_path = data/strategyqa/train.json
2022-04-11 15:19:37,903 - INFO - allennlp.common.params - vocabulary = <allennlp.common.lazy.Lazy object at 0x7f3e2cdb9f90>
2022-04-11 15:19:37,903 - INFO - allennlp.common.params - datasets_for_vocab_creation = None
2022-04-11 15:19:37,903 - INFO - allennlp.common.params - validation_dataset_reader.type = strategy_qa_reader
2022-04-11 15:19:37,904 - INFO - allennlp.common.params - validation_dataset_reader.lazy = False
2022-04-11 15:19:37,904 - INFO - allennlp.common.params - validation_dataset_reader.cache_directory = None
2022-04-11 15:19:37,904 - INFO - allennlp.common.params - validation_dataset_reader.max_instances = None
2022-04-11 15:19:37,904 - INFO - allennlp.common.params - validation_dataset_reader.manual_distributed_sharding = False
2022-04-11 15:19:37,904 - INFO - allennlp.common.params - validation_dataset_reader.manual_multi_process_sharding = False
2022-04-11 15:19:37,904 - INFO - allennlp.common.params - validation_dataset_reader.tokenizer_wrapper.type = default
2022-04-11 15:19:37,904 - INFO - allennlp.common.params - validation_dataset_reader.tokenizer_wrapper.pretrained_model = roberta-large
2022-04-11 15:19:37,904 - INFO - allennlp.common.params - validation_dataset_reader.tokenizer_wrapper.call_kwargs.return_offsets_mapping = True
2022-04-11 15:19:37,905 - INFO - allennlp.common.params - validation_dataset_reader.tokenizer_wrapper.call_kwargs.return_special_tokens_mask = True
2022-04-11 15:19:37,905 - INFO - allennlp.common.params - validation_dataset_reader.tokenizer_wrapper.call_kwargs.truncation = only_first
2022-04-11 15:19:39,099 - INFO - allennlp.common.params - validation_dataset_reader.save_tokenizer = False
2022-04-11 15:19:39,099 - INFO - allennlp.common.params - validation_dataset_reader.is_training = False
2022-04-11 15:19:39,100 - INFO - allennlp.common.params - validation_dataset_reader.pickle.action = None
2022-04-11 15:19:39,100 - INFO - allennlp.common.params - validation_dataset_reader.pickle.file_name = None
2022-04-11 15:19:39,100 - INFO - allennlp.common.params - validation_dataset_reader.pickle.path = ../pickle
2022-04-11 15:19:39,100 - INFO - allennlp.common.params - validation_dataset_reader.pickle.save_even_when_max_instances = False
2022-04-11 15:19:39,100 - INFO - allennlp.common.params - validation_dataset_reader.paragraphs_source = IR-ORA-D
2022-04-11 15:19:39,100 - INFO - allennlp.common.params - validation_dataset_reader.answer_last_decomposition_step = False
2022-04-11 15:19:39,100 - INFO - allennlp.common.params - validation_dataset_reader.skip_if_context_missing = True
2022-04-11 15:19:39,100 - INFO - allennlp.common.params - validation_dataset_reader.paragraphs_limit = 10
2022-04-11 15:19:39,100 - INFO - allennlp.common.params - validation_dataset_reader.generated_decompositions_paths = None
2022-04-11 15:19:39,100 - INFO - allennlp.common.params - validation_dataset_reader.save_elasticsearch_cache = True
2022-04-11 15:19:39,100 - INFO - filelock - Lock 139905016351504 acquired on cache.lock
2022-04-11 15:19:39,932 - INFO - filelock - Lock 139905016351504 released on cache.lock
2022-04-11 15:19:39,932 - INFO - allennlp.common.params - validation_data_path = data/strategyqa/dev.json
2022-04-11 15:19:39,932 - INFO - allennlp.common.params - validation_data_loader = None
2022-04-11 15:19:39,932 - INFO - allennlp.common.params - test_data_path = None
2022-04-11 15:19:39,932 - INFO - allennlp.common.params - evaluate_on_test = True
2022-04-11 15:19:39,932 - INFO - allennlp.common.params - batch_weight_key = 
2022-04-11 15:19:39,932 - INFO - allennlp.training.util - Reading training data from data/strategyqa/train.json
2022-04-11 15:19:39,933 - INFO - tqdm - reading instances: 0it [00:00, ?it/s]
2022-04-11 15:19:39,933 - INFO - src.data.dataset_readers.base.base_dataset_reader - Reading the dataset from scratch
2022-04-11 15:19:39,933 - INFO - src.data.dataset_readers.strategy_qa_reader - Reading the dataset:
2022-04-11 15:19:39,933 - INFO - src.data.dataset_readers.strategy_qa_reader - Reading file at data/strategyqa/train.json
2022-04-11 15:19:47,544 - INFO - filelock - Lock 139904188643024 acquired on cache.lock
2022-04-11 15:19:48,345 - INFO - filelock - Lock 139904188643024 released on cache.lock
2022-04-11 15:19:48,382 - INFO - allennlp.training.util - Reading validation data from data/strategyqa/dev.json
2022-04-11 15:19:48,383 - INFO - tqdm - reading instances: 0it [00:00, ?it/s]
2022-04-11 15:19:48,383 - INFO - src.data.dataset_readers.base.base_dataset_reader - Reading the dataset from scratch
2022-04-11 15:19:48,383 - INFO - src.data.dataset_readers.strategy_qa_reader - Reading the dataset:
2022-04-11 15:19:48,383 - INFO - src.data.dataset_readers.strategy_qa_reader - Reading file at data/strategyqa/dev.json
2022-04-11 15:19:48,841 - INFO - src.data.dataset_readers.utils.elasticsearch_utils - Empty results from Elasticsearch for: actions commercial pilots take concerning engine noise arriving departing Orange County, California?
2022-04-11 15:19:49,176 - INFO - filelock - Lock 139904196059664 acquired on cache.lock
2022-04-11 15:19:49,964 - INFO - filelock - Lock 139904196059664 released on cache.lock
2022-04-11 15:19:50,001 - INFO - allennlp.common.params - type = from_instances
2022-04-11 15:19:50,001 - INFO - allennlp.common.params - min_count = None
2022-04-11 15:19:50,002 - INFO - allennlp.common.params - max_vocab_size = None
2022-04-11 15:19:50,002 - INFO - allennlp.common.params - non_padded_namespaces = ('*tags', '*labels')
2022-04-11 15:19:50,002 - INFO - allennlp.common.params - pretrained_files = None
2022-04-11 15:19:50,002 - INFO - allennlp.common.params - only_include_pretrained_words = False
2022-04-11 15:19:50,002 - INFO - allennlp.common.params - tokens_to_add = None
2022-04-11 15:19:50,002 - INFO - allennlp.common.params - min_pretrained_embeddings = None
2022-04-11 15:19:50,002 - INFO - allennlp.common.params - padding_token = @@PADDING@@
2022-04-11 15:19:50,002 - INFO - allennlp.common.params - oov_token = @@UNKNOWN@@
2022-04-11 15:19:50,002 - INFO - allennlp.data.vocabulary - Fitting token dictionary from dataset.
2022-04-11 15:19:50,002 - INFO - tqdm - building vocab: 0it [00:00, ?it/s]
2022-04-11 15:19:50,008 - INFO - allennlp.common.params - model.type = hf_classifier
2022-04-11 15:19:50,009 - INFO - allennlp.common.params - model.regularizer = None
2022-04-11 15:19:50,009 - INFO - allennlp.common.params - model.pretrained_model = roberta-large
2022-04-11 15:19:50,009 - INFO - allennlp.common.params - model.tokenizer_wrapper.type = default
2022-04-11 15:19:50,009 - INFO - allennlp.common.params - model.tokenizer_wrapper.pretrained_model = roberta-large
2022-04-11 15:19:50,009 - INFO - allennlp.common.params - model.tokenizer_wrapper.call_kwargs.return_offsets_mapping = True
2022-04-11 15:19:50,009 - INFO - allennlp.common.params - model.tokenizer_wrapper.call_kwargs.return_special_tokens_mask = True
2022-04-11 15:19:50,009 - INFO - allennlp.common.params - model.tokenizer_wrapper.call_kwargs.truncation = only_first
2022-04-11 15:19:51,201 - INFO - allennlp.common.params - model.num_labels = 2
2022-04-11 15:19:51,201 - INFO - allennlp.common.params - model.label_namespace = labels
2022-04-11 15:19:51,201 - INFO - allennlp.common.params - model.transformer_weights_path = None
2022-04-11 15:19:51,202 - INFO - allennlp.common.params - model.initializer.regexes.0.1.type = pretrained
2022-04-11 15:19:51,202 - INFO - allennlp.common.params - model.initializer.regexes.0.1.weights_file_path = output_roberta-star/model.tar.gz
2022-04-11 15:19:51,203 - INFO - allennlp.models.archival - extracting archive file output_roberta-star/model.tar.gz to temp dir /tmp/tmpb6711wcn
2022-04-11 15:20:03,267 - INFO - allennlp.models.archival - removing temporary unarchived model dir at /tmp/tmpb6711wcn
2022-04-11 15:20:03,456 - INFO - allennlp.common.params - model.initializer.prevent_regexes = None
2022-04-11 15:20:18,447 - INFO - allennlp.nn.initializers - Initializing parameters
2022-04-11 15:20:18,448 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.embeddings.word_embeddings.weight using .* initializer
2022-04-11 15:20:18,457 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.embeddings.position_embeddings.weight using .* initializer
2022-04-11 15:20:18,457 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.embeddings.token_type_embeddings.weight using .* initializer
2022-04-11 15:20:18,457 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.embeddings.LayerNorm.weight using .* initializer
2022-04-11 15:20:18,457 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.embeddings.LayerNorm.bias using .* initializer
2022-04-11 15:20:18,458 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.self.query.weight using .* initializer
2022-04-11 15:20:18,458 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.self.query.bias using .* initializer
2022-04-11 15:20:18,458 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.self.key.weight using .* initializer
2022-04-11 15:20:18,458 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.self.key.bias using .* initializer
2022-04-11 15:20:18,459 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.self.value.weight using .* initializer
2022-04-11 15:20:18,459 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.self.value.bias using .* initializer
2022-04-11 15:20:18,459 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.output.dense.weight using .* initializer
2022-04-11 15:20:18,459 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.output.dense.bias using .* initializer
2022-04-11 15:20:18,459 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.output.LayerNorm.weight using .* initializer
2022-04-11 15:20:18,459 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.output.LayerNorm.bias using .* initializer
2022-04-11 15:20:18,459 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.intermediate.dense.weight using .* initializer
2022-04-11 15:20:18,460 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.intermediate.dense.bias using .* initializer
2022-04-11 15:20:18,460 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.output.dense.weight using .* initializer
2022-04-11 15:20:18,461 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.output.dense.bias using .* initializer
2022-04-11 15:20:18,461 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.output.LayerNorm.weight using .* initializer
2022-04-11 15:20:18,462 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.output.LayerNorm.bias using .* initializer
2022-04-11 15:20:18,462 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.self.query.weight using .* initializer
2022-04-11 15:20:18,462 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.self.query.bias using .* initializer
2022-04-11 15:20:18,462 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.self.key.weight using .* initializer
2022-04-11 15:20:18,462 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.self.key.bias using .* initializer
2022-04-11 15:20:18,462 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.self.value.weight using .* initializer
2022-04-11 15:20:18,463 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.self.value.bias using .* initializer
2022-04-11 15:20:18,463 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.output.dense.weight using .* initializer
2022-04-11 15:20:18,463 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.output.dense.bias using .* initializer
2022-04-11 15:20:18,463 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.output.LayerNorm.weight using .* initializer
2022-04-11 15:20:18,463 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.output.LayerNorm.bias using .* initializer
2022-04-11 15:20:18,463 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.intermediate.dense.weight using .* initializer
2022-04-11 15:20:18,464 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.intermediate.dense.bias using .* initializer
2022-04-11 15:20:18,464 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.output.dense.weight using .* initializer
2022-04-11 15:20:18,465 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.output.dense.bias using .* initializer
2022-04-11 15:20:18,465 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.output.LayerNorm.weight using .* initializer
2022-04-11 15:20:18,465 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.output.LayerNorm.bias using .* initializer
2022-04-11 15:20:18,465 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.self.query.weight using .* initializer
2022-04-11 15:20:18,466 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.self.query.bias using .* initializer
2022-04-11 15:20:18,466 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.self.key.weight using .* initializer
2022-04-11 15:20:18,466 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.self.key.bias using .* initializer
2022-04-11 15:20:18,466 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.self.value.weight using .* initializer
2022-04-11 15:20:18,467 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.self.value.bias using .* initializer
2022-04-11 15:20:18,467 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.output.dense.weight using .* initializer
2022-04-11 15:20:18,467 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.output.dense.bias using .* initializer
2022-04-11 15:20:18,467 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.output.LayerNorm.weight using .* initializer
2022-04-11 15:20:18,467 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.output.LayerNorm.bias using .* initializer
2022-04-11 15:20:18,467 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.intermediate.dense.weight using .* initializer
2022-04-11 15:20:18,468 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.intermediate.dense.bias using .* initializer
2022-04-11 15:20:18,468 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.output.dense.weight using .* initializer
2022-04-11 15:20:18,469 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.output.dense.bias using .* initializer
2022-04-11 15:20:18,469 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.output.LayerNorm.weight using .* initializer
2022-04-11 15:20:18,469 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.output.LayerNorm.bias using .* initializer
2022-04-11 15:20:18,469 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.self.query.weight using .* initializer
2022-04-11 15:20:18,470 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.self.query.bias using .* initializer
2022-04-11 15:20:18,470 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.self.key.weight using .* initializer
2022-04-11 15:20:18,470 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.self.key.bias using .* initializer
2022-04-11 15:20:18,470 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.self.value.weight using .* initializer
2022-04-11 15:20:18,471 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.self.value.bias using .* initializer
2022-04-11 15:20:18,471 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.output.dense.weight using .* initializer
2022-04-11 15:20:18,471 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.output.dense.bias using .* initializer
2022-04-11 15:20:18,471 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.output.LayerNorm.weight using .* initializer
2022-04-11 15:20:18,471 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.output.LayerNorm.bias using .* initializer
2022-04-11 15:20:18,471 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.intermediate.dense.weight using .* initializer
2022-04-11 15:20:18,472 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.intermediate.dense.bias using .* initializer
2022-04-11 15:20:18,472 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.output.dense.weight using .* initializer
2022-04-11 15:20:18,473 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.output.dense.bias using .* initializer
2022-04-11 15:20:18,473 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.output.LayerNorm.weight using .* initializer
2022-04-11 15:20:18,473 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.output.LayerNorm.bias using .* initializer
2022-04-11 15:20:18,473 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.self.query.weight using .* initializer
2022-04-11 15:20:18,474 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.self.query.bias using .* initializer
2022-04-11 15:20:18,474 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.self.key.weight using .* initializer
2022-04-11 15:20:18,474 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.self.key.bias using .* initializer
2022-04-11 15:20:18,474 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.self.value.weight using .* initializer
2022-04-11 15:20:18,474 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.self.value.bias using .* initializer
2022-04-11 15:20:18,475 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.output.dense.weight using .* initializer
2022-04-11 15:20:18,475 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.output.dense.bias using .* initializer
2022-04-11 15:20:18,475 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.output.LayerNorm.weight using .* initializer
2022-04-11 15:20:18,475 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.output.LayerNorm.bias using .* initializer
2022-04-11 15:20:18,475 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.intermediate.dense.weight using .* initializer
2022-04-11 15:20:18,476 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.intermediate.dense.bias using .* initializer
2022-04-11 15:20:18,476 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.output.dense.weight using .* initializer
2022-04-11 15:20:18,477 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.output.dense.bias using .* initializer
2022-04-11 15:20:18,477 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.output.LayerNorm.weight using .* initializer
2022-04-11 15:20:18,477 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.output.LayerNorm.bias using .* initializer
2022-04-11 15:20:18,477 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.self.query.weight using .* initializer
2022-04-11 15:20:18,478 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.self.query.bias using .* initializer
2022-04-11 15:20:18,478 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.self.key.weight using .* initializer
2022-04-11 15:20:18,478 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.self.key.bias using .* initializer
2022-04-11 15:20:18,478 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.self.value.weight using .* initializer
2022-04-11 15:20:18,478 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.self.value.bias using .* initializer
2022-04-11 15:20:18,479 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.output.dense.weight using .* initializer
2022-04-11 15:20:18,479 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.output.dense.bias using .* initializer
2022-04-11 15:20:18,479 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.output.LayerNorm.weight using .* initializer
2022-04-11 15:20:18,479 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.output.LayerNorm.bias using .* initializer
2022-04-11 15:20:18,479 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.intermediate.dense.weight using .* initializer
2022-04-11 15:20:18,480 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.intermediate.dense.bias using .* initializer
2022-04-11 15:20:18,480 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.output.dense.weight using .* initializer
2022-04-11 15:20:18,481 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.output.dense.bias using .* initializer
2022-04-11 15:20:18,481 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.output.LayerNorm.weight using .* initializer
2022-04-11 15:20:18,481 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.output.LayerNorm.bias using .* initializer
2022-04-11 15:20:18,481 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.self.query.weight using .* initializer
2022-04-11 15:20:18,482 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.self.query.bias using .* initializer
2022-04-11 15:20:18,482 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.self.key.weight using .* initializer
2022-04-11 15:20:18,482 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.self.key.bias using .* initializer
2022-04-11 15:20:18,482 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.self.value.weight using .* initializer
2022-04-11 15:20:18,482 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.self.value.bias using .* initializer
2022-04-11 15:20:18,483 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.output.dense.weight using .* initializer
2022-04-11 15:20:18,483 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.output.dense.bias using .* initializer
2022-04-11 15:20:18,483 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.output.LayerNorm.weight using .* initializer
2022-04-11 15:20:18,483 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.output.LayerNorm.bias using .* initializer
2022-04-11 15:20:18,483 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.intermediate.dense.weight using .* initializer
2022-04-11 15:20:18,484 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.intermediate.dense.bias using .* initializer
2022-04-11 15:20:18,484 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.output.dense.weight using .* initializer
2022-04-11 15:20:18,485 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.output.dense.bias using .* initializer
2022-04-11 15:20:18,485 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.output.LayerNorm.weight using .* initializer
2022-04-11 15:20:18,485 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.output.LayerNorm.bias using .* initializer
2022-04-11 15:20:18,485 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.self.query.weight using .* initializer
2022-04-11 15:20:18,485 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.self.query.bias using .* initializer
2022-04-11 15:20:18,486 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.self.key.weight using .* initializer
2022-04-11 15:20:18,486 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.self.key.bias using .* initializer
2022-04-11 15:20:18,486 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.self.value.weight using .* initializer
2022-04-11 15:20:18,486 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.self.value.bias using .* initializer
2022-04-11 15:20:18,486 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.output.dense.weight using .* initializer
2022-04-11 15:20:18,487 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.output.dense.bias using .* initializer
2022-04-11 15:20:18,487 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.output.LayerNorm.weight using .* initializer
2022-04-11 15:20:18,487 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.output.LayerNorm.bias using .* initializer
2022-04-11 15:20:18,487 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.intermediate.dense.weight using .* initializer
2022-04-11 15:20:18,488 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.intermediate.dense.bias using .* initializer
2022-04-11 15:20:18,488 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.output.dense.weight using .* initializer
2022-04-11 15:20:18,489 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.output.dense.bias using .* initializer
2022-04-11 15:20:18,489 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.output.LayerNorm.weight using .* initializer
2022-04-11 15:20:18,489 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.output.LayerNorm.bias using .* initializer
2022-04-11 15:20:18,489 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.self.query.weight using .* initializer
2022-04-11 15:20:18,489 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.self.query.bias using .* initializer
2022-04-11 15:20:18,490 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.self.key.weight using .* initializer
2022-04-11 15:20:18,490 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.self.key.bias using .* initializer
2022-04-11 15:20:18,490 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.self.value.weight using .* initializer
2022-04-11 15:20:18,490 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.self.value.bias using .* initializer
2022-04-11 15:20:18,490 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.output.dense.weight using .* initializer
2022-04-11 15:20:18,491 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.output.dense.bias using .* initializer
2022-04-11 15:20:18,491 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.output.LayerNorm.weight using .* initializer
2022-04-11 15:20:18,491 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.output.LayerNorm.bias using .* initializer
2022-04-11 15:20:18,491 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.intermediate.dense.weight using .* initializer
2022-04-11 15:20:18,492 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.intermediate.dense.bias using .* initializer
2022-04-11 15:20:18,492 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.output.dense.weight using .* initializer
2022-04-11 15:20:18,493 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.output.dense.bias using .* initializer
2022-04-11 15:20:18,493 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.output.LayerNorm.weight using .* initializer
2022-04-11 15:20:18,493 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.output.LayerNorm.bias using .* initializer
2022-04-11 15:20:18,493 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.self.query.weight using .* initializer
2022-04-11 15:20:18,493 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.self.query.bias using .* initializer
2022-04-11 15:20:18,493 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.self.key.weight using .* initializer
2022-04-11 15:20:18,494 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.self.key.bias using .* initializer
2022-04-11 15:20:18,494 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.self.value.weight using .* initializer
2022-04-11 15:20:18,494 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.self.value.bias using .* initializer
2022-04-11 15:20:18,494 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.output.dense.weight using .* initializer
2022-04-11 15:20:18,494 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.output.dense.bias using .* initializer
2022-04-11 15:20:18,495 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.output.LayerNorm.weight using .* initializer
2022-04-11 15:20:18,495 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.output.LayerNorm.bias using .* initializer
2022-04-11 15:20:18,495 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.intermediate.dense.weight using .* initializer
2022-04-11 15:20:18,496 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.intermediate.dense.bias using .* initializer
2022-04-11 15:20:18,496 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.output.dense.weight using .* initializer
2022-04-11 15:20:18,497 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.output.dense.bias using .* initializer
2022-04-11 15:20:18,497 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.output.LayerNorm.weight using .* initializer
2022-04-11 15:20:18,497 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.output.LayerNorm.bias using .* initializer
2022-04-11 15:20:18,497 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.self.query.weight using .* initializer
2022-04-11 15:20:18,497 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.self.query.bias using .* initializer
2022-04-11 15:20:18,497 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.self.key.weight using .* initializer
2022-04-11 15:20:18,498 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.self.key.bias using .* initializer
2022-04-11 15:20:18,498 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.self.value.weight using .* initializer
2022-04-11 15:20:18,498 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.self.value.bias using .* initializer
2022-04-11 15:20:18,498 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.output.dense.weight using .* initializer
2022-04-11 15:20:18,499 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.output.dense.bias using .* initializer
2022-04-11 15:20:18,499 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.output.LayerNorm.weight using .* initializer
2022-04-11 15:20:18,499 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.output.LayerNorm.bias using .* initializer
2022-04-11 15:20:18,499 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.intermediate.dense.weight using .* initializer
2022-04-11 15:20:18,500 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.intermediate.dense.bias using .* initializer
2022-04-11 15:20:18,500 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.output.dense.weight using .* initializer
2022-04-11 15:20:18,501 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.output.dense.bias using .* initializer
2022-04-11 15:20:18,501 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.output.LayerNorm.weight using .* initializer
2022-04-11 15:20:18,501 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.output.LayerNorm.bias using .* initializer
2022-04-11 15:20:18,501 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.self.query.weight using .* initializer
2022-04-11 15:20:18,501 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.self.query.bias using .* initializer
2022-04-11 15:20:18,501 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.self.key.weight using .* initializer
2022-04-11 15:20:18,502 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.self.key.bias using .* initializer
2022-04-11 15:20:18,502 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.self.value.weight using .* initializer
2022-04-11 15:20:18,502 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.self.value.bias using .* initializer
2022-04-11 15:20:18,502 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.output.dense.weight using .* initializer
2022-04-11 15:20:18,502 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.output.dense.bias using .* initializer
2022-04-11 15:20:18,502 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.output.LayerNorm.weight using .* initializer
2022-04-11 15:20:18,503 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.output.LayerNorm.bias using .* initializer
2022-04-11 15:20:18,503 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.intermediate.dense.weight using .* initializer
2022-04-11 15:20:18,503 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.intermediate.dense.bias using .* initializer
2022-04-11 15:20:18,504 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.output.dense.weight using .* initializer
2022-04-11 15:20:18,504 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.output.dense.bias using .* initializer
2022-04-11 15:20:18,505 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.output.LayerNorm.weight using .* initializer
2022-04-11 15:20:18,505 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.output.LayerNorm.bias using .* initializer
2022-04-11 15:20:18,505 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.self.query.weight using .* initializer
2022-04-11 15:20:18,505 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.self.query.bias using .* initializer
2022-04-11 15:20:18,505 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.self.key.weight using .* initializer
2022-04-11 15:20:18,505 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.self.key.bias using .* initializer
2022-04-11 15:20:18,505 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.self.value.weight using .* initializer
2022-04-11 15:20:18,506 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.self.value.bias using .* initializer
2022-04-11 15:20:18,506 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.output.dense.weight using .* initializer
2022-04-11 15:20:18,506 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.output.dense.bias using .* initializer
2022-04-11 15:20:18,506 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.output.LayerNorm.weight using .* initializer
2022-04-11 15:20:18,506 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.output.LayerNorm.bias using .* initializer
2022-04-11 15:20:18,506 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.intermediate.dense.weight using .* initializer
2022-04-11 15:20:18,507 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.intermediate.dense.bias using .* initializer
2022-04-11 15:20:18,507 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.output.dense.weight using .* initializer
2022-04-11 15:20:18,508 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.output.dense.bias using .* initializer
2022-04-11 15:20:18,508 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.output.LayerNorm.weight using .* initializer
2022-04-11 15:20:18,509 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.output.LayerNorm.bias using .* initializer
2022-04-11 15:20:18,509 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.self.query.weight using .* initializer
2022-04-11 15:20:18,509 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.self.query.bias using .* initializer
2022-04-11 15:20:18,509 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.self.key.weight using .* initializer
2022-04-11 15:20:18,509 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.self.key.bias using .* initializer
2022-04-11 15:20:18,509 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.self.value.weight using .* initializer
2022-04-11 15:20:18,510 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.self.value.bias using .* initializer
2022-04-11 15:20:18,510 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.output.dense.weight using .* initializer
2022-04-11 15:20:18,510 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.output.dense.bias using .* initializer
2022-04-11 15:20:18,510 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.output.LayerNorm.weight using .* initializer
2022-04-11 15:20:18,510 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.output.LayerNorm.bias using .* initializer
2022-04-11 15:20:18,510 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.intermediate.dense.weight using .* initializer
2022-04-11 15:20:18,511 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.intermediate.dense.bias using .* initializer
2022-04-11 15:20:18,511 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.output.dense.weight using .* initializer
2022-04-11 15:20:18,512 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.output.dense.bias using .* initializer
2022-04-11 15:20:18,512 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.output.LayerNorm.weight using .* initializer
2022-04-11 15:20:18,512 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.output.LayerNorm.bias using .* initializer
2022-04-11 15:20:18,513 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.self.query.weight using .* initializer
2022-04-11 15:20:18,513 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.self.query.bias using .* initializer
2022-04-11 15:20:18,513 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.self.key.weight using .* initializer
2022-04-11 15:20:18,513 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.self.key.bias using .* initializer
2022-04-11 15:20:18,513 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.self.value.weight using .* initializer
2022-04-11 15:20:18,514 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.self.value.bias using .* initializer
2022-04-11 15:20:18,514 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.output.dense.weight using .* initializer
2022-04-11 15:20:18,514 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.output.dense.bias using .* initializer
2022-04-11 15:20:18,514 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.output.LayerNorm.weight using .* initializer
2022-04-11 15:20:18,514 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.output.LayerNorm.bias using .* initializer
2022-04-11 15:20:18,514 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.intermediate.dense.weight using .* initializer
2022-04-11 15:20:18,515 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.intermediate.dense.bias using .* initializer
2022-04-11 15:20:18,515 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.output.dense.weight using .* initializer
2022-04-11 15:20:18,516 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.output.dense.bias using .* initializer
2022-04-11 15:20:18,516 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.output.LayerNorm.weight using .* initializer
2022-04-11 15:20:18,516 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.output.LayerNorm.bias using .* initializer
2022-04-11 15:20:18,516 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.self.query.weight using .* initializer
2022-04-11 15:20:18,517 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.self.query.bias using .* initializer
2022-04-11 15:20:18,517 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.self.key.weight using .* initializer
2022-04-11 15:20:18,517 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.self.key.bias using .* initializer
2022-04-11 15:20:18,517 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.self.value.weight using .* initializer
2022-04-11 15:20:18,518 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.self.value.bias using .* initializer
2022-04-11 15:20:18,518 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.output.dense.weight using .* initializer
2022-04-11 15:20:18,518 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.output.dense.bias using .* initializer
2022-04-11 15:20:18,518 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.output.LayerNorm.weight using .* initializer
2022-04-11 15:20:18,518 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.output.LayerNorm.bias using .* initializer
2022-04-11 15:20:18,518 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.intermediate.dense.weight using .* initializer
2022-04-11 15:20:18,519 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.intermediate.dense.bias using .* initializer
2022-04-11 15:20:18,519 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.output.dense.weight using .* initializer
2022-04-11 15:20:18,520 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.output.dense.bias using .* initializer
2022-04-11 15:20:18,520 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.output.LayerNorm.weight using .* initializer
2022-04-11 15:20:18,520 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.output.LayerNorm.bias using .* initializer
2022-04-11 15:20:18,520 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.self.query.weight using .* initializer
2022-04-11 15:20:18,521 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.self.query.bias using .* initializer
2022-04-11 15:20:18,521 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.self.key.weight using .* initializer
2022-04-11 15:20:18,521 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.self.key.bias using .* initializer
2022-04-11 15:20:18,521 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.self.value.weight using .* initializer
2022-04-11 15:20:18,521 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.self.value.bias using .* initializer
2022-04-11 15:20:18,522 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.output.dense.weight using .* initializer
2022-04-11 15:20:18,522 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.output.dense.bias using .* initializer
2022-04-11 15:20:18,522 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.output.LayerNorm.weight using .* initializer
2022-04-11 15:20:18,522 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.output.LayerNorm.bias using .* initializer
2022-04-11 15:20:18,522 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.intermediate.dense.weight using .* initializer
2022-04-11 15:20:18,523 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.intermediate.dense.bias using .* initializer
2022-04-11 15:20:18,523 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.output.dense.weight using .* initializer
2022-04-11 15:20:18,524 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.output.dense.bias using .* initializer
2022-04-11 15:20:18,524 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.output.LayerNorm.weight using .* initializer
2022-04-11 15:20:18,524 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.output.LayerNorm.bias using .* initializer
2022-04-11 15:20:18,524 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.self.query.weight using .* initializer
2022-04-11 15:20:18,524 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.self.query.bias using .* initializer
2022-04-11 15:20:18,525 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.self.key.weight using .* initializer
2022-04-11 15:20:18,525 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.self.key.bias using .* initializer
2022-04-11 15:20:18,525 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.self.value.weight using .* initializer
2022-04-11 15:20:18,525 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.self.value.bias using .* initializer
2022-04-11 15:20:18,525 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.output.dense.weight using .* initializer
2022-04-11 15:20:18,526 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.output.dense.bias using .* initializer
2022-04-11 15:20:18,526 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.output.LayerNorm.weight using .* initializer
2022-04-11 15:20:18,526 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.output.LayerNorm.bias using .* initializer
2022-04-11 15:20:18,526 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.intermediate.dense.weight using .* initializer
2022-04-11 15:20:18,527 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.intermediate.dense.bias using .* initializer
2022-04-11 15:20:18,527 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.output.dense.weight using .* initializer
2022-04-11 15:20:18,528 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.output.dense.bias using .* initializer
2022-04-11 15:20:18,528 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.output.LayerNorm.weight using .* initializer
2022-04-11 15:20:18,528 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.output.LayerNorm.bias using .* initializer
2022-04-11 15:20:18,528 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.self.query.weight using .* initializer
2022-04-11 15:20:18,528 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.self.query.bias using .* initializer
2022-04-11 15:20:18,529 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.self.key.weight using .* initializer
2022-04-11 15:20:18,529 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.self.key.bias using .* initializer
2022-04-11 15:20:18,529 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.self.value.weight using .* initializer
2022-04-11 15:20:18,529 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.self.value.bias using .* initializer
2022-04-11 15:20:18,529 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.output.dense.weight using .* initializer
2022-04-11 15:20:18,530 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.output.dense.bias using .* initializer
2022-04-11 15:20:18,530 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.output.LayerNorm.weight using .* initializer
2022-04-11 15:20:18,530 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.output.LayerNorm.bias using .* initializer
2022-04-11 15:20:18,530 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.intermediate.dense.weight using .* initializer
2022-04-11 15:20:18,531 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.intermediate.dense.bias using .* initializer
2022-04-11 15:20:18,531 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.output.dense.weight using .* initializer
2022-04-11 15:20:18,532 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.output.dense.bias using .* initializer
2022-04-11 15:20:18,532 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.output.LayerNorm.weight using .* initializer
2022-04-11 15:20:18,532 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.output.LayerNorm.bias using .* initializer
2022-04-11 15:20:18,532 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.self.query.weight using .* initializer
2022-04-11 15:20:18,532 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.self.query.bias using .* initializer
2022-04-11 15:20:18,532 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.self.key.weight using .* initializer
2022-04-11 15:20:18,533 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.self.key.bias using .* initializer
2022-04-11 15:20:18,533 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.self.value.weight using .* initializer
2022-04-11 15:20:18,533 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.self.value.bias using .* initializer
2022-04-11 15:20:18,533 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.output.dense.weight using .* initializer
2022-04-11 15:20:18,533 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.output.dense.bias using .* initializer
2022-04-11 15:20:18,534 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.output.LayerNorm.weight using .* initializer
2022-04-11 15:20:18,534 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.output.LayerNorm.bias using .* initializer
2022-04-11 15:20:18,534 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.intermediate.dense.weight using .* initializer
2022-04-11 15:20:18,535 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.intermediate.dense.bias using .* initializer
2022-04-11 15:20:18,535 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.output.dense.weight using .* initializer
2022-04-11 15:20:18,536 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.output.dense.bias using .* initializer
2022-04-11 15:20:18,536 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.output.LayerNorm.weight using .* initializer
2022-04-11 15:20:18,536 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.output.LayerNorm.bias using .* initializer
2022-04-11 15:20:18,536 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.self.query.weight using .* initializer
2022-04-11 15:20:18,536 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.self.query.bias using .* initializer
2022-04-11 15:20:18,536 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.self.key.weight using .* initializer
2022-04-11 15:20:18,536 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.self.key.bias using .* initializer
2022-04-11 15:20:18,537 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.self.value.weight using .* initializer
2022-04-11 15:20:18,537 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.self.value.bias using .* initializer
2022-04-11 15:20:18,537 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.output.dense.weight using .* initializer
2022-04-11 15:20:18,537 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.output.dense.bias using .* initializer
2022-04-11 15:20:18,537 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.output.LayerNorm.weight using .* initializer
2022-04-11 15:20:18,537 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.output.LayerNorm.bias using .* initializer
2022-04-11 15:20:18,538 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.intermediate.dense.weight using .* initializer
2022-04-11 15:20:18,538 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.intermediate.dense.bias using .* initializer
2022-04-11 15:20:18,539 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.output.dense.weight using .* initializer
2022-04-11 15:20:18,539 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.output.dense.bias using .* initializer
2022-04-11 15:20:18,540 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.output.LayerNorm.weight using .* initializer
2022-04-11 15:20:18,540 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.output.LayerNorm.bias using .* initializer
2022-04-11 15:20:18,540 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.self.query.weight using .* initializer
2022-04-11 15:20:18,540 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.self.query.bias using .* initializer
2022-04-11 15:20:18,540 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.self.key.weight using .* initializer
2022-04-11 15:20:18,540 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.self.key.bias using .* initializer
2022-04-11 15:20:18,540 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.self.value.weight using .* initializer
2022-04-11 15:20:18,541 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.self.value.bias using .* initializer
2022-04-11 15:20:18,541 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.output.dense.weight using .* initializer
2022-04-11 15:20:18,541 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.output.dense.bias using .* initializer
2022-04-11 15:20:18,541 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.output.LayerNorm.weight using .* initializer
2022-04-11 15:20:18,541 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.output.LayerNorm.bias using .* initializer
2022-04-11 15:20:18,541 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.intermediate.dense.weight using .* initializer
2022-04-11 15:20:18,542 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.intermediate.dense.bias using .* initializer
2022-04-11 15:20:18,542 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.output.dense.weight using .* initializer
2022-04-11 15:20:18,543 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.output.dense.bias using .* initializer
2022-04-11 15:20:18,543 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.output.LayerNorm.weight using .* initializer
2022-04-11 15:20:18,543 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.output.LayerNorm.bias using .* initializer
2022-04-11 15:20:18,543 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.self.query.weight using .* initializer
2022-04-11 15:20:18,544 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.self.query.bias using .* initializer
2022-04-11 15:20:18,544 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.self.key.weight using .* initializer
2022-04-11 15:20:18,544 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.self.key.bias using .* initializer
2022-04-11 15:20:18,544 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.self.value.weight using .* initializer
2022-04-11 15:20:18,545 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.self.value.bias using .* initializer
2022-04-11 15:20:18,545 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.output.dense.weight using .* initializer
2022-04-11 15:20:18,545 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.output.dense.bias using .* initializer
2022-04-11 15:20:18,545 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.output.LayerNorm.weight using .* initializer
2022-04-11 15:20:18,545 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.output.LayerNorm.bias using .* initializer
2022-04-11 15:20:18,545 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.intermediate.dense.weight using .* initializer
2022-04-11 15:20:18,546 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.intermediate.dense.bias using .* initializer
2022-04-11 15:20:18,546 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.output.dense.weight using .* initializer
2022-04-11 15:20:18,547 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.output.dense.bias using .* initializer
2022-04-11 15:20:18,547 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.output.LayerNorm.weight using .* initializer
2022-04-11 15:20:18,547 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.output.LayerNorm.bias using .* initializer
2022-04-11 15:20:18,547 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.self.query.weight using .* initializer
2022-04-11 15:20:18,548 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.self.query.bias using .* initializer
2022-04-11 15:20:18,548 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.self.key.weight using .* initializer
2022-04-11 15:20:18,548 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.self.key.bias using .* initializer
2022-04-11 15:20:18,548 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.self.value.weight using .* initializer
2022-04-11 15:20:18,549 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.self.value.bias using .* initializer
2022-04-11 15:20:18,549 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.output.dense.weight using .* initializer
2022-04-11 15:20:18,549 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.output.dense.bias using .* initializer
2022-04-11 15:20:18,549 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.output.LayerNorm.weight using .* initializer
2022-04-11 15:20:18,549 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.output.LayerNorm.bias using .* initializer
2022-04-11 15:20:18,549 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.intermediate.dense.weight using .* initializer
2022-04-11 15:20:18,550 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.intermediate.dense.bias using .* initializer
2022-04-11 15:20:18,550 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.output.dense.weight using .* initializer
2022-04-11 15:20:18,551 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.output.dense.bias using .* initializer
2022-04-11 15:20:18,551 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.output.LayerNorm.weight using .* initializer
2022-04-11 15:20:18,551 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.output.LayerNorm.bias using .* initializer
2022-04-11 15:20:18,551 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.pooler.dense.weight using .* initializer
2022-04-11 15:20:18,551 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.pooler.dense.bias using .* initializer
2022-04-11 15:20:18,552 - INFO - allennlp.nn.initializers - Initializing _classifier.classifier.dense.weight using .* initializer
2022-04-11 15:20:18,552 - INFO - allennlp.nn.initializers - Initializing _classifier.classifier.dense.bias using .* initializer
2022-04-11 15:20:18,552 - INFO - allennlp.nn.initializers - Initializing _classifier.classifier.out_proj.weight using .* initializer
2022-04-11 15:20:18,552 - INFO - allennlp.nn.initializers - Initializing _classifier.classifier.out_proj.bias using .* initializer
2022-04-11 15:20:18,552 - INFO - allennlp.nn.initializers - Done initializing parameters; the following parameters are using their default initialization from their code
2022-04-11 15:20:19,104 - INFO - filelock - Lock 139904188642960 acquired on ./output_ir-ora-d_hyper7/vocabulary/.lock
2022-04-11 15:20:19,105 - INFO - filelock - Lock 139904188642960 released on ./output_ir-ora-d_hyper7/vocabulary/.lock
2022-04-11 15:20:19,105 - INFO - allennlp.common.params - data_loader.type = pytorch_dataloader
2022-04-11 15:20:19,105 - INFO - allennlp.common.params - data_loader.batch_size = 1
2022-04-11 15:20:19,105 - INFO - allennlp.common.params - data_loader.shuffle = False
2022-04-11 15:20:19,105 - INFO - allennlp.common.params - data_loader.sampler = None
2022-04-11 15:20:19,106 - INFO - allennlp.common.params - data_loader.num_workers = 0
2022-04-11 15:20:19,106 - INFO - allennlp.common.params - data_loader.pin_memory = False
2022-04-11 15:20:19,106 - INFO - allennlp.common.params - data_loader.drop_last = False
2022-04-11 15:20:19,106 - INFO - allennlp.common.params - data_loader.timeout = 0
2022-04-11 15:20:19,106 - INFO - allennlp.common.params - data_loader.worker_init_fn = None
2022-04-11 15:20:19,106 - INFO - allennlp.common.params - data_loader.multiprocessing_context = None
2022-04-11 15:20:19,106 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None
2022-04-11 15:20:19,106 - INFO - allennlp.common.params - data_loader.batch_sampler.type = bucket
2022-04-11 15:20:19,106 - INFO - allennlp.common.params - data_loader.batch_sampler.batch_size = 2
2022-04-11 15:20:19,106 - INFO - allennlp.common.params - data_loader.batch_sampler.sorting_keys = None
2022-04-11 15:20:19,106 - INFO - allennlp.common.params - data_loader.batch_sampler.padding_noise = 0.1
2022-04-11 15:20:19,106 - INFO - allennlp.common.params - data_loader.batch_sampler.drop_last = False
2022-04-11 15:20:19,107 - INFO - allennlp.common.params - data_loader.type = pytorch_dataloader
2022-04-11 15:20:19,107 - INFO - allennlp.common.params - data_loader.batch_size = 1
2022-04-11 15:20:19,107 - INFO - allennlp.common.params - data_loader.shuffle = False
2022-04-11 15:20:19,107 - INFO - allennlp.common.params - data_loader.sampler = None
2022-04-11 15:20:19,107 - INFO - allennlp.common.params - data_loader.num_workers = 0
2022-04-11 15:20:19,107 - INFO - allennlp.common.params - data_loader.pin_memory = False
2022-04-11 15:20:19,108 - INFO - allennlp.common.params - data_loader.drop_last = False
2022-04-11 15:20:19,108 - INFO - allennlp.common.params - data_loader.timeout = 0
2022-04-11 15:20:19,108 - INFO - allennlp.common.params - data_loader.worker_init_fn = None
2022-04-11 15:20:19,108 - INFO - allennlp.common.params - data_loader.multiprocessing_context = None
2022-04-11 15:20:19,108 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None
2022-04-11 15:20:19,108 - INFO - allennlp.common.params - data_loader.batch_sampler.type = bucket
2022-04-11 15:20:19,108 - INFO - allennlp.common.params - data_loader.batch_sampler.batch_size = 2
2022-04-11 15:20:19,108 - INFO - allennlp.common.params - data_loader.batch_sampler.sorting_keys = None
2022-04-11 15:20:19,108 - INFO - allennlp.common.params - data_loader.batch_sampler.padding_noise = 0.1
2022-04-11 15:20:19,108 - INFO - allennlp.common.params - data_loader.batch_sampler.drop_last = False
2022-04-11 15:20:19,109 - INFO - allennlp.common.params - trainer.type = gradient_descent
2022-04-11 15:20:19,109 - INFO - allennlp.common.params - trainer.patience = None
2022-04-11 15:20:19,109 - INFO - allennlp.common.params - trainer.validation_metric = +accuracy
2022-04-11 15:20:19,109 - INFO - allennlp.common.params - trainer.num_epochs = 15
2022-04-11 15:20:19,109 - INFO - allennlp.common.params - trainer.cuda_device = 0
2022-04-11 15:20:19,109 - INFO - allennlp.common.params - trainer.grad_norm = None
2022-04-11 15:20:19,109 - INFO - allennlp.common.params - trainer.grad_clipping = None
2022-04-11 15:20:19,109 - INFO - allennlp.common.params - trainer.distributed = False
2022-04-11 15:20:19,109 - INFO - allennlp.common.params - trainer.world_size = 1
2022-04-11 15:20:19,109 - INFO - allennlp.common.params - trainer.num_gradient_accumulation_steps = 16
2022-04-11 15:20:19,110 - INFO - allennlp.common.params - trainer.use_amp = False
2022-04-11 15:20:19,110 - INFO - allennlp.common.params - trainer.no_grad = None
2022-04-11 15:20:19,110 - INFO - allennlp.common.params - trainer.momentum_scheduler = None
2022-04-11 15:20:19,110 - INFO - allennlp.common.params - trainer.tensorboard_writer = <allennlp.common.lazy.Lazy object at 0x7f3e22970290>
2022-04-11 15:20:19,110 - INFO - allennlp.common.params - trainer.moving_average = None
2022-04-11 15:20:19,110 - INFO - allennlp.common.params - trainer.checkpointer = <allennlp.common.lazy.Lazy object at 0x7f3e22970310>
2022-04-11 15:20:19,110 - INFO - allennlp.common.params - trainer.batch_callbacks = None
2022-04-11 15:20:19,110 - INFO - allennlp.common.params - trainer.epoch_callbacks = None
2022-04-11 15:20:19,110 - INFO - allennlp.common.params - trainer.end_callbacks = None
2022-04-11 15:20:19,110 - INFO - allennlp.common.params - trainer.trainer_callbacks = None
2022-04-11 15:20:21,873 - INFO - allennlp.common.params - trainer.optimizer.type = huggingface_adamw
2022-04-11 15:20:21,874 - INFO - allennlp.common.params - trainer.optimizer.parameter_groups = None
2022-04-11 15:20:21,874 - INFO - allennlp.common.params - trainer.optimizer.lr = 5e-06
2022-04-11 15:20:21,874 - INFO - allennlp.common.params - trainer.optimizer.betas = [0.9, 0.98]
2022-04-11 15:20:21,874 - INFO - allennlp.common.params - trainer.optimizer.eps = 1e-06
2022-04-11 15:20:21,874 - INFO - allennlp.common.params - trainer.optimizer.weight_decay = 0.1
2022-04-11 15:20:21,874 - INFO - allennlp.common.params - trainer.optimizer.correct_bias = True
2022-04-11 15:20:21,874 - INFO - allennlp.training.optimizers - Number of trainable parameters: 356411394
2022-04-11 15:20:21,876 - INFO - allennlp.common.util - The following parameters are Frozen (without gradient):
2022-04-11 15:20:21,879 - INFO - allennlp.common.util - The following parameters are Tunable (with gradient):
2022-04-11 15:20:21,879 - INFO - allennlp.common.util - _classifier.roberta.embeddings.word_embeddings.weight
2022-04-11 15:20:21,879 - INFO - allennlp.common.util - _classifier.roberta.embeddings.position_embeddings.weight
2022-04-11 15:20:21,879 - INFO - allennlp.common.util - _classifier.roberta.embeddings.token_type_embeddings.weight
2022-04-11 15:20:21,879 - INFO - allennlp.common.util - _classifier.roberta.embeddings.LayerNorm.weight
2022-04-11 15:20:21,879 - INFO - allennlp.common.util - _classifier.roberta.embeddings.LayerNorm.bias
2022-04-11 15:20:21,879 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.self.query.weight
2022-04-11 15:20:21,879 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.self.query.bias
2022-04-11 15:20:21,879 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.self.key.weight
2022-04-11 15:20:21,879 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.self.key.bias
2022-04-11 15:20:21,879 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.self.value.weight
2022-04-11 15:20:21,879 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.self.value.bias
2022-04-11 15:20:21,879 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.output.dense.weight
2022-04-11 15:20:21,879 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.output.dense.bias
2022-04-11 15:20:21,879 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.output.LayerNorm.weight
2022-04-11 15:20:21,880 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.output.LayerNorm.bias
2022-04-11 15:20:21,880 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.intermediate.dense.weight
2022-04-11 15:20:21,880 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.intermediate.dense.bias
2022-04-11 15:20:21,880 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.output.dense.weight
2022-04-11 15:20:21,880 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.output.dense.bias
2022-04-11 15:20:21,880 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.output.LayerNorm.weight
2022-04-11 15:20:21,880 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.output.LayerNorm.bias
2022-04-11 15:20:21,880 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.self.query.weight
2022-04-11 15:20:21,880 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.self.query.bias
2022-04-11 15:20:21,880 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.self.key.weight
2022-04-11 15:20:21,880 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.self.key.bias
2022-04-11 15:20:21,880 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.self.value.weight
2022-04-11 15:20:21,880 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.self.value.bias
2022-04-11 15:20:21,880 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.output.dense.weight
2022-04-11 15:20:21,880 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.output.dense.bias
2022-04-11 15:20:21,880 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.output.LayerNorm.weight
2022-04-11 15:20:21,880 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.output.LayerNorm.bias
2022-04-11 15:20:21,880 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.intermediate.dense.weight
2022-04-11 15:20:21,880 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.intermediate.dense.bias
2022-04-11 15:20:21,880 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.output.dense.weight
2022-04-11 15:20:21,880 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.output.dense.bias
2022-04-11 15:20:21,881 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.output.LayerNorm.weight
2022-04-11 15:20:21,881 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.output.LayerNorm.bias
2022-04-11 15:20:21,881 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.self.query.weight
2022-04-11 15:20:21,881 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.self.query.bias
2022-04-11 15:20:21,881 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.self.key.weight
2022-04-11 15:20:21,881 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.self.key.bias
2022-04-11 15:20:21,881 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.self.value.weight
2022-04-11 15:20:21,881 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.self.value.bias
2022-04-11 15:20:21,881 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.output.dense.weight
2022-04-11 15:20:21,881 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.output.dense.bias
2022-04-11 15:20:21,881 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.output.LayerNorm.weight
2022-04-11 15:20:21,881 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.output.LayerNorm.bias
2022-04-11 15:20:21,881 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.intermediate.dense.weight
2022-04-11 15:20:21,881 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.intermediate.dense.bias
2022-04-11 15:20:21,881 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.output.dense.weight
2022-04-11 15:20:21,881 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.output.dense.bias
2022-04-11 15:20:21,881 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.output.LayerNorm.weight
2022-04-11 15:20:21,881 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.output.LayerNorm.bias
2022-04-11 15:20:21,881 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.self.query.weight
2022-04-11 15:20:21,881 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.self.query.bias
2022-04-11 15:20:21,881 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.self.key.weight
2022-04-11 15:20:21,881 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.self.key.bias
2022-04-11 15:20:21,882 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.self.value.weight
2022-04-11 15:20:21,882 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.self.value.bias
2022-04-11 15:20:21,882 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.output.dense.weight
2022-04-11 15:20:21,882 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.output.dense.bias
2022-04-11 15:20:21,882 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.output.LayerNorm.weight
2022-04-11 15:20:21,882 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.output.LayerNorm.bias
2022-04-11 15:20:21,882 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.intermediate.dense.weight
2022-04-11 15:20:21,882 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.intermediate.dense.bias
2022-04-11 15:20:21,882 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.output.dense.weight
2022-04-11 15:20:21,882 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.output.dense.bias
2022-04-11 15:20:21,882 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.output.LayerNorm.weight
2022-04-11 15:20:21,882 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.output.LayerNorm.bias
2022-04-11 15:20:21,882 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.self.query.weight
2022-04-11 15:20:21,882 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.self.query.bias
2022-04-11 15:20:21,882 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.self.key.weight
2022-04-11 15:20:21,882 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.self.key.bias
2022-04-11 15:20:21,882 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.self.value.weight
2022-04-11 15:20:21,882 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.self.value.bias
2022-04-11 15:20:21,882 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.output.dense.weight
2022-04-11 15:20:21,882 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.output.dense.bias
2022-04-11 15:20:21,882 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.output.LayerNorm.weight
2022-04-11 15:20:21,883 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.output.LayerNorm.bias
2022-04-11 15:20:21,883 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.intermediate.dense.weight
2022-04-11 15:20:21,883 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.intermediate.dense.bias
2022-04-11 15:20:21,883 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.output.dense.weight
2022-04-11 15:20:21,883 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.output.dense.bias
2022-04-11 15:20:21,883 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.output.LayerNorm.weight
2022-04-11 15:20:21,883 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.output.LayerNorm.bias
2022-04-11 15:20:21,883 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.self.query.weight
2022-04-11 15:20:21,883 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.self.query.bias
2022-04-11 15:20:21,883 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.self.key.weight
2022-04-11 15:20:21,883 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.self.key.bias
2022-04-11 15:20:21,883 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.self.value.weight
2022-04-11 15:20:21,883 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.self.value.bias
2022-04-11 15:20:21,883 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.output.dense.weight
2022-04-11 15:20:21,883 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.output.dense.bias
2022-04-11 15:20:21,883 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.output.LayerNorm.weight
2022-04-11 15:20:21,883 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.output.LayerNorm.bias
2022-04-11 15:20:21,883 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.intermediate.dense.weight
2022-04-11 15:20:21,883 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.intermediate.dense.bias
2022-04-11 15:20:21,883 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.output.dense.weight
2022-04-11 15:20:21,883 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.output.dense.bias
2022-04-11 15:20:21,883 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.output.LayerNorm.weight
2022-04-11 15:20:21,884 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.output.LayerNorm.bias
2022-04-11 15:20:21,884 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.self.query.weight
2022-04-11 15:20:21,884 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.self.query.bias
2022-04-11 15:20:21,884 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.self.key.weight
2022-04-11 15:20:21,884 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.self.key.bias
2022-04-11 15:20:21,884 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.self.value.weight
2022-04-11 15:20:21,884 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.self.value.bias
2022-04-11 15:20:21,884 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.output.dense.weight
2022-04-11 15:20:21,884 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.output.dense.bias
2022-04-11 15:20:21,884 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.output.LayerNorm.weight
2022-04-11 15:20:21,884 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.output.LayerNorm.bias
2022-04-11 15:20:21,884 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.intermediate.dense.weight
2022-04-11 15:20:21,884 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.intermediate.dense.bias
2022-04-11 15:20:21,884 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.output.dense.weight
2022-04-11 15:20:21,884 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.output.dense.bias
2022-04-11 15:20:21,884 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.output.LayerNorm.weight
2022-04-11 15:20:21,884 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.output.LayerNorm.bias
2022-04-11 15:20:21,884 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.self.query.weight
2022-04-11 15:20:21,884 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.self.query.bias
2022-04-11 15:20:21,884 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.self.key.weight
2022-04-11 15:20:21,884 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.self.key.bias
2022-04-11 15:20:21,884 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.self.value.weight
2022-04-11 15:20:21,885 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.self.value.bias
2022-04-11 15:20:21,885 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.output.dense.weight
2022-04-11 15:20:21,885 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.output.dense.bias
2022-04-11 15:20:21,885 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.output.LayerNorm.weight
2022-04-11 15:20:21,885 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.output.LayerNorm.bias
2022-04-11 15:20:21,885 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.intermediate.dense.weight
2022-04-11 15:20:21,885 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.intermediate.dense.bias
2022-04-11 15:20:21,885 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.output.dense.weight
2022-04-11 15:20:21,885 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.output.dense.bias
2022-04-11 15:20:21,885 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.output.LayerNorm.weight
2022-04-11 15:20:21,885 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.output.LayerNorm.bias
2022-04-11 15:20:21,885 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.self.query.weight
2022-04-11 15:20:21,885 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.self.query.bias
2022-04-11 15:20:21,885 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.self.key.weight
2022-04-11 15:20:21,885 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.self.key.bias
2022-04-11 15:20:21,885 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.self.value.weight
2022-04-11 15:20:21,885 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.self.value.bias
2022-04-11 15:20:21,885 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.output.dense.weight
2022-04-11 15:20:21,885 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.output.dense.bias
2022-04-11 15:20:21,885 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.output.LayerNorm.weight
2022-04-11 15:20:21,885 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.output.LayerNorm.bias
2022-04-11 15:20:21,885 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.intermediate.dense.weight
2022-04-11 15:20:21,886 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.intermediate.dense.bias
2022-04-11 15:20:21,886 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.output.dense.weight
2022-04-11 15:20:21,886 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.output.dense.bias
2022-04-11 15:20:21,886 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.output.LayerNorm.weight
2022-04-11 15:20:21,886 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.output.LayerNorm.bias
2022-04-11 15:20:21,886 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.self.query.weight
2022-04-11 15:20:21,886 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.self.query.bias
2022-04-11 15:20:21,886 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.self.key.weight
2022-04-11 15:20:21,886 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.self.key.bias
2022-04-11 15:20:21,886 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.self.value.weight
2022-04-11 15:20:21,886 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.self.value.bias
2022-04-11 15:20:21,886 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.output.dense.weight
2022-04-11 15:20:21,886 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.output.dense.bias
2022-04-11 15:20:21,886 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.output.LayerNorm.weight
2022-04-11 15:20:21,886 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.output.LayerNorm.bias
2022-04-11 15:20:21,886 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.intermediate.dense.weight
2022-04-11 15:20:21,886 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.intermediate.dense.bias
2022-04-11 15:20:21,886 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.output.dense.weight
2022-04-11 15:20:21,886 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.output.dense.bias
2022-04-11 15:20:21,886 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.output.LayerNorm.weight
2022-04-11 15:20:21,886 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.output.LayerNorm.bias
2022-04-11 15:20:21,887 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.self.query.weight
2022-04-11 15:20:21,887 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.self.query.bias
2022-04-11 15:20:21,887 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.self.key.weight
2022-04-11 15:20:21,887 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.self.key.bias
2022-04-11 15:20:21,887 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.self.value.weight
2022-04-11 15:20:21,887 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.self.value.bias
2022-04-11 15:20:21,887 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.output.dense.weight
2022-04-11 15:20:21,887 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.output.dense.bias
2022-04-11 15:20:21,887 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.output.LayerNorm.weight
2022-04-11 15:20:21,887 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.output.LayerNorm.bias
2022-04-11 15:20:21,887 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.intermediate.dense.weight
2022-04-11 15:20:21,887 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.intermediate.dense.bias
2022-04-11 15:20:21,887 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.output.dense.weight
2022-04-11 15:20:21,887 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.output.dense.bias
2022-04-11 15:20:21,887 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.output.LayerNorm.weight
2022-04-11 15:20:21,887 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.output.LayerNorm.bias
2022-04-11 15:20:21,887 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.self.query.weight
2022-04-11 15:20:21,887 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.self.query.bias
2022-04-11 15:20:21,887 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.self.key.weight
2022-04-11 15:20:21,887 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.self.key.bias
2022-04-11 15:20:21,888 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.self.value.weight
2022-04-11 15:20:21,888 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.self.value.bias
2022-04-11 15:20:21,888 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.output.dense.weight
2022-04-11 15:20:21,888 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.output.dense.bias
2022-04-11 15:20:21,888 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.output.LayerNorm.weight
2022-04-11 15:20:21,888 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.output.LayerNorm.bias
2022-04-11 15:20:21,888 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.intermediate.dense.weight
2022-04-11 15:20:21,888 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.intermediate.dense.bias
2022-04-11 15:20:21,888 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.output.dense.weight
2022-04-11 15:20:21,888 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.output.dense.bias
2022-04-11 15:20:21,888 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.output.LayerNorm.weight
2022-04-11 15:20:21,888 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.output.LayerNorm.bias
2022-04-11 15:20:21,888 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.self.query.weight
2022-04-11 15:20:21,888 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.self.query.bias
2022-04-11 15:20:21,888 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.self.key.weight
2022-04-11 15:20:21,888 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.self.key.bias
2022-04-11 15:20:21,888 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.self.value.weight
2022-04-11 15:20:21,888 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.self.value.bias
2022-04-11 15:20:21,889 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.output.dense.weight
2022-04-11 15:20:21,889 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.output.dense.bias
2022-04-11 15:20:21,889 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.output.LayerNorm.weight
2022-04-11 15:20:21,889 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.output.LayerNorm.bias
2022-04-11 15:20:21,889 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.intermediate.dense.weight
2022-04-11 15:20:21,889 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.intermediate.dense.bias
2022-04-11 15:20:21,889 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.output.dense.weight
2022-04-11 15:20:21,889 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.output.dense.bias
2022-04-11 15:20:21,889 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.output.LayerNorm.weight
2022-04-11 15:20:21,889 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.output.LayerNorm.bias
2022-04-11 15:20:21,889 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.self.query.weight
2022-04-11 15:20:21,889 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.self.query.bias
2022-04-11 15:20:21,889 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.self.key.weight
2022-04-11 15:20:21,889 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.self.key.bias
2022-04-11 15:20:21,889 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.self.value.weight
2022-04-11 15:20:21,889 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.self.value.bias
2022-04-11 15:20:21,889 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.output.dense.weight
2022-04-11 15:20:21,889 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.output.dense.bias
2022-04-11 15:20:21,889 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.output.LayerNorm.weight
2022-04-11 15:20:21,889 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.output.LayerNorm.bias
2022-04-11 15:20:21,889 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.intermediate.dense.weight
2022-04-11 15:20:21,890 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.intermediate.dense.bias
2022-04-11 15:20:21,890 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.output.dense.weight
2022-04-11 15:20:21,890 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.output.dense.bias
2022-04-11 15:20:21,890 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.output.LayerNorm.weight
2022-04-11 15:20:21,890 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.output.LayerNorm.bias
2022-04-11 15:20:21,890 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.self.query.weight
2022-04-11 15:20:21,890 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.self.query.bias
2022-04-11 15:20:21,890 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.self.key.weight
2022-04-11 15:20:21,890 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.self.key.bias
2022-04-11 15:20:21,890 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.self.value.weight
2022-04-11 15:20:21,890 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.self.value.bias
2022-04-11 15:20:21,890 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.output.dense.weight
2022-04-11 15:20:21,890 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.output.dense.bias
2022-04-11 15:20:21,890 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.output.LayerNorm.weight
2022-04-11 15:20:21,890 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.output.LayerNorm.bias
2022-04-11 15:20:21,890 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.intermediate.dense.weight
2022-04-11 15:20:21,890 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.intermediate.dense.bias
2022-04-11 15:20:21,890 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.output.dense.weight
2022-04-11 15:20:21,890 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.output.dense.bias
2022-04-11 15:20:21,890 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.output.LayerNorm.weight
2022-04-11 15:20:21,890 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.output.LayerNorm.bias
2022-04-11 15:20:21,891 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.self.query.weight
2022-04-11 15:20:21,891 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.self.query.bias
2022-04-11 15:20:21,891 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.self.key.weight
2022-04-11 15:20:21,891 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.self.key.bias
2022-04-11 15:20:21,891 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.self.value.weight
2022-04-11 15:20:21,891 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.self.value.bias
2022-04-11 15:20:21,891 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.output.dense.weight
2022-04-11 15:20:21,891 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.output.dense.bias
2022-04-11 15:20:21,891 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.output.LayerNorm.weight
2022-04-11 15:20:21,891 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.output.LayerNorm.bias
2022-04-11 15:20:21,891 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.intermediate.dense.weight
2022-04-11 15:20:21,891 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.intermediate.dense.bias
2022-04-11 15:20:21,891 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.output.dense.weight
2022-04-11 15:20:21,891 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.output.dense.bias
2022-04-11 15:20:21,891 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.output.LayerNorm.weight
2022-04-11 15:20:21,891 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.output.LayerNorm.bias
2022-04-11 15:20:21,891 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.self.query.weight
2022-04-11 15:20:21,891 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.self.query.bias
2022-04-11 15:20:21,891 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.self.key.weight
2022-04-11 15:20:21,891 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.self.key.bias
2022-04-11 15:20:21,891 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.self.value.weight
2022-04-11 15:20:21,891 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.self.value.bias
2022-04-11 15:20:21,892 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.output.dense.weight
2022-04-11 15:20:21,892 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.output.dense.bias
2022-04-11 15:20:21,892 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.output.LayerNorm.weight
2022-04-11 15:20:21,892 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.output.LayerNorm.bias
2022-04-11 15:20:21,892 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.intermediate.dense.weight
2022-04-11 15:20:21,892 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.intermediate.dense.bias
2022-04-11 15:20:21,892 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.output.dense.weight
2022-04-11 15:20:21,892 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.output.dense.bias
2022-04-11 15:20:21,892 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.output.LayerNorm.weight
2022-04-11 15:20:21,892 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.output.LayerNorm.bias
2022-04-11 15:20:21,892 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.self.query.weight
2022-04-11 15:20:21,892 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.self.query.bias
2022-04-11 15:20:21,892 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.self.key.weight
2022-04-11 15:20:21,892 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.self.key.bias
2022-04-11 15:20:21,892 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.self.value.weight
2022-04-11 15:20:21,892 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.self.value.bias
2022-04-11 15:20:21,892 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.output.dense.weight
2022-04-11 15:20:21,892 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.output.dense.bias
2022-04-11 15:20:21,892 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.output.LayerNorm.weight
2022-04-11 15:20:21,892 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.output.LayerNorm.bias
2022-04-11 15:20:21,892 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.intermediate.dense.weight
2022-04-11 15:20:21,892 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.intermediate.dense.bias
2022-04-11 15:20:21,893 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.output.dense.weight
2022-04-11 15:20:21,893 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.output.dense.bias
2022-04-11 15:20:21,893 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.output.LayerNorm.weight
2022-04-11 15:20:21,893 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.output.LayerNorm.bias
2022-04-11 15:20:21,893 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.self.query.weight
2022-04-11 15:20:21,893 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.self.query.bias
2022-04-11 15:20:21,893 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.self.key.weight
2022-04-11 15:20:21,893 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.self.key.bias
2022-04-11 15:20:21,893 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.self.value.weight
2022-04-11 15:20:21,893 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.self.value.bias
2022-04-11 15:20:21,893 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.output.dense.weight
2022-04-11 15:20:21,893 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.output.dense.bias
2022-04-11 15:20:21,893 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.output.LayerNorm.weight
2022-04-11 15:20:21,893 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.output.LayerNorm.bias
2022-04-11 15:20:21,893 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.intermediate.dense.weight
2022-04-11 15:20:21,893 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.intermediate.dense.bias
2022-04-11 15:20:21,893 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.output.dense.weight
2022-04-11 15:20:21,893 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.output.dense.bias
2022-04-11 15:20:21,893 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.output.LayerNorm.weight
2022-04-11 15:20:21,893 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.output.LayerNorm.bias
2022-04-11 15:20:21,893 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.self.query.weight
2022-04-11 15:20:21,894 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.self.query.bias
2022-04-11 15:20:21,894 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.self.key.weight
2022-04-11 15:20:21,894 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.self.key.bias
2022-04-11 15:20:21,894 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.self.value.weight
2022-04-11 15:20:21,894 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.self.value.bias
2022-04-11 15:20:21,894 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.output.dense.weight
2022-04-11 15:20:21,894 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.output.dense.bias
2022-04-11 15:20:21,894 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.output.LayerNorm.weight
2022-04-11 15:20:21,894 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.output.LayerNorm.bias
2022-04-11 15:20:21,894 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.intermediate.dense.weight
2022-04-11 15:20:21,894 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.intermediate.dense.bias
2022-04-11 15:20:21,894 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.output.dense.weight
2022-04-11 15:20:21,894 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.output.dense.bias
2022-04-11 15:20:21,894 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.output.LayerNorm.weight
2022-04-11 15:20:21,894 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.output.LayerNorm.bias
2022-04-11 15:20:21,894 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.self.query.weight
2022-04-11 15:20:21,894 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.self.query.bias
2022-04-11 15:20:21,894 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.self.key.weight
2022-04-11 15:20:21,894 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.self.key.bias
2022-04-11 15:20:21,894 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.self.value.weight
2022-04-11 15:20:21,894 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.self.value.bias
2022-04-11 15:20:21,894 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.output.dense.weight
2022-04-11 15:20:21,895 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.output.dense.bias
2022-04-11 15:20:21,895 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.output.LayerNorm.weight
2022-04-11 15:20:21,895 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.output.LayerNorm.bias
2022-04-11 15:20:21,895 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.intermediate.dense.weight
2022-04-11 15:20:21,895 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.intermediate.dense.bias
2022-04-11 15:20:21,895 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.output.dense.weight
2022-04-11 15:20:21,895 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.output.dense.bias
2022-04-11 15:20:21,895 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.output.LayerNorm.weight
2022-04-11 15:20:21,895 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.output.LayerNorm.bias
2022-04-11 15:20:21,895 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.self.query.weight
2022-04-11 15:20:21,895 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.self.query.bias
2022-04-11 15:20:21,895 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.self.key.weight
2022-04-11 15:20:21,895 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.self.key.bias
2022-04-11 15:20:21,895 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.self.value.weight
2022-04-11 15:20:21,895 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.self.value.bias
2022-04-11 15:20:21,895 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.output.dense.weight
2022-04-11 15:20:21,895 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.output.dense.bias
2022-04-11 15:20:21,895 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.output.LayerNorm.weight
2022-04-11 15:20:21,895 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.output.LayerNorm.bias
2022-04-11 15:20:21,895 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.intermediate.dense.weight
2022-04-11 15:20:21,895 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.intermediate.dense.bias
2022-04-11 15:20:21,895 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.output.dense.weight
2022-04-11 15:20:21,896 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.output.dense.bias
2022-04-11 15:20:21,896 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.output.LayerNorm.weight
2022-04-11 15:20:21,896 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.output.LayerNorm.bias
2022-04-11 15:20:21,896 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.self.query.weight
2022-04-11 15:20:21,896 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.self.query.bias
2022-04-11 15:20:21,896 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.self.key.weight
2022-04-11 15:20:21,896 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.self.key.bias
2022-04-11 15:20:21,896 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.self.value.weight
2022-04-11 15:20:21,896 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.self.value.bias
2022-04-11 15:20:21,896 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.output.dense.weight
2022-04-11 15:20:21,896 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.output.dense.bias
2022-04-11 15:20:21,896 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.output.LayerNorm.weight
2022-04-11 15:20:21,896 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.output.LayerNorm.bias
2022-04-11 15:20:21,896 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.intermediate.dense.weight
2022-04-11 15:20:21,896 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.intermediate.dense.bias
2022-04-11 15:20:21,896 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.output.dense.weight
2022-04-11 15:20:21,896 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.output.dense.bias
2022-04-11 15:20:21,896 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.output.LayerNorm.weight
2022-04-11 15:20:21,896 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.output.LayerNorm.bias
2022-04-11 15:20:21,896 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.self.query.weight
2022-04-11 15:20:21,896 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.self.query.bias
2022-04-11 15:20:21,897 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.self.key.weight
2022-04-11 15:20:21,897 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.self.key.bias
2022-04-11 15:20:21,897 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.self.value.weight
2022-04-11 15:20:21,897 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.self.value.bias
2022-04-11 15:20:21,897 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.output.dense.weight
2022-04-11 15:20:21,897 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.output.dense.bias
2022-04-11 15:20:21,897 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.output.LayerNorm.weight
2022-04-11 15:20:21,897 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.output.LayerNorm.bias
2022-04-11 15:20:21,897 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.intermediate.dense.weight
2022-04-11 15:20:21,897 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.intermediate.dense.bias
2022-04-11 15:20:21,897 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.output.dense.weight
2022-04-11 15:20:21,897 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.output.dense.bias
2022-04-11 15:20:21,897 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.output.LayerNorm.weight
2022-04-11 15:20:21,897 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.output.LayerNorm.bias
2022-04-11 15:20:21,897 - INFO - allennlp.common.util - _classifier.roberta.pooler.dense.weight
2022-04-11 15:20:21,897 - INFO - allennlp.common.util - _classifier.roberta.pooler.dense.bias
2022-04-11 15:20:21,897 - INFO - allennlp.common.util - _classifier.classifier.dense.weight
2022-04-11 15:20:21,897 - INFO - allennlp.common.util - _classifier.classifier.dense.bias
2022-04-11 15:20:21,897 - INFO - allennlp.common.util - _classifier.classifier.out_proj.weight
2022-04-11 15:20:21,897 - INFO - allennlp.common.util - _classifier.classifier.out_proj.bias
2022-04-11 15:20:21,898 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.type = slanted_triangular
2022-04-11 15:20:21,898 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.cut_frac = 0.12
2022-04-11 15:20:21,898 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.ratio = 32
2022-04-11 15:20:21,898 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.last_epoch = -1
2022-04-11 15:20:21,898 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.gradual_unfreezing = False
2022-04-11 15:20:21,898 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.discriminative_fine_tuning = False
2022-04-11 15:20:21,898 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.decay_factor = 0.38
2022-04-11 15:20:21,898 - INFO - allennlp.common.params - type = default
2022-04-11 15:20:21,899 - INFO - allennlp.common.params - keep_serialized_model_every_num_seconds = None
2022-04-11 15:20:21,899 - INFO - allennlp.common.params - num_serialized_models_to_keep = 2
2022-04-11 15:20:21,899 - INFO - allennlp.common.params - model_save_interval = None
2022-04-11 15:20:21,899 - INFO - allennlp.common.params - summary_interval = 100
2022-04-11 15:20:21,899 - INFO - allennlp.common.params - histogram_interval = None
2022-04-11 15:20:21,899 - INFO - allennlp.common.params - batch_size_interval = None
2022-04-11 15:20:21,899 - INFO - allennlp.common.params - should_log_parameter_statistics = True
2022-04-11 15:20:21,899 - INFO - allennlp.common.params - should_log_learning_rate = False
2022-04-11 15:20:21,899 - INFO - allennlp.common.params - get_batch_num_total = None
2022-04-11 15:20:21,901 - WARNING - allennlp.training.trainer - You provided a validation dataset but patience was set to None, meaning that early stopping is disabled
2022-04-11 15:20:21,993 - INFO - allennlp.training.trainer - Beginning training.
2022-04-11 15:20:21,993 - INFO - allennlp.training.trainer - Epoch 0/14
2022-04-11 15:20:21,993 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-11 15:20:21,993 - INFO - allennlp.training.trainer - GPU 0 memory usage: 1.3G
2022-04-11 15:20:21,995 - INFO - allennlp.training.trainer - Training
2022-04-11 15:20:21,995 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-11 15:20:21,995 - INFO - allennlp.data.samplers.bucket_batch_sampler - No sorting keys given; trying to guess a good one
2022-04-11 15:20:21,995 - INFO - allennlp.data.samplers.bucket_batch_sampler - Using ['tokens'] as the sorting keys
2022-04-11 15:20:41,885 - INFO - tqdm - accuracy: 0.5156, batch_loss: 1.5624, loss: 1.5557 ||:   3%|3         | 2/65 [00:19<10:26,  9.95s/it]
2022-04-11 15:20:51,923 - INFO - tqdm - accuracy: 0.5208, batch_loss: 2.0294, loss: 1.7136 ||:   5%|4         | 3/65 [00:29<10:19,  9.99s/it]
2022-04-11 15:21:02,016 - INFO - tqdm - accuracy: 0.5469, batch_loss: 1.3658, loss: 1.6267 ||:   6%|6         | 4/65 [00:40<10:11, 10.03s/it]
2022-04-11 15:21:12,175 - INFO - tqdm - accuracy: 0.5312, batch_loss: 1.2696, loss: 1.5553 ||:   8%|7         | 5/65 [00:50<10:04, 10.08s/it]
2022-04-11 15:21:22,397 - INFO - tqdm - accuracy: 0.5208, batch_loss: 1.7749, loss: 1.5919 ||:   9%|9         | 6/65 [01:00<09:57, 10.13s/it]
2022-04-11 15:21:32,687 - INFO - tqdm - accuracy: 0.5223, batch_loss: 1.5466, loss: 1.5854 ||:  11%|#         | 7/65 [01:10<09:50, 10.18s/it]
2022-04-11 15:21:42,995 - INFO - tqdm - accuracy: 0.5117, batch_loss: 1.6490, loss: 1.5934 ||:  12%|#2        | 8/65 [01:20<09:42, 10.22s/it]
2022-04-11 15:21:53,365 - INFO - tqdm - accuracy: 0.5035, batch_loss: 1.8843, loss: 1.6257 ||:  14%|#3        | 9/65 [01:31<09:34, 10.27s/it]
2022-04-11 15:22:03,644 - INFO - tqdm - accuracy: 0.4938, batch_loss: 1.7199, loss: 1.6351 ||:  15%|#5        | 10/65 [01:41<09:24, 10.27s/it]
2022-04-11 15:22:14,098 - INFO - tqdm - accuracy: 0.4943, batch_loss: 1.6306, loss: 1.6347 ||:  17%|#6        | 11/65 [01:52<09:17, 10.33s/it]
2022-04-11 15:22:24,511 - INFO - tqdm - accuracy: 0.5026, batch_loss: 1.1837, loss: 1.5971 ||:  18%|#8        | 12/65 [02:02<09:08, 10.35s/it]
2022-04-11 15:22:34,955 - INFO - tqdm - accuracy: 0.5120, batch_loss: 1.1488, loss: 1.5626 ||:  20%|##        | 13/65 [02:12<08:59, 10.38s/it]
2022-04-11 15:22:45,318 - INFO - tqdm - accuracy: 0.5112, batch_loss: 1.7782, loss: 1.5780 ||:  22%|##1       | 14/65 [02:23<08:49, 10.38s/it]
2022-04-11 15:22:55,842 - INFO - tqdm - accuracy: 0.5167, batch_loss: 1.4206, loss: 1.5675 ||:  23%|##3       | 15/65 [02:33<08:41, 10.42s/it]
2022-04-11 15:23:06,402 - INFO - tqdm - accuracy: 0.5117, batch_loss: 1.4023, loss: 1.5572 ||:  25%|##4       | 16/65 [02:44<08:32, 10.46s/it]
2022-04-11 15:23:16,991 - INFO - tqdm - accuracy: 0.5165, batch_loss: 0.8197, loss: 1.5138 ||:  26%|##6       | 17/65 [02:54<08:24, 10.50s/it]
2022-04-11 15:23:27,614 - INFO - tqdm - accuracy: 0.5208, batch_loss: 1.0832, loss: 1.4899 ||:  28%|##7       | 18/65 [03:05<08:15, 10.54s/it]
2022-04-11 15:23:38,255 - INFO - tqdm - accuracy: 0.5230, batch_loss: 1.1803, loss: 1.4736 ||:  29%|##9       | 19/65 [03:16<08:06, 10.57s/it]
2022-04-11 15:23:48,712 - INFO - tqdm - accuracy: 0.5219, batch_loss: 1.0608, loss: 1.4530 ||:  31%|###       | 20/65 [03:26<07:54, 10.54s/it]
2022-04-11 15:23:59,394 - INFO - tqdm - accuracy: 0.5164, batch_loss: 1.7846, loss: 1.4687 ||:  32%|###2      | 21/65 [03:37<07:45, 10.58s/it]
2022-04-11 15:24:10,026 - INFO - tqdm - accuracy: 0.5142, batch_loss: 1.2502, loss: 1.4588 ||:  34%|###3      | 22/65 [03:48<07:35, 10.59s/it]
2022-04-11 15:24:20,778 - INFO - tqdm - accuracy: 0.5136, batch_loss: 1.2420, loss: 1.4494 ||:  35%|###5      | 23/65 [03:58<07:26, 10.64s/it]
2022-04-11 15:24:31,568 - INFO - tqdm - accuracy: 0.5143, batch_loss: 1.2049, loss: 1.4392 ||:  37%|###6      | 24/65 [04:09<07:18, 10.69s/it]
2022-04-11 15:24:42,367 - INFO - tqdm - accuracy: 0.5100, batch_loss: 1.3418, loss: 1.4353 ||:  38%|###8      | 25/65 [04:20<07:08, 10.72s/it]
2022-04-11 15:24:53,171 - INFO - tqdm - accuracy: 0.5024, batch_loss: 1.2245, loss: 1.4272 ||:  40%|####      | 26/65 [04:31<06:59, 10.75s/it]
2022-04-11 15:25:03,995 - INFO - tqdm - accuracy: 0.5012, batch_loss: 0.9899, loss: 1.4110 ||:  42%|####1     | 27/65 [04:41<06:49, 10.77s/it]
2022-04-11 15:25:14,825 - INFO - tqdm - accuracy: 0.5033, batch_loss: 0.9256, loss: 1.3937 ||:  43%|####3     | 28/65 [04:52<06:39, 10.79s/it]
2022-04-11 15:25:25,650 - INFO - tqdm - accuracy: 0.5011, batch_loss: 1.3654, loss: 1.3927 ||:  45%|####4     | 29/65 [05:03<06:28, 10.80s/it]
2022-04-11 15:25:36,508 - INFO - tqdm - accuracy: 0.4990, batch_loss: 1.4116, loss: 1.3933 ||:  46%|####6     | 30/65 [05:14<06:18, 10.82s/it]
2022-04-11 15:25:47,372 - INFO - tqdm - accuracy: 0.5050, batch_loss: 0.7558, loss: 1.3727 ||:  48%|####7     | 31/65 [05:25<06:08, 10.83s/it]
2022-04-11 15:25:58,253 - INFO - tqdm - accuracy: 0.4980, batch_loss: 1.5265, loss: 1.3776 ||:  49%|####9     | 32/65 [05:36<05:57, 10.85s/it]
2022-04-11 15:26:09,138 - INFO - tqdm - accuracy: 0.5028, batch_loss: 0.6578, loss: 1.3557 ||:  51%|#####     | 33/65 [05:47<05:47, 10.86s/it]
2022-04-11 15:26:20,027 - INFO - tqdm - accuracy: 0.5000, batch_loss: 1.0508, loss: 1.3468 ||:  52%|#####2    | 34/65 [05:58<05:36, 10.87s/it]
2022-04-11 15:26:30,904 - INFO - tqdm - accuracy: 0.4964, batch_loss: 1.0476, loss: 1.3382 ||:  54%|#####3    | 35/65 [06:08<05:26, 10.87s/it]
2022-04-11 15:26:41,793 - INFO - tqdm - accuracy: 0.4983, batch_loss: 0.8716, loss: 1.3253 ||:  55%|#####5    | 36/65 [06:19<05:15, 10.88s/it]
2022-04-11 15:26:52,704 - INFO - tqdm - accuracy: 0.4975, batch_loss: 0.9194, loss: 1.3143 ||:  57%|#####6    | 37/65 [06:30<05:04, 10.89s/it]
2022-04-11 15:27:03,612 - INFO - tqdm - accuracy: 0.4984, batch_loss: 0.7265, loss: 1.2988 ||:  58%|#####8    | 38/65 [06:41<04:54, 10.89s/it]
2022-04-11 15:27:14,526 - INFO - tqdm - accuracy: 0.4976, batch_loss: 0.8980, loss: 1.2886 ||:  60%|######    | 39/65 [06:52<04:43, 10.90s/it]
2022-04-11 15:27:25,432 - INFO - tqdm - accuracy: 0.5008, batch_loss: 0.6999, loss: 1.2738 ||:  62%|######1   | 40/65 [07:03<04:32, 10.90s/it]
2022-04-11 15:27:36,340 - INFO - tqdm - accuracy: 0.5008, batch_loss: 0.6367, loss: 1.2583 ||:  63%|######3   | 41/65 [07:14<04:21, 10.90s/it]
2022-04-11 15:27:47,265 - INFO - tqdm - accuracy: 0.5037, batch_loss: 0.6711, loss: 1.2443 ||:  65%|######4   | 42/65 [07:25<04:10, 10.91s/it]
2022-04-11 15:27:58,180 - INFO - tqdm - accuracy: 0.5036, batch_loss: 0.9092, loss: 1.2365 ||:  66%|######6   | 43/65 [07:36<04:00, 10.91s/it]
2022-04-11 15:28:09,106 - INFO - tqdm - accuracy: 0.5043, batch_loss: 0.7349, loss: 1.2251 ||:  68%|######7   | 44/65 [07:47<03:49, 10.92s/it]
2022-04-11 15:28:20,042 - INFO - tqdm - accuracy: 0.5069, batch_loss: 0.7029, loss: 1.2135 ||:  69%|######9   | 45/65 [07:58<03:38, 10.92s/it]
2022-04-11 15:28:30,953 - INFO - tqdm - accuracy: 0.5061, batch_loss: 0.7337, loss: 1.2031 ||:  71%|#######   | 46/65 [08:08<03:27, 10.92s/it]
2022-04-11 15:28:41,876 - INFO - tqdm - accuracy: 0.5080, batch_loss: 0.7637, loss: 1.1937 ||:  72%|#######2  | 47/65 [08:19<03:16, 10.92s/it]
2022-04-11 15:28:52,793 - INFO - tqdm - accuracy: 0.5052, batch_loss: 0.7781, loss: 1.1851 ||:  74%|#######3  | 48/65 [08:30<03:05, 10.92s/it]
2022-04-11 15:29:03,724 - INFO - tqdm - accuracy: 0.5045, batch_loss: 0.7844, loss: 1.1769 ||:  75%|#######5  | 49/65 [08:41<02:54, 10.92s/it]
2022-04-11 15:29:14,677 - INFO - tqdm - accuracy: 0.5069, batch_loss: 0.5895, loss: 1.1652 ||:  77%|#######6  | 50/65 [08:52<02:43, 10.93s/it]
2022-04-11 15:29:25,602 - INFO - tqdm - accuracy: 0.5080, batch_loss: 0.7151, loss: 1.1563 ||:  78%|#######8  | 51/65 [09:03<02:33, 10.93s/it]
2022-04-11 15:29:36,529 - INFO - tqdm - accuracy: 0.5078, batch_loss: 0.6475, loss: 1.1465 ||:  80%|########  | 52/65 [09:14<02:22, 10.93s/it]
2022-04-11 15:29:47,473 - INFO - tqdm - accuracy: 0.5065, batch_loss: 0.7320, loss: 1.1387 ||:  82%|########1 | 53/65 [09:25<02:11, 10.93s/it]
2022-04-11 15:29:58,081 - INFO - tqdm - accuracy: 0.5107, batch_loss: 0.5858, loss: 1.1285 ||:  83%|########3 | 54/65 [09:36<01:59, 10.84s/it]
2022-04-11 15:30:09,023 - INFO - tqdm - accuracy: 0.5082, batch_loss: 0.7118, loss: 1.1209 ||:  85%|########4 | 55/65 [09:47<01:48, 10.87s/it]
2022-04-11 15:30:19,972 - INFO - tqdm - accuracy: 0.5092, batch_loss: 0.6848, loss: 1.1131 ||:  86%|########6 | 56/65 [09:57<01:38, 10.89s/it]
2022-04-11 15:30:30,862 - INFO - tqdm - accuracy: 0.5058, batch_loss: 0.7236, loss: 1.1063 ||:  88%|########7 | 57/65 [10:08<01:27, 10.89s/it]
2022-04-11 15:30:41,754 - INFO - tqdm - accuracy: 0.5073, batch_loss: 0.6775, loss: 1.0989 ||:  89%|########9 | 58/65 [10:19<01:16, 10.89s/it]
2022-04-11 15:30:52,675 - INFO - tqdm - accuracy: 0.5072, batch_loss: 0.7232, loss: 1.0925 ||:  91%|######### | 59/65 [10:30<01:05, 10.90s/it]
2022-04-11 15:31:03,561 - INFO - tqdm - accuracy: 0.5081, batch_loss: 0.6533, loss: 1.0852 ||:  92%|#########2| 60/65 [10:41<00:54, 10.90s/it]
2022-04-11 15:31:14,488 - INFO - tqdm - accuracy: 0.5095, batch_loss: 0.6641, loss: 1.0783 ||:  94%|#########3| 61/65 [10:52<00:43, 10.91s/it]
2022-04-11 15:31:25,404 - INFO - tqdm - accuracy: 0.5108, batch_loss: 0.6820, loss: 1.0719 ||:  95%|#########5| 62/65 [11:03<00:32, 10.91s/it]
2022-04-11 15:31:36,302 - INFO - tqdm - accuracy: 0.5112, batch_loss: 0.6807, loss: 1.0657 ||:  97%|#########6| 63/65 [11:14<00:21, 10.91s/it]
2022-04-11 15:31:47,210 - INFO - tqdm - accuracy: 0.5085, batch_loss: 0.7733, loss: 1.0611 ||:  98%|#########8| 64/65 [11:25<00:10, 10.91s/it]
2022-04-11 15:31:52,067 - INFO - tqdm - accuracy: 0.5090, batch_loss: 0.7630, loss: 1.0565 ||: 100%|##########| 65/65 [11:30<00:00,  9.09s/it]
2022-04-11 15:31:52,067 - INFO - tqdm - accuracy: 0.5090, batch_loss: 0.7630, loss: 1.0565 ||: 100%|##########| 65/65 [11:30<00:00, 10.62s/it]
2022-04-11 15:31:54,956 - INFO - allennlp.training.trainer - Validating
2022-04-11 15:31:54,957 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-11 15:31:54,958 - INFO - allennlp.data.samplers.bucket_batch_sampler - No sorting keys given; trying to guess a good one
2022-04-11 15:31:54,958 - INFO - allennlp.data.samplers.bucket_batch_sampler - Using ['tokens'] as the sorting keys
2022-04-11 15:32:04,979 - INFO - tqdm - accuracy: 0.4405, batch_loss: 0.1995, loss: 0.7143 ||:  37%|###6      | 42/115 [00:10<00:17,  4.17it/s]
2022-04-11 15:32:15,213 - INFO - tqdm - accuracy: 0.5176, batch_loss: 0.5015, loss: 0.6950 ||:  74%|#######3  | 85/115 [00:20<00:07,  4.19it/s]
2022-04-11 15:32:22,271 - INFO - tqdm - accuracy: 0.5328, batch_loss: 0.7036, loss: 0.6954 ||: 100%|##########| 115/115 [00:27<00:00,  4.18it/s]
2022-04-11 15:32:22,271 - INFO - tqdm - accuracy: 0.5328, batch_loss: 0.7036, loss: 0.6954 ||: 100%|##########| 115/115 [00:27<00:00,  4.21it/s]
2022-04-11 15:32:22,271 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-11 15:32:22,273 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.509  |     0.533
2022-04-11 15:32:22,274 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  1359.606  |       N/A
2022-04-11 15:32:22,274 - INFO - allennlp.training.tensorboard_writer - loss               |     1.057  |     0.695
2022-04-11 15:32:22,275 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5804.656  |       N/A
2022-04-11 15:32:28,148 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to './output_ir-ora-d_hyper7/best.th'.
2022-04-11 15:32:29,417 - INFO - allennlp.training.trainer - Epoch duration: 0:12:07.423666
2022-04-11 15:32:29,417 - INFO - allennlp.training.trainer - Estimated training time remaining: 2:49:43
2022-04-11 15:32:29,417 - INFO - allennlp.training.trainer - Epoch 1/14
2022-04-11 15:32:29,417 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-11 15:32:29,418 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-11 15:32:29,419 - INFO - allennlp.training.trainer - Training
2022-04-11 15:32:29,419 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-11 15:32:40,246 - INFO - tqdm - accuracy: 0.4688, batch_loss: 0.7061, loss: 0.7061 ||:   2%|1         | 1/65 [00:10<11:32, 10.83s/it]
2022-04-11 15:32:51,068 - INFO - tqdm - accuracy: 0.5625, batch_loss: 0.6519, loss: 0.6790 ||:   3%|3         | 2/65 [00:21<11:21, 10.82s/it]
2022-04-11 15:33:01,901 - INFO - tqdm - accuracy: 0.5521, batch_loss: 0.7331, loss: 0.6970 ||:   5%|4         | 3/65 [00:32<11:11, 10.83s/it]
2022-04-11 15:33:12,742 - INFO - tqdm - accuracy: 0.5547, batch_loss: 0.6736, loss: 0.6912 ||:   6%|6         | 4/65 [00:43<11:00, 10.83s/it]
2022-04-11 15:33:23,587 - INFO - tqdm - accuracy: 0.5437, batch_loss: 0.6808, loss: 0.6891 ||:   8%|7         | 5/65 [00:54<10:50, 10.84s/it]
2022-04-11 15:33:34,439 - INFO - tqdm - accuracy: 0.5573, batch_loss: 0.6848, loss: 0.6884 ||:   9%|9         | 6/65 [01:05<10:39, 10.84s/it]
2022-04-11 15:33:45,307 - INFO - tqdm - accuracy: 0.5625, batch_loss: 0.6737, loss: 0.6863 ||:  11%|#         | 7/65 [01:15<10:29, 10.85s/it]
2022-04-11 15:33:56,191 - INFO - tqdm - accuracy: 0.5625, batch_loss: 0.6695, loss: 0.6842 ||:  12%|#2        | 8/65 [01:26<10:19, 10.86s/it]
2022-04-11 15:34:07,082 - INFO - tqdm - accuracy: 0.5729, batch_loss: 0.6509, loss: 0.6805 ||:  14%|#3        | 9/65 [01:37<10:08, 10.87s/it]
2022-04-11 15:34:17,980 - INFO - tqdm - accuracy: 0.5813, batch_loss: 0.6501, loss: 0.6775 ||:  15%|#5        | 10/65 [01:48<09:58, 10.88s/it]
2022-04-11 15:34:28,917 - INFO - tqdm - accuracy: 0.5795, batch_loss: 0.7082, loss: 0.6803 ||:  17%|#6        | 11/65 [01:59<09:48, 10.90s/it]
2022-04-11 15:34:39,839 - INFO - tqdm - accuracy: 0.5781, batch_loss: 0.6603, loss: 0.6786 ||:  18%|#8        | 12/65 [02:10<09:37, 10.90s/it]
2022-04-11 15:34:50,778 - INFO - tqdm - accuracy: 0.5625, batch_loss: 0.7566, loss: 0.6846 ||:  20%|##        | 13/65 [02:21<09:27, 10.91s/it]
2022-04-11 15:35:01,596 - INFO - tqdm - accuracy: 0.5625, batch_loss: 0.7054, loss: 0.6861 ||:  22%|##1       | 14/65 [02:32<09:15, 10.89s/it]
2022-04-11 15:35:12,534 - INFO - tqdm - accuracy: 0.5604, batch_loss: 0.7011, loss: 0.6871 ||:  23%|##3       | 15/65 [02:43<09:05, 10.90s/it]
2022-04-11 15:35:23,463 - INFO - tqdm - accuracy: 0.5586, batch_loss: 0.7256, loss: 0.6895 ||:  25%|##4       | 16/65 [02:54<08:54, 10.91s/it]
2022-04-11 15:35:34,416 - INFO - tqdm - accuracy: 0.5570, batch_loss: 0.7393, loss: 0.6924 ||:  26%|##6       | 17/65 [03:04<08:44, 10.92s/it]
2022-04-11 15:35:45,348 - INFO - tqdm - accuracy: 0.5625, batch_loss: 0.6292, loss: 0.6889 ||:  28%|##7       | 18/65 [03:15<08:33, 10.93s/it]
2022-04-11 15:35:56,287 - INFO - tqdm - accuracy: 0.5641, batch_loss: 0.6811, loss: 0.6885 ||:  29%|##9       | 19/65 [03:26<08:22, 10.93s/it]
2022-04-11 15:36:07,230 - INFO - tqdm - accuracy: 0.5672, batch_loss: 0.6558, loss: 0.6869 ||:  31%|###       | 20/65 [03:37<08:12, 10.93s/it]
2022-04-11 15:36:18,178 - INFO - tqdm - accuracy: 0.5699, batch_loss: 0.6182, loss: 0.6836 ||:  32%|###2      | 21/65 [03:48<08:01, 10.94s/it]
2022-04-11 15:36:28,995 - INFO - tqdm - accuracy: 0.5696, batch_loss: 0.6859, loss: 0.6837 ||:  34%|###3      | 22/65 [03:59<07:48, 10.90s/it]
2022-04-11 15:36:39,938 - INFO - tqdm - accuracy: 0.5720, batch_loss: 0.6732, loss: 0.6832 ||:  35%|###5      | 23/65 [04:10<07:38, 10.91s/it]
2022-04-11 15:36:50,870 - INFO - tqdm - accuracy: 0.5703, batch_loss: 0.7133, loss: 0.6845 ||:  37%|###6      | 24/65 [04:21<07:27, 10.92s/it]
2022-04-11 15:37:01,830 - INFO - tqdm - accuracy: 0.5763, batch_loss: 0.6047, loss: 0.6813 ||:  38%|###8      | 25/65 [04:32<07:17, 10.93s/it]
2022-04-11 15:37:12,802 - INFO - tqdm - accuracy: 0.5721, batch_loss: 0.6941, loss: 0.6818 ||:  40%|####      | 26/65 [04:43<07:06, 10.94s/it]
2022-04-11 15:37:23,753 - INFO - tqdm - accuracy: 0.5694, batch_loss: 0.6707, loss: 0.6814 ||:  42%|####1     | 27/65 [04:54<06:55, 10.95s/it]
2022-04-11 15:37:34,720 - INFO - tqdm - accuracy: 0.5703, batch_loss: 0.6725, loss: 0.6811 ||:  43%|####3     | 28/65 [05:05<06:45, 10.95s/it]
2022-04-11 15:37:45,665 - INFO - tqdm - accuracy: 0.5690, batch_loss: 0.6853, loss: 0.6812 ||:  45%|####4     | 29/65 [05:16<06:34, 10.95s/it]
2022-04-11 15:37:56,604 - INFO - tqdm - accuracy: 0.5667, batch_loss: 0.6929, loss: 0.6816 ||:  46%|####6     | 30/65 [05:27<06:23, 10.95s/it]
2022-04-11 15:38:07,560 - INFO - tqdm - accuracy: 0.5625, batch_loss: 0.6966, loss: 0.6821 ||:  48%|####7     | 31/65 [05:38<06:12, 10.95s/it]
2022-04-11 15:38:18,494 - INFO - tqdm - accuracy: 0.5635, batch_loss: 0.6467, loss: 0.6810 ||:  49%|####9     | 32/65 [05:49<06:01, 10.94s/it]
2022-04-11 15:38:29,416 - INFO - tqdm - accuracy: 0.5625, batch_loss: 0.7134, loss: 0.6820 ||:  51%|#####     | 33/65 [05:59<05:50, 10.94s/it]
2022-04-11 15:38:40,341 - INFO - tqdm - accuracy: 0.5625, batch_loss: 0.6783, loss: 0.6819 ||:  52%|#####2    | 34/65 [06:10<05:38, 10.93s/it]
2022-04-11 15:38:51,538 - INFO - tqdm - accuracy: 0.5589, batch_loss: 0.6921, loss: 0.6821 ||:  54%|#####3    | 35/65 [06:22<05:30, 11.01s/it]
2022-04-11 15:39:02,444 - INFO - tqdm - accuracy: 0.5616, batch_loss: 0.6643, loss: 0.6817 ||:  55%|#####5    | 36/65 [06:33<05:18, 10.98s/it]
2022-04-11 15:39:13,364 - INFO - tqdm - accuracy: 0.5617, batch_loss: 0.6462, loss: 0.6807 ||:  57%|#####6    | 37/65 [06:43<05:06, 10.96s/it]
2022-04-11 15:39:24,301 - INFO - tqdm - accuracy: 0.5633, batch_loss: 0.6772, loss: 0.6806 ||:  58%|#####8    | 38/65 [06:54<04:55, 10.96s/it]
2022-04-11 15:39:35,228 - INFO - tqdm - accuracy: 0.5617, batch_loss: 0.7150, loss: 0.6815 ||:  60%|######    | 39/65 [07:05<04:44, 10.95s/it]
2022-04-11 15:39:46,192 - INFO - tqdm - accuracy: 0.5617, batch_loss: 0.6653, loss: 0.6811 ||:  62%|######1   | 40/65 [07:16<04:33, 10.95s/it]
2022-04-11 15:39:57,156 - INFO - tqdm - accuracy: 0.5602, batch_loss: 0.6962, loss: 0.6814 ||:  63%|######3   | 41/65 [07:27<04:22, 10.96s/it]
2022-04-11 15:40:08,117 - INFO - tqdm - accuracy: 0.5662, batch_loss: 0.6237, loss: 0.6801 ||:  65%|######4   | 42/65 [07:38<04:12, 10.96s/it]
2022-04-11 15:40:19,081 - INFO - tqdm - accuracy: 0.5632, batch_loss: 0.7141, loss: 0.6809 ||:  66%|######6   | 43/65 [07:49<04:01, 10.96s/it]
2022-04-11 15:40:30,046 - INFO - tqdm - accuracy: 0.5632, batch_loss: 0.6550, loss: 0.6803 ||:  68%|######7   | 44/65 [08:00<03:50, 10.96s/it]
2022-04-11 15:40:41,007 - INFO - tqdm - accuracy: 0.5639, batch_loss: 0.6348, loss: 0.6793 ||:  69%|######9   | 45/65 [08:11<03:39, 10.96s/it]
2022-04-11 15:40:51,963 - INFO - tqdm - accuracy: 0.5645, batch_loss: 0.6556, loss: 0.6788 ||:  71%|#######   | 46/65 [08:22<03:28, 10.96s/it]
2022-04-11 15:41:02,907 - INFO - tqdm - accuracy: 0.5652, batch_loss: 0.7165, loss: 0.6796 ||:  72%|#######2  | 47/65 [08:33<03:17, 10.95s/it]
2022-04-11 15:41:13,849 - INFO - tqdm - accuracy: 0.5632, batch_loss: 0.7540, loss: 0.6811 ||:  74%|#######3  | 48/65 [08:44<03:06, 10.95s/it]
2022-04-11 15:41:24,778 - INFO - tqdm - accuracy: 0.5612, batch_loss: 0.7704, loss: 0.6829 ||:  75%|#######5  | 49/65 [08:55<02:55, 10.94s/it]
2022-04-11 15:41:35,700 - INFO - tqdm - accuracy: 0.5631, batch_loss: 0.6936, loss: 0.6831 ||:  77%|#######6  | 50/65 [09:06<02:44, 10.94s/it]
2022-04-11 15:41:46,608 - INFO - tqdm - accuracy: 0.5637, batch_loss: 0.6835, loss: 0.6832 ||:  78%|#######8  | 51/65 [09:17<02:33, 10.93s/it]
2022-04-11 15:41:57,536 - INFO - tqdm - accuracy: 0.5643, batch_loss: 0.6975, loss: 0.6834 ||:  80%|########  | 52/65 [09:28<02:22, 10.93s/it]
2022-04-11 15:42:08,464 - INFO - tqdm - accuracy: 0.5678, batch_loss: 0.6124, loss: 0.6821 ||:  82%|########1 | 53/65 [09:39<02:11, 10.93s/it]
2022-04-11 15:42:19,367 - INFO - tqdm - accuracy: 0.5683, batch_loss: 0.7262, loss: 0.6829 ||:  83%|########3 | 54/65 [09:49<02:00, 10.92s/it]
2022-04-11 15:42:30,311 - INFO - tqdm - accuracy: 0.5710, batch_loss: 0.5911, loss: 0.6812 ||:  85%|########4 | 55/65 [10:00<01:49, 10.93s/it]
2022-04-11 15:42:41,240 - INFO - tqdm - accuracy: 0.5709, batch_loss: 0.6832, loss: 0.6813 ||:  86%|########6 | 56/65 [10:11<01:38, 10.93s/it]
2022-04-11 15:42:51,982 - INFO - tqdm - accuracy: 0.5724, batch_loss: 0.6070, loss: 0.6800 ||:  88%|########7 | 57/65 [10:22<01:26, 10.87s/it]
2022-04-11 15:43:02,948 - INFO - tqdm - accuracy: 0.5711, batch_loss: 0.7051, loss: 0.6804 ||:  89%|########9 | 58/65 [10:33<01:16, 10.90s/it]
2022-04-11 15:43:13,907 - INFO - tqdm - accuracy: 0.5694, batch_loss: 0.6750, loss: 0.6803 ||:  91%|######### | 59/65 [10:44<01:05, 10.92s/it]
2022-04-11 15:43:24,891 - INFO - tqdm - accuracy: 0.5698, batch_loss: 0.6928, loss: 0.6805 ||:  92%|#########2| 60/65 [10:55<00:54, 10.94s/it]
2022-04-11 15:43:35,859 - INFO - tqdm - accuracy: 0.5722, batch_loss: 0.5893, loss: 0.6790 ||:  94%|#########3| 61/65 [11:06<00:43, 10.95s/it]
2022-04-11 15:43:46,821 - INFO - tqdm - accuracy: 0.5736, batch_loss: 0.5601, loss: 0.6771 ||:  95%|#########5| 62/65 [11:17<00:32, 10.95s/it]
2022-04-11 15:43:57,768 - INFO - tqdm - accuracy: 0.5734, batch_loss: 0.6403, loss: 0.6765 ||:  97%|#########6| 63/65 [11:28<00:21, 10.95s/it]
2022-04-11 15:44:08,382 - INFO - tqdm - accuracy: 0.5725, batch_loss: 0.7424, loss: 0.6775 ||:  98%|#########8| 64/65 [11:38<00:10, 10.85s/it]
2022-04-11 15:44:13,238 - INFO - tqdm - accuracy: 0.5735, batch_loss: 0.6770, loss: 0.6775 ||: 100%|##########| 65/65 [11:43<00:00,  9.05s/it]
2022-04-11 15:44:13,238 - INFO - tqdm - accuracy: 0.5735, batch_loss: 0.6770, loss: 0.6775 ||: 100%|##########| 65/65 [11:43<00:00, 10.83s/it]
2022-04-11 15:44:16,117 - INFO - allennlp.training.trainer - Validating
2022-04-11 15:44:16,119 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-11 15:44:26,336 - INFO - tqdm - accuracy: 0.6279, batch_loss: 0.7673, loss: 0.7099 ||:  37%|###7      | 43/115 [00:10<00:17,  4.16it/s]
2022-04-11 15:44:36,427 - INFO - tqdm - accuracy: 0.6000, batch_loss: 0.5702, loss: 0.7007 ||:  74%|#######3  | 85/115 [00:20<00:07,  4.15it/s]
2022-04-11 15:44:43,544 - INFO - tqdm - accuracy: 0.5939, batch_loss: 0.6866, loss: 0.7259 ||: 100%|##########| 115/115 [00:27<00:00,  4.13it/s]
2022-04-11 15:44:43,544 - INFO - tqdm - accuracy: 0.5939, batch_loss: 0.6866, loss: 0.7259 ||: 100%|##########| 115/115 [00:27<00:00,  4.19it/s]
2022-04-11 15:44:43,544 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-11 15:44:43,545 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.574  |     0.594
2022-04-11 15:44:43,545 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-11 15:44:43,546 - INFO - allennlp.training.tensorboard_writer - loss               |     0.678  |     0.726
2022-04-11 15:44:43,547 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5804.656  |       N/A
2022-04-11 15:44:49,217 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to './output_ir-ora-d_hyper7/best.th'.
2022-04-11 15:45:07,545 - INFO - allennlp.training.trainer - Epoch duration: 0:12:38.128067
2022-04-11 15:45:07,546 - INFO - allennlp.training.trainer - Estimated training time remaining: 2:40:56
2022-04-11 15:45:07,546 - INFO - allennlp.training.trainer - Epoch 2/14
2022-04-11 15:45:07,546 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-11 15:45:07,546 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-11 15:45:07,548 - INFO - allennlp.training.trainer - Training
2022-04-11 15:45:07,548 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-11 15:45:18,149 - INFO - tqdm - accuracy: 0.4688, batch_loss: 0.7393, loss: 0.7393 ||:   2%|1         | 1/65 [00:10<11:18, 10.60s/it]
2022-04-11 15:45:28,774 - INFO - tqdm - accuracy: 0.6094, batch_loss: 0.5603, loss: 0.6498 ||:   3%|3         | 2/65 [00:21<11:08, 10.61s/it]
2022-04-11 15:45:39,432 - INFO - tqdm - accuracy: 0.5833, batch_loss: 0.6272, loss: 0.6423 ||:   5%|4         | 3/65 [00:31<10:59, 10.63s/it]
2022-04-11 15:45:50,123 - INFO - tqdm - accuracy: 0.5938, batch_loss: 0.5890, loss: 0.6289 ||:   6%|6         | 4/65 [00:42<10:50, 10.66s/it]
2022-04-11 15:46:00,863 - INFO - tqdm - accuracy: 0.6062, batch_loss: 0.5374, loss: 0.6106 ||:   8%|7         | 5/65 [00:53<10:41, 10.69s/it]
2022-04-11 15:46:11,494 - INFO - tqdm - accuracy: 0.6250, batch_loss: 0.5623, loss: 0.6026 ||:   9%|9         | 6/65 [01:03<10:29, 10.67s/it]
2022-04-11 15:46:22,253 - INFO - tqdm - accuracy: 0.6116, batch_loss: 0.7064, loss: 0.6174 ||:  11%|#         | 7/65 [01:14<10:20, 10.70s/it]
2022-04-11 15:46:33,039 - INFO - tqdm - accuracy: 0.6094, batch_loss: 0.6381, loss: 0.6200 ||:  12%|#2        | 8/65 [01:25<10:11, 10.73s/it]
2022-04-11 15:46:43,850 - INFO - tqdm - accuracy: 0.6215, batch_loss: 0.5850, loss: 0.6161 ||:  14%|#3        | 9/65 [01:36<10:02, 10.75s/it]
2022-04-11 15:46:54,678 - INFO - tqdm - accuracy: 0.6219, batch_loss: 0.6145, loss: 0.6160 ||:  15%|#5        | 10/65 [01:47<09:52, 10.78s/it]
2022-04-11 15:47:05,519 - INFO - tqdm - accuracy: 0.6108, batch_loss: 0.6595, loss: 0.6199 ||:  17%|#6        | 11/65 [01:57<09:42, 10.80s/it]
2022-04-11 15:47:16,379 - INFO - tqdm - accuracy: 0.6224, batch_loss: 0.5842, loss: 0.6169 ||:  18%|#8        | 12/65 [02:08<09:33, 10.82s/it]
2022-04-11 15:47:26,921 - INFO - tqdm - accuracy: 0.6072, batch_loss: 0.7935, loss: 0.6305 ||:  20%|##        | 13/65 [02:19<09:18, 10.73s/it]
2022-04-11 15:47:37,791 - INFO - tqdm - accuracy: 0.6107, batch_loss: 0.6385, loss: 0.6311 ||:  22%|##1       | 14/65 [02:30<09:09, 10.77s/it]
2022-04-11 15:47:48,688 - INFO - tqdm - accuracy: 0.6096, batch_loss: 0.7905, loss: 0.6417 ||:  23%|##3       | 15/65 [02:41<09:00, 10.81s/it]
2022-04-11 15:47:59,598 - INFO - tqdm - accuracy: 0.6204, batch_loss: 0.5596, loss: 0.6366 ||:  25%|##4       | 16/65 [02:52<08:51, 10.84s/it]
2022-04-11 15:48:10,530 - INFO - tqdm - accuracy: 0.6188, batch_loss: 0.7123, loss: 0.6410 ||:  26%|##6       | 17/65 [03:02<08:41, 10.87s/it]
2022-04-11 15:48:21,470 - INFO - tqdm - accuracy: 0.6139, batch_loss: 0.7586, loss: 0.6476 ||:  28%|##7       | 18/65 [03:13<08:31, 10.89s/it]
2022-04-11 15:48:32,397 - INFO - tqdm - accuracy: 0.6161, batch_loss: 0.6316, loss: 0.6467 ||:  29%|##9       | 19/65 [03:24<08:21, 10.90s/it]
2022-04-11 15:48:43,322 - INFO - tqdm - accuracy: 0.6228, batch_loss: 0.6229, loss: 0.6455 ||:  31%|###       | 20/65 [03:35<08:10, 10.91s/it]
2022-04-11 15:48:54,259 - INFO - tqdm - accuracy: 0.6200, batch_loss: 0.6702, loss: 0.6467 ||:  32%|###2      | 21/65 [03:46<08:00, 10.92s/it]
2022-04-11 15:49:05,188 - INFO - tqdm - accuracy: 0.6230, batch_loss: 0.5897, loss: 0.6441 ||:  34%|###3      | 22/65 [03:57<07:49, 10.92s/it]
2022-04-11 15:49:16,105 - INFO - tqdm - accuracy: 0.6272, batch_loss: 0.5714, loss: 0.6410 ||:  35%|###5      | 23/65 [04:08<07:38, 10.92s/it]
2022-04-11 15:49:26,929 - INFO - tqdm - accuracy: 0.6297, batch_loss: 0.6377, loss: 0.6408 ||:  37%|###6      | 24/65 [04:19<07:26, 10.89s/it]
2022-04-11 15:49:37,868 - INFO - tqdm - accuracy: 0.6320, batch_loss: 0.6168, loss: 0.6399 ||:  38%|###8      | 25/65 [04:30<07:16, 10.91s/it]
2022-04-11 15:49:48,802 - INFO - tqdm - accuracy: 0.6330, batch_loss: 0.6267, loss: 0.6394 ||:  40%|####      | 26/65 [04:41<07:05, 10.91s/it]
2022-04-11 15:49:59,743 - INFO - tqdm - accuracy: 0.6327, batch_loss: 0.5999, loss: 0.6379 ||:  42%|####1     | 27/65 [04:52<06:55, 10.92s/it]
2022-04-11 15:50:10,690 - INFO - tqdm - accuracy: 0.6391, batch_loss: 0.5117, loss: 0.6334 ||:  43%|####3     | 28/65 [05:03<06:44, 10.93s/it]
2022-04-11 15:50:21,607 - INFO - tqdm - accuracy: 0.6375, batch_loss: 0.6166, loss: 0.6328 ||:  45%|####4     | 29/65 [05:14<06:33, 10.93s/it]
2022-04-11 15:50:32,545 - INFO - tqdm - accuracy: 0.6382, batch_loss: 0.5798, loss: 0.6310 ||:  46%|####6     | 30/65 [05:24<06:22, 10.93s/it]
2022-04-11 15:50:43,513 - INFO - tqdm - accuracy: 0.6377, batch_loss: 0.5800, loss: 0.6294 ||:  48%|####7     | 31/65 [05:35<06:11, 10.94s/it]
2022-04-11 15:50:54,444 - INFO - tqdm - accuracy: 0.6413, batch_loss: 0.5666, loss: 0.6274 ||:  49%|####9     | 32/65 [05:46<06:00, 10.94s/it]
2022-04-11 15:51:05,368 - INFO - tqdm - accuracy: 0.6398, batch_loss: 0.7258, loss: 0.6304 ||:  51%|#####     | 33/65 [05:57<05:49, 10.93s/it]
2022-04-11 15:51:16,286 - INFO - tqdm - accuracy: 0.6385, batch_loss: 0.6627, loss: 0.6314 ||:  52%|#####2    | 34/65 [06:08<05:38, 10.93s/it]
2022-04-11 15:51:27,214 - INFO - tqdm - accuracy: 0.6390, batch_loss: 0.5523, loss: 0.6291 ||:  54%|#####3    | 35/65 [06:19<05:27, 10.93s/it]
2022-04-11 15:51:38,154 - INFO - tqdm - accuracy: 0.6412, batch_loss: 0.5812, loss: 0.6278 ||:  55%|#####5    | 36/65 [06:30<05:17, 10.93s/it]
2022-04-11 15:51:49,105 - INFO - tqdm - accuracy: 0.6407, batch_loss: 0.6076, loss: 0.6272 ||:  57%|#####6    | 37/65 [06:41<05:06, 10.94s/it]
2022-04-11 15:52:00,048 - INFO - tqdm - accuracy: 0.6461, batch_loss: 0.4273, loss: 0.6220 ||:  58%|#####8    | 38/65 [06:52<04:55, 10.94s/it]
2022-04-11 15:52:10,996 - INFO - tqdm - accuracy: 0.6464, batch_loss: 0.5433, loss: 0.6199 ||:  60%|######    | 39/65 [07:03<04:44, 10.94s/it]
2022-04-11 15:52:21,936 - INFO - tqdm - accuracy: 0.6443, batch_loss: 0.6625, loss: 0.6210 ||:  62%|######1   | 40/65 [07:14<04:33, 10.94s/it]
2022-04-11 15:52:32,902 - INFO - tqdm - accuracy: 0.6445, batch_loss: 0.5849, loss: 0.6201 ||:  63%|######3   | 41/65 [07:25<04:22, 10.95s/it]
2022-04-11 15:52:43,743 - INFO - tqdm - accuracy: 0.6456, batch_loss: 0.5779, loss: 0.6191 ||:  65%|######4   | 42/65 [07:36<04:11, 10.92s/it]
2022-04-11 15:52:54,705 - INFO - tqdm - accuracy: 0.6429, batch_loss: 0.6244, loss: 0.6192 ||:  66%|######6   | 43/65 [07:47<04:00, 10.93s/it]
2022-04-11 15:53:05,676 - INFO - tqdm - accuracy: 0.6439, batch_loss: 0.5696, loss: 0.6181 ||:  68%|######7   | 44/65 [07:58<03:49, 10.94s/it]
2022-04-11 15:53:16,647 - INFO - tqdm - accuracy: 0.6463, batch_loss: 0.5928, loss: 0.6176 ||:  69%|######9   | 45/65 [08:09<03:39, 10.95s/it]
2022-04-11 15:53:27,569 - INFO - tqdm - accuracy: 0.6479, batch_loss: 0.5666, loss: 0.6164 ||:  71%|#######   | 46/65 [08:20<03:27, 10.94s/it]
2022-04-11 15:53:38,499 - INFO - tqdm - accuracy: 0.6460, batch_loss: 0.7298, loss: 0.6189 ||:  72%|#######2  | 47/65 [08:30<03:16, 10.94s/it]
2022-04-11 15:53:49,424 - INFO - tqdm - accuracy: 0.6456, batch_loss: 0.6980, loss: 0.6205 ||:  74%|#######3  | 48/65 [08:41<03:05, 10.93s/it]
2022-04-11 15:54:00,350 - INFO - tqdm - accuracy: 0.6445, batch_loss: 0.5847, loss: 0.6198 ||:  75%|#######5  | 49/65 [08:52<02:54, 10.93s/it]
2022-04-11 15:54:11,267 - INFO - tqdm - accuracy: 0.6423, batch_loss: 0.6354, loss: 0.6201 ||:  77%|#######6  | 50/65 [09:03<02:43, 10.93s/it]
2022-04-11 15:54:22,195 - INFO - tqdm - accuracy: 0.6426, batch_loss: 0.6233, loss: 0.6202 ||:  78%|#######8  | 51/65 [09:14<02:32, 10.93s/it]
2022-04-11 15:54:33,110 - INFO - tqdm - accuracy: 0.6416, batch_loss: 0.6161, loss: 0.6201 ||:  80%|########  | 52/65 [09:25<02:22, 10.92s/it]
2022-04-11 15:54:44,048 - INFO - tqdm - accuracy: 0.6442, batch_loss: 0.5175, loss: 0.6181 ||:  82%|########1 | 53/65 [09:36<02:11, 10.93s/it]
2022-04-11 15:54:54,971 - INFO - tqdm - accuracy: 0.6450, batch_loss: 0.6267, loss: 0.6183 ||:  83%|########3 | 54/65 [09:47<02:00, 10.93s/it]
2022-04-11 15:55:05,921 - INFO - tqdm - accuracy: 0.6453, batch_loss: 0.6340, loss: 0.6186 ||:  85%|########4 | 55/65 [09:58<01:49, 10.93s/it]
2022-04-11 15:55:16,850 - INFO - tqdm - accuracy: 0.6454, batch_loss: 0.6198, loss: 0.6186 ||:  86%|########6 | 56/65 [10:09<01:38, 10.93s/it]
2022-04-11 15:55:27,787 - INFO - tqdm - accuracy: 0.6440, batch_loss: 0.7567, loss: 0.6210 ||:  88%|########7 | 57/65 [10:20<01:27, 10.93s/it]
2022-04-11 15:55:38,490 - INFO - tqdm - accuracy: 0.6442, batch_loss: 0.5236, loss: 0.6194 ||:  89%|########9 | 58/65 [10:30<01:16, 10.86s/it]
2022-04-11 15:55:49,423 - INFO - tqdm - accuracy: 0.6460, batch_loss: 0.4651, loss: 0.6167 ||:  91%|######### | 59/65 [10:41<01:05, 10.89s/it]
2022-04-11 15:56:00,358 - INFO - tqdm - accuracy: 0.6441, batch_loss: 0.7031, loss: 0.6182 ||:  92%|#########2| 60/65 [10:52<00:54, 10.90s/it]
2022-04-11 15:56:11,285 - INFO - tqdm - accuracy: 0.6468, batch_loss: 0.5163, loss: 0.6165 ||:  94%|#########3| 61/65 [11:03<00:43, 10.91s/it]
2022-04-11 15:56:22,200 - INFO - tqdm - accuracy: 0.6460, batch_loss: 0.7209, loss: 0.6182 ||:  95%|#########5| 62/65 [11:14<00:32, 10.91s/it]
2022-04-11 15:56:33,146 - INFO - tqdm - accuracy: 0.6457, batch_loss: 0.5779, loss: 0.6176 ||:  97%|#########6| 63/65 [11:25<00:21, 10.92s/it]
2022-04-11 15:56:44,091 - INFO - tqdm - accuracy: 0.6439, batch_loss: 0.7114, loss: 0.6190 ||:  98%|#########8| 64/65 [11:36<00:10, 10.93s/it]
2022-04-11 15:56:48,955 - INFO - tqdm - accuracy: 0.6443, batch_loss: 0.7314, loss: 0.6207 ||: 100%|##########| 65/65 [11:41<00:00,  9.11s/it]
2022-04-11 15:56:48,956 - INFO - tqdm - accuracy: 0.6443, batch_loss: 0.7314, loss: 0.6207 ||: 100%|##########| 65/65 [11:41<00:00, 10.79s/it]
2022-04-11 15:56:51,838 - INFO - allennlp.training.trainer - Validating
2022-04-11 15:56:51,839 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-11 15:57:01,917 - INFO - tqdm - accuracy: 0.6548, batch_loss: 0.7136, loss: 0.7101 ||:  37%|###6      | 42/115 [00:10<00:17,  4.14it/s]
2022-04-11 15:57:12,073 - INFO - tqdm - accuracy: 0.6450, batch_loss: 0.1110, loss: 0.6535 ||:  74%|#######3  | 85/115 [00:20<00:07,  4.14it/s]
2022-04-11 15:57:19,296 - INFO - tqdm - accuracy: 0.6332, batch_loss: 1.3063, loss: 0.6793 ||: 100%|##########| 115/115 [00:27<00:00,  4.15it/s]
2022-04-11 15:57:19,296 - INFO - tqdm - accuracy: 0.6332, batch_loss: 1.3063, loss: 0.6793 ||: 100%|##########| 115/115 [00:27<00:00,  4.19it/s]
2022-04-11 15:57:19,297 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-11 15:57:19,297 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.644  |     0.633
2022-04-11 15:57:19,299 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-11 15:57:19,299 - INFO - allennlp.training.tensorboard_writer - loss               |     0.621  |     0.679
2022-04-11 15:57:19,299 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5804.656  |       N/A
2022-04-11 15:57:25,126 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to './output_ir-ora-d_hyper7/best.th'.
2022-04-11 15:57:40,209 - INFO - allennlp.training.trainer - Epoch duration: 0:12:32.662664
2022-04-11 15:57:40,209 - INFO - allennlp.training.trainer - Estimated training time remaining: 2:29:12
2022-04-11 15:57:40,209 - INFO - allennlp.training.trainer - Epoch 3/14
2022-04-11 15:57:40,209 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-11 15:57:40,209 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-11 15:57:40,211 - INFO - allennlp.training.trainer - Training
2022-04-11 15:57:40,211 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-11 15:57:50,840 - INFO - tqdm - accuracy: 0.5938, batch_loss: 0.6114, loss: 0.6114 ||:   2%|1         | 1/65 [00:10<11:20, 10.63s/it]
2022-04-11 15:58:01,513 - INFO - tqdm - accuracy: 0.5625, batch_loss: 0.6973, loss: 0.6543 ||:   3%|3         | 2/65 [00:21<11:11, 10.65s/it]
2022-04-11 15:58:12,231 - INFO - tqdm - accuracy: 0.5938, batch_loss: 0.6075, loss: 0.6387 ||:   5%|4         | 3/65 [00:32<11:02, 10.68s/it]
2022-04-11 15:58:22,969 - INFO - tqdm - accuracy: 0.6250, batch_loss: 0.5415, loss: 0.6144 ||:   6%|6         | 4/65 [00:42<10:53, 10.71s/it]
2022-04-11 15:58:34,153 - INFO - tqdm - accuracy: 0.6500, batch_loss: 0.5313, loss: 0.5978 ||:   8%|7         | 5/65 [00:53<10:52, 10.88s/it]
2022-04-11 15:58:44,897 - INFO - tqdm - accuracy: 0.6667, batch_loss: 0.5681, loss: 0.5929 ||:   9%|9         | 6/65 [01:04<10:39, 10.83s/it]
2022-04-11 15:58:55,694 - INFO - tqdm - accuracy: 0.6786, batch_loss: 0.5296, loss: 0.5838 ||:  11%|#         | 7/65 [01:15<10:27, 10.82s/it]
2022-04-11 15:59:06,392 - INFO - tqdm - accuracy: 0.6875, batch_loss: 0.5184, loss: 0.5756 ||:  12%|#2        | 8/65 [01:26<10:14, 10.78s/it]
2022-04-11 15:59:17,222 - INFO - tqdm - accuracy: 0.6875, batch_loss: 0.4970, loss: 0.5669 ||:  14%|#3        | 9/65 [01:37<10:04, 10.80s/it]
2022-04-11 15:59:28,083 - INFO - tqdm - accuracy: 0.6969, batch_loss: 0.4959, loss: 0.5598 ||:  15%|#5        | 10/65 [01:47<09:54, 10.82s/it]
2022-04-11 15:59:38,812 - INFO - tqdm - accuracy: 0.7017, batch_loss: 0.5235, loss: 0.5565 ||:  17%|#6        | 11/65 [01:58<09:42, 10.79s/it]
2022-04-11 15:59:49,683 - INFO - tqdm - accuracy: 0.7109, batch_loss: 0.4357, loss: 0.5464 ||:  18%|#8        | 12/65 [02:09<09:33, 10.81s/it]
2022-04-11 16:00:00,554 - INFO - tqdm - accuracy: 0.7163, batch_loss: 0.4461, loss: 0.5387 ||:  20%|##        | 13/65 [02:20<09:23, 10.83s/it]
2022-04-11 16:00:11,429 - INFO - tqdm - accuracy: 0.7098, batch_loss: 0.5598, loss: 0.5402 ||:  22%|##1       | 14/65 [02:31<09:13, 10.84s/it]
2022-04-11 16:00:22,320 - INFO - tqdm - accuracy: 0.7146, batch_loss: 0.5148, loss: 0.5385 ||:  23%|##3       | 15/65 [02:42<09:02, 10.86s/it]
2022-04-11 16:00:33,226 - INFO - tqdm - accuracy: 0.7090, batch_loss: 0.5701, loss: 0.5405 ||:  25%|##4       | 16/65 [02:53<08:52, 10.87s/it]
2022-04-11 16:00:44,133 - INFO - tqdm - accuracy: 0.7077, batch_loss: 0.5233, loss: 0.5395 ||:  26%|##6       | 17/65 [03:03<08:42, 10.88s/it]
2022-04-11 16:00:55,037 - INFO - tqdm - accuracy: 0.7188, batch_loss: 0.3456, loss: 0.5287 ||:  28%|##7       | 18/65 [03:14<08:31, 10.89s/it]
2022-04-11 16:01:05,958 - INFO - tqdm - accuracy: 0.7056, batch_loss: 0.7456, loss: 0.5401 ||:  29%|##9       | 19/65 [03:25<08:21, 10.90s/it]
2022-04-11 16:01:16,877 - INFO - tqdm - accuracy: 0.7047, batch_loss: 0.5289, loss: 0.5396 ||:  31%|###       | 20/65 [03:36<08:10, 10.90s/it]
2022-04-11 16:01:27,807 - INFO - tqdm - accuracy: 0.7083, batch_loss: 0.4797, loss: 0.5367 ||:  32%|###2      | 21/65 [03:47<08:00, 10.91s/it]
2022-04-11 16:01:38,730 - INFO - tqdm - accuracy: 0.7074, batch_loss: 0.6362, loss: 0.5412 ||:  34%|###3      | 22/65 [03:58<07:49, 10.92s/it]
2022-04-11 16:01:49,658 - INFO - tqdm - accuracy: 0.7106, batch_loss: 0.4041, loss: 0.5353 ||:  35%|###5      | 23/65 [04:09<07:38, 10.92s/it]
2022-04-11 16:02:00,583 - INFO - tqdm - accuracy: 0.7135, batch_loss: 0.4308, loss: 0.5309 ||:  37%|###6      | 24/65 [04:20<07:27, 10.92s/it]
2022-04-11 16:02:11,511 - INFO - tqdm - accuracy: 0.7200, batch_loss: 0.3835, loss: 0.5250 ||:  38%|###8      | 25/65 [04:31<07:16, 10.92s/it]
2022-04-11 16:02:22,433 - INFO - tqdm - accuracy: 0.7248, batch_loss: 0.4527, loss: 0.5222 ||:  40%|####      | 26/65 [04:42<07:05, 10.92s/it]
2022-04-11 16:02:33,366 - INFO - tqdm - accuracy: 0.7245, batch_loss: 0.5998, loss: 0.5251 ||:  42%|####1     | 27/65 [04:53<06:55, 10.93s/it]
2022-04-11 16:02:44,295 - INFO - tqdm - accuracy: 0.7266, batch_loss: 0.4361, loss: 0.5219 ||:  43%|####3     | 28/65 [05:04<06:44, 10.93s/it]
2022-04-11 16:02:55,220 - INFO - tqdm - accuracy: 0.7295, batch_loss: 0.3818, loss: 0.5171 ||:  45%|####4     | 29/65 [05:15<06:33, 10.93s/it]
2022-04-11 16:03:06,164 - INFO - tqdm - accuracy: 0.7312, batch_loss: 0.4219, loss: 0.5139 ||:  46%|####6     | 30/65 [05:25<06:22, 10.93s/it]
2022-04-11 16:03:17,103 - INFO - tqdm - accuracy: 0.7359, batch_loss: 0.4241, loss: 0.5110 ||:  48%|####7     | 31/65 [05:36<06:11, 10.93s/it]
2022-04-11 16:03:27,937 - INFO - tqdm - accuracy: 0.7363, batch_loss: 0.5260, loss: 0.5115 ||:  49%|####9     | 32/65 [05:47<05:59, 10.90s/it]
2022-04-11 16:03:38,863 - INFO - tqdm - accuracy: 0.7339, batch_loss: 0.5621, loss: 0.5130 ||:  51%|#####     | 33/65 [05:58<05:49, 10.91s/it]
2022-04-11 16:03:49,802 - INFO - tqdm - accuracy: 0.7316, batch_loss: 0.5967, loss: 0.5155 ||:  52%|#####2    | 34/65 [06:09<05:38, 10.92s/it]
2022-04-11 16:04:00,723 - INFO - tqdm - accuracy: 0.7304, batch_loss: 0.5252, loss: 0.5158 ||:  54%|#####3    | 35/65 [06:20<05:27, 10.92s/it]
2022-04-11 16:04:11,663 - INFO - tqdm - accuracy: 0.7292, batch_loss: 0.5542, loss: 0.5168 ||:  55%|#####5    | 36/65 [06:31<05:16, 10.93s/it]
2022-04-11 16:04:22,597 - INFO - tqdm - accuracy: 0.7280, batch_loss: 0.5003, loss: 0.5164 ||:  57%|#####6    | 37/65 [06:42<05:05, 10.93s/it]
2022-04-11 16:04:33,525 - INFO - tqdm - accuracy: 0.7286, batch_loss: 0.5054, loss: 0.5161 ||:  58%|#####8    | 38/65 [06:53<04:55, 10.93s/it]
2022-04-11 16:04:44,455 - INFO - tqdm - accuracy: 0.7260, batch_loss: 0.6836, loss: 0.5204 ||:  60%|######    | 39/65 [07:04<04:44, 10.93s/it]
2022-04-11 16:04:55,376 - INFO - tqdm - accuracy: 0.7234, batch_loss: 0.5563, loss: 0.5213 ||:  62%|######1   | 40/65 [07:15<04:33, 10.93s/it]
2022-04-11 16:05:06,299 - INFO - tqdm - accuracy: 0.7264, batch_loss: 0.5425, loss: 0.5218 ||:  63%|######3   | 41/65 [07:26<04:22, 10.93s/it]
2022-04-11 16:05:17,241 - INFO - tqdm - accuracy: 0.7277, batch_loss: 0.4594, loss: 0.5203 ||:  65%|######4   | 42/65 [07:37<04:11, 10.93s/it]
2022-04-11 16:05:28,180 - INFO - tqdm - accuracy: 0.7304, batch_loss: 0.3971, loss: 0.5175 ||:  66%|######6   | 43/65 [07:47<04:00, 10.93s/it]
2022-04-11 16:05:39,117 - INFO - tqdm - accuracy: 0.7301, batch_loss: 0.5528, loss: 0.5183 ||:  68%|######7   | 44/65 [07:58<03:49, 10.93s/it]
2022-04-11 16:05:50,036 - INFO - tqdm - accuracy: 0.7299, batch_loss: 0.6069, loss: 0.5202 ||:  69%|######9   | 45/65 [08:09<03:38, 10.93s/it]
2022-04-11 16:06:00,982 - INFO - tqdm - accuracy: 0.7289, batch_loss: 0.5212, loss: 0.5203 ||:  71%|#######   | 46/65 [08:20<03:27, 10.93s/it]
2022-04-11 16:06:11,915 - INFO - tqdm - accuracy: 0.7287, batch_loss: 0.4940, loss: 0.5197 ||:  72%|#######2  | 47/65 [08:31<03:16, 10.93s/it]
2022-04-11 16:06:22,836 - INFO - tqdm - accuracy: 0.7311, batch_loss: 0.3734, loss: 0.5166 ||:  74%|#######3  | 48/65 [08:42<03:05, 10.93s/it]
2022-04-11 16:06:33,749 - INFO - tqdm - accuracy: 0.7296, batch_loss: 0.5706, loss: 0.5178 ||:  75%|#######5  | 49/65 [08:53<02:54, 10.93s/it]
2022-04-11 16:06:44,701 - INFO - tqdm - accuracy: 0.7294, batch_loss: 0.5742, loss: 0.5189 ||:  77%|#######6  | 50/65 [09:04<02:43, 10.93s/it]
2022-04-11 16:06:55,650 - INFO - tqdm - accuracy: 0.7292, batch_loss: 0.6444, loss: 0.5213 ||:  78%|#######8  | 51/65 [09:15<02:33, 10.94s/it]
2022-04-11 16:07:06,351 - INFO - tqdm - accuracy: 0.7278, batch_loss: 0.5425, loss: 0.5217 ||:  80%|########  | 52/65 [09:26<02:21, 10.87s/it]
2022-04-11 16:07:17,301 - INFO - tqdm - accuracy: 0.7264, batch_loss: 0.5320, loss: 0.5219 ||:  82%|########1 | 53/65 [09:37<02:10, 10.89s/it]
2022-04-11 16:07:28,253 - INFO - tqdm - accuracy: 0.7245, batch_loss: 0.6046, loss: 0.5235 ||:  83%|########3 | 54/65 [09:48<02:00, 10.91s/it]
2022-04-11 16:07:39,219 - INFO - tqdm - accuracy: 0.7244, batch_loss: 0.5576, loss: 0.5241 ||:  85%|########4 | 55/65 [09:59<01:49, 10.93s/it]
2022-04-11 16:07:50,155 - INFO - tqdm - accuracy: 0.7232, batch_loss: 0.6359, loss: 0.5261 ||:  86%|########6 | 56/65 [10:09<01:38, 10.93s/it]
2022-04-11 16:08:01,089 - INFO - tqdm - accuracy: 0.7237, batch_loss: 0.5387, loss: 0.5263 ||:  88%|########7 | 57/65 [10:20<01:27, 10.93s/it]
2022-04-11 16:08:11,996 - INFO - tqdm - accuracy: 0.7252, batch_loss: 0.4500, loss: 0.5250 ||:  89%|########9 | 58/65 [10:31<01:16, 10.92s/it]
2022-04-11 16:08:22,585 - INFO - tqdm - accuracy: 0.7250, batch_loss: 0.5329, loss: 0.5251 ||:  91%|######### | 59/65 [10:42<01:04, 10.82s/it]
2022-04-11 16:08:33,513 - INFO - tqdm - accuracy: 0.7269, batch_loss: 0.4180, loss: 0.5233 ||:  92%|#########2| 60/65 [10:53<00:54, 10.85s/it]
2022-04-11 16:08:44,443 - INFO - tqdm - accuracy: 0.7268, batch_loss: 0.5238, loss: 0.5233 ||:  94%|#########3| 61/65 [11:04<00:43, 10.88s/it]
2022-04-11 16:08:55,389 - INFO - tqdm - accuracy: 0.7282, batch_loss: 0.5263, loss: 0.5234 ||:  95%|#########5| 62/65 [11:15<00:32, 10.90s/it]
2022-04-11 16:09:06,319 - INFO - tqdm - accuracy: 0.7280, batch_loss: 0.5932, loss: 0.5245 ||:  97%|#########6| 63/65 [11:26<00:21, 10.91s/it]
2022-04-11 16:09:17,252 - INFO - tqdm - accuracy: 0.7294, batch_loss: 0.4589, loss: 0.5235 ||:  98%|#########8| 64/65 [11:37<00:10, 10.92s/it]
2022-04-11 16:09:22,112 - INFO - tqdm - accuracy: 0.7288, batch_loss: 0.5009, loss: 0.5231 ||: 100%|##########| 65/65 [11:41<00:00,  9.10s/it]
2022-04-11 16:09:22,112 - INFO - tqdm - accuracy: 0.7288, batch_loss: 0.5009, loss: 0.5231 ||: 100%|##########| 65/65 [11:41<00:00, 10.80s/it]
2022-04-11 16:09:24,966 - INFO - allennlp.training.trainer - Validating
2022-04-11 16:09:24,968 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-11 16:09:35,027 - INFO - tqdm - accuracy: 0.5952, batch_loss: 0.9111, loss: 0.8952 ||:  37%|###6      | 42/115 [00:10<00:17,  4.18it/s]
2022-04-11 16:09:45,053 - INFO - tqdm - accuracy: 0.6488, batch_loss: 1.2876, loss: 0.7554 ||:  73%|#######3  | 84/115 [00:20<00:07,  4.16it/s]
2022-04-11 16:09:52,405 - INFO - tqdm - accuracy: 0.6507, batch_loss: 0.0560, loss: 0.7456 ||: 100%|##########| 115/115 [00:27<00:00,  4.21it/s]
2022-04-11 16:09:52,405 - INFO - tqdm - accuracy: 0.6507, batch_loss: 0.0560, loss: 0.7456 ||: 100%|##########| 115/115 [00:27<00:00,  4.19it/s]
2022-04-11 16:09:52,405 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-11 16:09:52,406 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.729  |     0.651
2022-04-11 16:09:52,407 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-11 16:09:52,408 - INFO - allennlp.training.tensorboard_writer - loss               |     0.523  |     0.746
2022-04-11 16:09:52,408 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5804.656  |       N/A
2022-04-11 16:09:58,239 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to './output_ir-ora-d_hyper7/best.th'.
2022-04-11 16:10:14,735 - INFO - allennlp.training.trainer - Epoch duration: 0:12:34.526514
2022-04-11 16:10:14,736 - INFO - allennlp.training.trainer - Estimated training time remaining: 2:17:10
2022-04-11 16:10:14,736 - INFO - allennlp.training.trainer - Epoch 4/14
2022-04-11 16:10:14,736 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-11 16:10:14,736 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-11 16:10:14,737 - INFO - allennlp.training.trainer - Training
2022-04-11 16:10:14,738 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-11 16:10:25,336 - INFO - tqdm - accuracy: 0.8125, batch_loss: 0.4819, loss: 0.4819 ||:   2%|1         | 1/65 [00:10<11:18, 10.60s/it]
2022-04-11 16:10:35,965 - INFO - tqdm - accuracy: 0.8281, batch_loss: 0.3791, loss: 0.4305 ||:   3%|3         | 2/65 [00:21<11:08, 10.62s/it]
2022-04-11 16:10:46,668 - INFO - tqdm - accuracy: 0.7917, batch_loss: 0.5342, loss: 0.4651 ||:   5%|4         | 3/65 [00:31<11:00, 10.66s/it]
2022-04-11 16:10:57,378 - INFO - tqdm - accuracy: 0.7969, batch_loss: 0.4938, loss: 0.4723 ||:   6%|6         | 4/65 [00:42<10:51, 10.68s/it]
2022-04-11 16:11:08,132 - INFO - tqdm - accuracy: 0.7937, batch_loss: 0.4562, loss: 0.4690 ||:   8%|7         | 5/65 [00:53<10:42, 10.71s/it]
2022-04-11 16:11:18,906 - INFO - tqdm - accuracy: 0.8073, batch_loss: 0.4858, loss: 0.4718 ||:   9%|9         | 6/65 [01:04<10:32, 10.73s/it]
2022-04-11 16:11:29,696 - INFO - tqdm - accuracy: 0.8036, batch_loss: 0.4403, loss: 0.4673 ||:  11%|#         | 7/65 [01:14<10:23, 10.75s/it]
2022-04-11 16:11:40,492 - INFO - tqdm - accuracy: 0.8086, batch_loss: 0.3348, loss: 0.4508 ||:  12%|#2        | 8/65 [01:25<10:13, 10.76s/it]
2022-04-11 16:11:51,312 - INFO - tqdm - accuracy: 0.8090, batch_loss: 0.3575, loss: 0.4404 ||:  14%|#3        | 9/65 [01:36<10:03, 10.78s/it]
2022-04-11 16:12:02,155 - INFO - tqdm - accuracy: 0.8094, batch_loss: 0.3688, loss: 0.4332 ||:  15%|#5        | 10/65 [01:47<09:54, 10.80s/it]
2022-04-11 16:12:12,982 - INFO - tqdm - accuracy: 0.8097, batch_loss: 0.3997, loss: 0.4302 ||:  17%|#6        | 11/65 [01:58<09:43, 10.81s/it]
2022-04-11 16:12:23,787 - INFO - tqdm - accuracy: 0.8047, batch_loss: 0.4696, loss: 0.4335 ||:  18%|#8        | 12/65 [02:09<09:32, 10.81s/it]
2022-04-11 16:12:34,651 - INFO - tqdm - accuracy: 0.7981, batch_loss: 0.4838, loss: 0.4373 ||:  20%|##        | 13/65 [02:19<09:22, 10.82s/it]
2022-04-11 16:12:45,516 - INFO - tqdm - accuracy: 0.8013, batch_loss: 0.3774, loss: 0.4331 ||:  22%|##1       | 14/65 [02:30<09:12, 10.84s/it]
2022-04-11 16:12:56,392 - INFO - tqdm - accuracy: 0.8042, batch_loss: 0.3585, loss: 0.4281 ||:  23%|##3       | 15/65 [02:41<09:02, 10.85s/it]
2022-04-11 16:13:07,276 - INFO - tqdm - accuracy: 0.8086, batch_loss: 0.3828, loss: 0.4253 ||:  25%|##4       | 16/65 [02:52<08:52, 10.86s/it]
2022-04-11 16:13:18,157 - INFO - tqdm - accuracy: 0.8088, batch_loss: 0.3624, loss: 0.4216 ||:  26%|##6       | 17/65 [03:03<08:41, 10.87s/it]
2022-04-11 16:13:29,064 - INFO - tqdm - accuracy: 0.8108, batch_loss: 0.3538, loss: 0.4178 ||:  28%|##7       | 18/65 [03:14<08:31, 10.88s/it]
2022-04-11 16:13:39,955 - INFO - tqdm - accuracy: 0.8125, batch_loss: 0.4304, loss: 0.4185 ||:  29%|##9       | 19/65 [03:25<08:20, 10.88s/it]
2022-04-11 16:13:50,840 - INFO - tqdm - accuracy: 0.8078, batch_loss: 0.5068, loss: 0.4229 ||:  31%|###       | 20/65 [03:36<08:09, 10.88s/it]
2022-04-11 16:14:01,761 - INFO - tqdm - accuracy: 0.8036, batch_loss: 0.5894, loss: 0.4308 ||:  32%|###2      | 21/65 [03:47<07:59, 10.89s/it]
2022-04-11 16:14:12,686 - INFO - tqdm - accuracy: 0.8040, batch_loss: 0.3667, loss: 0.4279 ||:  34%|###3      | 22/65 [03:57<07:48, 10.90s/it]
2022-04-11 16:14:23,611 - INFO - tqdm - accuracy: 0.7989, batch_loss: 0.5379, loss: 0.4327 ||:  35%|###5      | 23/65 [04:08<07:38, 10.91s/it]
2022-04-11 16:14:34,525 - INFO - tqdm - accuracy: 0.7969, batch_loss: 0.3961, loss: 0.4312 ||:  37%|###6      | 24/65 [04:19<07:27, 10.91s/it]
2022-04-11 16:14:45,458 - INFO - tqdm - accuracy: 0.7900, batch_loss: 0.5536, loss: 0.4360 ||:  38%|###8      | 25/65 [04:30<07:16, 10.92s/it]
2022-04-11 16:14:56,402 - INFO - tqdm - accuracy: 0.7921, batch_loss: 0.3326, loss: 0.4321 ||:  40%|####      | 26/65 [04:41<07:06, 10.93s/it]
2022-04-11 16:15:07,325 - INFO - tqdm - accuracy: 0.7963, batch_loss: 0.2611, loss: 0.4257 ||:  42%|####1     | 27/65 [04:52<06:55, 10.92s/it]
2022-04-11 16:15:18,259 - INFO - tqdm - accuracy: 0.7980, batch_loss: 0.4434, loss: 0.4264 ||:  43%|####3     | 28/65 [05:03<06:44, 10.93s/it]
2022-04-11 16:15:29,202 - INFO - tqdm - accuracy: 0.8006, batch_loss: 0.3186, loss: 0.4227 ||:  45%|####4     | 29/65 [05:14<06:33, 10.93s/it]
2022-04-11 16:15:40,016 - INFO - tqdm - accuracy: 0.8000, batch_loss: 0.5243, loss: 0.4260 ||:  46%|####6     | 30/65 [05:25<06:21, 10.90s/it]
2022-04-11 16:15:50,949 - INFO - tqdm - accuracy: 0.8034, batch_loss: 0.3473, loss: 0.4235 ||:  48%|####7     | 31/65 [05:36<06:10, 10.91s/it]
2022-04-11 16:16:01,894 - INFO - tqdm - accuracy: 0.8018, batch_loss: 0.3924, loss: 0.4225 ||:  49%|####9     | 32/65 [05:47<06:00, 10.92s/it]
2022-04-11 16:16:12,861 - INFO - tqdm - accuracy: 0.8030, batch_loss: 0.3955, loss: 0.4217 ||:  51%|#####     | 33/65 [05:58<05:49, 10.93s/it]
2022-04-11 16:16:23,817 - INFO - tqdm - accuracy: 0.8033, batch_loss: 0.3598, loss: 0.4199 ||:  52%|#####2    | 34/65 [06:09<05:39, 10.94s/it]
2022-04-11 16:16:34,755 - INFO - tqdm - accuracy: 0.8054, batch_loss: 0.2968, loss: 0.4164 ||:  54%|#####3    | 35/65 [06:20<05:28, 10.94s/it]
2022-04-11 16:16:45,714 - INFO - tqdm - accuracy: 0.8082, batch_loss: 0.3745, loss: 0.4152 ||:  55%|#####5    | 36/65 [06:30<05:17, 10.95s/it]
2022-04-11 16:16:56,336 - INFO - tqdm - accuracy: 0.8081, batch_loss: 0.3990, loss: 0.4148 ||:  57%|#####6    | 37/65 [06:41<05:03, 10.85s/it]
2022-04-11 16:17:07,294 - INFO - tqdm - accuracy: 0.8115, batch_loss: 0.2545, loss: 0.4106 ||:  58%|#####8    | 38/65 [06:52<04:53, 10.88s/it]
2022-04-11 16:17:18,247 - INFO - tqdm - accuracy: 0.8107, batch_loss: 0.3393, loss: 0.4087 ||:  60%|######    | 39/65 [07:03<04:43, 10.90s/it]
2022-04-11 16:17:29,604 - INFO - tqdm - accuracy: 0.8108, batch_loss: 0.4420, loss: 0.4096 ||:  62%|######1   | 40/65 [07:14<04:35, 11.04s/it]
2022-04-11 16:17:40,535 - INFO - tqdm - accuracy: 0.8124, batch_loss: 0.3734, loss: 0.4087 ||:  63%|######3   | 41/65 [07:25<04:24, 11.01s/it]
2022-04-11 16:17:51,487 - INFO - tqdm - accuracy: 0.8109, batch_loss: 0.4381, loss: 0.4094 ||:  65%|######4   | 42/65 [07:36<04:12, 10.99s/it]
2022-04-11 16:18:02,445 - INFO - tqdm - accuracy: 0.8102, batch_loss: 0.4807, loss: 0.4110 ||:  66%|######6   | 43/65 [07:47<04:01, 10.98s/it]
2022-04-11 16:18:13,360 - INFO - tqdm - accuracy: 0.8088, batch_loss: 0.4049, loss: 0.4109 ||:  68%|######7   | 44/65 [07:58<03:50, 10.96s/it]
2022-04-11 16:18:24,150 - INFO - tqdm - accuracy: 0.8103, batch_loss: 0.2663, loss: 0.4077 ||:  69%|######9   | 45/65 [08:09<03:38, 10.91s/it]
2022-04-11 16:18:35,083 - INFO - tqdm - accuracy: 0.8090, batch_loss: 0.5366, loss: 0.4105 ||:  71%|#######   | 46/65 [08:20<03:27, 10.92s/it]
2022-04-11 16:18:45,999 - INFO - tqdm - accuracy: 0.8084, batch_loss: 0.4150, loss: 0.4106 ||:  72%|#######2  | 47/65 [08:31<03:16, 10.92s/it]
2022-04-11 16:18:56,930 - INFO - tqdm - accuracy: 0.8065, batch_loss: 0.4689, loss: 0.4118 ||:  74%|#######3  | 48/65 [08:42<03:05, 10.92s/it]
2022-04-11 16:19:07,859 - INFO - tqdm - accuracy: 0.8079, batch_loss: 0.3288, loss: 0.4101 ||:  75%|#######5  | 49/65 [08:53<02:54, 10.92s/it]
2022-04-11 16:19:18,787 - INFO - tqdm - accuracy: 0.8061, batch_loss: 0.4875, loss: 0.4116 ||:  77%|#######6  | 50/65 [09:04<02:43, 10.92s/it]
2022-04-11 16:19:29,709 - INFO - tqdm - accuracy: 0.8081, batch_loss: 0.2516, loss: 0.4085 ||:  78%|#######8  | 51/65 [09:14<02:32, 10.92s/it]
2022-04-11 16:19:40,636 - INFO - tqdm - accuracy: 0.8088, batch_loss: 0.3318, loss: 0.4070 ||:  80%|########  | 52/65 [09:25<02:22, 10.92s/it]
2022-04-11 16:19:51,569 - INFO - tqdm - accuracy: 0.8094, batch_loss: 0.4028, loss: 0.4070 ||:  82%|########1 | 53/65 [09:36<02:11, 10.93s/it]
2022-04-11 16:20:02,474 - INFO - tqdm - accuracy: 0.8095, batch_loss: 0.4056, loss: 0.4069 ||:  83%|########3 | 54/65 [09:47<02:00, 10.92s/it]
2022-04-11 16:20:13,427 - INFO - tqdm - accuracy: 0.8101, batch_loss: 0.3680, loss: 0.4062 ||:  85%|########4 | 55/65 [09:58<01:49, 10.93s/it]
2022-04-11 16:20:24,359 - INFO - tqdm - accuracy: 0.8113, batch_loss: 0.3429, loss: 0.4051 ||:  86%|########6 | 56/65 [10:09<01:38, 10.93s/it]
2022-04-11 16:20:35,292 - INFO - tqdm - accuracy: 0.8102, batch_loss: 0.5245, loss: 0.4072 ||:  88%|########7 | 57/65 [10:20<01:27, 10.93s/it]
2022-04-11 16:20:46,227 - INFO - tqdm - accuracy: 0.8108, batch_loss: 0.3954, loss: 0.4070 ||:  89%|########9 | 58/65 [10:31<01:16, 10.93s/it]
2022-04-11 16:20:57,156 - INFO - tqdm - accuracy: 0.8119, batch_loss: 0.3368, loss: 0.4058 ||:  91%|######### | 59/65 [10:42<01:05, 10.93s/it]
2022-04-11 16:21:08,093 - INFO - tqdm - accuracy: 0.8108, batch_loss: 0.4387, loss: 0.4063 ||:  92%|#########2| 60/65 [10:53<00:54, 10.93s/it]
2022-04-11 16:21:19,016 - INFO - tqdm - accuracy: 0.8104, batch_loss: 0.4083, loss: 0.4064 ||:  94%|#########3| 61/65 [11:04<00:43, 10.93s/it]
2022-04-11 16:21:29,719 - INFO - tqdm - accuracy: 0.8109, batch_loss: 0.3565, loss: 0.4056 ||:  95%|#########5| 62/65 [11:14<00:32, 10.86s/it]
2022-04-11 16:21:40,648 - INFO - tqdm - accuracy: 0.8119, batch_loss: 0.2773, loss: 0.4035 ||:  97%|#########6| 63/65 [11:25<00:21, 10.88s/it]
2022-04-11 16:21:51,589 - INFO - tqdm - accuracy: 0.8124, batch_loss: 0.3791, loss: 0.4032 ||:  98%|#########8| 64/65 [11:36<00:10, 10.90s/it]
2022-04-11 16:21:56,438 - INFO - tqdm - accuracy: 0.8127, batch_loss: 0.3973, loss: 0.4031 ||: 100%|##########| 65/65 [11:41<00:00,  9.08s/it]
2022-04-11 16:21:56,439 - INFO - tqdm - accuracy: 0.8127, batch_loss: 0.3973, loss: 0.4031 ||: 100%|##########| 65/65 [11:41<00:00, 10.80s/it]
2022-04-11 16:21:59,316 - INFO - allennlp.training.trainer - Validating
2022-04-11 16:21:59,317 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-11 16:22:09,361 - INFO - tqdm - accuracy: 0.6310, batch_loss: 3.9423, loss: 1.0770 ||:  37%|###6      | 42/115 [00:10<00:17,  4.16it/s]
2022-04-11 16:22:19,474 - INFO - tqdm - accuracy: 0.6429, batch_loss: 0.1770, loss: 0.9948 ||:  73%|#######3  | 84/115 [00:20<00:07,  4.13it/s]
2022-04-11 16:22:26,744 - INFO - tqdm - accuracy: 0.6463, batch_loss: 0.1105, loss: 0.9905 ||: 100%|##########| 115/115 [00:27<00:00,  4.14it/s]
2022-04-11 16:22:26,744 - INFO - tqdm - accuracy: 0.6463, batch_loss: 0.1105, loss: 0.9905 ||: 100%|##########| 115/115 [00:27<00:00,  4.19it/s]
2022-04-11 16:22:26,744 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-11 16:22:26,744 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.813  |     0.646
2022-04-11 16:22:26,745 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-11 16:22:26,746 - INFO - allennlp.training.tensorboard_writer - loss               |     0.403  |     0.990
2022-04-11 16:22:26,746 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5804.656  |       N/A
2022-04-11 16:22:32,960 - INFO - allennlp.training.trainer - Epoch duration: 0:12:18.224256
2022-04-11 16:22:32,960 - INFO - allennlp.training.trainer - Estimated training time remaining: 2:04:21
2022-04-11 16:22:32,960 - INFO - allennlp.training.trainer - Epoch 5/14
2022-04-11 16:22:32,961 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-11 16:22:32,961 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-11 16:22:32,962 - INFO - allennlp.training.trainer - Training
2022-04-11 16:22:32,962 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-11 16:22:43,629 - INFO - tqdm - accuracy: 0.8750, batch_loss: 0.1997, loss: 0.1997 ||:   2%|1         | 1/65 [00:10<11:22, 10.67s/it]
2022-04-11 16:22:54,425 - INFO - tqdm - accuracy: 0.8438, batch_loss: 0.3117, loss: 0.2557 ||:   3%|3         | 2/65 [00:21<11:16, 10.74s/it]
2022-04-11 16:23:05,239 - INFO - tqdm - accuracy: 0.8333, batch_loss: 0.3668, loss: 0.2927 ||:   5%|4         | 3/65 [00:32<11:08, 10.78s/it]
2022-04-11 16:23:16,099 - INFO - tqdm - accuracy: 0.8281, batch_loss: 0.3435, loss: 0.3054 ||:   6%|6         | 4/65 [00:43<10:59, 10.81s/it]
2022-04-11 16:23:26,922 - INFO - tqdm - accuracy: 0.8438, batch_loss: 0.2329, loss: 0.2909 ||:   8%|7         | 5/65 [00:53<10:48, 10.81s/it]
2022-04-11 16:23:37,759 - INFO - tqdm - accuracy: 0.8490, batch_loss: 0.3226, loss: 0.2962 ||:   9%|9         | 6/65 [01:04<10:38, 10.82s/it]
2022-04-11 16:23:48,507 - INFO - tqdm - accuracy: 0.8527, batch_loss: 0.2488, loss: 0.2894 ||:  11%|#         | 7/65 [01:15<10:26, 10.80s/it]
2022-04-11 16:23:59,381 - INFO - tqdm - accuracy: 0.8672, batch_loss: 0.1798, loss: 0.2757 ||:  12%|#2        | 8/65 [01:26<10:16, 10.82s/it]
2022-04-11 16:24:10,251 - INFO - tqdm - accuracy: 0.8750, batch_loss: 0.2071, loss: 0.2681 ||:  14%|#3        | 9/65 [01:37<10:06, 10.84s/it]
2022-04-11 16:24:21,150 - INFO - tqdm - accuracy: 0.8781, batch_loss: 0.3082, loss: 0.2721 ||:  15%|#5        | 10/65 [01:48<09:57, 10.86s/it]
2022-04-11 16:24:32,055 - INFO - tqdm - accuracy: 0.8778, batch_loss: 0.3073, loss: 0.2753 ||:  17%|#6        | 11/65 [01:59<09:47, 10.87s/it]
2022-04-11 16:24:42,980 - INFO - tqdm - accuracy: 0.8802, batch_loss: 0.2393, loss: 0.2723 ||:  18%|#8        | 12/65 [02:10<09:37, 10.89s/it]
2022-04-11 16:24:53,906 - INFO - tqdm - accuracy: 0.8798, batch_loss: 0.2751, loss: 0.2725 ||:  20%|##        | 13/65 [02:20<09:26, 10.90s/it]
2022-04-11 16:25:04,834 - INFO - tqdm - accuracy: 0.8795, batch_loss: 0.4061, loss: 0.2821 ||:  22%|##1       | 14/65 [02:31<09:16, 10.91s/it]
2022-04-11 16:25:15,751 - INFO - tqdm - accuracy: 0.8854, batch_loss: 0.1607, loss: 0.2740 ||:  23%|##3       | 15/65 [02:42<09:05, 10.91s/it]
2022-04-11 16:25:26,330 - INFO - tqdm - accuracy: 0.8806, batch_loss: 0.3230, loss: 0.2770 ||:  25%|##4       | 16/65 [02:53<08:49, 10.81s/it]
2022-04-11 16:25:37,239 - INFO - tqdm - accuracy: 0.8803, batch_loss: 0.3261, loss: 0.2799 ||:  26%|##6       | 17/65 [03:04<08:40, 10.84s/it]
2022-04-11 16:25:48,176 - INFO - tqdm - accuracy: 0.8800, batch_loss: 0.2894, loss: 0.2804 ||:  28%|##7       | 18/65 [03:15<08:30, 10.87s/it]
2022-04-11 16:25:59,116 - INFO - tqdm - accuracy: 0.8830, batch_loss: 0.1982, loss: 0.2761 ||:  29%|##9       | 19/65 [03:26<08:20, 10.89s/it]
2022-04-11 16:26:10,059 - INFO - tqdm - accuracy: 0.8842, batch_loss: 0.2114, loss: 0.2729 ||:  31%|###       | 20/65 [03:37<08:10, 10.91s/it]
2022-04-11 16:26:20,994 - INFO - tqdm - accuracy: 0.8867, batch_loss: 0.2446, loss: 0.2715 ||:  32%|###2      | 21/65 [03:48<08:00, 10.91s/it]
2022-04-11 16:26:31,913 - INFO - tqdm - accuracy: 0.8848, batch_loss: 0.4012, loss: 0.2774 ||:  34%|###3      | 22/65 [03:58<07:49, 10.92s/it]
2022-04-11 16:26:42,832 - INFO - tqdm - accuracy: 0.8816, batch_loss: 0.3640, loss: 0.2812 ||:  35%|###5      | 23/65 [04:09<07:38, 10.92s/it]
2022-04-11 16:26:53,725 - INFO - tqdm - accuracy: 0.8827, batch_loss: 0.2596, loss: 0.2803 ||:  37%|###6      | 24/65 [04:20<07:27, 10.91s/it]
2022-04-11 16:27:04,620 - INFO - tqdm - accuracy: 0.8811, batch_loss: 0.3886, loss: 0.2846 ||:  38%|###8      | 25/65 [04:31<07:16, 10.91s/it]
2022-04-11 16:27:15,524 - INFO - tqdm - accuracy: 0.8833, batch_loss: 0.1842, loss: 0.2808 ||:  40%|####      | 26/65 [04:42<07:05, 10.90s/it]
2022-04-11 16:27:26,420 - INFO - tqdm - accuracy: 0.8795, batch_loss: 0.3484, loss: 0.2833 ||:  42%|####1     | 27/65 [04:53<06:54, 10.90s/it]
2022-04-11 16:27:37,317 - INFO - tqdm - accuracy: 0.8804, batch_loss: 0.2037, loss: 0.2804 ||:  43%|####3     | 28/65 [05:04<06:43, 10.90s/it]
2022-04-11 16:27:48,216 - INFO - tqdm - accuracy: 0.8846, batch_loss: 0.1151, loss: 0.2747 ||:  45%|####4     | 29/65 [05:15<06:32, 10.90s/it]
2022-04-11 16:27:59,106 - INFO - tqdm - accuracy: 0.8843, batch_loss: 0.2840, loss: 0.2750 ||:  46%|####6     | 30/65 [05:26<06:21, 10.90s/it]
2022-04-11 16:28:09,974 - INFO - tqdm - accuracy: 0.8870, batch_loss: 0.1956, loss: 0.2725 ||:  48%|####7     | 31/65 [05:37<06:10, 10.89s/it]
2022-04-11 16:28:20,853 - INFO - tqdm - accuracy: 0.8886, batch_loss: 0.2606, loss: 0.2721 ||:  49%|####9     | 32/65 [05:47<05:59, 10.89s/it]
2022-04-11 16:28:31,746 - INFO - tqdm - accuracy: 0.8872, batch_loss: 0.2353, loss: 0.2710 ||:  51%|#####     | 33/65 [05:58<05:48, 10.89s/it]
2022-04-11 16:28:42,639 - INFO - tqdm - accuracy: 0.8887, batch_loss: 0.2810, loss: 0.2713 ||:  52%|#####2    | 34/65 [06:09<05:37, 10.89s/it]
2022-04-11 16:28:53,554 - INFO - tqdm - accuracy: 0.8892, batch_loss: 0.2551, loss: 0.2708 ||:  54%|#####3    | 35/65 [06:20<05:26, 10.90s/it]
2022-04-11 16:29:04,464 - INFO - tqdm - accuracy: 0.8897, batch_loss: 0.2044, loss: 0.2690 ||:  55%|#####5    | 36/65 [06:31<05:16, 10.90s/it]
2022-04-11 16:29:15,384 - INFO - tqdm - accuracy: 0.8884, batch_loss: 0.3451, loss: 0.2710 ||:  57%|#####6    | 37/65 [06:42<05:05, 10.91s/it]
2022-04-11 16:29:26,293 - INFO - tqdm - accuracy: 0.8881, batch_loss: 0.2246, loss: 0.2698 ||:  58%|#####8    | 38/65 [06:53<04:54, 10.91s/it]
2022-04-11 16:29:37,214 - INFO - tqdm - accuracy: 0.8885, batch_loss: 0.2200, loss: 0.2685 ||:  60%|######    | 39/65 [07:04<04:43, 10.91s/it]
2022-04-11 16:29:48,134 - INFO - tqdm - accuracy: 0.8866, batch_loss: 0.5250, loss: 0.2749 ||:  62%|######1   | 40/65 [07:15<04:32, 10.91s/it]
2022-04-11 16:29:59,062 - INFO - tqdm - accuracy: 0.8886, batch_loss: 0.1451, loss: 0.2718 ||:  63%|######3   | 41/65 [07:26<04:22, 10.92s/it]
2022-04-11 16:30:10,011 - INFO - tqdm - accuracy: 0.8876, batch_loss: 0.4100, loss: 0.2751 ||:  65%|######4   | 42/65 [07:37<04:11, 10.93s/it]
2022-04-11 16:30:20,957 - INFO - tqdm - accuracy: 0.8873, batch_loss: 0.3195, loss: 0.2761 ||:  66%|######6   | 43/65 [07:47<04:00, 10.93s/it]
2022-04-11 16:30:31,886 - INFO - tqdm - accuracy: 0.8827, batch_loss: 0.6139, loss: 0.2838 ||:  68%|######7   | 44/65 [07:58<03:49, 10.93s/it]
2022-04-11 16:30:42,578 - INFO - tqdm - accuracy: 0.8839, batch_loss: 0.2327, loss: 0.2826 ||:  69%|######9   | 45/65 [08:09<03:37, 10.86s/it]
2022-04-11 16:30:53,511 - INFO - tqdm - accuracy: 0.8838, batch_loss: 0.2622, loss: 0.2822 ||:  71%|#######   | 46/65 [08:20<03:26, 10.88s/it]
2022-04-11 16:31:04,444 - INFO - tqdm - accuracy: 0.8836, batch_loss: 0.2714, loss: 0.2820 ||:  72%|#######2  | 47/65 [08:31<03:16, 10.90s/it]
2022-04-11 16:31:15,385 - INFO - tqdm - accuracy: 0.8808, batch_loss: 0.4939, loss: 0.2864 ||:  74%|#######3  | 48/65 [08:42<03:05, 10.91s/it]
2022-04-11 16:31:26,295 - INFO - tqdm - accuracy: 0.8807, batch_loss: 0.2048, loss: 0.2847 ||:  75%|#######5  | 49/65 [08:53<02:54, 10.91s/it]
2022-04-11 16:31:37,203 - INFO - tqdm - accuracy: 0.8806, batch_loss: 0.2866, loss: 0.2848 ||:  77%|#######6  | 50/65 [09:04<02:43, 10.91s/it]
2022-04-11 16:31:48,095 - INFO - tqdm - accuracy: 0.8823, batch_loss: 0.1503, loss: 0.2821 ||:  78%|#######8  | 51/65 [09:15<02:32, 10.90s/it]
2022-04-11 16:31:59,038 - INFO - tqdm - accuracy: 0.8815, batch_loss: 0.3021, loss: 0.2825 ||:  80%|########  | 52/65 [09:26<02:21, 10.92s/it]
2022-04-11 16:32:10,006 - INFO - tqdm - accuracy: 0.8820, batch_loss: 0.2450, loss: 0.2818 ||:  82%|########1 | 53/65 [09:37<02:11, 10.93s/it]
2022-04-11 16:32:20,984 - INFO - tqdm - accuracy: 0.8825, batch_loss: 0.2746, loss: 0.2817 ||:  83%|########3 | 54/65 [09:48<02:00, 10.95s/it]
2022-04-11 16:32:31,950 - INFO - tqdm - accuracy: 0.8818, batch_loss: 0.3505, loss: 0.2829 ||:  85%|########4 | 55/65 [09:58<01:49, 10.95s/it]
2022-04-11 16:32:42,915 - INFO - tqdm - accuracy: 0.8811, batch_loss: 0.2901, loss: 0.2830 ||:  86%|########6 | 56/65 [10:09<01:38, 10.96s/it]
2022-04-11 16:32:53,848 - INFO - tqdm - accuracy: 0.8821, batch_loss: 0.2424, loss: 0.2823 ||:  88%|########7 | 57/65 [10:20<01:27, 10.95s/it]
2022-04-11 16:33:04,793 - INFO - tqdm - accuracy: 0.8803, batch_loss: 0.3880, loss: 0.2842 ||:  89%|########9 | 58/65 [10:31<01:16, 10.95s/it]
2022-04-11 16:33:15,604 - INFO - tqdm - accuracy: 0.8802, batch_loss: 0.2841, loss: 0.2842 ||:  91%|######### | 59/65 [10:42<01:05, 10.91s/it]
2022-04-11 16:33:26,513 - INFO - tqdm - accuracy: 0.8801, batch_loss: 0.3796, loss: 0.2857 ||:  92%|#########2| 60/65 [10:53<00:54, 10.91s/it]
2022-04-11 16:33:37,402 - INFO - tqdm - accuracy: 0.8801, batch_loss: 0.3117, loss: 0.2862 ||:  94%|#########3| 61/65 [11:04<00:43, 10.90s/it]
2022-04-11 16:33:48,328 - INFO - tqdm - accuracy: 0.8800, batch_loss: 0.2564, loss: 0.2857 ||:  95%|#########5| 62/65 [11:15<00:32, 10.91s/it]
2022-04-11 16:33:59,256 - INFO - tqdm - accuracy: 0.8804, batch_loss: 0.2754, loss: 0.2855 ||:  97%|#########6| 63/65 [11:26<00:21, 10.91s/it]
2022-04-11 16:34:10,192 - INFO - tqdm - accuracy: 0.8803, batch_loss: 0.2622, loss: 0.2852 ||:  98%|#########8| 64/65 [11:37<00:10, 10.92s/it]
2022-04-11 16:34:15,034 - INFO - tqdm - accuracy: 0.8792, batch_loss: 0.4002, loss: 0.2869 ||: 100%|##########| 65/65 [11:42<00:00,  9.10s/it]
2022-04-11 16:34:15,034 - INFO - tqdm - accuracy: 0.8792, batch_loss: 0.4002, loss: 0.2869 ||: 100%|##########| 65/65 [11:42<00:00, 10.80s/it]
2022-04-11 16:34:17,916 - INFO - allennlp.training.trainer - Validating
2022-04-11 16:34:17,918 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-11 16:34:27,976 - INFO - tqdm - accuracy: 0.5833, batch_loss: 2.4621, loss: 1.4603 ||:  37%|###6      | 42/115 [00:10<00:17,  4.20it/s]
2022-04-11 16:34:38,111 - INFO - tqdm - accuracy: 0.6331, batch_loss: 2.6304, loss: 1.2240 ||:  74%|#######3  | 85/115 [00:20<00:07,  4.19it/s]
2022-04-11 16:34:45,332 - INFO - tqdm - accuracy: 0.6245, batch_loss: 0.1357, loss: 1.2117 ||: 100%|##########| 115/115 [00:27<00:00,  4.15it/s]
2022-04-11 16:34:45,332 - INFO - tqdm - accuracy: 0.6245, batch_loss: 0.1357, loss: 1.2117 ||: 100%|##########| 115/115 [00:27<00:00,  4.19it/s]
2022-04-11 16:34:45,333 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-11 16:34:45,333 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.879  |     0.624
2022-04-11 16:34:45,334 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-11 16:34:45,335 - INFO - allennlp.training.tensorboard_writer - loss               |     0.287  |     1.212
2022-04-11 16:34:45,335 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5804.656  |       N/A
2022-04-11 16:34:51,531 - INFO - allennlp.training.trainer - Epoch duration: 0:12:18.570572
2022-04-11 16:34:51,531 - INFO - allennlp.training.trainer - Estimated training time remaining: 1:51:44
2022-04-11 16:34:51,531 - INFO - allennlp.training.trainer - Epoch 6/14
2022-04-11 16:34:51,532 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-11 16:34:51,532 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-11 16:34:51,533 - INFO - allennlp.training.trainer - Training
2022-04-11 16:34:51,534 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-11 16:35:02,347 - INFO - tqdm - accuracy: 0.8750, batch_loss: 0.2839, loss: 0.2839 ||:   2%|1         | 1/65 [00:10<11:32, 10.81s/it]
2022-04-11 16:35:13,171 - INFO - tqdm - accuracy: 0.9062, batch_loss: 0.1841, loss: 0.2340 ||:   3%|3         | 2/65 [00:21<11:21, 10.82s/it]
2022-04-11 16:35:24,030 - INFO - tqdm - accuracy: 0.9375, batch_loss: 0.1552, loss: 0.2077 ||:   5%|4         | 3/65 [00:32<11:11, 10.84s/it]
2022-04-11 16:35:34,876 - INFO - tqdm - accuracy: 0.9297, batch_loss: 0.2302, loss: 0.2134 ||:   6%|6         | 4/65 [00:43<11:01, 10.84s/it]
2022-04-11 16:35:45,739 - INFO - tqdm - accuracy: 0.9250, batch_loss: 0.2346, loss: 0.2176 ||:   8%|7         | 5/65 [00:54<10:50, 10.85s/it]
2022-04-11 16:35:56,639 - INFO - tqdm - accuracy: 0.9375, batch_loss: 0.1257, loss: 0.2023 ||:   9%|9         | 6/65 [01:05<10:41, 10.87s/it]
2022-04-11 16:36:07,527 - INFO - tqdm - accuracy: 0.9241, batch_loss: 0.3882, loss: 0.2288 ||:  11%|#         | 7/65 [01:15<10:30, 10.87s/it]
2022-04-11 16:36:18,426 - INFO - tqdm - accuracy: 0.9297, batch_loss: 0.1905, loss: 0.2241 ||:  12%|#2        | 8/65 [01:26<10:20, 10.88s/it]
2022-04-11 16:36:29,333 - INFO - tqdm - accuracy: 0.9271, batch_loss: 0.2252, loss: 0.2242 ||:  14%|#3        | 9/65 [01:37<10:09, 10.89s/it]
2022-04-11 16:36:40,748 - INFO - tqdm - accuracy: 0.9219, batch_loss: 0.2811, loss: 0.2299 ||:  15%|#5        | 10/65 [01:49<10:07, 11.05s/it]
2022-04-11 16:36:51,624 - INFO - tqdm - accuracy: 0.9119, batch_loss: 0.3672, loss: 0.2424 ||:  17%|#6        | 11/65 [02:00<09:53, 11.00s/it]
2022-04-11 16:37:02,553 - INFO - tqdm - accuracy: 0.9115, batch_loss: 0.1625, loss: 0.2357 ||:  18%|#8        | 12/65 [02:11<09:41, 10.98s/it]
2022-04-11 16:37:13,499 - INFO - tqdm - accuracy: 0.9111, batch_loss: 0.2126, loss: 0.2339 ||:  20%|##        | 13/65 [02:21<09:30, 10.97s/it]
2022-04-11 16:37:24,438 - INFO - tqdm - accuracy: 0.9107, batch_loss: 0.1998, loss: 0.2315 ||:  22%|##1       | 14/65 [02:32<09:18, 10.96s/it]
2022-04-11 16:37:35,402 - INFO - tqdm - accuracy: 0.9104, batch_loss: 0.2006, loss: 0.2294 ||:  23%|##3       | 15/65 [02:43<09:08, 10.96s/it]
2022-04-11 16:37:46,358 - INFO - tqdm - accuracy: 0.9121, batch_loss: 0.1931, loss: 0.2272 ||:  25%|##4       | 16/65 [02:54<08:56, 10.96s/it]
2022-04-11 16:37:57,306 - INFO - tqdm - accuracy: 0.9154, batch_loss: 0.0968, loss: 0.2195 ||:  26%|##6       | 17/65 [03:05<08:45, 10.96s/it]
2022-04-11 16:38:08,282 - INFO - tqdm - accuracy: 0.9132, batch_loss: 0.2117, loss: 0.2191 ||:  28%|##7       | 18/65 [03:16<08:35, 10.96s/it]
2022-04-11 16:38:19,271 - INFO - tqdm - accuracy: 0.9128, batch_loss: 0.2383, loss: 0.2201 ||:  29%|##9       | 19/65 [03:27<08:24, 10.97s/it]
2022-04-11 16:38:30,236 - INFO - tqdm - accuracy: 0.9172, batch_loss: 0.1250, loss: 0.2153 ||:  31%|###       | 20/65 [03:38<08:13, 10.97s/it]
2022-04-11 16:38:41,228 - INFO - tqdm - accuracy: 0.9152, batch_loss: 0.2047, loss: 0.2148 ||:  32%|###2      | 21/65 [03:49<08:02, 10.98s/it]
2022-04-11 16:38:52,212 - INFO - tqdm - accuracy: 0.9148, batch_loss: 0.2117, loss: 0.2147 ||:  34%|###3      | 22/65 [04:00<07:52, 10.98s/it]
2022-04-11 16:39:03,187 - INFO - tqdm - accuracy: 0.9171, batch_loss: 0.2515, loss: 0.2163 ||:  35%|###5      | 23/65 [04:11<07:41, 10.98s/it]
2022-04-11 16:39:14,158 - INFO - tqdm - accuracy: 0.9154, batch_loss: 0.2724, loss: 0.2186 ||:  37%|###6      | 24/65 [04:22<07:29, 10.98s/it]
2022-04-11 16:39:25,117 - INFO - tqdm - accuracy: 0.9125, batch_loss: 0.4082, loss: 0.2262 ||:  38%|###8      | 25/65 [04:33<07:18, 10.97s/it]
2022-04-11 16:39:36,084 - INFO - tqdm - accuracy: 0.9135, batch_loss: 0.2437, loss: 0.2269 ||:  40%|####      | 26/65 [04:44<07:07, 10.97s/it]
2022-04-11 16:39:47,045 - INFO - tqdm - accuracy: 0.9167, batch_loss: 0.0529, loss: 0.2204 ||:  42%|####1     | 27/65 [04:55<06:56, 10.97s/it]
2022-04-11 16:39:57,997 - INFO - tqdm - accuracy: 0.9174, batch_loss: 0.1743, loss: 0.2188 ||:  43%|####3     | 28/65 [05:06<06:45, 10.96s/it]
2022-04-11 16:40:08,944 - INFO - tqdm - accuracy: 0.9203, batch_loss: 0.0696, loss: 0.2136 ||:  45%|####4     | 29/65 [05:17<06:34, 10.96s/it]
2022-04-11 16:40:19,781 - INFO - tqdm - accuracy: 0.9198, batch_loss: 0.1861, loss: 0.2127 ||:  46%|####6     | 30/65 [05:28<06:22, 10.92s/it]
2022-04-11 16:40:30,752 - INFO - tqdm - accuracy: 0.9214, batch_loss: 0.1092, loss: 0.2094 ||:  48%|####7     | 31/65 [05:39<06:11, 10.94s/it]
2022-04-11 16:40:41,705 - INFO - tqdm - accuracy: 0.9219, batch_loss: 0.1514, loss: 0.2076 ||:  49%|####9     | 32/65 [05:50<06:01, 10.94s/it]
2022-04-11 16:40:52,637 - INFO - tqdm - accuracy: 0.9214, batch_loss: 0.1679, loss: 0.2064 ||:  51%|#####     | 33/65 [06:01<05:50, 10.94s/it]
2022-04-11 16:41:03,583 - INFO - tqdm - accuracy: 0.9228, batch_loss: 0.1501, loss: 0.2047 ||:  52%|#####2    | 34/65 [06:12<05:39, 10.94s/it]
2022-04-11 16:41:14,525 - INFO - tqdm - accuracy: 0.9223, batch_loss: 0.1876, loss: 0.2042 ||:  54%|#####3    | 35/65 [06:22<05:28, 10.94s/it]
2022-04-11 16:41:25,463 - INFO - tqdm - accuracy: 0.9210, batch_loss: 0.2117, loss: 0.2044 ||:  55%|#####5    | 36/65 [06:33<05:17, 10.94s/it]
2022-04-11 16:41:36,418 - INFO - tqdm - accuracy: 0.9215, batch_loss: 0.1514, loss: 0.2030 ||:  57%|#####6    | 37/65 [06:44<05:06, 10.94s/it]
2022-04-11 16:41:47,362 - INFO - tqdm - accuracy: 0.9202, batch_loss: 0.2794, loss: 0.2050 ||:  58%|#####8    | 38/65 [06:55<04:55, 10.94s/it]
2022-04-11 16:41:58,316 - INFO - tqdm - accuracy: 0.9199, batch_loss: 0.2131, loss: 0.2052 ||:  60%|######    | 39/65 [07:06<04:44, 10.95s/it]
2022-04-11 16:42:09,253 - INFO - tqdm - accuracy: 0.9187, batch_loss: 0.3438, loss: 0.2087 ||:  62%|######1   | 40/65 [07:17<04:33, 10.94s/it]
2022-04-11 16:42:19,868 - INFO - tqdm - accuracy: 0.9199, batch_loss: 0.1135, loss: 0.2064 ||:  63%|######3   | 41/65 [07:28<04:20, 10.85s/it]
2022-04-11 16:42:30,802 - INFO - tqdm - accuracy: 0.9196, batch_loss: 0.3252, loss: 0.2092 ||:  65%|######4   | 42/65 [07:39<04:10, 10.87s/it]
2022-04-11 16:42:41,516 - INFO - tqdm - accuracy: 0.9207, batch_loss: 0.1328, loss: 0.2074 ||:  66%|######6   | 43/65 [07:49<03:58, 10.82s/it]
2022-04-11 16:42:52,444 - INFO - tqdm - accuracy: 0.9204, batch_loss: 0.2205, loss: 0.2077 ||:  68%|######7   | 44/65 [08:00<03:47, 10.86s/it]
2022-04-11 16:43:03,406 - INFO - tqdm - accuracy: 0.9194, batch_loss: 0.2625, loss: 0.2089 ||:  69%|######9   | 45/65 [08:11<03:37, 10.89s/it]
2022-04-11 16:43:14,345 - INFO - tqdm - accuracy: 0.9198, batch_loss: 0.2079, loss: 0.2089 ||:  71%|#######   | 46/65 [08:22<03:27, 10.90s/it]
2022-04-11 16:43:25,289 - INFO - tqdm - accuracy: 0.9202, batch_loss: 0.1317, loss: 0.2073 ||:  72%|#######2  | 47/65 [08:33<03:16, 10.92s/it]
2022-04-11 16:43:36,252 - INFO - tqdm - accuracy: 0.9205, batch_loss: 0.3562, loss: 0.2104 ||:  74%|#######3  | 48/65 [08:44<03:05, 10.93s/it]
2022-04-11 16:43:47,210 - INFO - tqdm - accuracy: 0.9215, batch_loss: 0.1298, loss: 0.2087 ||:  75%|#######5  | 49/65 [08:55<02:55, 10.94s/it]
2022-04-11 16:43:58,162 - INFO - tqdm - accuracy: 0.9231, batch_loss: 0.0606, loss: 0.2058 ||:  77%|#######6  | 50/65 [09:06<02:44, 10.94s/it]
2022-04-11 16:44:09,121 - INFO - tqdm - accuracy: 0.9227, batch_loss: 0.1686, loss: 0.2050 ||:  78%|#######8  | 51/65 [09:17<02:33, 10.95s/it]
2022-04-11 16:44:19,957 - INFO - tqdm - accuracy: 0.9230, batch_loss: 0.1503, loss: 0.2040 ||:  80%|########  | 52/65 [09:28<02:21, 10.91s/it]
2022-04-11 16:44:30,910 - INFO - tqdm - accuracy: 0.9233, batch_loss: 0.1850, loss: 0.2036 ||:  82%|########1 | 53/65 [09:39<02:11, 10.93s/it]
2022-04-11 16:44:41,863 - INFO - tqdm - accuracy: 0.9241, batch_loss: 0.1285, loss: 0.2022 ||:  83%|########3 | 54/65 [09:50<02:00, 10.93s/it]
2022-04-11 16:44:52,822 - INFO - tqdm - accuracy: 0.9255, batch_loss: 0.0763, loss: 0.1999 ||:  85%|########4 | 55/65 [10:01<01:49, 10.94s/it]
2022-04-11 16:45:03,676 - INFO - tqdm - accuracy: 0.9263, batch_loss: 0.0769, loss: 0.1977 ||:  86%|########6 | 56/65 [10:12<01:38, 10.92s/it]
2022-04-11 16:45:14,630 - INFO - tqdm - accuracy: 0.9248, batch_loss: 0.3406, loss: 0.2002 ||:  88%|########7 | 57/65 [10:23<01:27, 10.93s/it]
2022-04-11 16:45:25,603 - INFO - tqdm - accuracy: 0.9261, batch_loss: 0.0403, loss: 0.1975 ||:  89%|########9 | 58/65 [10:34<01:16, 10.94s/it]
2022-04-11 16:45:36,583 - INFO - tqdm - accuracy: 0.9263, batch_loss: 0.0944, loss: 0.1957 ||:  91%|######### | 59/65 [10:45<01:05, 10.95s/it]
2022-04-11 16:45:47,552 - INFO - tqdm - accuracy: 0.9265, batch_loss: 0.1032, loss: 0.1942 ||:  92%|#########2| 60/65 [10:56<00:54, 10.96s/it]
2022-04-11 16:45:58,522 - INFO - tqdm - accuracy: 0.9267, batch_loss: 0.1827, loss: 0.1940 ||:  94%|#########3| 61/65 [11:06<00:43, 10.96s/it]
2022-04-11 16:46:09,481 - INFO - tqdm - accuracy: 0.9264, batch_loss: 0.2723, loss: 0.1953 ||:  95%|#########5| 62/65 [11:17<00:32, 10.96s/it]
2022-04-11 16:46:20,455 - INFO - tqdm - accuracy: 0.9261, batch_loss: 0.2083, loss: 0.1955 ||:  97%|#########6| 63/65 [11:28<00:21, 10.96s/it]
2022-04-11 16:46:31,410 - INFO - tqdm - accuracy: 0.9248, batch_loss: 0.2518, loss: 0.1964 ||:  98%|#########8| 64/65 [11:39<00:10, 10.96s/it]
2022-04-11 16:46:36,280 - INFO - tqdm - accuracy: 0.9253, batch_loss: 0.1011, loss: 0.1949 ||: 100%|##########| 65/65 [11:44<00:00,  9.13s/it]
2022-04-11 16:46:36,280 - INFO - tqdm - accuracy: 0.9253, batch_loss: 0.1011, loss: 0.1949 ||: 100%|##########| 65/65 [11:44<00:00, 10.84s/it]
2022-04-11 16:46:39,169 - INFO - allennlp.training.trainer - Validating
2022-04-11 16:46:39,171 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-11 16:46:49,380 - INFO - tqdm - accuracy: 0.6471, batch_loss: 2.1613, loss: 1.4400 ||:  37%|###7      | 43/115 [00:10<00:17,  4.13it/s]
2022-04-11 16:46:59,409 - INFO - tqdm - accuracy: 0.6568, batch_loss: 0.1072, loss: 1.4561 ||:  74%|#######3  | 85/115 [00:20<00:07,  4.26it/s]
2022-04-11 16:47:06,634 - INFO - tqdm - accuracy: 0.6638, batch_loss: 2.1673, loss: 1.3623 ||: 100%|##########| 115/115 [00:27<00:00,  4.15it/s]
2022-04-11 16:47:06,634 - INFO - tqdm - accuracy: 0.6638, batch_loss: 2.1673, loss: 1.3623 ||: 100%|##########| 115/115 [00:27<00:00,  4.19it/s]
2022-04-11 16:47:06,635 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-11 16:47:06,635 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.925  |     0.664
2022-04-11 16:47:06,636 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-11 16:47:06,637 - INFO - allennlp.training.tensorboard_writer - loss               |     0.195  |     1.362
2022-04-11 16:47:06,638 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5804.656  |       N/A
2022-04-11 16:47:12,317 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to './output_ir-ora-d_hyper7/best.th'.
2022-04-11 16:47:30,858 - INFO - allennlp.training.trainer - Epoch duration: 0:12:39.326357
2022-04-11 16:47:30,858 - INFO - allennlp.training.trainer - Estimated training time remaining: 1:39:35
2022-04-11 16:47:30,858 - INFO - allennlp.training.trainer - Epoch 7/14
2022-04-11 16:47:30,858 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-11 16:47:30,859 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-11 16:47:30,860 - INFO - allennlp.training.trainer - Training
2022-04-11 16:47:30,860 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-11 16:47:41,468 - INFO - tqdm - accuracy: 0.9375, batch_loss: 0.1368, loss: 0.1368 ||:   2%|1         | 1/65 [00:10<11:18, 10.61s/it]
2022-04-11 16:47:52,120 - INFO - tqdm - accuracy: 0.9375, batch_loss: 0.1063, loss: 0.1215 ||:   3%|3         | 2/65 [00:21<11:09, 10.63s/it]
2022-04-11 16:48:02,829 - INFO - tqdm - accuracy: 0.9271, batch_loss: 0.1485, loss: 0.1305 ||:   5%|4         | 3/65 [00:31<11:01, 10.67s/it]
2022-04-11 16:48:13,564 - INFO - tqdm - accuracy: 0.9297, batch_loss: 0.1638, loss: 0.1388 ||:   6%|6         | 4/65 [00:42<10:52, 10.69s/it]
2022-04-11 16:48:24,322 - INFO - tqdm - accuracy: 0.9125, batch_loss: 0.3487, loss: 0.1808 ||:   8%|7         | 5/65 [00:53<10:43, 10.72s/it]
2022-04-11 16:48:35,116 - INFO - tqdm - accuracy: 0.9219, batch_loss: 0.0988, loss: 0.1671 ||:   9%|9         | 6/65 [01:04<10:33, 10.74s/it]
2022-04-11 16:48:45,931 - INFO - tqdm - accuracy: 0.9330, batch_loss: 0.0374, loss: 0.1486 ||:  11%|#         | 7/65 [01:15<10:24, 10.77s/it]
2022-04-11 16:48:56,760 - INFO - tqdm - accuracy: 0.9336, batch_loss: 0.1199, loss: 0.1450 ||:  12%|#2        | 8/65 [01:25<10:14, 10.79s/it]
2022-04-11 16:49:07,596 - INFO - tqdm - accuracy: 0.9375, batch_loss: 0.0732, loss: 0.1370 ||:  14%|#3        | 9/65 [01:36<10:04, 10.80s/it]
2022-04-11 16:49:18,454 - INFO - tqdm - accuracy: 0.9375, batch_loss: 0.1837, loss: 0.1417 ||:  15%|#5        | 10/65 [01:47<09:55, 10.82s/it]
2022-04-11 16:49:29,301 - INFO - tqdm - accuracy: 0.9403, batch_loss: 0.1134, loss: 0.1391 ||:  17%|#6        | 11/65 [01:58<09:44, 10.83s/it]
2022-04-11 16:49:40,192 - INFO - tqdm - accuracy: 0.9323, batch_loss: 0.5813, loss: 0.1760 ||:  18%|#8        | 12/65 [02:09<09:34, 10.85s/it]
2022-04-11 16:49:51,081 - INFO - tqdm - accuracy: 0.9255, batch_loss: 0.3635, loss: 0.1904 ||:  20%|##        | 13/65 [02:20<09:24, 10.86s/it]
2022-04-11 16:50:01,999 - INFO - tqdm - accuracy: 0.9286, batch_loss: 0.1160, loss: 0.1851 ||:  22%|##1       | 14/65 [02:31<09:14, 10.88s/it]
2022-04-11 16:50:12,806 - INFO - tqdm - accuracy: 0.9292, batch_loss: 0.1025, loss: 0.1796 ||:  23%|##3       | 15/65 [02:41<09:02, 10.86s/it]
2022-04-11 16:50:23,716 - INFO - tqdm - accuracy: 0.9336, batch_loss: 0.0510, loss: 0.1716 ||:  25%|##4       | 16/65 [02:52<08:52, 10.87s/it]
2022-04-11 16:50:34,508 - INFO - tqdm - accuracy: 0.9338, batch_loss: 0.1452, loss: 0.1700 ||:  26%|##6       | 17/65 [03:03<08:40, 10.85s/it]
2022-04-11 16:50:45,434 - INFO - tqdm - accuracy: 0.9375, batch_loss: 0.0547, loss: 0.1636 ||:  28%|##7       | 18/65 [03:14<08:30, 10.87s/it]
2022-04-11 16:50:56,368 - INFO - tqdm - accuracy: 0.9375, batch_loss: 0.1823, loss: 0.1646 ||:  29%|##9       | 19/65 [03:25<08:20, 10.89s/it]
2022-04-11 16:51:07,181 - INFO - tqdm - accuracy: 0.9359, batch_loss: 0.2728, loss: 0.1700 ||:  31%|###       | 20/65 [03:36<08:09, 10.87s/it]
2022-04-11 16:51:18,126 - INFO - tqdm - accuracy: 0.9360, batch_loss: 0.1653, loss: 0.1698 ||:  32%|###2      | 21/65 [03:47<07:59, 10.89s/it]
2022-04-11 16:51:29,102 - INFO - tqdm - accuracy: 0.9332, batch_loss: 0.2050, loss: 0.1714 ||:  34%|###3      | 22/65 [03:58<07:49, 10.92s/it]
2022-04-11 16:51:40,059 - INFO - tqdm - accuracy: 0.9348, batch_loss: 0.1362, loss: 0.1698 ||:  35%|###5      | 23/65 [04:09<07:38, 10.93s/it]
2022-04-11 16:51:51,014 - INFO - tqdm - accuracy: 0.9375, batch_loss: 0.0570, loss: 0.1651 ||:  37%|###6      | 24/65 [04:20<07:28, 10.94s/it]
2022-04-11 16:52:02,000 - INFO - tqdm - accuracy: 0.9387, batch_loss: 0.0921, loss: 0.1622 ||:  38%|###8      | 25/65 [04:31<07:18, 10.95s/it]
2022-04-11 16:52:12,969 - INFO - tqdm - accuracy: 0.9399, batch_loss: 0.1256, loss: 0.1608 ||:  40%|####      | 26/65 [04:42<07:07, 10.96s/it]
2022-04-11 16:52:23,957 - INFO - tqdm - accuracy: 0.9421, batch_loss: 0.0455, loss: 0.1565 ||:  42%|####1     | 27/65 [04:53<06:56, 10.97s/it]
2022-04-11 16:52:34,885 - INFO - tqdm - accuracy: 0.9442, batch_loss: 0.0386, loss: 0.1523 ||:  43%|####3     | 28/65 [05:04<06:45, 10.95s/it]
2022-04-11 16:52:45,809 - INFO - tqdm - accuracy: 0.9440, batch_loss: 0.1888, loss: 0.1536 ||:  45%|####4     | 29/65 [05:14<06:34, 10.95s/it]
2022-04-11 16:52:56,755 - INFO - tqdm - accuracy: 0.9437, batch_loss: 0.1727, loss: 0.1542 ||:  46%|####6     | 30/65 [05:25<06:23, 10.95s/it]
2022-04-11 16:53:07,702 - INFO - tqdm - accuracy: 0.9435, batch_loss: 0.1645, loss: 0.1546 ||:  48%|####7     | 31/65 [05:36<06:12, 10.95s/it]
2022-04-11 16:53:18,640 - INFO - tqdm - accuracy: 0.9453, batch_loss: 0.1115, loss: 0.1532 ||:  49%|####9     | 32/65 [05:47<06:01, 10.94s/it]
2022-04-11 16:53:29,592 - INFO - tqdm - accuracy: 0.9460, batch_loss: 0.0675, loss: 0.1506 ||:  51%|#####     | 33/65 [05:58<05:50, 10.95s/it]
2022-04-11 16:53:40,557 - INFO - tqdm - accuracy: 0.9449, batch_loss: 0.3240, loss: 0.1557 ||:  52%|#####2    | 34/65 [06:09<05:39, 10.95s/it]
2022-04-11 16:53:51,524 - INFO - tqdm - accuracy: 0.9437, batch_loss: 0.2206, loss: 0.1576 ||:  54%|#####3    | 35/65 [06:20<05:28, 10.96s/it]
2022-04-11 16:54:02,482 - INFO - tqdm - accuracy: 0.9418, batch_loss: 0.2295, loss: 0.1596 ||:  55%|#####5    | 36/65 [06:31<05:17, 10.96s/it]
2022-04-11 16:54:13,443 - INFO - tqdm - accuracy: 0.9434, batch_loss: 0.0607, loss: 0.1569 ||:  57%|#####6    | 37/65 [06:42<05:06, 10.96s/it]
2022-04-11 16:54:24,387 - INFO - tqdm - accuracy: 0.9449, batch_loss: 0.0659, loss: 0.1545 ||:  58%|#####8    | 38/65 [06:53<04:55, 10.95s/it]
2022-04-11 16:54:35,341 - INFO - tqdm - accuracy: 0.9447, batch_loss: 0.1382, loss: 0.1541 ||:  60%|######    | 39/65 [07:04<04:44, 10.95s/it]
2022-04-11 16:54:46,302 - INFO - tqdm - accuracy: 0.9445, batch_loss: 0.1956, loss: 0.1551 ||:  62%|######1   | 40/65 [07:15<04:33, 10.96s/it]
2022-04-11 16:54:57,265 - INFO - tqdm - accuracy: 0.9451, batch_loss: 0.1688, loss: 0.1555 ||:  63%|######3   | 41/65 [07:26<04:22, 10.96s/it]
2022-04-11 16:55:08,223 - INFO - tqdm - accuracy: 0.9449, batch_loss: 0.1377, loss: 0.1550 ||:  65%|######4   | 42/65 [07:37<04:12, 10.96s/it]
2022-04-11 16:55:19,192 - INFO - tqdm - accuracy: 0.9448, batch_loss: 0.1608, loss: 0.1552 ||:  66%|######6   | 43/65 [07:48<04:01, 10.96s/it]
2022-04-11 16:55:30,156 - INFO - tqdm - accuracy: 0.9439, batch_loss: 0.1591, loss: 0.1553 ||:  68%|######7   | 44/65 [07:59<03:50, 10.96s/it]
2022-04-11 16:55:41,609 - INFO - tqdm - accuracy: 0.9444, batch_loss: 0.0390, loss: 0.1527 ||:  69%|######9   | 45/65 [08:10<03:42, 11.11s/it]
2022-04-11 16:55:52,559 - INFO - tqdm - accuracy: 0.9450, batch_loss: 0.1451, loss: 0.1525 ||:  71%|#######   | 46/65 [08:21<03:30, 11.06s/it]
2022-04-11 16:56:03,504 - INFO - tqdm - accuracy: 0.9455, batch_loss: 0.0822, loss: 0.1510 ||:  72%|#######2  | 47/65 [08:32<03:18, 11.03s/it]
2022-04-11 16:56:14,465 - INFO - tqdm - accuracy: 0.9460, batch_loss: 0.1475, loss: 0.1509 ||:  74%|#######3  | 48/65 [08:43<03:07, 11.01s/it]
2022-04-11 16:56:25,437 - INFO - tqdm - accuracy: 0.9471, batch_loss: 0.0433, loss: 0.1487 ||:  75%|#######5  | 49/65 [08:54<02:55, 11.00s/it]
2022-04-11 16:56:36,412 - INFO - tqdm - accuracy: 0.9475, batch_loss: 0.1037, loss: 0.1478 ||:  77%|#######6  | 50/65 [09:05<02:44, 10.99s/it]
2022-04-11 16:56:47,367 - INFO - tqdm - accuracy: 0.9479, batch_loss: 0.1547, loss: 0.1480 ||:  78%|#######8  | 51/65 [09:16<02:33, 10.98s/it]
2022-04-11 16:56:58,328 - INFO - tqdm - accuracy: 0.9471, batch_loss: 0.2939, loss: 0.1508 ||:  80%|########  | 52/65 [09:27<02:22, 10.97s/it]
2022-04-11 16:57:09,311 - INFO - tqdm - accuracy: 0.9475, batch_loss: 0.1651, loss: 0.1510 ||:  82%|########1 | 53/65 [09:38<02:11, 10.98s/it]
2022-04-11 16:57:20,299 - INFO - tqdm - accuracy: 0.9468, batch_loss: 0.1535, loss: 0.1511 ||:  83%|########3 | 54/65 [09:49<02:00, 10.98s/it]
2022-04-11 16:57:31,269 - INFO - tqdm - accuracy: 0.9466, batch_loss: 0.1552, loss: 0.1512 ||:  85%|########4 | 55/65 [10:00<01:49, 10.98s/it]
2022-04-11 16:57:42,246 - INFO - tqdm - accuracy: 0.9464, batch_loss: 0.1739, loss: 0.1516 ||:  86%|########6 | 56/65 [10:11<01:38, 10.98s/it]
2022-04-11 16:57:53,220 - INFO - tqdm - accuracy: 0.9474, batch_loss: 0.0508, loss: 0.1498 ||:  88%|########7 | 57/65 [10:22<01:27, 10.98s/it]
2022-04-11 16:58:04,193 - INFO - tqdm - accuracy: 0.9472, batch_loss: 0.2594, loss: 0.1517 ||:  89%|########9 | 58/65 [10:33<01:16, 10.98s/it]
2022-04-11 16:58:14,625 - INFO - tqdm - accuracy: 0.9470, batch_loss: 0.1683, loss: 0.1520 ||:  91%|######### | 59/65 [10:43<01:04, 10.81s/it]
2022-04-11 16:58:25,606 - INFO - tqdm - accuracy: 0.9479, batch_loss: 0.0450, loss: 0.1502 ||:  92%|#########2| 60/65 [10:54<00:54, 10.86s/it]
2022-04-11 16:58:36,582 - INFO - tqdm - accuracy: 0.9482, batch_loss: 0.0980, loss: 0.1493 ||:  94%|#########3| 61/65 [11:05<00:43, 10.90s/it]
2022-04-11 16:58:47,561 - INFO - tqdm - accuracy: 0.9476, batch_loss: 0.2197, loss: 0.1505 ||:  95%|#########5| 62/65 [11:16<00:32, 10.92s/it]
2022-04-11 16:58:58,552 - INFO - tqdm - accuracy: 0.9484, batch_loss: 0.0460, loss: 0.1488 ||:  97%|#########6| 63/65 [11:27<00:21, 10.94s/it]
2022-04-11 16:59:09,522 - INFO - tqdm - accuracy: 0.9487, batch_loss: 0.1126, loss: 0.1482 ||:  98%|#########8| 64/65 [11:38<00:10, 10.95s/it]
2022-04-11 16:59:14,396 - INFO - tqdm - accuracy: 0.9491, batch_loss: 0.0312, loss: 0.1464 ||: 100%|##########| 65/65 [11:43<00:00,  9.13s/it]
2022-04-11 16:59:14,396 - INFO - tqdm - accuracy: 0.9491, batch_loss: 0.0312, loss: 0.1464 ||: 100%|##########| 65/65 [11:43<00:00, 10.82s/it]
2022-04-11 16:59:17,259 - INFO - allennlp.training.trainer - Validating
2022-04-11 16:59:17,260 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-11 16:59:27,279 - INFO - tqdm - accuracy: 0.7262, batch_loss: 0.0204, loss: 1.1771 ||:  37%|###6      | 42/115 [00:10<00:17,  4.20it/s]
2022-04-11 16:59:37,296 - INFO - tqdm - accuracy: 0.6826, batch_loss: 0.2871, loss: 1.3665 ||:  73%|#######3  | 84/115 [00:20<00:07,  4.12it/s]
2022-04-11 16:59:44,777 - INFO - tqdm - accuracy: 0.6594, batch_loss: 2.7468, loss: 1.5072 ||: 100%|##########| 115/115 [00:27<00:00,  4.14it/s]
2022-04-11 16:59:44,777 - INFO - tqdm - accuracy: 0.6594, batch_loss: 2.7468, loss: 1.5072 ||: 100%|##########| 115/115 [00:27<00:00,  4.18it/s]
2022-04-11 16:59:44,777 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-11 16:59:44,778 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.949  |     0.659
2022-04-11 16:59:44,778 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-11 16:59:44,779 - INFO - allennlp.training.tensorboard_writer - loss               |     0.146  |     1.507
2022-04-11 16:59:44,779 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5804.656  |       N/A
2022-04-11 16:59:51,018 - INFO - allennlp.training.trainer - Epoch duration: 0:12:20.160125
2022-04-11 16:59:51,019 - INFO - allennlp.training.trainer - Estimated training time remaining: 1:27:02
2022-04-11 16:59:51,019 - INFO - allennlp.training.trainer - Epoch 8/14
2022-04-11 16:59:51,019 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-11 16:59:51,019 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-11 16:59:51,021 - INFO - allennlp.training.trainer - Training
2022-04-11 16:59:51,021 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-11 17:00:01,822 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.1090, loss: 0.1090 ||:   2%|1         | 1/65 [00:10<11:31, 10.80s/it]
2022-04-11 17:00:12,663 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0317, loss: 0.0704 ||:   3%|3         | 2/65 [00:21<11:21, 10.82s/it]
2022-04-11 17:00:23,536 - INFO - tqdm - accuracy: 0.9583, batch_loss: 0.1750, loss: 0.1053 ||:   5%|4         | 3/65 [00:32<11:12, 10.85s/it]
2022-04-11 17:00:34,416 - INFO - tqdm - accuracy: 0.9609, batch_loss: 0.0962, loss: 0.1030 ||:   6%|6         | 4/65 [00:43<11:02, 10.86s/it]
2022-04-11 17:00:45,311 - INFO - tqdm - accuracy: 0.9625, batch_loss: 0.1052, loss: 0.1034 ||:   8%|7         | 5/65 [00:54<10:52, 10.87s/it]
2022-04-11 17:00:56,201 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.0325, loss: 0.0916 ||:   9%|9         | 6/65 [01:05<10:41, 10.88s/it]
2022-04-11 17:01:07,120 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.0843, loss: 0.0906 ||:  11%|#         | 7/65 [01:16<10:31, 10.89s/it]
2022-04-11 17:01:18,019 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.0769, loss: 0.0889 ||:  12%|#2        | 8/65 [01:26<10:20, 10.89s/it]
2022-04-11 17:01:28,926 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.0864, loss: 0.0886 ||:  14%|#3        | 9/65 [01:37<10:10, 10.90s/it]
2022-04-11 17:01:39,846 - INFO - tqdm - accuracy: 0.9656, batch_loss: 0.1222, loss: 0.0919 ||:  15%|#5        | 10/65 [01:48<09:59, 10.90s/it]
2022-04-11 17:01:50,769 - INFO - tqdm - accuracy: 0.9631, batch_loss: 0.1009, loss: 0.0928 ||:  17%|#6        | 11/65 [01:59<09:49, 10.91s/it]
2022-04-11 17:02:01,714 - INFO - tqdm - accuracy: 0.9635, batch_loss: 0.0835, loss: 0.0920 ||:  18%|#8        | 12/65 [02:10<09:38, 10.92s/it]
2022-04-11 17:02:12,669 - INFO - tqdm - accuracy: 0.9639, batch_loss: 0.1085, loss: 0.0932 ||:  20%|##        | 13/65 [02:21<09:28, 10.93s/it]
2022-04-11 17:02:23,632 - INFO - tqdm - accuracy: 0.9643, batch_loss: 0.0965, loss: 0.0935 ||:  22%|##1       | 14/65 [02:32<09:17, 10.94s/it]
2022-04-11 17:02:34,594 - INFO - tqdm - accuracy: 0.9667, batch_loss: 0.0567, loss: 0.0910 ||:  23%|##3       | 15/65 [02:43<09:07, 10.95s/it]
2022-04-11 17:02:45,567 - INFO - tqdm - accuracy: 0.9668, batch_loss: 0.0829, loss: 0.0905 ||:  25%|##4       | 16/65 [02:54<08:56, 10.95s/it]
2022-04-11 17:02:56,524 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.0436, loss: 0.0878 ||:  26%|##6       | 17/65 [03:05<08:45, 10.96s/it]
2022-04-11 17:03:07,486 - INFO - tqdm - accuracy: 0.9635, batch_loss: 0.1841, loss: 0.0931 ||:  28%|##7       | 18/65 [03:16<08:34, 10.96s/it]
2022-04-11 17:03:18,442 - INFO - tqdm - accuracy: 0.9655, batch_loss: 0.0445, loss: 0.0905 ||:  29%|##9       | 19/65 [03:27<08:24, 10.96s/it]
2022-04-11 17:03:29,397 - INFO - tqdm - accuracy: 0.9625, batch_loss: 0.2170, loss: 0.0969 ||:  31%|###       | 20/65 [03:38<08:13, 10.96s/it]
2022-04-11 17:03:40,343 - INFO - tqdm - accuracy: 0.9628, batch_loss: 0.1132, loss: 0.0976 ||:  32%|###2      | 21/65 [03:49<08:01, 10.95s/it]
2022-04-11 17:03:51,316 - INFO - tqdm - accuracy: 0.9645, batch_loss: 0.0332, loss: 0.0947 ||:  34%|###3      | 22/65 [04:00<07:51, 10.96s/it]
2022-04-11 17:04:02,298 - INFO - tqdm - accuracy: 0.9660, batch_loss: 0.0373, loss: 0.0922 ||:  35%|###5      | 23/65 [04:11<07:40, 10.97s/it]
2022-04-11 17:04:13,269 - INFO - tqdm - accuracy: 0.9674, batch_loss: 0.0268, loss: 0.0895 ||:  37%|###6      | 24/65 [04:22<07:29, 10.97s/it]
2022-04-11 17:04:24,231 - INFO - tqdm - accuracy: 0.9650, batch_loss: 0.1538, loss: 0.0921 ||:  38%|###8      | 25/65 [04:33<07:18, 10.97s/it]
2022-04-11 17:04:35,193 - INFO - tqdm - accuracy: 0.9651, batch_loss: 0.0919, loss: 0.0921 ||:  40%|####      | 26/65 [04:44<07:07, 10.96s/it]
2022-04-11 17:04:45,928 - INFO - tqdm - accuracy: 0.9641, batch_loss: 0.1894, loss: 0.0957 ||:  42%|####1     | 27/65 [04:54<06:54, 10.90s/it]
2022-04-11 17:04:56,893 - INFO - tqdm - accuracy: 0.9643, batch_loss: 0.1100, loss: 0.0962 ||:  43%|####3     | 28/65 [05:05<06:43, 10.92s/it]
2022-04-11 17:05:07,837 - INFO - tqdm - accuracy: 0.9655, batch_loss: 0.0305, loss: 0.0939 ||:  45%|####4     | 29/65 [05:16<06:33, 10.92s/it]
2022-04-11 17:05:18,817 - INFO - tqdm - accuracy: 0.9667, batch_loss: 0.0288, loss: 0.0917 ||:  46%|####6     | 30/65 [05:27<06:22, 10.94s/it]
2022-04-11 17:05:29,790 - INFO - tqdm - accuracy: 0.9667, batch_loss: 0.0529, loss: 0.0905 ||:  48%|####7     | 31/65 [05:38<06:12, 10.95s/it]
2022-04-11 17:05:40,758 - INFO - tqdm - accuracy: 0.9668, batch_loss: 0.0662, loss: 0.0897 ||:  49%|####9     | 32/65 [05:49<06:01, 10.96s/it]
2022-04-11 17:05:51,734 - INFO - tqdm - accuracy: 0.9669, batch_loss: 0.1023, loss: 0.0901 ||:  51%|#####     | 33/65 [06:00<05:50, 10.96s/it]
2022-04-11 17:06:02,683 - INFO - tqdm - accuracy: 0.9651, batch_loss: 0.2937, loss: 0.0961 ||:  52%|#####2    | 34/65 [06:11<05:39, 10.96s/it]
2022-04-11 17:06:13,656 - INFO - tqdm - accuracy: 0.9661, batch_loss: 0.0217, loss: 0.0940 ||:  54%|#####3    | 35/65 [06:22<05:28, 10.96s/it]
2022-04-11 17:06:24,604 - INFO - tqdm - accuracy: 0.9661, batch_loss: 0.0680, loss: 0.0933 ||:  55%|#####5    | 36/65 [06:33<05:17, 10.96s/it]
2022-04-11 17:06:35,555 - INFO - tqdm - accuracy: 0.9662, batch_loss: 0.0881, loss: 0.0931 ||:  57%|#####6    | 37/65 [06:44<05:06, 10.96s/it]
2022-04-11 17:06:46,514 - INFO - tqdm - accuracy: 0.9671, batch_loss: 0.0471, loss: 0.0919 ||:  58%|#####8    | 38/65 [06:55<04:55, 10.96s/it]
2022-04-11 17:06:57,234 - INFO - tqdm - accuracy: 0.9655, batch_loss: 0.2461, loss: 0.0959 ||:  60%|######    | 39/65 [07:06<04:43, 10.89s/it]
2022-04-11 17:07:08,199 - INFO - tqdm - accuracy: 0.9656, batch_loss: 0.1008, loss: 0.0960 ||:  62%|######1   | 40/65 [07:17<04:32, 10.91s/it]
2022-04-11 17:07:19,168 - INFO - tqdm - accuracy: 0.9649, batch_loss: 0.1241, loss: 0.0967 ||:  63%|######3   | 41/65 [07:28<04:22, 10.93s/it]
2022-04-11 17:07:30,124 - INFO - tqdm - accuracy: 0.9643, batch_loss: 0.1719, loss: 0.0985 ||:  65%|######4   | 42/65 [07:39<04:11, 10.94s/it]
2022-04-11 17:07:41,082 - INFO - tqdm - accuracy: 0.9629, batch_loss: 0.2106, loss: 0.1011 ||:  66%|######6   | 43/65 [07:50<04:00, 10.94s/it]
2022-04-11 17:07:52,057 - INFO - tqdm - accuracy: 0.9624, batch_loss: 0.2214, loss: 0.1038 ||:  68%|######7   | 44/65 [08:01<03:49, 10.95s/it]
2022-04-11 17:08:03,013 - INFO - tqdm - accuracy: 0.9632, batch_loss: 0.0556, loss: 0.1027 ||:  69%|######9   | 45/65 [08:11<03:39, 10.95s/it]
2022-04-11 17:08:13,977 - INFO - tqdm - accuracy: 0.9626, batch_loss: 0.0904, loss: 0.1025 ||:  71%|#######   | 46/65 [08:22<03:28, 10.96s/it]
2022-04-11 17:08:24,950 - INFO - tqdm - accuracy: 0.9634, batch_loss: 0.0236, loss: 0.1008 ||:  72%|#######2  | 47/65 [08:33<03:17, 10.96s/it]
2022-04-11 17:08:35,881 - INFO - tqdm - accuracy: 0.9642, batch_loss: 0.0348, loss: 0.0994 ||:  74%|#######3  | 48/65 [08:44<03:06, 10.95s/it]
2022-04-11 17:08:46,830 - INFO - tqdm - accuracy: 0.9649, batch_loss: 0.0361, loss: 0.0981 ||:  75%|#######5  | 49/65 [08:55<02:55, 10.95s/it]
2022-04-11 17:08:57,801 - INFO - tqdm - accuracy: 0.9650, batch_loss: 0.0765, loss: 0.0977 ||:  77%|#######6  | 50/65 [09:06<02:44, 10.96s/it]
2022-04-11 17:09:08,746 - INFO - tqdm - accuracy: 0.9651, batch_loss: 0.0983, loss: 0.0977 ||:  78%|#######8  | 51/65 [09:17<02:33, 10.95s/it]
2022-04-11 17:09:19,712 - INFO - tqdm - accuracy: 0.9657, batch_loss: 0.0395, loss: 0.0966 ||:  80%|########  | 52/65 [09:28<02:22, 10.96s/it]
2022-04-11 17:09:30,693 - INFO - tqdm - accuracy: 0.9658, batch_loss: 0.0770, loss: 0.0962 ||:  82%|########1 | 53/65 [09:39<02:11, 10.96s/it]
2022-04-11 17:09:41,644 - INFO - tqdm - accuracy: 0.9659, batch_loss: 0.0727, loss: 0.0958 ||:  83%|########3 | 54/65 [09:50<02:00, 10.96s/it]
2022-04-11 17:09:52,598 - INFO - tqdm - accuracy: 0.9665, batch_loss: 0.0103, loss: 0.0942 ||:  85%|########4 | 55/65 [10:01<01:49, 10.96s/it]
2022-04-11 17:10:03,557 - INFO - tqdm - accuracy: 0.9643, batch_loss: 0.3006, loss: 0.0979 ||:  86%|########6 | 56/65 [10:12<01:38, 10.96s/it]
2022-04-11 17:10:14,526 - INFO - tqdm - accuracy: 0.9644, batch_loss: 0.0896, loss: 0.0978 ||:  88%|########7 | 57/65 [10:23<01:27, 10.96s/it]
2022-04-11 17:10:25,498 - INFO - tqdm - accuracy: 0.9644, batch_loss: 0.0658, loss: 0.0972 ||:  89%|########9 | 58/65 [10:34<01:16, 10.96s/it]
2022-04-11 17:10:36,445 - INFO - tqdm - accuracy: 0.9645, batch_loss: 0.1014, loss: 0.0973 ||:  91%|######### | 59/65 [10:45<01:05, 10.96s/it]
2022-04-11 17:10:47,075 - INFO - tqdm - accuracy: 0.9651, batch_loss: 0.0329, loss: 0.0962 ||:  92%|#########2| 60/65 [10:56<00:54, 10.86s/it]
2022-04-11 17:10:58,010 - INFO - tqdm - accuracy: 0.9646, batch_loss: 0.1204, loss: 0.0966 ||:  94%|#########3| 61/65 [11:06<00:43, 10.88s/it]
2022-04-11 17:11:08,958 - INFO - tqdm - accuracy: 0.9647, batch_loss: 0.1844, loss: 0.0980 ||:  95%|#########5| 62/65 [11:17<00:32, 10.90s/it]
2022-04-11 17:11:19,879 - INFO - tqdm - accuracy: 0.9638, batch_loss: 0.1338, loss: 0.0986 ||:  97%|#########6| 63/65 [11:28<00:21, 10.91s/it]
2022-04-11 17:11:30,823 - INFO - tqdm - accuracy: 0.9638, batch_loss: 0.0995, loss: 0.0986 ||:  98%|#########8| 64/65 [11:39<00:10, 10.92s/it]
2022-04-11 17:11:35,553 - INFO - tqdm - accuracy: 0.9641, batch_loss: 0.0090, loss: 0.0972 ||: 100%|##########| 65/65 [11:44<00:00,  9.06s/it]
2022-04-11 17:11:35,553 - INFO - tqdm - accuracy: 0.9641, batch_loss: 0.0090, loss: 0.0972 ||: 100%|##########| 65/65 [11:44<00:00, 10.84s/it]
2022-04-11 17:11:38,409 - INFO - allennlp.training.trainer - Validating
2022-04-11 17:11:38,410 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-11 17:11:48,499 - INFO - tqdm - accuracy: 0.6190, batch_loss: 1.1598, loss: 1.8842 ||:  37%|###6      | 42/115 [00:10<00:17,  4.17it/s]
2022-04-11 17:11:58,593 - INFO - tqdm - accuracy: 0.6310, batch_loss: 0.0250, loss: 1.7408 ||:  73%|#######3  | 84/115 [00:20<00:07,  4.15it/s]
2022-04-11 17:12:05,866 - INFO - tqdm - accuracy: 0.6419, batch_loss: 0.5026, loss: 1.6218 ||: 100%|##########| 115/115 [00:27<00:00,  4.25it/s]
2022-04-11 17:12:05,866 - INFO - tqdm - accuracy: 0.6419, batch_loss: 0.5026, loss: 1.6218 ||: 100%|##########| 115/115 [00:27<00:00,  4.19it/s]
2022-04-11 17:12:05,867 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-11 17:12:05,867 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.964  |     0.642
2022-04-11 17:12:05,868 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-11 17:12:05,869 - INFO - allennlp.training.tensorboard_writer - loss               |     0.097  |     1.622
2022-04-11 17:12:05,869 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5804.656  |       N/A
2022-04-11 17:12:12,114 - INFO - allennlp.training.trainer - Epoch duration: 0:12:21.095737
2022-04-11 17:12:12,115 - INFO - allennlp.training.trainer - Estimated training time remaining: 1:14:33
2022-04-11 17:12:12,115 - INFO - allennlp.training.trainer - Epoch 9/14
2022-04-11 17:12:12,115 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-11 17:12:12,115 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-11 17:12:12,116 - INFO - allennlp.training.trainer - Training
2022-04-11 17:12:12,117 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-11 17:12:22,955 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.0495, loss: 0.0495 ||:   2%|1         | 1/65 [00:10<11:33, 10.84s/it]
2022-04-11 17:12:33,790 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0265, loss: 0.0380 ||:   3%|3         | 2/65 [00:21<11:22, 10.84s/it]
2022-04-11 17:12:44,645 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.1429, loss: 0.0730 ||:   5%|4         | 3/65 [00:32<11:12, 10.84s/it]
2022-04-11 17:12:55,425 - INFO - tqdm - accuracy: 0.9766, batch_loss: 0.0316, loss: 0.0626 ||:   6%|6         | 4/65 [00:43<10:59, 10.82s/it]
2022-04-11 17:13:06,320 - INFO - tqdm - accuracy: 0.9750, batch_loss: 0.0906, loss: 0.0682 ||:   8%|7         | 5/65 [00:54<10:50, 10.85s/it]
2022-04-11 17:13:17,228 - INFO - tqdm - accuracy: 0.9740, batch_loss: 0.0737, loss: 0.0691 ||:   9%|9         | 6/65 [01:05<10:41, 10.87s/it]
2022-04-11 17:13:28,125 - INFO - tqdm - accuracy: 0.9777, batch_loss: 0.0230, loss: 0.0626 ||:  11%|#         | 7/65 [01:16<10:30, 10.88s/it]
2022-04-11 17:13:39,024 - INFO - tqdm - accuracy: 0.9727, batch_loss: 0.1687, loss: 0.0758 ||:  12%|#2        | 8/65 [01:26<10:20, 10.88s/it]
2022-04-11 17:13:49,944 - INFO - tqdm - accuracy: 0.9757, batch_loss: 0.0067, loss: 0.0681 ||:  14%|#3        | 9/65 [01:37<10:10, 10.90s/it]
2022-04-11 17:14:00,864 - INFO - tqdm - accuracy: 0.9781, batch_loss: 0.0172, loss: 0.0631 ||:  15%|#5        | 10/65 [01:48<09:59, 10.90s/it]
2022-04-11 17:14:11,791 - INFO - tqdm - accuracy: 0.9773, batch_loss: 0.0842, loss: 0.0650 ||:  17%|#6        | 11/65 [01:59<09:49, 10.91s/it]
2022-04-11 17:14:22,721 - INFO - tqdm - accuracy: 0.9766, batch_loss: 0.0947, loss: 0.0675 ||:  18%|#8        | 12/65 [02:10<09:38, 10.92s/it]
2022-04-11 17:14:33,666 - INFO - tqdm - accuracy: 0.9784, batch_loss: 0.0243, loss: 0.0641 ||:  20%|##        | 13/65 [02:21<09:28, 10.93s/it]
2022-04-11 17:14:44,601 - INFO - tqdm - accuracy: 0.9799, batch_loss: 0.0643, loss: 0.0641 ||:  22%|##1       | 14/65 [02:32<09:17, 10.93s/it]
2022-04-11 17:14:55,953 - INFO - tqdm - accuracy: 0.9792, batch_loss: 0.0869, loss: 0.0657 ||:  23%|##3       | 15/65 [02:43<09:12, 11.06s/it]
2022-04-11 17:15:06,885 - INFO - tqdm - accuracy: 0.9805, batch_loss: 0.0318, loss: 0.0635 ||:  25%|##4       | 16/65 [02:54<08:59, 11.02s/it]
2022-04-11 17:15:17,824 - INFO - tqdm - accuracy: 0.9816, batch_loss: 0.0065, loss: 0.0602 ||:  26%|##6       | 17/65 [03:05<08:47, 10.99s/it]
2022-04-11 17:15:28,782 - INFO - tqdm - accuracy: 0.9826, batch_loss: 0.0294, loss: 0.0585 ||:  28%|##7       | 18/65 [03:16<08:36, 10.98s/it]
2022-04-11 17:15:39,728 - INFO - tqdm - accuracy: 0.9836, batch_loss: 0.0273, loss: 0.0568 ||:  29%|##9       | 19/65 [03:27<08:24, 10.97s/it]
2022-04-11 17:15:50,676 - INFO - tqdm - accuracy: 0.9781, batch_loss: 0.3072, loss: 0.0694 ||:  31%|###       | 20/65 [03:38<08:13, 10.97s/it]
2022-04-11 17:16:01,649 - INFO - tqdm - accuracy: 0.9792, batch_loss: 0.0147, loss: 0.0668 ||:  32%|###2      | 21/65 [03:49<08:02, 10.97s/it]
2022-04-11 17:16:12,605 - INFO - tqdm - accuracy: 0.9787, batch_loss: 0.0433, loss: 0.0657 ||:  34%|###3      | 22/65 [04:00<07:51, 10.96s/it]
2022-04-11 17:16:23,568 - INFO - tqdm - accuracy: 0.9783, batch_loss: 0.0783, loss: 0.0662 ||:  35%|###5      | 23/65 [04:11<07:40, 10.96s/it]
2022-04-11 17:16:34,542 - INFO - tqdm - accuracy: 0.9753, batch_loss: 0.1191, loss: 0.0684 ||:  37%|###6      | 24/65 [04:22<07:29, 10.97s/it]
2022-04-11 17:16:45,492 - INFO - tqdm - accuracy: 0.9762, batch_loss: 0.0245, loss: 0.0667 ||:  38%|###8      | 25/65 [04:33<07:18, 10.96s/it]
2022-04-11 17:16:56,458 - INFO - tqdm - accuracy: 0.9772, batch_loss: 0.0135, loss: 0.0646 ||:  40%|####      | 26/65 [04:44<07:07, 10.96s/it]
2022-04-11 17:17:07,409 - INFO - tqdm - accuracy: 0.9769, batch_loss: 0.0594, loss: 0.0644 ||:  42%|####1     | 27/65 [04:55<06:56, 10.96s/it]
2022-04-11 17:17:18,348 - INFO - tqdm - accuracy: 0.9766, batch_loss: 0.0842, loss: 0.0651 ||:  43%|####3     | 28/65 [05:06<06:45, 10.95s/it]
2022-04-11 17:17:29,312 - INFO - tqdm - accuracy: 0.9774, batch_loss: 0.0593, loss: 0.0649 ||:  45%|####4     | 29/65 [05:17<06:34, 10.96s/it]
2022-04-11 17:17:40,143 - INFO - tqdm - accuracy: 0.9781, batch_loss: 0.0118, loss: 0.0632 ||:  46%|####6     | 30/65 [05:28<06:22, 10.92s/it]
2022-04-11 17:17:51,114 - INFO - tqdm - accuracy: 0.9778, batch_loss: 0.0432, loss: 0.0625 ||:  48%|####7     | 31/65 [05:38<06:11, 10.93s/it]
2022-04-11 17:18:02,103 - INFO - tqdm - accuracy: 0.9785, batch_loss: 0.0213, loss: 0.0612 ||:  49%|####9     | 32/65 [05:49<06:01, 10.95s/it]
2022-04-11 17:18:13,075 - INFO - tqdm - accuracy: 0.9773, batch_loss: 0.1046, loss: 0.0626 ||:  51%|#####     | 33/65 [06:00<05:50, 10.96s/it]
2022-04-11 17:18:24,033 - INFO - tqdm - accuracy: 0.9770, batch_loss: 0.1165, loss: 0.0641 ||:  52%|#####2    | 34/65 [06:11<05:39, 10.96s/it]
2022-04-11 17:18:34,664 - INFO - tqdm - accuracy: 0.9777, batch_loss: 0.0164, loss: 0.0628 ||:  54%|#####3    | 35/65 [06:22<05:25, 10.86s/it]
2022-04-11 17:18:45,634 - INFO - tqdm - accuracy: 0.9774, batch_loss: 0.0564, loss: 0.0626 ||:  55%|#####5    | 36/65 [06:33<05:15, 10.89s/it]
2022-04-11 17:18:56,597 - INFO - tqdm - accuracy: 0.9772, batch_loss: 0.0842, loss: 0.0632 ||:  57%|#####6    | 37/65 [06:44<05:05, 10.91s/it]
2022-04-11 17:19:07,564 - INFO - tqdm - accuracy: 0.9778, batch_loss: 0.0274, loss: 0.0622 ||:  58%|#####8    | 38/65 [06:55<04:55, 10.93s/it]
2022-04-11 17:19:18,537 - INFO - tqdm - accuracy: 0.9783, batch_loss: 0.0266, loss: 0.0613 ||:  60%|######    | 39/65 [07:06<04:44, 10.94s/it]
2022-04-11 17:19:29,519 - INFO - tqdm - accuracy: 0.9781, batch_loss: 0.0708, loss: 0.0616 ||:  62%|######1   | 40/65 [07:17<04:33, 10.95s/it]
2022-04-11 17:19:40,496 - INFO - tqdm - accuracy: 0.9786, batch_loss: 0.0260, loss: 0.0607 ||:  63%|######3   | 41/65 [07:28<04:23, 10.96s/it]
2022-04-11 17:19:51,467 - INFO - tqdm - accuracy: 0.9784, batch_loss: 0.0549, loss: 0.0606 ||:  65%|######4   | 42/65 [07:39<04:12, 10.96s/it]
2022-04-11 17:20:02,432 - INFO - tqdm - accuracy: 0.9789, batch_loss: 0.0279, loss: 0.0598 ||:  66%|######6   | 43/65 [07:50<04:01, 10.96s/it]
2022-04-11 17:20:13,406 - INFO - tqdm - accuracy: 0.9787, batch_loss: 0.1565, loss: 0.0620 ||:  68%|######7   | 44/65 [08:01<03:50, 10.97s/it]
2022-04-11 17:20:24,366 - INFO - tqdm - accuracy: 0.9785, batch_loss: 0.1486, loss: 0.0639 ||:  69%|######9   | 45/65 [08:12<03:39, 10.97s/it]
2022-04-11 17:20:35,334 - INFO - tqdm - accuracy: 0.9776, batch_loss: 0.1793, loss: 0.0664 ||:  71%|#######   | 46/65 [08:23<03:28, 10.97s/it]
2022-04-11 17:20:46,307 - INFO - tqdm - accuracy: 0.9774, batch_loss: 0.1567, loss: 0.0683 ||:  72%|#######2  | 47/65 [08:34<03:17, 10.97s/it]
2022-04-11 17:20:57,273 - INFO - tqdm - accuracy: 0.9772, batch_loss: 0.0742, loss: 0.0685 ||:  74%|#######3  | 48/65 [08:45<03:06, 10.97s/it]
2022-04-11 17:21:08,250 - INFO - tqdm - accuracy: 0.9764, batch_loss: 0.0855, loss: 0.0688 ||:  75%|#######5  | 49/65 [08:56<02:55, 10.97s/it]
2022-04-11 17:21:18,989 - INFO - tqdm - accuracy: 0.9762, batch_loss: 0.0580, loss: 0.0686 ||:  77%|#######6  | 50/65 [09:06<02:43, 10.90s/it]
2022-04-11 17:21:29,966 - INFO - tqdm - accuracy: 0.9761, batch_loss: 0.0474, loss: 0.0682 ||:  78%|#######8  | 51/65 [09:17<02:32, 10.92s/it]
2022-04-11 17:21:40,933 - INFO - tqdm - accuracy: 0.9765, batch_loss: 0.0240, loss: 0.0673 ||:  80%|########  | 52/65 [09:28<02:22, 10.94s/it]
2022-04-11 17:21:51,918 - INFO - tqdm - accuracy: 0.9758, batch_loss: 0.1164, loss: 0.0683 ||:  82%|########1 | 53/65 [09:39<02:11, 10.95s/it]
2022-04-11 17:22:02,884 - INFO - tqdm - accuracy: 0.9757, batch_loss: 0.0755, loss: 0.0684 ||:  83%|########3 | 54/65 [09:50<02:00, 10.96s/it]
2022-04-11 17:22:13,860 - INFO - tqdm - accuracy: 0.9761, batch_loss: 0.0163, loss: 0.0674 ||:  85%|########4 | 55/65 [10:01<01:49, 10.96s/it]
2022-04-11 17:22:24,835 - INFO - tqdm - accuracy: 0.9760, batch_loss: 0.0850, loss: 0.0678 ||:  86%|########6 | 56/65 [10:12<01:38, 10.97s/it]
2022-04-11 17:22:35,807 - INFO - tqdm - accuracy: 0.9759, batch_loss: 0.0511, loss: 0.0675 ||:  88%|########7 | 57/65 [10:23<01:27, 10.97s/it]
2022-04-11 17:22:46,781 - INFO - tqdm - accuracy: 0.9757, batch_loss: 0.0791, loss: 0.0677 ||:  89%|########9 | 58/65 [10:34<01:16, 10.97s/it]
2022-04-11 17:22:57,740 - INFO - tqdm - accuracy: 0.9756, batch_loss: 0.0690, loss: 0.0677 ||:  91%|######### | 59/65 [10:45<01:05, 10.97s/it]
2022-04-11 17:23:08,721 - INFO - tqdm - accuracy: 0.9760, batch_loss: 0.0448, loss: 0.0673 ||:  92%|#########2| 60/65 [10:56<00:54, 10.97s/it]
2022-04-11 17:23:19,687 - INFO - tqdm - accuracy: 0.9764, batch_loss: 0.0202, loss: 0.0665 ||:  94%|#########3| 61/65 [11:07<00:43, 10.97s/it]
2022-04-11 17:23:30,659 - INFO - tqdm - accuracy: 0.9768, batch_loss: 0.0368, loss: 0.0661 ||:  95%|#########5| 62/65 [11:18<00:32, 10.97s/it]
2022-04-11 17:23:41,613 - INFO - tqdm - accuracy: 0.9772, batch_loss: 0.0234, loss: 0.0654 ||:  97%|#########6| 63/65 [11:29<00:21, 10.97s/it]
2022-04-11 17:23:52,591 - INFO - tqdm - accuracy: 0.9770, batch_loss: 0.0775, loss: 0.0656 ||:  98%|#########8| 64/65 [11:40<00:10, 10.97s/it]
2022-04-11 17:23:57,463 - INFO - tqdm - accuracy: 0.9767, batch_loss: 0.1728, loss: 0.0672 ||: 100%|##########| 65/65 [11:45<00:00,  9.14s/it]
2022-04-11 17:23:57,464 - INFO - tqdm - accuracy: 0.9767, batch_loss: 0.1728, loss: 0.0672 ||: 100%|##########| 65/65 [11:45<00:00, 10.85s/it]
2022-04-11 17:24:00,328 - INFO - allennlp.training.trainer - Validating
2022-04-11 17:24:00,330 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-11 17:24:10,485 - INFO - tqdm - accuracy: 0.6353, batch_loss: 4.2421, loss: 1.8526 ||:  37%|###7      | 43/115 [00:10<00:17,  4.16it/s]
2022-04-11 17:24:20,613 - INFO - tqdm - accuracy: 0.6568, batch_loss: 0.4711, loss: 1.7652 ||:  74%|#######3  | 85/115 [00:20<00:07,  4.17it/s]
2022-04-11 17:24:27,862 - INFO - tqdm - accuracy: 0.6419, batch_loss: 0.8604, loss: 1.7064 ||: 100%|##########| 115/115 [00:27<00:00,  4.13it/s]
2022-04-11 17:24:27,862 - INFO - tqdm - accuracy: 0.6419, batch_loss: 0.8604, loss: 1.7064 ||: 100%|##########| 115/115 [00:27<00:00,  4.18it/s]
2022-04-11 17:24:27,862 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-11 17:24:27,863 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.977  |     0.642
2022-04-11 17:24:27,864 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-11 17:24:27,864 - INFO - allennlp.training.tensorboard_writer - loss               |     0.067  |     1.706
2022-04-11 17:24:27,864 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5804.656  |       N/A
2022-04-11 17:24:33,952 - INFO - allennlp.training.trainer - Epoch duration: 0:12:21.837541
2022-04-11 17:24:33,953 - INFO - allennlp.training.trainer - Estimated training time remaining: 1:02:05
2022-04-11 17:24:33,953 - INFO - allennlp.training.trainer - Epoch 10/14
2022-04-11 17:24:33,953 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-11 17:24:33,953 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-11 17:24:33,955 - INFO - allennlp.training.trainer - Training
2022-04-11 17:24:33,955 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-11 17:24:44,751 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.0619, loss: 0.0619 ||:   2%|1         | 1/65 [00:10<11:30, 10.80s/it]
2022-04-11 17:24:55,353 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.0654, loss: 0.0637 ||:   3%|3         | 2/65 [00:21<11:12, 10.68s/it]
2022-04-11 17:25:06,216 - INFO - tqdm - accuracy: 0.9792, batch_loss: 0.0342, loss: 0.0538 ||:   5%|4         | 3/65 [00:32<11:07, 10.76s/it]
2022-04-11 17:25:17,105 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0332, loss: 0.0487 ||:   6%|6         | 4/65 [00:43<10:59, 10.81s/it]
2022-04-11 17:25:27,869 - INFO - tqdm - accuracy: 0.9875, batch_loss: 0.0212, loss: 0.0432 ||:   8%|7         | 5/65 [00:53<10:47, 10.80s/it]
2022-04-11 17:25:38,766 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0390, loss: 0.0425 ||:   9%|9         | 6/65 [01:04<10:38, 10.83s/it]
2022-04-11 17:25:49,664 - INFO - tqdm - accuracy: 0.9866, batch_loss: 0.0427, loss: 0.0425 ||:  11%|#         | 7/65 [01:15<10:29, 10.85s/it]
2022-04-11 17:26:00,561 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0469, loss: 0.0431 ||:  12%|#2        | 8/65 [01:26<10:19, 10.87s/it]
2022-04-11 17:26:11,486 - INFO - tqdm - accuracy: 0.9861, batch_loss: 0.0148, loss: 0.0399 ||:  14%|#3        | 9/65 [01:37<10:09, 10.88s/it]
2022-04-11 17:26:22,411 - INFO - tqdm - accuracy: 0.9875, batch_loss: 0.0182, loss: 0.0378 ||:  15%|#5        | 10/65 [01:48<09:59, 10.90s/it]
2022-04-11 17:26:33,318 - INFO - tqdm - accuracy: 0.9858, batch_loss: 0.1459, loss: 0.0476 ||:  17%|#6        | 11/65 [01:59<09:48, 10.90s/it]
2022-04-11 17:26:44,241 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.1492, loss: 0.0561 ||:  18%|#8        | 12/65 [02:10<09:38, 10.91s/it]
2022-04-11 17:26:55,166 - INFO - tqdm - accuracy: 0.9856, batch_loss: 0.0335, loss: 0.0543 ||:  20%|##        | 13/65 [02:21<09:27, 10.91s/it]
2022-04-11 17:27:06,113 - INFO - tqdm - accuracy: 0.9821, batch_loss: 0.1642, loss: 0.0622 ||:  22%|##1       | 14/65 [02:32<09:17, 10.92s/it]
2022-04-11 17:27:17,050 - INFO - tqdm - accuracy: 0.9833, batch_loss: 0.0172, loss: 0.0592 ||:  23%|##3       | 15/65 [02:43<09:06, 10.93s/it]
2022-04-11 17:27:27,986 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0233, loss: 0.0569 ||:  25%|##4       | 16/65 [02:54<08:55, 10.93s/it]
2022-04-11 17:27:38,930 - INFO - tqdm - accuracy: 0.9853, batch_loss: 0.0037, loss: 0.0538 ||:  26%|##6       | 17/65 [03:04<08:44, 10.93s/it]
2022-04-11 17:27:49,870 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0868, loss: 0.0556 ||:  28%|##7       | 18/65 [03:15<08:33, 10.94s/it]
2022-04-11 17:28:00,802 - INFO - tqdm - accuracy: 0.9852, batch_loss: 0.0206, loss: 0.0538 ||:  29%|##9       | 19/65 [03:26<08:22, 10.93s/it]
2022-04-11 17:28:11,759 - INFO - tqdm - accuracy: 0.9859, batch_loss: 0.0711, loss: 0.0547 ||:  31%|###       | 20/65 [03:37<08:12, 10.94s/it]
2022-04-11 17:28:22,710 - INFO - tqdm - accuracy: 0.9866, batch_loss: 0.0475, loss: 0.0543 ||:  32%|###2      | 21/65 [03:48<08:01, 10.94s/it]
2022-04-11 17:28:33,652 - INFO - tqdm - accuracy: 0.9858, batch_loss: 0.0929, loss: 0.0561 ||:  34%|###3      | 22/65 [03:59<07:50, 10.94s/it]
2022-04-11 17:28:44,274 - INFO - tqdm - accuracy: 0.9864, batch_loss: 0.0271, loss: 0.0548 ||:  35%|###5      | 23/65 [04:10<07:35, 10.85s/it]
2022-04-11 17:28:55,174 - INFO - tqdm - accuracy: 0.9870, batch_loss: 0.0504, loss: 0.0546 ||:  37%|###6      | 24/65 [04:21<07:25, 10.86s/it]
2022-04-11 17:29:06,144 - INFO - tqdm - accuracy: 0.9862, batch_loss: 0.0657, loss: 0.0551 ||:  38%|###8      | 25/65 [04:32<07:15, 10.90s/it]
2022-04-11 17:29:16,951 - INFO - tqdm - accuracy: 0.9856, batch_loss: 0.0344, loss: 0.0543 ||:  40%|####      | 26/65 [04:42<07:03, 10.87s/it]
2022-04-11 17:29:27,892 - INFO - tqdm - accuracy: 0.9861, batch_loss: 0.0641, loss: 0.0546 ||:  42%|####1     | 27/65 [04:53<06:53, 10.89s/it]
2022-04-11 17:29:38,836 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0907, loss: 0.0559 ||:  43%|####3     | 28/65 [05:04<06:43, 10.91s/it]
2022-04-11 17:29:49,789 - INFO - tqdm - accuracy: 0.9827, batch_loss: 0.1187, loss: 0.0581 ||:  45%|####4     | 29/65 [05:15<06:33, 10.92s/it]
2022-04-11 17:30:00,724 - INFO - tqdm - accuracy: 0.9833, batch_loss: 0.0133, loss: 0.0566 ||:  46%|####6     | 30/65 [05:26<06:22, 10.92s/it]
2022-04-11 17:30:11,662 - INFO - tqdm - accuracy: 0.9828, batch_loss: 0.0549, loss: 0.0565 ||:  48%|####7     | 31/65 [05:37<06:11, 10.93s/it]
2022-04-11 17:30:22,630 - INFO - tqdm - accuracy: 0.9834, batch_loss: 0.0171, loss: 0.0553 ||:  49%|####9     | 32/65 [05:48<06:01, 10.94s/it]
2022-04-11 17:30:33,603 - INFO - tqdm - accuracy: 0.9820, batch_loss: 0.1105, loss: 0.0570 ||:  51%|#####     | 33/65 [05:59<05:50, 10.95s/it]
2022-04-11 17:30:44,567 - INFO - tqdm - accuracy: 0.9825, batch_loss: 0.0232, loss: 0.0560 ||:  52%|#####2    | 34/65 [06:10<05:39, 10.95s/it]
2022-04-11 17:30:55,529 - INFO - tqdm - accuracy: 0.9821, batch_loss: 0.0641, loss: 0.0562 ||:  54%|#####3    | 35/65 [06:21<05:28, 10.96s/it]
2022-04-11 17:31:06,496 - INFO - tqdm - accuracy: 0.9826, batch_loss: 0.0103, loss: 0.0549 ||:  55%|#####5    | 36/65 [06:32<05:17, 10.96s/it]
2022-04-11 17:31:17,440 - INFO - tqdm - accuracy: 0.9831, batch_loss: 0.0079, loss: 0.0537 ||:  57%|#####6    | 37/65 [06:43<05:06, 10.95s/it]
2022-04-11 17:31:28,411 - INFO - tqdm - accuracy: 0.9835, batch_loss: 0.0410, loss: 0.0533 ||:  58%|#####8    | 38/65 [06:54<04:55, 10.96s/it]
2022-04-11 17:31:39,359 - INFO - tqdm - accuracy: 0.9832, batch_loss: 0.0478, loss: 0.0532 ||:  60%|######    | 39/65 [07:05<04:44, 10.96s/it]
2022-04-11 17:31:50,308 - INFO - tqdm - accuracy: 0.9836, batch_loss: 0.0232, loss: 0.0524 ||:  62%|######1   | 40/65 [07:16<04:33, 10.95s/it]
2022-04-11 17:32:01,289 - INFO - tqdm - accuracy: 0.9840, batch_loss: 0.0075, loss: 0.0513 ||:  63%|######3   | 41/65 [07:27<04:23, 10.96s/it]
2022-04-11 17:32:12,273 - INFO - tqdm - accuracy: 0.9836, batch_loss: 0.0905, loss: 0.0523 ||:  65%|######4   | 42/65 [07:38<04:12, 10.97s/it]
2022-04-11 17:32:23,228 - INFO - tqdm - accuracy: 0.9833, batch_loss: 0.0893, loss: 0.0531 ||:  66%|######6   | 43/65 [07:49<04:01, 10.96s/it]
2022-04-11 17:32:34,178 - INFO - tqdm - accuracy: 0.9837, batch_loss: 0.0085, loss: 0.0521 ||:  68%|######7   | 44/65 [08:00<03:50, 10.96s/it]
2022-04-11 17:32:45,136 - INFO - tqdm - accuracy: 0.9826, batch_loss: 0.1554, loss: 0.0544 ||:  69%|######9   | 45/65 [08:11<03:39, 10.96s/it]
2022-04-11 17:32:56,082 - INFO - tqdm - accuracy: 0.9823, batch_loss: 0.0464, loss: 0.0542 ||:  71%|#######   | 46/65 [08:22<03:28, 10.96s/it]
2022-04-11 17:33:07,025 - INFO - tqdm - accuracy: 0.9820, batch_loss: 0.0912, loss: 0.0550 ||:  72%|#######2  | 47/65 [08:33<03:17, 10.95s/it]
2022-04-11 17:33:17,967 - INFO - tqdm - accuracy: 0.9818, batch_loss: 0.0839, loss: 0.0556 ||:  74%|#######3  | 48/65 [08:44<03:06, 10.95s/it]
2022-04-11 17:33:28,936 - INFO - tqdm - accuracy: 0.9821, batch_loss: 0.0133, loss: 0.0548 ||:  75%|#######5  | 49/65 [08:54<02:55, 10.95s/it]
2022-04-11 17:33:40,383 - INFO - tqdm - accuracy: 0.9825, batch_loss: 0.0179, loss: 0.0540 ||:  77%|#######6  | 50/65 [09:06<02:46, 11.10s/it]
2022-04-11 17:33:51,318 - INFO - tqdm - accuracy: 0.9828, batch_loss: 0.0093, loss: 0.0532 ||:  78%|#######8  | 51/65 [09:17<02:34, 11.05s/it]
2022-04-11 17:34:02,281 - INFO - tqdm - accuracy: 0.9832, batch_loss: 0.0138, loss: 0.0524 ||:  80%|########  | 52/65 [09:28<02:23, 11.03s/it]
2022-04-11 17:34:13,221 - INFO - tqdm - accuracy: 0.9835, batch_loss: 0.0155, loss: 0.0517 ||:  82%|########1 | 53/65 [09:39<02:11, 11.00s/it]
2022-04-11 17:34:24,171 - INFO - tqdm - accuracy: 0.9838, batch_loss: 0.0306, loss: 0.0513 ||:  83%|########3 | 54/65 [09:50<02:00, 10.99s/it]
2022-04-11 17:34:35,127 - INFO - tqdm - accuracy: 0.9829, batch_loss: 0.0973, loss: 0.0521 ||:  85%|########4 | 55/65 [10:01<01:49, 10.98s/it]
2022-04-11 17:34:46,076 - INFO - tqdm - accuracy: 0.9821, batch_loss: 0.1992, loss: 0.0548 ||:  86%|########6 | 56/65 [10:12<01:38, 10.97s/it]
2022-04-11 17:34:57,044 - INFO - tqdm - accuracy: 0.9824, batch_loss: 0.0336, loss: 0.0544 ||:  88%|########7 | 57/65 [10:23<01:27, 10.97s/it]
2022-04-11 17:35:08,007 - INFO - tqdm - accuracy: 0.9827, batch_loss: 0.0194, loss: 0.0538 ||:  89%|########9 | 58/65 [10:34<01:16, 10.97s/it]
2022-04-11 17:35:18,976 - INFO - tqdm - accuracy: 0.9825, batch_loss: 0.0495, loss: 0.0537 ||:  91%|######### | 59/65 [10:45<01:05, 10.97s/it]
2022-04-11 17:35:29,934 - INFO - tqdm - accuracy: 0.9823, batch_loss: 0.0581, loss: 0.0538 ||:  92%|#########2| 60/65 [10:55<00:54, 10.96s/it]
2022-04-11 17:35:40,894 - INFO - tqdm - accuracy: 0.9826, batch_loss: 0.0130, loss: 0.0531 ||:  94%|#########3| 61/65 [11:06<00:43, 10.96s/it]
2022-04-11 17:35:51,846 - INFO - tqdm - accuracy: 0.9829, batch_loss: 0.0156, loss: 0.0525 ||:  95%|#########5| 62/65 [11:17<00:32, 10.96s/it]
2022-04-11 17:36:02,800 - INFO - tqdm - accuracy: 0.9826, batch_loss: 0.0443, loss: 0.0524 ||:  97%|#########6| 63/65 [11:28<00:21, 10.96s/it]
2022-04-11 17:36:13,758 - INFO - tqdm - accuracy: 0.9824, batch_loss: 0.0814, loss: 0.0528 ||:  98%|#########8| 64/65 [11:39<00:10, 10.96s/it]
2022-04-11 17:36:18,606 - INFO - tqdm - accuracy: 0.9825, batch_loss: 0.0241, loss: 0.0524 ||: 100%|##########| 65/65 [11:44<00:00,  9.13s/it]
2022-04-11 17:36:18,606 - INFO - tqdm - accuracy: 0.9825, batch_loss: 0.0241, loss: 0.0524 ||: 100%|##########| 65/65 [11:44<00:00, 10.84s/it]
2022-04-11 17:36:21,460 - INFO - allennlp.training.trainer - Validating
2022-04-11 17:36:21,462 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-11 17:36:31,551 - INFO - tqdm - accuracy: 0.6548, batch_loss: 0.8917, loss: 1.4845 ||:  37%|###6      | 42/115 [00:10<00:17,  4.16it/s]
2022-04-11 17:36:41,744 - INFO - tqdm - accuracy: 0.6095, batch_loss: 0.0027, loss: 1.8183 ||:  74%|#######3  | 85/115 [00:20<00:07,  4.13it/s]
2022-04-11 17:36:48,971 - INFO - tqdm - accuracy: 0.6332, batch_loss: 3.9233, loss: 1.8263 ||: 100%|##########| 115/115 [00:27<00:00,  4.14it/s]
2022-04-11 17:36:48,971 - INFO - tqdm - accuracy: 0.6332, batch_loss: 3.9233, loss: 1.8263 ||: 100%|##########| 115/115 [00:27<00:00,  4.18it/s]
2022-04-11 17:36:48,971 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-11 17:36:48,972 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.983  |     0.633
2022-04-11 17:36:48,972 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-11 17:36:48,973 - INFO - allennlp.training.tensorboard_writer - loss               |     0.052  |     1.826
2022-04-11 17:36:48,973 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5804.656  |       N/A
2022-04-11 17:36:55,149 - INFO - allennlp.training.trainer - Epoch duration: 0:12:21.195961
2022-04-11 17:36:55,149 - INFO - allennlp.training.trainer - Estimated training time remaining: 0:49:39
2022-04-11 17:36:55,149 - INFO - allennlp.training.trainer - Epoch 11/14
2022-04-11 17:36:55,149 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-11 17:36:55,150 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-11 17:36:55,151 - INFO - allennlp.training.trainer - Training
2022-04-11 17:36:55,151 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-11 17:37:05,958 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0109, loss: 0.0109 ||:   2%|1         | 1/65 [00:10<11:31, 10.81s/it]
2022-04-11 17:37:16,769 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0244, loss: 0.0177 ||:   3%|3         | 2/65 [00:21<11:20, 10.81s/it]
2022-04-11 17:37:27,633 - INFO - tqdm - accuracy: 0.9792, batch_loss: 0.3239, loss: 0.1197 ||:   5%|4         | 3/65 [00:32<11:11, 10.83s/it]
2022-04-11 17:37:38,511 - INFO - tqdm - accuracy: 0.9766, batch_loss: 0.1917, loss: 0.1377 ||:   6%|6         | 4/65 [00:43<11:01, 10.85s/it]
2022-04-11 17:37:49,404 - INFO - tqdm - accuracy: 0.9812, batch_loss: 0.0185, loss: 0.1139 ||:   8%|7         | 5/65 [00:54<10:51, 10.87s/it]
2022-04-11 17:38:00,292 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0194, loss: 0.0982 ||:   9%|9         | 6/65 [01:05<10:41, 10.87s/it]
2022-04-11 17:38:11,194 - INFO - tqdm - accuracy: 0.9866, batch_loss: 0.0096, loss: 0.0855 ||:  11%|#         | 7/65 [01:16<10:31, 10.88s/it]
2022-04-11 17:38:22,119 - INFO - tqdm - accuracy: 0.9883, batch_loss: 0.0126, loss: 0.0764 ||:  12%|#2        | 8/65 [01:26<10:21, 10.90s/it]
2022-04-11 17:38:33,020 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.0081, loss: 0.0688 ||:  14%|#3        | 9/65 [01:37<10:10, 10.90s/it]
2022-04-11 17:38:43,952 - INFO - tqdm - accuracy: 0.9906, batch_loss: 0.0257, loss: 0.0645 ||:  15%|#5        | 10/65 [01:48<09:59, 10.91s/it]
2022-04-11 17:38:54,855 - INFO - tqdm - accuracy: 0.9915, batch_loss: 0.0199, loss: 0.0605 ||:  17%|#6        | 11/65 [01:59<09:48, 10.91s/it]
2022-04-11 17:39:05,809 - INFO - tqdm - accuracy: 0.9922, batch_loss: 0.0077, loss: 0.0561 ||:  18%|#8        | 12/65 [02:10<09:38, 10.92s/it]
2022-04-11 17:39:16,745 - INFO - tqdm - accuracy: 0.9904, batch_loss: 0.0981, loss: 0.0593 ||:  20%|##        | 13/65 [02:21<09:28, 10.93s/it]
2022-04-11 17:39:27,704 - INFO - tqdm - accuracy: 0.9866, batch_loss: 0.1503, loss: 0.0658 ||:  22%|##1       | 14/65 [02:32<09:17, 10.94s/it]
2022-04-11 17:39:38,653 - INFO - tqdm - accuracy: 0.9875, batch_loss: 0.0162, loss: 0.0625 ||:  23%|##3       | 15/65 [02:43<09:06, 10.94s/it]
2022-04-11 17:39:49,505 - INFO - tqdm - accuracy: 0.9883, batch_loss: 0.0051, loss: 0.0589 ||:  25%|##4       | 16/65 [02:54<08:54, 10.91s/it]
2022-04-11 17:40:00,471 - INFO - tqdm - accuracy: 0.9890, batch_loss: 0.0036, loss: 0.0557 ||:  26%|##6       | 17/65 [03:05<08:44, 10.93s/it]
2022-04-11 17:40:11,434 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.0266, loss: 0.0540 ||:  28%|##7       | 18/65 [03:16<08:34, 10.94s/it]
2022-04-11 17:40:22,408 - INFO - tqdm - accuracy: 0.9901, batch_loss: 0.0146, loss: 0.0520 ||:  29%|##9       | 19/65 [03:27<08:23, 10.95s/it]
2022-04-11 17:40:33,365 - INFO - tqdm - accuracy: 0.9906, batch_loss: 0.0289, loss: 0.0508 ||:  31%|###       | 20/65 [03:38<08:12, 10.95s/it]
2022-04-11 17:40:44,329 - INFO - tqdm - accuracy: 0.9911, batch_loss: 0.0166, loss: 0.0492 ||:  32%|###2      | 21/65 [03:49<08:02, 10.96s/it]
2022-04-11 17:40:55,311 - INFO - tqdm - accuracy: 0.9915, batch_loss: 0.0108, loss: 0.0474 ||:  34%|###3      | 22/65 [04:00<07:51, 10.96s/it]
2022-04-11 17:41:06,263 - INFO - tqdm - accuracy: 0.9905, batch_loss: 0.0390, loss: 0.0471 ||:  35%|###5      | 23/65 [04:11<07:40, 10.96s/it]
2022-04-11 17:41:17,228 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.0532, loss: 0.0473 ||:  37%|###6      | 24/65 [04:22<07:29, 10.96s/it]
2022-04-11 17:41:28,191 - INFO - tqdm - accuracy: 0.9888, batch_loss: 0.0470, loss: 0.0473 ||:  38%|###8      | 25/65 [04:33<07:18, 10.96s/it]
2022-04-11 17:41:39,145 - INFO - tqdm - accuracy: 0.9892, batch_loss: 0.0057, loss: 0.0457 ||:  40%|####      | 26/65 [04:43<07:07, 10.96s/it]
2022-04-11 17:41:50,101 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.0239, loss: 0.0449 ||:  42%|####1     | 27/65 [04:54<06:56, 10.96s/it]
2022-04-11 17:42:01,030 - INFO - tqdm - accuracy: 0.9900, batch_loss: 0.0222, loss: 0.0441 ||:  43%|####3     | 28/65 [05:05<06:45, 10.95s/it]
2022-04-11 17:42:11,967 - INFO - tqdm - accuracy: 0.9903, batch_loss: 0.0073, loss: 0.0428 ||:  45%|####4     | 29/65 [05:16<06:34, 10.95s/it]
2022-04-11 17:42:22,896 - INFO - tqdm - accuracy: 0.9906, batch_loss: 0.0087, loss: 0.0417 ||:  46%|####6     | 30/65 [05:27<06:22, 10.94s/it]
2022-04-11 17:42:33,837 - INFO - tqdm - accuracy: 0.9909, batch_loss: 0.0048, loss: 0.0405 ||:  48%|####7     | 31/65 [05:38<06:11, 10.94s/it]
2022-04-11 17:42:44,785 - INFO - tqdm - accuracy: 0.9912, batch_loss: 0.0139, loss: 0.0397 ||:  49%|####9     | 32/65 [05:49<06:01, 10.94s/it]
2022-04-11 17:42:55,733 - INFO - tqdm - accuracy: 0.9915, batch_loss: 0.0159, loss: 0.0389 ||:  51%|#####     | 33/65 [06:00<05:50, 10.94s/it]
2022-04-11 17:43:06,682 - INFO - tqdm - accuracy: 0.9908, batch_loss: 0.0344, loss: 0.0388 ||:  52%|#####2    | 34/65 [06:11<05:39, 10.95s/it]
2022-04-11 17:43:17,626 - INFO - tqdm - accuracy: 0.9911, batch_loss: 0.0200, loss: 0.0383 ||:  54%|#####3    | 35/65 [06:22<05:28, 10.95s/it]
2022-04-11 17:43:28,560 - INFO - tqdm - accuracy: 0.9913, batch_loss: 0.0096, loss: 0.0375 ||:  55%|#####5    | 36/65 [06:33<05:17, 10.94s/it]
2022-04-11 17:43:39,506 - INFO - tqdm - accuracy: 0.9907, batch_loss: 0.0570, loss: 0.0380 ||:  57%|#####6    | 37/65 [06:44<05:06, 10.94s/it]
2022-04-11 17:43:50,462 - INFO - tqdm - accuracy: 0.9901, batch_loss: 0.0443, loss: 0.0382 ||:  58%|#####8    | 38/65 [06:55<04:55, 10.95s/it]
2022-04-11 17:44:01,399 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.0689, loss: 0.0390 ||:  60%|######    | 39/65 [07:06<04:44, 10.94s/it]
2022-04-11 17:44:12,340 - INFO - tqdm - accuracy: 0.9891, batch_loss: 0.0497, loss: 0.0392 ||:  62%|######1   | 40/65 [07:17<04:33, 10.94s/it]
2022-04-11 17:44:23,299 - INFO - tqdm - accuracy: 0.9886, batch_loss: 0.0414, loss: 0.0393 ||:  63%|######3   | 41/65 [07:28<04:22, 10.95s/it]
2022-04-11 17:44:34,259 - INFO - tqdm - accuracy: 0.9881, batch_loss: 0.0319, loss: 0.0391 ||:  65%|######4   | 42/65 [07:39<04:11, 10.95s/it]
2022-04-11 17:44:45,080 - INFO - tqdm - accuracy: 0.9876, batch_loss: 0.0798, loss: 0.0400 ||:  66%|######6   | 43/65 [07:49<04:00, 10.91s/it]
2022-04-11 17:44:56,021 - INFO - tqdm - accuracy: 0.9879, batch_loss: 0.0149, loss: 0.0395 ||:  68%|######7   | 44/65 [08:00<03:49, 10.92s/it]
2022-04-11 17:45:06,650 - INFO - tqdm - accuracy: 0.9875, batch_loss: 0.0810, loss: 0.0404 ||:  69%|######9   | 45/65 [08:11<03:36, 10.83s/it]
2022-04-11 17:45:17,608 - INFO - tqdm - accuracy: 0.9878, batch_loss: 0.0350, loss: 0.0403 ||:  71%|#######   | 46/65 [08:22<03:26, 10.87s/it]
2022-04-11 17:45:28,571 - INFO - tqdm - accuracy: 0.9874, batch_loss: 0.0880, loss: 0.0413 ||:  72%|#######2  | 47/65 [08:33<03:16, 10.90s/it]
2022-04-11 17:45:39,520 - INFO - tqdm - accuracy: 0.9870, batch_loss: 0.0457, loss: 0.0414 ||:  74%|#######3  | 48/65 [08:44<03:05, 10.91s/it]
2022-04-11 17:45:50,460 - INFO - tqdm - accuracy: 0.9872, batch_loss: 0.0092, loss: 0.0407 ||:  75%|#######5  | 49/65 [08:55<02:54, 10.92s/it]
2022-04-11 17:46:01,422 - INFO - tqdm - accuracy: 0.9875, batch_loss: 0.0028, loss: 0.0400 ||:  77%|#######6  | 50/65 [09:06<02:44, 10.93s/it]
2022-04-11 17:46:12,360 - INFO - tqdm - accuracy: 0.9871, batch_loss: 0.1537, loss: 0.0422 ||:  78%|#######8  | 51/65 [09:17<02:33, 10.93s/it]
2022-04-11 17:46:23,311 - INFO - tqdm - accuracy: 0.9874, batch_loss: 0.0151, loss: 0.0417 ||:  80%|########  | 52/65 [09:28<02:22, 10.94s/it]
2022-04-11 17:46:34,272 - INFO - tqdm - accuracy: 0.9876, batch_loss: 0.0104, loss: 0.0411 ||:  82%|########1 | 53/65 [09:39<02:11, 10.95s/it]
2022-04-11 17:46:45,232 - INFO - tqdm - accuracy: 0.9878, batch_loss: 0.0084, loss: 0.0405 ||:  83%|########3 | 54/65 [09:50<02:00, 10.95s/it]
2022-04-11 17:46:56,192 - INFO - tqdm - accuracy: 0.9881, batch_loss: 0.0067, loss: 0.0399 ||:  85%|########4 | 55/65 [10:01<01:49, 10.95s/it]
2022-04-11 17:47:07,169 - INFO - tqdm - accuracy: 0.9883, batch_loss: 0.0057, loss: 0.0393 ||:  86%|########6 | 56/65 [10:12<01:38, 10.96s/it]
2022-04-11 17:47:17,886 - INFO - tqdm - accuracy: 0.9879, batch_loss: 0.0649, loss: 0.0397 ||:  88%|########7 | 57/65 [10:22<01:27, 10.89s/it]
2022-04-11 17:47:28,874 - INFO - tqdm - accuracy: 0.9881, batch_loss: 0.0348, loss: 0.0396 ||:  89%|########9 | 58/65 [10:33<01:16, 10.92s/it]
2022-04-11 17:47:39,858 - INFO - tqdm - accuracy: 0.9883, batch_loss: 0.0070, loss: 0.0391 ||:  91%|######### | 59/65 [10:44<01:05, 10.94s/it]
2022-04-11 17:47:50,818 - INFO - tqdm - accuracy: 0.9885, batch_loss: 0.0136, loss: 0.0386 ||:  92%|#########2| 60/65 [10:55<00:54, 10.94s/it]
2022-04-11 17:48:01,812 - INFO - tqdm - accuracy: 0.9887, batch_loss: 0.0219, loss: 0.0384 ||:  94%|#########3| 61/65 [11:06<00:43, 10.96s/it]
2022-04-11 17:48:12,778 - INFO - tqdm - accuracy: 0.9889, batch_loss: 0.0129, loss: 0.0380 ||:  95%|#########5| 62/65 [11:17<00:32, 10.96s/it]
2022-04-11 17:48:23,751 - INFO - tqdm - accuracy: 0.9891, batch_loss: 0.0070, loss: 0.0375 ||:  97%|#########6| 63/65 [11:28<00:21, 10.96s/it]
2022-04-11 17:48:34,712 - INFO - tqdm - accuracy: 0.9888, batch_loss: 0.0483, loss: 0.0376 ||:  98%|#########8| 64/65 [11:39<00:10, 10.96s/it]
2022-04-11 17:48:39,592 - INFO - tqdm - accuracy: 0.9884, batch_loss: 0.0969, loss: 0.0386 ||: 100%|##########| 65/65 [11:44<00:00,  9.14s/it]
2022-04-11 17:48:39,592 - INFO - tqdm - accuracy: 0.9884, batch_loss: 0.0969, loss: 0.0386 ||: 100%|##########| 65/65 [11:44<00:00, 10.84s/it]
2022-04-11 17:48:42,512 - INFO - allennlp.training.trainer - Validating
2022-04-11 17:48:42,514 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-11 17:48:52,567 - INFO - tqdm - accuracy: 0.6190, batch_loss: 0.0019, loss: 2.3153 ||:  37%|###6      | 42/115 [00:10<00:17,  4.17it/s]
2022-04-11 17:49:02,606 - INFO - tqdm - accuracy: 0.6168, batch_loss: 0.4638, loss: 2.0425 ||:  73%|#######3  | 84/115 [00:20<00:07,  4.29it/s]
2022-04-11 17:49:09,988 - INFO - tqdm - accuracy: 0.6419, batch_loss: 1.4313, loss: 1.8628 ||: 100%|##########| 115/115 [00:27<00:00,  4.13it/s]
2022-04-11 17:49:09,989 - INFO - tqdm - accuracy: 0.6419, batch_loss: 1.4313, loss: 1.8628 ||: 100%|##########| 115/115 [00:27<00:00,  4.19it/s]
2022-04-11 17:49:09,989 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-11 17:49:09,989 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.988  |     0.642
2022-04-11 17:49:09,991 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-11 17:49:09,991 - INFO - allennlp.training.tensorboard_writer - loss               |     0.039  |     1.863
2022-04-11 17:49:09,992 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5804.656  |       N/A
2022-04-11 17:49:16,145 - INFO - allennlp.training.trainer - Epoch duration: 0:12:20.995938
2022-04-11 17:49:16,145 - INFO - allennlp.training.trainer - Estimated training time remaining: 0:37:13
2022-04-11 17:49:16,145 - INFO - allennlp.training.trainer - Epoch 12/14
2022-04-11 17:49:16,145 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-11 17:49:16,146 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-11 17:49:16,147 - INFO - allennlp.training.trainer - Training
2022-04-11 17:49:16,147 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-11 17:49:26,932 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0386, loss: 0.0386 ||:   2%|1         | 1/65 [00:10<11:30, 10.78s/it]
2022-04-11 17:49:37,763 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0119, loss: 0.0253 ||:   3%|3         | 2/65 [00:21<11:21, 10.81s/it]
2022-04-11 17:49:48,496 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0103, loss: 0.0203 ||:   5%|4         | 3/65 [00:32<11:08, 10.78s/it]
2022-04-11 17:49:59,359 - INFO - tqdm - accuracy: 0.9922, batch_loss: 0.0357, loss: 0.0241 ||:   6%|6         | 4/65 [00:43<10:59, 10.81s/it]
2022-04-11 17:50:10,223 - INFO - tqdm - accuracy: 0.9938, batch_loss: 0.0136, loss: 0.0220 ||:   8%|7         | 5/65 [00:54<10:49, 10.83s/it]
2022-04-11 17:50:21,117 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.0871, loss: 0.0329 ||:   9%|9         | 6/65 [01:04<10:40, 10.85s/it]
2022-04-11 17:50:32,016 - INFO - tqdm - accuracy: 0.9911, batch_loss: 0.0125, loss: 0.0300 ||:  11%|#         | 7/65 [01:15<10:30, 10.87s/it]
2022-04-11 17:50:42,917 - INFO - tqdm - accuracy: 0.9922, batch_loss: 0.0053, loss: 0.0269 ||:  12%|#2        | 8/65 [01:26<10:20, 10.88s/it]
2022-04-11 17:50:53,833 - INFO - tqdm - accuracy: 0.9931, batch_loss: 0.0025, loss: 0.0242 ||:  14%|#3        | 9/65 [01:37<10:09, 10.89s/it]
2022-04-11 17:51:04,737 - INFO - tqdm - accuracy: 0.9938, batch_loss: 0.0029, loss: 0.0220 ||:  15%|#5        | 10/65 [01:48<09:59, 10.89s/it]
2022-04-11 17:51:15,654 - INFO - tqdm - accuracy: 0.9943, batch_loss: 0.0078, loss: 0.0207 ||:  17%|#6        | 11/65 [01:59<09:48, 10.90s/it]
2022-04-11 17:51:26,570 - INFO - tqdm - accuracy: 0.9948, batch_loss: 0.0104, loss: 0.0199 ||:  18%|#8        | 12/65 [02:10<09:38, 10.91s/it]
2022-04-11 17:51:37,173 - INFO - tqdm - accuracy: 0.9928, batch_loss: 0.0481, loss: 0.0220 ||:  20%|##        | 13/65 [02:21<09:22, 10.81s/it]
2022-04-11 17:51:48,072 - INFO - tqdm - accuracy: 0.9911, batch_loss: 0.0472, loss: 0.0238 ||:  22%|##1       | 14/65 [02:31<09:12, 10.84s/it]
2022-04-11 17:51:59,009 - INFO - tqdm - accuracy: 0.9916, batch_loss: 0.0180, loss: 0.0235 ||:  23%|##3       | 15/65 [02:42<09:03, 10.87s/it]
2022-04-11 17:52:09,925 - INFO - tqdm - accuracy: 0.9902, batch_loss: 0.0733, loss: 0.0266 ||:  25%|##4       | 16/65 [02:53<08:53, 10.88s/it]
2022-04-11 17:52:20,866 - INFO - tqdm - accuracy: 0.9908, batch_loss: 0.0061, loss: 0.0254 ||:  26%|##6       | 17/65 [03:04<08:43, 10.90s/it]
2022-04-11 17:52:31,820 - INFO - tqdm - accuracy: 0.9913, batch_loss: 0.0154, loss: 0.0248 ||:  28%|##7       | 18/65 [03:15<08:33, 10.92s/it]
2022-04-11 17:52:42,761 - INFO - tqdm - accuracy: 0.9918, batch_loss: 0.0060, loss: 0.0238 ||:  29%|##9       | 19/65 [03:26<08:22, 10.92s/it]
2022-04-11 17:52:54,100 - INFO - tqdm - accuracy: 0.9922, batch_loss: 0.0186, loss: 0.0236 ||:  31%|###       | 20/65 [03:37<08:17, 11.05s/it]
2022-04-11 17:53:05,024 - INFO - tqdm - accuracy: 0.9911, batch_loss: 0.0287, loss: 0.0238 ||:  32%|###2      | 21/65 [03:48<08:04, 11.01s/it]
2022-04-11 17:53:15,962 - INFO - tqdm - accuracy: 0.9915, batch_loss: 0.0128, loss: 0.0233 ||:  34%|###3      | 22/65 [03:59<07:52, 10.99s/it]
2022-04-11 17:53:26,892 - INFO - tqdm - accuracy: 0.9918, batch_loss: 0.0198, loss: 0.0232 ||:  35%|###5      | 23/65 [04:10<07:40, 10.97s/it]
2022-04-11 17:53:37,854 - INFO - tqdm - accuracy: 0.9909, batch_loss: 0.1612, loss: 0.0289 ||:  37%|###6      | 24/65 [04:21<07:29, 10.97s/it]
2022-04-11 17:53:48,571 - INFO - tqdm - accuracy: 0.9912, batch_loss: 0.0238, loss: 0.0287 ||:  38%|###8      | 25/65 [04:32<07:15, 10.89s/it]
2022-04-11 17:53:59,517 - INFO - tqdm - accuracy: 0.9916, batch_loss: 0.0104, loss: 0.0280 ||:  40%|####      | 26/65 [04:43<07:05, 10.91s/it]
2022-04-11 17:54:10,461 - INFO - tqdm - accuracy: 0.9919, batch_loss: 0.0044, loss: 0.0271 ||:  42%|####1     | 27/65 [04:54<06:54, 10.92s/it]
2022-04-11 17:54:21,419 - INFO - tqdm - accuracy: 0.9922, batch_loss: 0.0130, loss: 0.0266 ||:  43%|####3     | 28/65 [05:05<06:44, 10.93s/it]
2022-04-11 17:54:32,368 - INFO - tqdm - accuracy: 0.9924, batch_loss: 0.0343, loss: 0.0269 ||:  45%|####4     | 29/65 [05:16<06:33, 10.94s/it]
2022-04-11 17:54:43,331 - INFO - tqdm - accuracy: 0.9927, batch_loss: 0.0336, loss: 0.0271 ||:  46%|####6     | 30/65 [05:27<06:23, 10.94s/it]
2022-04-11 17:54:54,288 - INFO - tqdm - accuracy: 0.9929, batch_loss: 0.0033, loss: 0.0263 ||:  48%|####7     | 31/65 [05:38<06:12, 10.95s/it]
2022-04-11 17:55:05,129 - INFO - tqdm - accuracy: 0.9922, batch_loss: 0.0460, loss: 0.0270 ||:  49%|####9     | 32/65 [05:48<06:00, 10.92s/it]
2022-04-11 17:55:16,073 - INFO - tqdm - accuracy: 0.9924, batch_loss: 0.0058, loss: 0.0263 ||:  51%|#####     | 33/65 [05:59<05:49, 10.92s/it]
2022-04-11 17:55:27,031 - INFO - tqdm - accuracy: 0.9926, batch_loss: 0.0115, loss: 0.0259 ||:  52%|#####2    | 34/65 [06:10<05:38, 10.93s/it]
2022-04-11 17:55:37,995 - INFO - tqdm - accuracy: 0.9929, batch_loss: 0.0096, loss: 0.0254 ||:  54%|#####3    | 35/65 [06:21<05:28, 10.94s/it]
2022-04-11 17:55:48,936 - INFO - tqdm - accuracy: 0.9904, batch_loss: 0.1329, loss: 0.0284 ||:  55%|#####5    | 36/65 [06:32<05:17, 10.94s/it]
2022-04-11 17:55:59,903 - INFO - tqdm - accuracy: 0.9907, batch_loss: 0.0071, loss: 0.0278 ||:  57%|#####6    | 37/65 [06:43<05:06, 10.95s/it]
2022-04-11 17:56:10,857 - INFO - tqdm - accuracy: 0.9909, batch_loss: 0.0237, loss: 0.0277 ||:  58%|#####8    | 38/65 [06:54<04:55, 10.95s/it]
2022-04-11 17:56:21,821 - INFO - tqdm - accuracy: 0.9904, batch_loss: 0.0827, loss: 0.0291 ||:  60%|######    | 39/65 [07:05<04:44, 10.95s/it]
2022-04-11 17:56:32,780 - INFO - tqdm - accuracy: 0.9906, batch_loss: 0.0089, loss: 0.0286 ||:  62%|######1   | 40/65 [07:16<04:33, 10.96s/it]
2022-04-11 17:56:43,750 - INFO - tqdm - accuracy: 0.9893, batch_loss: 0.1780, loss: 0.0323 ||:  63%|######3   | 41/65 [07:27<04:23, 10.96s/it]
2022-04-11 17:56:54,713 - INFO - tqdm - accuracy: 0.9888, batch_loss: 0.0307, loss: 0.0322 ||:  65%|######4   | 42/65 [07:38<04:12, 10.96s/it]
2022-04-11 17:57:05,689 - INFO - tqdm - accuracy: 0.9891, batch_loss: 0.0033, loss: 0.0316 ||:  66%|######6   | 43/65 [07:49<04:01, 10.97s/it]
2022-04-11 17:57:16,659 - INFO - tqdm - accuracy: 0.9893, batch_loss: 0.0127, loss: 0.0311 ||:  68%|######7   | 44/65 [08:00<03:50, 10.97s/it]
2022-04-11 17:57:27,634 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.0087, loss: 0.0306 ||:  69%|######9   | 45/65 [08:11<03:39, 10.97s/it]
2022-04-11 17:57:38,612 - INFO - tqdm - accuracy: 0.9898, batch_loss: 0.0094, loss: 0.0302 ||:  71%|#######   | 46/65 [08:22<03:28, 10.97s/it]
2022-04-11 17:57:49,570 - INFO - tqdm - accuracy: 0.9900, batch_loss: 0.0015, loss: 0.0296 ||:  72%|#######2  | 47/65 [08:33<03:17, 10.97s/it]
2022-04-11 17:58:00,543 - INFO - tqdm - accuracy: 0.9902, batch_loss: 0.0340, loss: 0.0296 ||:  74%|#######3  | 48/65 [08:44<03:06, 10.97s/it]
2022-04-11 17:58:11,513 - INFO - tqdm - accuracy: 0.9904, batch_loss: 0.0028, loss: 0.0291 ||:  75%|#######5  | 49/65 [08:55<02:55, 10.97s/it]
2022-04-11 17:58:22,461 - INFO - tqdm - accuracy: 0.9906, batch_loss: 0.0034, loss: 0.0286 ||:  77%|#######6  | 50/65 [09:06<02:44, 10.96s/it]
2022-04-11 17:58:33,454 - INFO - tqdm - accuracy: 0.9908, batch_loss: 0.0138, loss: 0.0283 ||:  78%|#######8  | 51/65 [09:17<02:33, 10.97s/it]
2022-04-11 17:58:44,399 - INFO - tqdm - accuracy: 0.9904, batch_loss: 0.0834, loss: 0.0294 ||:  80%|########  | 52/65 [09:28<02:22, 10.96s/it]
2022-04-11 17:58:55,386 - INFO - tqdm - accuracy: 0.9906, batch_loss: 0.0195, loss: 0.0292 ||:  82%|########1 | 53/65 [09:39<02:11, 10.97s/it]
2022-04-11 17:59:06,355 - INFO - tqdm - accuracy: 0.9907, batch_loss: 0.0219, loss: 0.0290 ||:  83%|########3 | 54/65 [09:50<02:00, 10.97s/it]
2022-04-11 17:59:17,338 - INFO - tqdm - accuracy: 0.9909, batch_loss: 0.0191, loss: 0.0289 ||:  85%|########4 | 55/65 [10:01<01:49, 10.97s/it]
2022-04-11 17:59:28,298 - INFO - tqdm - accuracy: 0.9905, batch_loss: 0.0684, loss: 0.0296 ||:  86%|########6 | 56/65 [10:12<01:38, 10.97s/it]
2022-04-11 17:59:39,255 - INFO - tqdm - accuracy: 0.9907, batch_loss: 0.0116, loss: 0.0292 ||:  88%|########7 | 57/65 [10:23<01:27, 10.97s/it]
2022-04-11 17:59:50,244 - INFO - tqdm - accuracy: 0.9903, batch_loss: 0.0466, loss: 0.0295 ||:  89%|########9 | 58/65 [10:34<01:16, 10.97s/it]
2022-04-11 18:00:01,220 - INFO - tqdm - accuracy: 0.9899, batch_loss: 0.0885, loss: 0.0305 ||:  91%|######### | 59/65 [10:45<01:05, 10.97s/it]
2022-04-11 18:00:12,195 - INFO - tqdm - accuracy: 0.9901, batch_loss: 0.0085, loss: 0.0302 ||:  92%|#########2| 60/65 [10:56<00:54, 10.97s/it]
2022-04-11 18:00:23,165 - INFO - tqdm - accuracy: 0.9903, batch_loss: 0.0046, loss: 0.0298 ||:  94%|#########3| 61/65 [11:07<00:43, 10.97s/it]
2022-04-11 18:00:34,133 - INFO - tqdm - accuracy: 0.9904, batch_loss: 0.0457, loss: 0.0300 ||:  95%|#########5| 62/65 [11:17<00:32, 10.97s/it]
2022-04-11 18:00:45,098 - INFO - tqdm - accuracy: 0.9901, batch_loss: 0.0491, loss: 0.0303 ||:  97%|#########6| 63/65 [11:28<00:21, 10.97s/it]
2022-04-11 18:00:55,962 - INFO - tqdm - accuracy: 0.9893, batch_loss: 0.1488, loss: 0.0322 ||:  98%|#########8| 64/65 [11:39<00:10, 10.94s/it]
2022-04-11 18:01:00,835 - INFO - tqdm - accuracy: 0.9893, batch_loss: 0.0017, loss: 0.0317 ||: 100%|##########| 65/65 [11:44<00:00,  9.12s/it]
2022-04-11 18:01:00,835 - INFO - tqdm - accuracy: 0.9893, batch_loss: 0.0017, loss: 0.0317 ||: 100%|##########| 65/65 [11:44<00:00, 10.84s/it]
2022-04-11 18:01:03,873 - INFO - allennlp.training.trainer - Validating
2022-04-11 18:01:03,875 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-11 18:01:14,015 - INFO - tqdm - accuracy: 0.7529, batch_loss: 4.2314, loss: 1.6528 ||:  37%|###7      | 43/115 [00:10<00:17,  4.17it/s]
2022-04-11 18:01:24,162 - INFO - tqdm - accuracy: 0.7041, batch_loss: 0.0073, loss: 1.7570 ||:  74%|#######3  | 85/115 [00:20<00:07,  4.14it/s]
2022-04-11 18:01:31,400 - INFO - tqdm - accuracy: 0.6550, batch_loss: 2.1649, loss: 1.9106 ||: 100%|##########| 115/115 [00:27<00:00,  4.14it/s]
2022-04-11 18:01:31,400 - INFO - tqdm - accuracy: 0.6550, batch_loss: 2.1649, loss: 1.9106 ||: 100%|##########| 115/115 [00:27<00:00,  4.18it/s]
2022-04-11 18:01:31,400 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-11 18:01:31,400 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.989  |     0.655
2022-04-11 18:01:31,401 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-11 18:01:31,402 - INFO - allennlp.training.tensorboard_writer - loss               |     0.032  |     1.911
2022-04-11 18:01:31,402 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5804.656  |       N/A
2022-04-11 18:01:37,580 - INFO - allennlp.training.trainer - Epoch duration: 0:12:21.434897
2022-04-11 18:01:37,709 - INFO - allennlp.training.trainer - Estimated training time remaining: 0:24:48
2022-04-11 18:01:37,709 - INFO - allennlp.training.trainer - Epoch 13/14
2022-04-11 18:01:37,710 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-11 18:01:37,710 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-11 18:01:37,711 - INFO - allennlp.training.trainer - Training
2022-04-11 18:01:37,711 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-11 18:01:48,505 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.1088, loss: 0.1088 ||:   2%|1         | 1/65 [00:10<11:30, 10.79s/it]
2022-04-11 18:01:59,360 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0174, loss: 0.0631 ||:   3%|3         | 2/65 [00:21<11:22, 10.83s/it]
2022-04-11 18:02:10,212 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.0109, loss: 0.0457 ||:   5%|4         | 3/65 [00:32<11:12, 10.84s/it]
2022-04-11 18:02:21,078 - INFO - tqdm - accuracy: 0.9922, batch_loss: 0.0201, loss: 0.0393 ||:   6%|6         | 4/65 [00:43<11:01, 10.85s/it]
2022-04-11 18:02:31,975 - INFO - tqdm - accuracy: 0.9812, batch_loss: 0.1264, loss: 0.0567 ||:   8%|7         | 5/65 [00:54<10:52, 10.87s/it]
2022-04-11 18:02:42,873 - INFO - tqdm - accuracy: 0.9792, batch_loss: 0.0852, loss: 0.0615 ||:   9%|9         | 6/65 [01:05<10:41, 10.88s/it]
2022-04-11 18:02:53,783 - INFO - tqdm - accuracy: 0.9821, batch_loss: 0.0205, loss: 0.0556 ||:  11%|#         | 7/65 [01:16<10:31, 10.89s/it]
2022-04-11 18:03:04,698 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0040, loss: 0.0492 ||:  12%|#2        | 8/65 [01:26<10:21, 10.90s/it]
2022-04-11 18:03:15,620 - INFO - tqdm - accuracy: 0.9861, batch_loss: 0.0114, loss: 0.0450 ||:  14%|#3        | 9/65 [01:37<10:10, 10.90s/it]
2022-04-11 18:03:26,538 - INFO - tqdm - accuracy: 0.9875, batch_loss: 0.0163, loss: 0.0421 ||:  15%|#5        | 10/65 [01:48<09:59, 10.91s/it]
2022-04-11 18:03:37,447 - INFO - tqdm - accuracy: 0.9886, batch_loss: 0.0035, loss: 0.0386 ||:  17%|#6        | 11/65 [01:59<09:49, 10.91s/it]
2022-04-11 18:03:48,372 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.0064, loss: 0.0359 ||:  18%|#8        | 12/65 [02:10<09:38, 10.91s/it]
2022-04-11 18:03:59,318 - INFO - tqdm - accuracy: 0.9904, batch_loss: 0.0125, loss: 0.0341 ||:  20%|##        | 13/65 [02:21<09:28, 10.92s/it]
2022-04-11 18:04:10,262 - INFO - tqdm - accuracy: 0.9888, batch_loss: 0.0614, loss: 0.0360 ||:  22%|##1       | 14/65 [02:32<09:17, 10.93s/it]
2022-04-11 18:04:21,205 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.0019, loss: 0.0338 ||:  23%|##3       | 15/65 [02:43<09:06, 10.93s/it]
2022-04-11 18:04:32,162 - INFO - tqdm - accuracy: 0.9902, batch_loss: 0.0103, loss: 0.0323 ||:  25%|##4       | 16/65 [02:54<08:56, 10.94s/it]
2022-04-11 18:04:43,119 - INFO - tqdm - accuracy: 0.9908, batch_loss: 0.0027, loss: 0.0306 ||:  26%|##6       | 17/65 [03:05<08:45, 10.95s/it]
2022-04-11 18:04:53,741 - INFO - tqdm - accuracy: 0.9913, batch_loss: 0.0133, loss: 0.0296 ||:  28%|##7       | 18/65 [03:16<08:29, 10.85s/it]
2022-04-11 18:05:04,710 - INFO - tqdm - accuracy: 0.9918, batch_loss: 0.0078, loss: 0.0285 ||:  29%|##9       | 19/65 [03:26<08:20, 10.88s/it]
2022-04-11 18:05:15,666 - INFO - tqdm - accuracy: 0.9906, batch_loss: 0.0814, loss: 0.0311 ||:  31%|###       | 20/65 [03:37<08:10, 10.91s/it]
2022-04-11 18:05:26,622 - INFO - tqdm - accuracy: 0.9911, batch_loss: 0.0077, loss: 0.0300 ||:  32%|###2      | 21/65 [03:48<08:00, 10.92s/it]
2022-04-11 18:05:37,564 - INFO - tqdm - accuracy: 0.9900, batch_loss: 0.0456, loss: 0.0307 ||:  34%|###3      | 22/65 [03:59<07:49, 10.93s/it]
2022-04-11 18:05:48,539 - INFO - tqdm - accuracy: 0.9878, batch_loss: 0.1341, loss: 0.0352 ||:  35%|###5      | 23/65 [04:10<07:39, 10.94s/it]
2022-04-11 18:05:59,470 - INFO - tqdm - accuracy: 0.9883, batch_loss: 0.0197, loss: 0.0345 ||:  37%|###6      | 24/65 [04:21<07:28, 10.94s/it]
2022-04-11 18:06:10,436 - INFO - tqdm - accuracy: 0.9887, batch_loss: 0.0055, loss: 0.0334 ||:  38%|###8      | 25/65 [04:32<07:17, 10.95s/it]
2022-04-11 18:06:21,418 - INFO - tqdm - accuracy: 0.9892, batch_loss: 0.0119, loss: 0.0326 ||:  40%|####      | 26/65 [04:43<07:07, 10.96s/it]
2022-04-11 18:06:32,382 - INFO - tqdm - accuracy: 0.9884, batch_loss: 0.1070, loss: 0.0353 ||:  42%|####1     | 27/65 [04:54<06:56, 10.96s/it]
2022-04-11 18:06:43,364 - INFO - tqdm - accuracy: 0.9877, batch_loss: 0.0895, loss: 0.0372 ||:  43%|####3     | 28/65 [05:05<06:45, 10.97s/it]
2022-04-11 18:06:54,336 - INFO - tqdm - accuracy: 0.9881, batch_loss: 0.0316, loss: 0.0371 ||:  45%|####4     | 29/65 [05:16<06:34, 10.97s/it]
2022-04-11 18:07:05,312 - INFO - tqdm - accuracy: 0.9875, batch_loss: 0.1662, loss: 0.0414 ||:  46%|####6     | 30/65 [05:27<06:23, 10.97s/it]
2022-04-11 18:07:16,288 - INFO - tqdm - accuracy: 0.9879, batch_loss: 0.0047, loss: 0.0402 ||:  48%|####7     | 31/65 [05:38<06:13, 10.97s/it]
2022-04-11 18:07:27,253 - INFO - tqdm - accuracy: 0.9883, batch_loss: 0.0098, loss: 0.0392 ||:  49%|####9     | 32/65 [05:49<06:02, 10.97s/it]
2022-04-11 18:07:38,216 - INFO - tqdm - accuracy: 0.9877, batch_loss: 0.1326, loss: 0.0421 ||:  51%|#####     | 33/65 [06:00<05:50, 10.97s/it]
2022-04-11 18:07:49,043 - INFO - tqdm - accuracy: 0.9871, batch_loss: 0.0624, loss: 0.0427 ||:  52%|#####2    | 34/65 [06:11<05:38, 10.93s/it]
2022-04-11 18:08:00,009 - INFO - tqdm - accuracy: 0.9857, batch_loss: 0.1223, loss: 0.0449 ||:  54%|#####3    | 35/65 [06:22<05:28, 10.94s/it]
2022-04-11 18:08:10,981 - INFO - tqdm - accuracy: 0.9861, batch_loss: 0.0276, loss: 0.0444 ||:  55%|#####5    | 36/65 [06:33<05:17, 10.95s/it]
2022-04-11 18:08:21,946 - INFO - tqdm - accuracy: 0.9865, batch_loss: 0.0022, loss: 0.0433 ||:  57%|#####6    | 37/65 [06:44<05:06, 10.95s/it]
2022-04-11 18:08:32,902 - INFO - tqdm - accuracy: 0.9860, batch_loss: 0.0543, loss: 0.0436 ||:  58%|#####8    | 38/65 [06:55<04:55, 10.95s/it]
2022-04-11 18:08:43,869 - INFO - tqdm - accuracy: 0.9864, batch_loss: 0.0188, loss: 0.0430 ||:  60%|######    | 39/65 [07:06<04:44, 10.96s/it]
2022-04-11 18:08:54,824 - INFO - tqdm - accuracy: 0.9867, batch_loss: 0.0087, loss: 0.0421 ||:  62%|######1   | 40/65 [07:17<04:33, 10.96s/it]
2022-04-11 18:09:05,785 - INFO - tqdm - accuracy: 0.9863, batch_loss: 0.0802, loss: 0.0430 ||:  63%|######3   | 41/65 [07:28<04:22, 10.96s/it]
2022-04-11 18:09:16,759 - INFO - tqdm - accuracy: 0.9866, batch_loss: 0.0060, loss: 0.0421 ||:  65%|######4   | 42/65 [07:39<04:12, 10.96s/it]
2022-04-11 18:09:27,709 - INFO - tqdm - accuracy: 0.9869, batch_loss: 0.0172, loss: 0.0416 ||:  66%|######6   | 43/65 [07:49<04:01, 10.96s/it]
2022-04-11 18:09:38,677 - INFO - tqdm - accuracy: 0.9872, batch_loss: 0.0159, loss: 0.0410 ||:  68%|######7   | 44/65 [08:00<03:50, 10.96s/it]
2022-04-11 18:09:49,639 - INFO - tqdm - accuracy: 0.9875, batch_loss: 0.0130, loss: 0.0404 ||:  69%|######9   | 45/65 [08:11<03:39, 10.96s/it]
2022-04-11 18:10:00,357 - INFO - tqdm - accuracy: 0.9878, batch_loss: 0.0084, loss: 0.0397 ||:  71%|#######   | 46/65 [08:22<03:26, 10.89s/it]
2022-04-11 18:10:11,305 - INFO - tqdm - accuracy: 0.9880, batch_loss: 0.0137, loss: 0.0391 ||:  72%|#######2  | 47/65 [08:33<03:16, 10.91s/it]
2022-04-11 18:10:22,265 - INFO - tqdm - accuracy: 0.9883, batch_loss: 0.0048, loss: 0.0384 ||:  74%|#######3  | 48/65 [08:44<03:05, 10.92s/it]
2022-04-11 18:10:33,230 - INFO - tqdm - accuracy: 0.9885, batch_loss: 0.0334, loss: 0.0383 ||:  75%|#######5  | 49/65 [08:55<02:54, 10.94s/it]
2022-04-11 18:10:44,065 - INFO - tqdm - accuracy: 0.9887, batch_loss: 0.0195, loss: 0.0379 ||:  77%|#######6  | 50/65 [09:06<02:43, 10.91s/it]
2022-04-11 18:10:55,041 - INFO - tqdm - accuracy: 0.9890, batch_loss: 0.0147, loss: 0.0375 ||:  78%|#######8  | 51/65 [09:17<02:32, 10.93s/it]
2022-04-11 18:11:06,000 - INFO - tqdm - accuracy: 0.9892, batch_loss: 0.0064, loss: 0.0369 ||:  80%|########  | 52/65 [09:28<02:22, 10.94s/it]
2022-04-11 18:11:16,975 - INFO - tqdm - accuracy: 0.9888, batch_loss: 0.1280, loss: 0.0386 ||:  82%|########1 | 53/65 [09:39<02:11, 10.95s/it]
2022-04-11 18:11:27,945 - INFO - tqdm - accuracy: 0.9890, batch_loss: 0.0025, loss: 0.0379 ||:  83%|########3 | 54/65 [09:50<02:00, 10.95s/it]
2022-04-11 18:11:39,338 - INFO - tqdm - accuracy: 0.9881, batch_loss: 0.0997, loss: 0.0390 ||:  85%|########4 | 55/65 [10:01<01:50, 11.09s/it]
2022-04-11 18:11:50,277 - INFO - tqdm - accuracy: 0.9877, batch_loss: 0.0699, loss: 0.0396 ||:  86%|########6 | 56/65 [10:12<01:39, 11.04s/it]
2022-04-11 18:12:01,243 - INFO - tqdm - accuracy: 0.9874, batch_loss: 0.1160, loss: 0.0409 ||:  88%|########7 | 57/65 [10:23<01:28, 11.02s/it]
2022-04-11 18:12:12,204 - INFO - tqdm - accuracy: 0.9876, batch_loss: 0.0110, loss: 0.0404 ||:  89%|########9 | 58/65 [10:34<01:17, 11.00s/it]
2022-04-11 18:12:23,164 - INFO - tqdm - accuracy: 0.9878, batch_loss: 0.0031, loss: 0.0398 ||:  91%|######### | 59/65 [10:45<01:05, 10.99s/it]
2022-04-11 18:12:34,116 - INFO - tqdm - accuracy: 0.9880, batch_loss: 0.0047, loss: 0.0392 ||:  92%|#########2| 60/65 [10:56<00:54, 10.98s/it]
2022-04-11 18:12:45,077 - INFO - tqdm - accuracy: 0.9882, batch_loss: 0.0106, loss: 0.0387 ||:  94%|#########3| 61/65 [11:07<00:43, 10.97s/it]
2022-04-11 18:12:56,032 - INFO - tqdm - accuracy: 0.9884, batch_loss: 0.0157, loss: 0.0384 ||:  95%|#########5| 62/65 [11:18<00:32, 10.97s/it]
2022-04-11 18:13:06,996 - INFO - tqdm - accuracy: 0.9886, batch_loss: 0.0037, loss: 0.0378 ||:  97%|#########6| 63/65 [11:29<00:21, 10.97s/it]
2022-04-11 18:13:17,954 - INFO - tqdm - accuracy: 0.9888, batch_loss: 0.0170, loss: 0.0375 ||:  98%|#########8| 64/65 [11:40<00:10, 10.96s/it]
2022-04-11 18:13:22,820 - INFO - tqdm - accuracy: 0.9888, batch_loss: 0.0806, loss: 0.0381 ||: 100%|##########| 65/65 [11:45<00:00,  9.13s/it]
2022-04-11 18:13:22,820 - INFO - tqdm - accuracy: 0.9888, batch_loss: 0.0806, loss: 0.0381 ||: 100%|##########| 65/65 [11:45<00:00, 10.85s/it]
2022-04-11 18:13:25,691 - INFO - allennlp.training.trainer - Validating
2022-04-11 18:13:25,692 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-11 18:13:35,887 - INFO - tqdm - accuracy: 0.7529, batch_loss: 2.4833, loss: 1.4712 ||:  37%|###7      | 43/115 [00:10<00:17,  4.15it/s]
2022-04-11 18:13:46,022 - INFO - tqdm - accuracy: 0.6509, batch_loss: 0.3005, loss: 2.0604 ||:  74%|#######3  | 85/115 [00:20<00:07,  4.13it/s]
2022-04-11 18:13:53,177 - INFO - tqdm - accuracy: 0.6419, batch_loss: 0.1138, loss: 1.9619 ||: 100%|##########| 115/115 [00:27<00:00,  4.13it/s]
2022-04-11 18:13:53,178 - INFO - tqdm - accuracy: 0.6419, batch_loss: 0.1138, loss: 1.9619 ||: 100%|##########| 115/115 [00:27<00:00,  4.18it/s]
2022-04-11 18:13:53,178 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-11 18:13:53,178 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.989  |     0.642
2022-04-11 18:13:53,179 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-11 18:13:53,179 - INFO - allennlp.training.tensorboard_writer - loss               |     0.038  |     1.962
2022-04-11 18:13:53,179 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5804.656  |       N/A
2022-04-11 18:13:59,243 - INFO - allennlp.training.trainer - Epoch duration: 0:12:21.533821
2022-04-11 18:13:59,244 - INFO - allennlp.training.trainer - Estimated training time remaining: 0:12:24
2022-04-11 18:13:59,244 - INFO - allennlp.training.trainer - Epoch 14/14
2022-04-11 18:13:59,244 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-11 18:13:59,244 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-11 18:13:59,245 - INFO - allennlp.training.trainer - Training
2022-04-11 18:13:59,246 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-11 18:14:10,058 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0028, loss: 0.0028 ||:   2%|1         | 1/65 [00:10<11:31, 10.81s/it]
2022-04-11 18:14:20,876 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0218, loss: 0.0123 ||:   3%|3         | 2/65 [00:21<11:21, 10.82s/it]
2022-04-11 18:14:31,721 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0067, loss: 0.0104 ||:   5%|4         | 3/65 [00:32<11:11, 10.83s/it]
2022-04-11 18:14:42,598 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0111, loss: 0.0106 ||:   6%|6         | 4/65 [00:43<11:01, 10.85s/it]
2022-04-11 18:14:53,480 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0057, loss: 0.0096 ||:   8%|7         | 5/65 [00:54<10:51, 10.86s/it]
2022-04-11 18:15:04,372 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0060, loss: 0.0090 ||:   9%|9         | 6/65 [01:05<10:41, 10.87s/it]
2022-04-11 18:15:15,273 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0208, loss: 0.0107 ||:  11%|#         | 7/65 [01:16<10:31, 10.88s/it]
2022-04-11 18:15:26,177 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0039, loss: 0.0098 ||:  12%|#2        | 8/65 [01:26<10:20, 10.89s/it]
2022-04-11 18:15:37,082 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0031, loss: 0.0091 ||:  14%|#3        | 9/65 [01:37<10:10, 10.89s/it]
2022-04-11 18:15:47,996 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0057, loss: 0.0088 ||:  15%|#5        | 10/65 [01:48<09:59, 10.90s/it]
2022-04-11 18:15:58,922 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0076, loss: 0.0086 ||:  17%|#6        | 11/65 [01:59<09:49, 10.91s/it]
2022-04-11 18:16:09,837 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0059, loss: 0.0084 ||:  18%|#8        | 12/65 [02:10<09:38, 10.91s/it]
2022-04-11 18:16:20,758 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0152, loss: 0.0089 ||:  20%|##        | 13/65 [02:21<09:27, 10.91s/it]
2022-04-11 18:16:31,702 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0192, loss: 0.0097 ||:  22%|##1       | 14/65 [02:32<09:17, 10.92s/it]
2022-04-11 18:16:42,639 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0114, loss: 0.0098 ||:  23%|##3       | 15/65 [02:43<09:06, 10.93s/it]
2022-04-11 18:16:53,575 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0093, loss: 0.0097 ||:  25%|##4       | 16/65 [02:54<08:55, 10.93s/it]
2022-04-11 18:17:04,502 - INFO - tqdm - accuracy: 0.9982, batch_loss: 0.0310, loss: 0.0110 ||:  26%|##6       | 17/65 [03:05<08:44, 10.93s/it]
2022-04-11 18:17:15,414 - INFO - tqdm - accuracy: 0.9983, batch_loss: 0.0033, loss: 0.0106 ||:  28%|##7       | 18/65 [03:16<08:33, 10.92s/it]
2022-04-11 18:17:26,358 - INFO - tqdm - accuracy: 0.9984, batch_loss: 0.0018, loss: 0.0101 ||:  29%|##9       | 19/65 [03:27<08:22, 10.93s/it]
2022-04-11 18:17:37,300 - INFO - tqdm - accuracy: 0.9984, batch_loss: 0.0029, loss: 0.0097 ||:  31%|###       | 20/65 [03:38<08:12, 10.93s/it]
2022-04-11 18:17:48,248 - INFO - tqdm - accuracy: 0.9985, batch_loss: 0.0062, loss: 0.0096 ||:  32%|###2      | 21/65 [03:49<08:01, 10.94s/it]
2022-04-11 18:17:59,074 - INFO - tqdm - accuracy: 0.9986, batch_loss: 0.0315, loss: 0.0106 ||:  34%|###3      | 22/65 [03:59<07:48, 10.90s/it]
2022-04-11 18:18:10,014 - INFO - tqdm - accuracy: 0.9986, batch_loss: 0.0051, loss: 0.0103 ||:  35%|###5      | 23/65 [04:10<07:38, 10.91s/it]
2022-04-11 18:18:20,977 - INFO - tqdm - accuracy: 0.9987, batch_loss: 0.0104, loss: 0.0103 ||:  37%|###6      | 24/65 [04:21<07:28, 10.93s/it]
2022-04-11 18:18:31,941 - INFO - tqdm - accuracy: 0.9988, batch_loss: 0.0142, loss: 0.0105 ||:  38%|###8      | 25/65 [04:32<07:17, 10.94s/it]
2022-04-11 18:18:42,897 - INFO - tqdm - accuracy: 0.9988, batch_loss: 0.0039, loss: 0.0102 ||:  40%|####      | 26/65 [04:43<07:06, 10.94s/it]
2022-04-11 18:18:53,866 - INFO - tqdm - accuracy: 0.9977, batch_loss: 0.0361, loss: 0.0112 ||:  42%|####1     | 27/65 [04:54<06:56, 10.95s/it]
2022-04-11 18:19:04,817 - INFO - tqdm - accuracy: 0.9978, batch_loss: 0.0114, loss: 0.0112 ||:  43%|####3     | 28/65 [05:05<06:45, 10.95s/it]
2022-04-11 18:19:15,805 - INFO - tqdm - accuracy: 0.9978, batch_loss: 0.0048, loss: 0.0110 ||:  45%|####4     | 29/65 [05:16<06:34, 10.96s/it]
2022-04-11 18:19:26,776 - INFO - tqdm - accuracy: 0.9969, batch_loss: 0.0439, loss: 0.0121 ||:  46%|####6     | 30/65 [05:27<06:23, 10.96s/it]
2022-04-11 18:19:37,752 - INFO - tqdm - accuracy: 0.9970, batch_loss: 0.0142, loss: 0.0122 ||:  48%|####7     | 31/65 [05:38<06:12, 10.97s/it]
2022-04-11 18:19:48,712 - INFO - tqdm - accuracy: 0.9971, batch_loss: 0.0130, loss: 0.0122 ||:  49%|####9     | 32/65 [05:49<06:01, 10.97s/it]
2022-04-11 18:19:59,669 - INFO - tqdm - accuracy: 0.9972, batch_loss: 0.0065, loss: 0.0120 ||:  51%|#####     | 33/65 [06:00<05:50, 10.96s/it]
2022-04-11 18:20:10,630 - INFO - tqdm - accuracy: 0.9972, batch_loss: 0.0019, loss: 0.0117 ||:  52%|#####2    | 34/65 [06:11<05:39, 10.96s/it]
2022-04-11 18:20:21,609 - INFO - tqdm - accuracy: 0.9973, batch_loss: 0.0302, loss: 0.0122 ||:  54%|#####3    | 35/65 [06:22<05:29, 10.97s/it]
2022-04-11 18:20:32,575 - INFO - tqdm - accuracy: 0.9965, batch_loss: 0.0433, loss: 0.0131 ||:  55%|#####5    | 36/65 [06:33<05:18, 10.97s/it]
2022-04-11 18:20:43,540 - INFO - tqdm - accuracy: 0.9966, batch_loss: 0.0056, loss: 0.0129 ||:  57%|#####6    | 37/65 [06:44<05:07, 10.97s/it]
2022-04-11 18:20:54,513 - INFO - tqdm - accuracy: 0.9967, batch_loss: 0.0075, loss: 0.0128 ||:  58%|#####8    | 38/65 [06:55<04:56, 10.97s/it]
2022-04-11 18:21:05,479 - INFO - tqdm - accuracy: 0.9968, batch_loss: 0.0072, loss: 0.0126 ||:  60%|######    | 39/65 [07:06<04:45, 10.97s/it]
2022-04-11 18:21:16,445 - INFO - tqdm - accuracy: 0.9969, batch_loss: 0.0070, loss: 0.0125 ||:  62%|######1   | 40/65 [07:17<04:34, 10.97s/it]
2022-04-11 18:21:27,419 - INFO - tqdm - accuracy: 0.9970, batch_loss: 0.0155, loss: 0.0125 ||:  63%|######3   | 41/65 [07:28<04:23, 10.97s/it]
2022-04-11 18:21:38,417 - INFO - tqdm - accuracy: 0.9963, batch_loss: 0.0681, loss: 0.0139 ||:  65%|######4   | 42/65 [07:39<04:12, 10.98s/it]
2022-04-11 18:21:49,410 - INFO - tqdm - accuracy: 0.9964, batch_loss: 0.0251, loss: 0.0141 ||:  66%|######6   | 43/65 [07:50<04:01, 10.98s/it]
2022-04-11 18:22:00,375 - INFO - tqdm - accuracy: 0.9964, batch_loss: 0.0034, loss: 0.0139 ||:  68%|######7   | 44/65 [08:01<03:50, 10.98s/it]
2022-04-11 18:22:11,340 - INFO - tqdm - accuracy: 0.9965, batch_loss: 0.0276, loss: 0.0142 ||:  69%|######9   | 45/65 [08:12<03:39, 10.97s/it]
2022-04-11 18:22:22,317 - INFO - tqdm - accuracy: 0.9966, batch_loss: 0.0032, loss: 0.0139 ||:  71%|#######   | 46/65 [08:23<03:28, 10.97s/it]
2022-04-11 18:22:33,280 - INFO - tqdm - accuracy: 0.9967, batch_loss: 0.0199, loss: 0.0141 ||:  72%|#######2  | 47/65 [08:34<03:17, 10.97s/it]
2022-04-11 18:22:44,246 - INFO - tqdm - accuracy: 0.9961, batch_loss: 0.1243, loss: 0.0164 ||:  74%|#######3  | 48/65 [08:45<03:06, 10.97s/it]
2022-04-11 18:22:55,227 - INFO - tqdm - accuracy: 0.9962, batch_loss: 0.0074, loss: 0.0162 ||:  75%|#######5  | 49/65 [08:55<02:55, 10.97s/it]
2022-04-11 18:23:06,205 - INFO - tqdm - accuracy: 0.9962, batch_loss: 0.0050, loss: 0.0160 ||:  77%|#######6  | 50/65 [09:06<02:44, 10.97s/it]
2022-04-11 18:23:17,162 - INFO - tqdm - accuracy: 0.9963, batch_loss: 0.0039, loss: 0.0157 ||:  78%|#######8  | 51/65 [09:17<02:33, 10.97s/it]
2022-04-11 18:23:27,690 - INFO - tqdm - accuracy: 0.9964, batch_loss: 0.0177, loss: 0.0158 ||:  80%|########  | 52/65 [09:28<02:20, 10.84s/it]
2022-04-11 18:23:38,668 - INFO - tqdm - accuracy: 0.9965, batch_loss: 0.0124, loss: 0.0157 ||:  82%|########1 | 53/65 [09:39<02:10, 10.88s/it]
2022-04-11 18:23:49,619 - INFO - tqdm - accuracy: 0.9959, batch_loss: 0.0522, loss: 0.0164 ||:  83%|########3 | 54/65 [09:50<01:59, 10.90s/it]
2022-04-11 18:24:00,576 - INFO - tqdm - accuracy: 0.9960, batch_loss: 0.0041, loss: 0.0162 ||:  85%|########4 | 55/65 [10:01<01:49, 10.92s/it]
2022-04-11 18:24:11,310 - INFO - tqdm - accuracy: 0.9961, batch_loss: 0.0305, loss: 0.0164 ||:  86%|########6 | 56/65 [10:12<01:37, 10.86s/it]
2022-04-11 18:24:22,229 - INFO - tqdm - accuracy: 0.9956, batch_loss: 0.0295, loss: 0.0166 ||:  88%|########7 | 57/65 [10:22<01:27, 10.88s/it]
2022-04-11 18:24:33,185 - INFO - tqdm - accuracy: 0.9951, batch_loss: 0.0437, loss: 0.0171 ||:  89%|########9 | 58/65 [10:33<01:16, 10.90s/it]
2022-04-11 18:24:44,163 - INFO - tqdm - accuracy: 0.9952, batch_loss: 0.0069, loss: 0.0169 ||:  91%|######### | 59/65 [10:44<01:05, 10.93s/it]
2022-04-11 18:24:55,113 - INFO - tqdm - accuracy: 0.9953, batch_loss: 0.0215, loss: 0.0170 ||:  92%|#########2| 60/65 [10:55<00:54, 10.93s/it]
2022-04-11 18:25:06,078 - INFO - tqdm - accuracy: 0.9949, batch_loss: 0.0352, loss: 0.0173 ||:  94%|#########3| 61/65 [11:06<00:43, 10.94s/it]
2022-04-11 18:25:17,034 - INFO - tqdm - accuracy: 0.9950, batch_loss: 0.0032, loss: 0.0171 ||:  95%|#########5| 62/65 [11:17<00:32, 10.95s/it]
2022-04-11 18:25:27,997 - INFO - tqdm - accuracy: 0.9950, batch_loss: 0.0031, loss: 0.0169 ||:  97%|#########6| 63/65 [11:28<00:21, 10.95s/it]
2022-04-11 18:25:38,955 - INFO - tqdm - accuracy: 0.9951, batch_loss: 0.0023, loss: 0.0166 ||:  98%|#########8| 64/65 [11:39<00:10, 10.95s/it]
2022-04-11 18:25:43,829 - INFO - tqdm - accuracy: 0.9951, batch_loss: 0.0206, loss: 0.0167 ||: 100%|##########| 65/65 [11:44<00:00,  9.13s/it]
2022-04-11 18:25:43,829 - INFO - tqdm - accuracy: 0.9951, batch_loss: 0.0206, loss: 0.0167 ||: 100%|##########| 65/65 [11:44<00:00, 10.84s/it]
2022-04-11 18:25:46,671 - INFO - allennlp.training.trainer - Validating
2022-04-11 18:25:46,673 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-11 18:25:56,776 - INFO - tqdm - accuracy: 0.6548, batch_loss: 3.2458, loss: 1.7860 ||:  37%|###6      | 42/115 [00:10<00:17,  4.16it/s]
2022-04-11 18:26:06,948 - INFO - tqdm - accuracy: 0.6746, batch_loss: 3.4775, loss: 1.7598 ||:  74%|#######3  | 85/115 [00:20<00:06,  4.64it/s]
2022-04-11 18:26:14,196 - INFO - tqdm - accuracy: 0.6419, batch_loss: 3.2519, loss: 1.9711 ||: 100%|##########| 115/115 [00:27<00:00,  4.13it/s]
2022-04-11 18:26:14,196 - INFO - tqdm - accuracy: 0.6419, batch_loss: 3.2519, loss: 1.9711 ||: 100%|##########| 115/115 [00:27<00:00,  4.18it/s]
2022-04-11 18:26:14,196 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-11 18:26:14,197 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.995  |     0.642
2022-04-11 18:26:14,197 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-11 18:26:14,198 - INFO - allennlp.training.tensorboard_writer - loss               |     0.017  |     1.971
2022-04-11 18:26:14,199 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5804.656  |       N/A
2022-04-11 18:26:20,278 - INFO - allennlp.training.trainer - Epoch duration: 0:12:21.034456
2022-04-11 18:26:20,278 - INFO - allennlp.training.checkpointer - loading best weights
2022-04-11 18:26:21,254 - INFO - allennlp.common.util - Metrics: {
  "best_epoch": 6,
  "peak_worker_0_memory_MB": 5804.65625,
  "peak_gpu_0_memory_MB": 9580.5712890625,
  "training_duration": "3:05:52.206239",
  "training_start_epoch": 0,
  "training_epochs": 14,
  "epoch": 14,
  "training_accuracy": 0.995147986414362,
  "training_loss": 0.016691513155665933,
  "training_worker_0_memory_MB": 5804.65625,
  "training_gpu_0_memory_MB": 9580.5712890625,
  "validation_accuracy": 0.6419213973799127,
  "validation_loss": 1.9711409045378272,
  "best_validation_accuracy": 0.6637554585152838,
  "best_validation_loss": 1.3622769601845548
}
2022-04-11 18:26:21,255 - INFO - allennlp.models.archival - archiving weights and vocabulary to ./output_ir-ora-d_hyper7/model.tar.gz
