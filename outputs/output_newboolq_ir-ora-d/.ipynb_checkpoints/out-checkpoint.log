2022-04-08 21:34:59,038 - INFO - allennlp.common.params - random_seed = 42
2022-04-08 21:34:59,038 - INFO - allennlp.common.params - numpy_seed = 42
2022-04-08 21:34:59,039 - INFO - allennlp.common.params - pytorch_seed = 42
2022-04-08 21:34:59,715 - INFO - allennlp.common.checks - Pytorch version: 1.7.1
2022-04-08 21:34:59,715 - INFO - allennlp.common.params - type = default
2022-04-08 21:34:59,715 - INFO - allennlp.common.params - dataset_reader.type = strategy_qa_reader
2022-04-08 21:34:59,716 - INFO - allennlp.common.params - dataset_reader.lazy = False
2022-04-08 21:34:59,716 - INFO - allennlp.common.params - dataset_reader.cache_directory = None
2022-04-08 21:34:59,716 - INFO - allennlp.common.params - dataset_reader.max_instances = None
2022-04-08 21:34:59,716 - INFO - allennlp.common.params - dataset_reader.manual_distributed_sharding = False
2022-04-08 21:34:59,716 - INFO - allennlp.common.params - dataset_reader.manual_multi_process_sharding = False
2022-04-08 21:34:59,716 - INFO - allennlp.common.params - dataset_reader.tokenizer_wrapper.type = default
2022-04-08 21:34:59,716 - INFO - allennlp.common.params - dataset_reader.tokenizer_wrapper.pretrained_model = roberta-large
2022-04-08 21:34:59,716 - INFO - allennlp.common.params - dataset_reader.tokenizer_wrapper.call_kwargs.return_offsets_mapping = True
2022-04-08 21:34:59,716 - INFO - allennlp.common.params - dataset_reader.tokenizer_wrapper.call_kwargs.return_special_tokens_mask = True
2022-04-08 21:34:59,716 - INFO - allennlp.common.params - dataset_reader.tokenizer_wrapper.call_kwargs.truncation = only_first
2022-04-08 21:35:00,915 - INFO - allennlp.common.params - dataset_reader.save_tokenizer = True
2022-04-08 21:35:00,915 - INFO - allennlp.common.params - dataset_reader.is_training = True
2022-04-08 21:35:00,915 - INFO - allennlp.common.params - dataset_reader.pickle.action = None
2022-04-08 21:35:00,915 - INFO - allennlp.common.params - dataset_reader.pickle.file_name = None
2022-04-08 21:35:00,915 - INFO - allennlp.common.params - dataset_reader.pickle.path = ../pickle
2022-04-08 21:35:00,915 - INFO - allennlp.common.params - dataset_reader.pickle.save_even_when_max_instances = False
2022-04-08 21:35:00,915 - INFO - allennlp.common.params - dataset_reader.paragraphs_source = IR-ORA-D
2022-04-08 21:35:00,915 - INFO - allennlp.common.params - dataset_reader.answer_last_decomposition_step = False
2022-04-08 21:35:00,915 - INFO - allennlp.common.params - dataset_reader.skip_if_context_missing = True
2022-04-08 21:35:00,915 - INFO - allennlp.common.params - dataset_reader.paragraphs_limit = 10
2022-04-08 21:35:00,916 - INFO - allennlp.common.params - dataset_reader.generated_decompositions_paths = None
2022-04-08 21:35:00,916 - INFO - allennlp.common.params - dataset_reader.save_elasticsearch_cache = True
2022-04-08 21:35:01,370 - INFO - filelock - Lock 140015486777168 acquired on cache.lock
2022-04-08 21:35:02,217 - INFO - filelock - Lock 140015486777168 released on cache.lock
2022-04-08 21:35:02,217 - INFO - allennlp.common.params - train_data_path = data/strategyqa/train.json
2022-04-08 21:35:02,218 - INFO - allennlp.common.params - vocabulary = <allennlp.common.lazy.Lazy object at 0x7f57f01e2d10>
2022-04-08 21:35:02,218 - INFO - allennlp.common.params - datasets_for_vocab_creation = None
2022-04-08 21:35:02,218 - INFO - allennlp.common.params - validation_dataset_reader.type = strategy_qa_reader
2022-04-08 21:35:02,218 - INFO - allennlp.common.params - validation_dataset_reader.lazy = False
2022-04-08 21:35:02,218 - INFO - allennlp.common.params - validation_dataset_reader.cache_directory = None
2022-04-08 21:35:02,218 - INFO - allennlp.common.params - validation_dataset_reader.max_instances = None
2022-04-08 21:35:02,219 - INFO - allennlp.common.params - validation_dataset_reader.manual_distributed_sharding = False
2022-04-08 21:35:02,219 - INFO - allennlp.common.params - validation_dataset_reader.manual_multi_process_sharding = False
2022-04-08 21:35:02,219 - INFO - allennlp.common.params - validation_dataset_reader.tokenizer_wrapper.type = default
2022-04-08 21:35:02,219 - INFO - allennlp.common.params - validation_dataset_reader.tokenizer_wrapper.pretrained_model = roberta-large
2022-04-08 21:35:02,219 - INFO - allennlp.common.params - validation_dataset_reader.tokenizer_wrapper.call_kwargs.return_offsets_mapping = True
2022-04-08 21:35:02,219 - INFO - allennlp.common.params - validation_dataset_reader.tokenizer_wrapper.call_kwargs.return_special_tokens_mask = True
2022-04-08 21:35:02,219 - INFO - allennlp.common.params - validation_dataset_reader.tokenizer_wrapper.call_kwargs.truncation = only_first
2022-04-08 21:35:03,376 - INFO - allennlp.common.params - validation_dataset_reader.save_tokenizer = False
2022-04-08 21:35:03,377 - INFO - allennlp.common.params - validation_dataset_reader.is_training = False
2022-04-08 21:35:03,377 - INFO - allennlp.common.params - validation_dataset_reader.pickle.action = None
2022-04-08 21:35:03,377 - INFO - allennlp.common.params - validation_dataset_reader.pickle.file_name = None
2022-04-08 21:35:03,377 - INFO - allennlp.common.params - validation_dataset_reader.pickle.path = ../pickle
2022-04-08 21:35:03,377 - INFO - allennlp.common.params - validation_dataset_reader.pickle.save_even_when_max_instances = False
2022-04-08 21:35:03,377 - INFO - allennlp.common.params - validation_dataset_reader.paragraphs_source = IR-ORA-D
2022-04-08 21:35:03,377 - INFO - allennlp.common.params - validation_dataset_reader.answer_last_decomposition_step = False
2022-04-08 21:35:03,377 - INFO - allennlp.common.params - validation_dataset_reader.skip_if_context_missing = True
2022-04-08 21:35:03,377 - INFO - allennlp.common.params - validation_dataset_reader.paragraphs_limit = 10
2022-04-08 21:35:03,377 - INFO - allennlp.common.params - validation_dataset_reader.generated_decompositions_paths = None
2022-04-08 21:35:03,377 - INFO - allennlp.common.params - validation_dataset_reader.save_elasticsearch_cache = True
2022-04-08 21:35:03,377 - INFO - filelock - Lock 140015489015440 acquired on cache.lock
2022-04-08 21:35:04,204 - INFO - filelock - Lock 140015489015440 released on cache.lock
2022-04-08 21:35:04,205 - INFO - allennlp.common.params - validation_data_path = data/strategyqa/dev.json
2022-04-08 21:35:04,205 - INFO - allennlp.common.params - validation_data_loader = None
2022-04-08 21:35:04,205 - INFO - allennlp.common.params - test_data_path = None
2022-04-08 21:35:04,205 - INFO - allennlp.common.params - evaluate_on_test = True
2022-04-08 21:35:04,205 - INFO - allennlp.common.params - batch_weight_key = 
2022-04-08 21:35:04,205 - INFO - allennlp.training.util - Reading training data from data/strategyqa/train.json
2022-04-08 21:35:04,205 - INFO - tqdm - reading instances: 0it [00:00, ?it/s]
2022-04-08 21:35:04,206 - INFO - src.data.dataset_readers.base.base_dataset_reader - Reading the dataset from scratch
2022-04-08 21:35:04,206 - INFO - src.data.dataset_readers.strategy_qa_reader - Reading the dataset:
2022-04-08 21:35:04,206 - INFO - src.data.dataset_readers.strategy_qa_reader - Reading file at data/strategyqa/train.json
2022-04-08 21:35:11,374 - INFO - filelock - Lock 140014828086416 acquired on cache.lock
2022-04-08 21:35:12,150 - INFO - filelock - Lock 140014828086416 released on cache.lock
2022-04-08 21:35:12,186 - INFO - allennlp.training.util - Reading validation data from data/strategyqa/dev.json
2022-04-08 21:35:12,186 - INFO - tqdm - reading instances: 0it [00:00, ?it/s]
2022-04-08 21:35:12,186 - INFO - src.data.dataset_readers.base.base_dataset_reader - Reading the dataset from scratch
2022-04-08 21:35:12,186 - INFO - src.data.dataset_readers.strategy_qa_reader - Reading the dataset:
2022-04-08 21:35:12,186 - INFO - src.data.dataset_readers.strategy_qa_reader - Reading file at data/strategyqa/dev.json
2022-04-08 21:35:12,616 - INFO - src.data.dataset_readers.utils.elasticsearch_utils - Empty results from Elasticsearch for: actions commercial pilots take concerning engine noise arriving departing Orange County, California?
2022-04-08 21:35:12,941 - INFO - filelock - Lock 140014835985424 acquired on cache.lock
2022-04-08 21:35:13,695 - INFO - filelock - Lock 140014835985424 released on cache.lock
2022-04-08 21:35:13,729 - INFO - allennlp.common.params - type = from_instances
2022-04-08 21:35:13,730 - INFO - allennlp.common.params - min_count = None
2022-04-08 21:35:13,730 - INFO - allennlp.common.params - max_vocab_size = None
2022-04-08 21:35:13,730 - INFO - allennlp.common.params - non_padded_namespaces = ('*tags', '*labels')
2022-04-08 21:35:13,730 - INFO - allennlp.common.params - pretrained_files = None
2022-04-08 21:35:13,730 - INFO - allennlp.common.params - only_include_pretrained_words = False
2022-04-08 21:35:13,731 - INFO - allennlp.common.params - tokens_to_add = None
2022-04-08 21:35:13,731 - INFO - allennlp.common.params - min_pretrained_embeddings = None
2022-04-08 21:35:13,731 - INFO - allennlp.common.params - padding_token = @@PADDING@@
2022-04-08 21:35:13,731 - INFO - allennlp.common.params - oov_token = @@UNKNOWN@@
2022-04-08 21:35:13,731 - INFO - allennlp.data.vocabulary - Fitting token dictionary from dataset.
2022-04-08 21:35:13,731 - INFO - tqdm - building vocab: 0it [00:00, ?it/s]
2022-04-08 21:35:13,737 - INFO - allennlp.common.params - model.type = hf_classifier
2022-04-08 21:35:13,737 - INFO - allennlp.common.params - model.regularizer = None
2022-04-08 21:35:13,737 - INFO - allennlp.common.params - model.pretrained_model = roberta-large
2022-04-08 21:35:13,738 - INFO - allennlp.common.params - model.tokenizer_wrapper.type = default
2022-04-08 21:35:13,738 - INFO - allennlp.common.params - model.tokenizer_wrapper.pretrained_model = roberta-large
2022-04-08 21:35:13,738 - INFO - allennlp.common.params - model.tokenizer_wrapper.call_kwargs.return_offsets_mapping = True
2022-04-08 21:35:13,738 - INFO - allennlp.common.params - model.tokenizer_wrapper.call_kwargs.return_special_tokens_mask = True
2022-04-08 21:35:13,738 - INFO - allennlp.common.params - model.tokenizer_wrapper.call_kwargs.truncation = only_first
2022-04-08 21:35:14,924 - INFO - allennlp.common.params - model.num_labels = 2
2022-04-08 21:35:14,924 - INFO - allennlp.common.params - model.label_namespace = labels
2022-04-08 21:35:14,924 - INFO - allennlp.common.params - model.transformer_weights_path = None
2022-04-08 21:35:14,925 - INFO - allennlp.common.params - model.initializer.regexes.0.1.type = pretrained
2022-04-08 21:35:14,925 - INFO - allennlp.common.params - model.initializer.regexes.0.1.weights_file_path = output_newboolq/model.tar.gz
2022-04-08 21:35:14,926 - INFO - allennlp.models.archival - extracting archive file output_newboolq/model.tar.gz to temp dir /tmp/tmprkd1uknq
2022-04-08 21:35:26,460 - INFO - allennlp.models.archival - removing temporary unarchived model dir at /tmp/tmprkd1uknq
2022-04-08 21:35:26,629 - INFO - allennlp.common.params - model.initializer.prevent_regexes = None
2022-04-08 21:35:40,973 - INFO - allennlp.nn.initializers - Initializing parameters
2022-04-08 21:35:40,973 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.embeddings.word_embeddings.weight using .* initializer
2022-04-08 21:35:40,982 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.embeddings.position_embeddings.weight using .* initializer
2022-04-08 21:35:40,983 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.embeddings.token_type_embeddings.weight using .* initializer
2022-04-08 21:35:40,983 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.embeddings.LayerNorm.weight using .* initializer
2022-04-08 21:35:40,983 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.embeddings.LayerNorm.bias using .* initializer
2022-04-08 21:35:40,983 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.self.query.weight using .* initializer
2022-04-08 21:35:40,984 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.self.query.bias using .* initializer
2022-04-08 21:35:40,984 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.self.key.weight using .* initializer
2022-04-08 21:35:40,984 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.self.key.bias using .* initializer
2022-04-08 21:35:40,984 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.self.value.weight using .* initializer
2022-04-08 21:35:40,985 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.self.value.bias using .* initializer
2022-04-08 21:35:40,985 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.output.dense.weight using .* initializer
2022-04-08 21:35:40,985 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.output.dense.bias using .* initializer
2022-04-08 21:35:40,985 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.output.LayerNorm.weight using .* initializer
2022-04-08 21:35:40,986 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.output.LayerNorm.bias using .* initializer
2022-04-08 21:35:40,986 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.intermediate.dense.weight using .* initializer
2022-04-08 21:35:40,987 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.intermediate.dense.bias using .* initializer
2022-04-08 21:35:40,987 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.output.dense.weight using .* initializer
2022-04-08 21:35:40,988 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.output.dense.bias using .* initializer
2022-04-08 21:35:40,988 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.output.LayerNorm.weight using .* initializer
2022-04-08 21:35:40,988 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.output.LayerNorm.bias using .* initializer
2022-04-08 21:35:40,988 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.self.query.weight using .* initializer
2022-04-08 21:35:40,988 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.self.query.bias using .* initializer
2022-04-08 21:35:40,988 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.self.key.weight using .* initializer
2022-04-08 21:35:40,989 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.self.key.bias using .* initializer
2022-04-08 21:35:40,989 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.self.value.weight using .* initializer
2022-04-08 21:35:40,989 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.self.value.bias using .* initializer
2022-04-08 21:35:40,989 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.output.dense.weight using .* initializer
2022-04-08 21:35:40,990 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.output.dense.bias using .* initializer
2022-04-08 21:35:40,990 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.output.LayerNorm.weight using .* initializer
2022-04-08 21:35:40,990 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.output.LayerNorm.bias using .* initializer
2022-04-08 21:35:40,990 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.intermediate.dense.weight using .* initializer
2022-04-08 21:35:40,991 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.intermediate.dense.bias using .* initializer
2022-04-08 21:35:40,991 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.output.dense.weight using .* initializer
2022-04-08 21:35:40,992 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.output.dense.bias using .* initializer
2022-04-08 21:35:40,992 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.output.LayerNorm.weight using .* initializer
2022-04-08 21:35:40,992 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.output.LayerNorm.bias using .* initializer
2022-04-08 21:35:40,993 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.self.query.weight using .* initializer
2022-04-08 21:35:40,993 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.self.query.bias using .* initializer
2022-04-08 21:35:40,993 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.self.key.weight using .* initializer
2022-04-08 21:35:40,993 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.self.key.bias using .* initializer
2022-04-08 21:35:40,993 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.self.value.weight using .* initializer
2022-04-08 21:35:40,994 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.self.value.bias using .* initializer
2022-04-08 21:35:40,994 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.output.dense.weight using .* initializer
2022-04-08 21:35:40,994 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.output.dense.bias using .* initializer
2022-04-08 21:35:40,994 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.output.LayerNorm.weight using .* initializer
2022-04-08 21:35:40,995 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.output.LayerNorm.bias using .* initializer
2022-04-08 21:35:40,995 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.intermediate.dense.weight using .* initializer
2022-04-08 21:35:40,996 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.intermediate.dense.bias using .* initializer
2022-04-08 21:35:40,996 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.output.dense.weight using .* initializer
2022-04-08 21:35:40,997 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.output.dense.bias using .* initializer
2022-04-08 21:35:40,997 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.output.LayerNorm.weight using .* initializer
2022-04-08 21:35:40,997 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.output.LayerNorm.bias using .* initializer
2022-04-08 21:35:40,997 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.self.query.weight using .* initializer
2022-04-08 21:35:40,997 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.self.query.bias using .* initializer
2022-04-08 21:35:40,997 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.self.key.weight using .* initializer
2022-04-08 21:35:40,998 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.self.key.bias using .* initializer
2022-04-08 21:35:40,998 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.self.value.weight using .* initializer
2022-04-08 21:35:40,998 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.self.value.bias using .* initializer
2022-04-08 21:35:40,998 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.output.dense.weight using .* initializer
2022-04-08 21:35:40,999 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.output.dense.bias using .* initializer
2022-04-08 21:35:40,999 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.output.LayerNorm.weight using .* initializer
2022-04-08 21:35:40,999 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.output.LayerNorm.bias using .* initializer
2022-04-08 21:35:40,999 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.intermediate.dense.weight using .* initializer
2022-04-08 21:35:41,000 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.intermediate.dense.bias using .* initializer
2022-04-08 21:35:41,000 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.output.dense.weight using .* initializer
2022-04-08 21:35:41,001 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.output.dense.bias using .* initializer
2022-04-08 21:35:41,001 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.output.LayerNorm.weight using .* initializer
2022-04-08 21:35:41,001 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.output.LayerNorm.bias using .* initializer
2022-04-08 21:35:41,001 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.self.query.weight using .* initializer
2022-04-08 21:35:41,002 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.self.query.bias using .* initializer
2022-04-08 21:35:41,002 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.self.key.weight using .* initializer
2022-04-08 21:35:41,002 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.self.key.bias using .* initializer
2022-04-08 21:35:41,002 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.self.value.weight using .* initializer
2022-04-08 21:35:41,003 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.self.value.bias using .* initializer
2022-04-08 21:35:41,003 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.output.dense.weight using .* initializer
2022-04-08 21:35:41,003 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.output.dense.bias using .* initializer
2022-04-08 21:35:41,003 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.output.LayerNorm.weight using .* initializer
2022-04-08 21:35:41,003 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.output.LayerNorm.bias using .* initializer
2022-04-08 21:35:41,004 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.intermediate.dense.weight using .* initializer
2022-04-08 21:35:41,004 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.intermediate.dense.bias using .* initializer
2022-04-08 21:35:41,005 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.output.dense.weight using .* initializer
2022-04-08 21:35:41,005 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.output.dense.bias using .* initializer
2022-04-08 21:35:41,006 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.output.LayerNorm.weight using .* initializer
2022-04-08 21:35:41,006 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.output.LayerNorm.bias using .* initializer
2022-04-08 21:35:41,006 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.self.query.weight using .* initializer
2022-04-08 21:35:41,006 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.self.query.bias using .* initializer
2022-04-08 21:35:41,006 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.self.key.weight using .* initializer
2022-04-08 21:35:41,007 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.self.key.bias using .* initializer
2022-04-08 21:35:41,007 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.self.value.weight using .* initializer
2022-04-08 21:35:41,007 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.self.value.bias using .* initializer
2022-04-08 21:35:41,007 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.output.dense.weight using .* initializer
2022-04-08 21:35:41,008 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.output.dense.bias using .* initializer
2022-04-08 21:35:41,008 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.output.LayerNorm.weight using .* initializer
2022-04-08 21:35:41,008 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.output.LayerNorm.bias using .* initializer
2022-04-08 21:35:41,008 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.intermediate.dense.weight using .* initializer
2022-04-08 21:35:41,009 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.intermediate.dense.bias using .* initializer
2022-04-08 21:35:41,009 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.output.dense.weight using .* initializer
2022-04-08 21:35:41,010 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.output.dense.bias using .* initializer
2022-04-08 21:35:41,010 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.output.LayerNorm.weight using .* initializer
2022-04-08 21:35:41,010 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.output.LayerNorm.bias using .* initializer
2022-04-08 21:35:41,010 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.self.query.weight using .* initializer
2022-04-08 21:35:41,011 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.self.query.bias using .* initializer
2022-04-08 21:35:41,011 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.self.key.weight using .* initializer
2022-04-08 21:35:41,011 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.self.key.bias using .* initializer
2022-04-08 21:35:41,011 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.self.value.weight using .* initializer
2022-04-08 21:35:41,012 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.self.value.bias using .* initializer
2022-04-08 21:35:41,012 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.output.dense.weight using .* initializer
2022-04-08 21:35:41,012 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.output.dense.bias using .* initializer
2022-04-08 21:35:41,012 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.output.LayerNorm.weight using .* initializer
2022-04-08 21:35:41,012 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.output.LayerNorm.bias using .* initializer
2022-04-08 21:35:41,013 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.intermediate.dense.weight using .* initializer
2022-04-08 21:35:41,013 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.intermediate.dense.bias using .* initializer
2022-04-08 21:35:41,014 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.output.dense.weight using .* initializer
2022-04-08 21:35:41,014 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.output.dense.bias using .* initializer
2022-04-08 21:35:41,015 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.output.LayerNorm.weight using .* initializer
2022-04-08 21:35:41,015 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.output.LayerNorm.bias using .* initializer
2022-04-08 21:35:41,015 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.self.query.weight using .* initializer
2022-04-08 21:35:41,015 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.self.query.bias using .* initializer
2022-04-08 21:35:41,015 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.self.key.weight using .* initializer
2022-04-08 21:35:41,016 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.self.key.bias using .* initializer
2022-04-08 21:35:41,016 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.self.value.weight using .* initializer
2022-04-08 21:35:41,016 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.self.value.bias using .* initializer
2022-04-08 21:35:41,016 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.output.dense.weight using .* initializer
2022-04-08 21:35:41,017 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.output.dense.bias using .* initializer
2022-04-08 21:35:41,017 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.output.LayerNorm.weight using .* initializer
2022-04-08 21:35:41,017 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.output.LayerNorm.bias using .* initializer
2022-04-08 21:35:41,017 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.intermediate.dense.weight using .* initializer
2022-04-08 21:35:41,018 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.intermediate.dense.bias using .* initializer
2022-04-08 21:35:41,018 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.output.dense.weight using .* initializer
2022-04-08 21:35:41,019 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.output.dense.bias using .* initializer
2022-04-08 21:35:41,019 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.output.LayerNorm.weight using .* initializer
2022-04-08 21:35:41,019 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.output.LayerNorm.bias using .* initializer
2022-04-08 21:35:41,019 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.self.query.weight using .* initializer
2022-04-08 21:35:41,020 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.self.query.bias using .* initializer
2022-04-08 21:35:41,020 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.self.key.weight using .* initializer
2022-04-08 21:35:41,020 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.self.key.bias using .* initializer
2022-04-08 21:35:41,020 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.self.value.weight using .* initializer
2022-04-08 21:35:41,020 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.self.value.bias using .* initializer
2022-04-08 21:35:41,021 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.output.dense.weight using .* initializer
2022-04-08 21:35:41,021 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.output.dense.bias using .* initializer
2022-04-08 21:35:41,021 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.output.LayerNorm.weight using .* initializer
2022-04-08 21:35:41,021 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.output.LayerNorm.bias using .* initializer
2022-04-08 21:35:41,021 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.intermediate.dense.weight using .* initializer
2022-04-08 21:35:41,022 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.intermediate.dense.bias using .* initializer
2022-04-08 21:35:41,022 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.output.dense.weight using .* initializer
2022-04-08 21:35:41,023 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.output.dense.bias using .* initializer
2022-04-08 21:35:41,023 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.output.LayerNorm.weight using .* initializer
2022-04-08 21:35:41,024 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.output.LayerNorm.bias using .* initializer
2022-04-08 21:35:41,024 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.self.query.weight using .* initializer
2022-04-08 21:35:41,024 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.self.query.bias using .* initializer
2022-04-08 21:35:41,024 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.self.key.weight using .* initializer
2022-04-08 21:35:41,024 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.self.key.bias using .* initializer
2022-04-08 21:35:41,025 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.self.value.weight using .* initializer
2022-04-08 21:35:41,025 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.self.value.bias using .* initializer
2022-04-08 21:35:41,025 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.output.dense.weight using .* initializer
2022-04-08 21:35:41,025 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.output.dense.bias using .* initializer
2022-04-08 21:35:41,026 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.output.LayerNorm.weight using .* initializer
2022-04-08 21:35:41,026 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.output.LayerNorm.bias using .* initializer
2022-04-08 21:35:41,026 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.intermediate.dense.weight using .* initializer
2022-04-08 21:35:41,027 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.intermediate.dense.bias using .* initializer
2022-04-08 21:35:41,027 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.output.dense.weight using .* initializer
2022-04-08 21:35:41,028 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.output.dense.bias using .* initializer
2022-04-08 21:35:41,028 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.output.LayerNorm.weight using .* initializer
2022-04-08 21:35:41,028 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.output.LayerNorm.bias using .* initializer
2022-04-08 21:35:41,028 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.self.query.weight using .* initializer
2022-04-08 21:35:41,028 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.self.query.bias using .* initializer
2022-04-08 21:35:41,029 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.self.key.weight using .* initializer
2022-04-08 21:35:41,029 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.self.key.bias using .* initializer
2022-04-08 21:35:41,029 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.self.value.weight using .* initializer
2022-04-08 21:35:41,029 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.self.value.bias using .* initializer
2022-04-08 21:35:41,030 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.output.dense.weight using .* initializer
2022-04-08 21:35:41,030 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.output.dense.bias using .* initializer
2022-04-08 21:35:41,030 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.output.LayerNorm.weight using .* initializer
2022-04-08 21:35:41,030 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.output.LayerNorm.bias using .* initializer
2022-04-08 21:35:41,030 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.intermediate.dense.weight using .* initializer
2022-04-08 21:35:41,031 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.intermediate.dense.bias using .* initializer
2022-04-08 21:35:41,031 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.output.dense.weight using .* initializer
2022-04-08 21:35:41,032 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.output.dense.bias using .* initializer
2022-04-08 21:35:41,032 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.output.LayerNorm.weight using .* initializer
2022-04-08 21:35:41,032 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.output.LayerNorm.bias using .* initializer
2022-04-08 21:35:41,033 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.self.query.weight using .* initializer
2022-04-08 21:35:41,033 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.self.query.bias using .* initializer
2022-04-08 21:35:41,033 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.self.key.weight using .* initializer
2022-04-08 21:35:41,033 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.self.key.bias using .* initializer
2022-04-08 21:35:41,034 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.self.value.weight using .* initializer
2022-04-08 21:35:41,034 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.self.value.bias using .* initializer
2022-04-08 21:35:41,034 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.output.dense.weight using .* initializer
2022-04-08 21:35:41,034 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.output.dense.bias using .* initializer
2022-04-08 21:35:41,034 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.output.LayerNorm.weight using .* initializer
2022-04-08 21:35:41,035 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.output.LayerNorm.bias using .* initializer
2022-04-08 21:35:41,035 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.intermediate.dense.weight using .* initializer
2022-04-08 21:35:41,036 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.intermediate.dense.bias using .* initializer
2022-04-08 21:35:41,036 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.output.dense.weight using .* initializer
2022-04-08 21:35:41,037 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.output.dense.bias using .* initializer
2022-04-08 21:35:41,037 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.output.LayerNorm.weight using .* initializer
2022-04-08 21:35:41,037 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.output.LayerNorm.bias using .* initializer
2022-04-08 21:35:41,037 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.self.query.weight using .* initializer
2022-04-08 21:35:41,037 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.self.query.bias using .* initializer
2022-04-08 21:35:41,037 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.self.key.weight using .* initializer
2022-04-08 21:35:41,038 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.self.key.bias using .* initializer
2022-04-08 21:35:41,038 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.self.value.weight using .* initializer
2022-04-08 21:35:41,038 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.self.value.bias using .* initializer
2022-04-08 21:35:41,038 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.output.dense.weight using .* initializer
2022-04-08 21:35:41,039 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.output.dense.bias using .* initializer
2022-04-08 21:35:41,039 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.output.LayerNorm.weight using .* initializer
2022-04-08 21:35:41,039 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.output.LayerNorm.bias using .* initializer
2022-04-08 21:35:41,039 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.intermediate.dense.weight using .* initializer
2022-04-08 21:35:41,040 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.intermediate.dense.bias using .* initializer
2022-04-08 21:35:41,040 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.output.dense.weight using .* initializer
2022-04-08 21:35:41,041 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.output.dense.bias using .* initializer
2022-04-08 21:35:41,041 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.output.LayerNorm.weight using .* initializer
2022-04-08 21:35:41,041 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.output.LayerNorm.bias using .* initializer
2022-04-08 21:35:41,041 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.self.query.weight using .* initializer
2022-04-08 21:35:41,042 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.self.query.bias using .* initializer
2022-04-08 21:35:41,042 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.self.key.weight using .* initializer
2022-04-08 21:35:41,042 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.self.key.bias using .* initializer
2022-04-08 21:35:41,042 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.self.value.weight using .* initializer
2022-04-08 21:35:41,043 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.self.value.bias using .* initializer
2022-04-08 21:35:41,043 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.output.dense.weight using .* initializer
2022-04-08 21:35:41,043 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.output.dense.bias using .* initializer
2022-04-08 21:35:41,043 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.output.LayerNorm.weight using .* initializer
2022-04-08 21:35:41,044 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.output.LayerNorm.bias using .* initializer
2022-04-08 21:35:41,044 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.intermediate.dense.weight using .* initializer
2022-04-08 21:35:41,045 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.intermediate.dense.bias using .* initializer
2022-04-08 21:35:41,045 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.output.dense.weight using .* initializer
2022-04-08 21:35:41,046 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.output.dense.bias using .* initializer
2022-04-08 21:35:41,046 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.output.LayerNorm.weight using .* initializer
2022-04-08 21:35:41,046 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.output.LayerNorm.bias using .* initializer
2022-04-08 21:35:41,046 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.self.query.weight using .* initializer
2022-04-08 21:35:41,047 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.self.query.bias using .* initializer
2022-04-08 21:35:41,047 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.self.key.weight using .* initializer
2022-04-08 21:35:41,047 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.self.key.bias using .* initializer
2022-04-08 21:35:41,047 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.self.value.weight using .* initializer
2022-04-08 21:35:41,048 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.self.value.bias using .* initializer
2022-04-08 21:35:41,048 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.output.dense.weight using .* initializer
2022-04-08 21:35:41,048 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.output.dense.bias using .* initializer
2022-04-08 21:35:41,048 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.output.LayerNorm.weight using .* initializer
2022-04-08 21:35:41,048 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.output.LayerNorm.bias using .* initializer
2022-04-08 21:35:41,049 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.intermediate.dense.weight using .* initializer
2022-04-08 21:35:41,049 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.intermediate.dense.bias using .* initializer
2022-04-08 21:35:41,050 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.output.dense.weight using .* initializer
2022-04-08 21:35:41,050 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.output.dense.bias using .* initializer
2022-04-08 21:35:41,051 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.output.LayerNorm.weight using .* initializer
2022-04-08 21:35:41,051 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.output.LayerNorm.bias using .* initializer
2022-04-08 21:35:41,051 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.self.query.weight using .* initializer
2022-04-08 21:35:41,051 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.self.query.bias using .* initializer
2022-04-08 21:35:41,051 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.self.key.weight using .* initializer
2022-04-08 21:35:41,052 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.self.key.bias using .* initializer
2022-04-08 21:35:41,052 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.self.value.weight using .* initializer
2022-04-08 21:35:41,052 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.self.value.bias using .* initializer
2022-04-08 21:35:41,052 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.output.dense.weight using .* initializer
2022-04-08 21:35:41,053 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.output.dense.bias using .* initializer
2022-04-08 21:35:41,053 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.output.LayerNorm.weight using .* initializer
2022-04-08 21:35:41,053 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.output.LayerNorm.bias using .* initializer
2022-04-08 21:35:41,053 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.intermediate.dense.weight using .* initializer
2022-04-08 21:35:41,054 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.intermediate.dense.bias using .* initializer
2022-04-08 21:35:41,054 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.output.dense.weight using .* initializer
2022-04-08 21:35:41,055 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.output.dense.bias using .* initializer
2022-04-08 21:35:41,055 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.output.LayerNorm.weight using .* initializer
2022-04-08 21:35:41,055 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.output.LayerNorm.bias using .* initializer
2022-04-08 21:35:41,055 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.self.query.weight using .* initializer
2022-04-08 21:35:41,056 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.self.query.bias using .* initializer
2022-04-08 21:35:41,056 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.self.key.weight using .* initializer
2022-04-08 21:35:41,056 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.self.key.bias using .* initializer
2022-04-08 21:35:41,056 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.self.value.weight using .* initializer
2022-04-08 21:35:41,057 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.self.value.bias using .* initializer
2022-04-08 21:35:41,057 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.output.dense.weight using .* initializer
2022-04-08 21:35:41,057 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.output.dense.bias using .* initializer
2022-04-08 21:35:41,057 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.output.LayerNorm.weight using .* initializer
2022-04-08 21:35:41,057 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.output.LayerNorm.bias using .* initializer
2022-04-08 21:35:41,058 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.intermediate.dense.weight using .* initializer
2022-04-08 21:35:41,058 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.intermediate.dense.bias using .* initializer
2022-04-08 21:35:41,059 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.output.dense.weight using .* initializer
2022-04-08 21:35:41,059 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.output.dense.bias using .* initializer
2022-04-08 21:35:41,060 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.output.LayerNorm.weight using .* initializer
2022-04-08 21:35:41,060 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.output.LayerNorm.bias using .* initializer
2022-04-08 21:35:41,060 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.self.query.weight using .* initializer
2022-04-08 21:35:41,060 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.self.query.bias using .* initializer
2022-04-08 21:35:41,060 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.self.key.weight using .* initializer
2022-04-08 21:35:41,061 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.self.key.bias using .* initializer
2022-04-08 21:35:41,061 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.self.value.weight using .* initializer
2022-04-08 21:35:41,061 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.self.value.bias using .* initializer
2022-04-08 21:35:41,061 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.output.dense.weight using .* initializer
2022-04-08 21:35:41,062 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.output.dense.bias using .* initializer
2022-04-08 21:35:41,062 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.output.LayerNorm.weight using .* initializer
2022-04-08 21:35:41,062 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.output.LayerNorm.bias using .* initializer
2022-04-08 21:35:41,062 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.intermediate.dense.weight using .* initializer
2022-04-08 21:35:41,063 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.intermediate.dense.bias using .* initializer
2022-04-08 21:35:41,063 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.output.dense.weight using .* initializer
2022-04-08 21:35:41,064 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.output.dense.bias using .* initializer
2022-04-08 21:35:41,064 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.output.LayerNorm.weight using .* initializer
2022-04-08 21:35:41,064 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.output.LayerNorm.bias using .* initializer
2022-04-08 21:35:41,064 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.self.query.weight using .* initializer
2022-04-08 21:35:41,065 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.self.query.bias using .* initializer
2022-04-08 21:35:41,065 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.self.key.weight using .* initializer
2022-04-08 21:35:41,065 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.self.key.bias using .* initializer
2022-04-08 21:35:41,065 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.self.value.weight using .* initializer
2022-04-08 21:35:41,066 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.self.value.bias using .* initializer
2022-04-08 21:35:41,066 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.output.dense.weight using .* initializer
2022-04-08 21:35:41,066 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.output.dense.bias using .* initializer
2022-04-08 21:35:41,066 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.output.LayerNorm.weight using .* initializer
2022-04-08 21:35:41,066 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.output.LayerNorm.bias using .* initializer
2022-04-08 21:35:41,067 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.intermediate.dense.weight using .* initializer
2022-04-08 21:35:41,067 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.intermediate.dense.bias using .* initializer
2022-04-08 21:35:41,068 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.output.dense.weight using .* initializer
2022-04-08 21:35:41,068 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.output.dense.bias using .* initializer
2022-04-08 21:35:41,069 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.output.LayerNorm.weight using .* initializer
2022-04-08 21:35:41,069 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.output.LayerNorm.bias using .* initializer
2022-04-08 21:35:41,069 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.self.query.weight using .* initializer
2022-04-08 21:35:41,069 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.self.query.bias using .* initializer
2022-04-08 21:35:41,069 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.self.key.weight using .* initializer
2022-04-08 21:35:41,070 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.self.key.bias using .* initializer
2022-04-08 21:35:41,070 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.self.value.weight using .* initializer
2022-04-08 21:35:41,070 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.self.value.bias using .* initializer
2022-04-08 21:35:41,070 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.output.dense.weight using .* initializer
2022-04-08 21:35:41,071 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.output.dense.bias using .* initializer
2022-04-08 21:35:41,071 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.output.LayerNorm.weight using .* initializer
2022-04-08 21:35:41,071 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.output.LayerNorm.bias using .* initializer
2022-04-08 21:35:41,071 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.intermediate.dense.weight using .* initializer
2022-04-08 21:35:41,072 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.intermediate.dense.bias using .* initializer
2022-04-08 21:35:41,072 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.output.dense.weight using .* initializer
2022-04-08 21:35:41,073 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.output.dense.bias using .* initializer
2022-04-08 21:35:41,073 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.output.LayerNorm.weight using .* initializer
2022-04-08 21:35:41,073 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.output.LayerNorm.bias using .* initializer
2022-04-08 21:35:41,073 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.self.query.weight using .* initializer
2022-04-08 21:35:41,074 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.self.query.bias using .* initializer
2022-04-08 21:35:41,074 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.self.key.weight using .* initializer
2022-04-08 21:35:41,074 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.self.key.bias using .* initializer
2022-04-08 21:35:41,074 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.self.value.weight using .* initializer
2022-04-08 21:35:41,075 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.self.value.bias using .* initializer
2022-04-08 21:35:41,075 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.output.dense.weight using .* initializer
2022-04-08 21:35:41,075 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.output.dense.bias using .* initializer
2022-04-08 21:35:41,075 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.output.LayerNorm.weight using .* initializer
2022-04-08 21:35:41,075 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.output.LayerNorm.bias using .* initializer
2022-04-08 21:35:41,076 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.intermediate.dense.weight using .* initializer
2022-04-08 21:35:41,076 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.intermediate.dense.bias using .* initializer
2022-04-08 21:35:41,077 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.output.dense.weight using .* initializer
2022-04-08 21:35:41,077 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.output.dense.bias using .* initializer
2022-04-08 21:35:41,078 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.output.LayerNorm.weight using .* initializer
2022-04-08 21:35:41,078 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.output.LayerNorm.bias using .* initializer
2022-04-08 21:35:41,078 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.self.query.weight using .* initializer
2022-04-08 21:35:41,078 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.self.query.bias using .* initializer
2022-04-08 21:35:41,078 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.self.key.weight using .* initializer
2022-04-08 21:35:41,079 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.self.key.bias using .* initializer
2022-04-08 21:35:41,079 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.self.value.weight using .* initializer
2022-04-08 21:35:41,079 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.self.value.bias using .* initializer
2022-04-08 21:35:41,079 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.output.dense.weight using .* initializer
2022-04-08 21:35:41,080 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.output.dense.bias using .* initializer
2022-04-08 21:35:41,080 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.output.LayerNorm.weight using .* initializer
2022-04-08 21:35:41,080 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.output.LayerNorm.bias using .* initializer
2022-04-08 21:35:41,080 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.intermediate.dense.weight using .* initializer
2022-04-08 21:35:41,081 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.intermediate.dense.bias using .* initializer
2022-04-08 21:35:41,081 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.output.dense.weight using .* initializer
2022-04-08 21:35:41,082 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.output.dense.bias using .* initializer
2022-04-08 21:35:41,082 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.output.LayerNorm.weight using .* initializer
2022-04-08 21:35:41,082 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.output.LayerNorm.bias using .* initializer
2022-04-08 21:35:41,082 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.self.query.weight using .* initializer
2022-04-08 21:35:41,083 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.self.query.bias using .* initializer
2022-04-08 21:35:41,083 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.self.key.weight using .* initializer
2022-04-08 21:35:41,083 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.self.key.bias using .* initializer
2022-04-08 21:35:41,083 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.self.value.weight using .* initializer
2022-04-08 21:35:41,084 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.self.value.bias using .* initializer
2022-04-08 21:35:41,084 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.output.dense.weight using .* initializer
2022-04-08 21:35:41,084 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.output.dense.bias using .* initializer
2022-04-08 21:35:41,084 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.output.LayerNorm.weight using .* initializer
2022-04-08 21:35:41,084 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.output.LayerNorm.bias using .* initializer
2022-04-08 21:35:41,084 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.intermediate.dense.weight using .* initializer
2022-04-08 21:35:41,085 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.intermediate.dense.bias using .* initializer
2022-04-08 21:35:41,085 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.output.dense.weight using .* initializer
2022-04-08 21:35:41,086 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.output.dense.bias using .* initializer
2022-04-08 21:35:41,086 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.output.LayerNorm.weight using .* initializer
2022-04-08 21:35:41,087 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.output.LayerNorm.bias using .* initializer
2022-04-08 21:35:41,087 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.self.query.weight using .* initializer
2022-04-08 21:35:41,087 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.self.query.bias using .* initializer
2022-04-08 21:35:41,087 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.self.key.weight using .* initializer
2022-04-08 21:35:41,088 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.self.key.bias using .* initializer
2022-04-08 21:35:41,088 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.self.value.weight using .* initializer
2022-04-08 21:35:41,088 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.self.value.bias using .* initializer
2022-04-08 21:35:41,088 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.output.dense.weight using .* initializer
2022-04-08 21:35:41,088 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.output.dense.bias using .* initializer
2022-04-08 21:35:41,089 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.output.LayerNorm.weight using .* initializer
2022-04-08 21:35:41,089 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.output.LayerNorm.bias using .* initializer
2022-04-08 21:35:41,089 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.intermediate.dense.weight using .* initializer
2022-04-08 21:35:41,090 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.intermediate.dense.bias using .* initializer
2022-04-08 21:35:41,090 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.output.dense.weight using .* initializer
2022-04-08 21:35:41,091 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.output.dense.bias using .* initializer
2022-04-08 21:35:41,091 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.output.LayerNorm.weight using .* initializer
2022-04-08 21:35:41,091 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.output.LayerNorm.bias using .* initializer
2022-04-08 21:35:41,091 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.pooler.dense.weight using .* initializer
2022-04-08 21:35:41,092 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.pooler.dense.bias using .* initializer
2022-04-08 21:35:41,092 - INFO - allennlp.nn.initializers - Initializing _classifier.classifier.dense.weight using .* initializer
2022-04-08 21:35:41,092 - INFO - allennlp.nn.initializers - Initializing _classifier.classifier.dense.bias using .* initializer
2022-04-08 21:35:41,092 - INFO - allennlp.nn.initializers - Initializing _classifier.classifier.out_proj.weight using .* initializer
2022-04-08 21:35:41,092 - INFO - allennlp.nn.initializers - Initializing _classifier.classifier.out_proj.bias using .* initializer
2022-04-08 21:35:41,092 - INFO - allennlp.nn.initializers - Done initializing parameters; the following parameters are using their default initialization from their code
2022-04-08 21:35:41,637 - INFO - filelock - Lock 140014828086352 acquired on ./output_newboolq_ir-ora-d/vocabulary/.lock
2022-04-08 21:35:41,637 - INFO - filelock - Lock 140014828086352 released on ./output_newboolq_ir-ora-d/vocabulary/.lock
2022-04-08 21:35:41,637 - INFO - allennlp.common.params - data_loader.type = pytorch_dataloader
2022-04-08 21:35:41,638 - INFO - allennlp.common.params - data_loader.batch_size = 1
2022-04-08 21:35:41,638 - INFO - allennlp.common.params - data_loader.shuffle = False
2022-04-08 21:35:41,638 - INFO - allennlp.common.params - data_loader.sampler = None
2022-04-08 21:35:41,638 - INFO - allennlp.common.params - data_loader.num_workers = 0
2022-04-08 21:35:41,638 - INFO - allennlp.common.params - data_loader.pin_memory = False
2022-04-08 21:35:41,638 - INFO - allennlp.common.params - data_loader.drop_last = False
2022-04-08 21:35:41,638 - INFO - allennlp.common.params - data_loader.timeout = 0
2022-04-08 21:35:41,639 - INFO - allennlp.common.params - data_loader.worker_init_fn = None
2022-04-08 21:35:41,639 - INFO - allennlp.common.params - data_loader.multiprocessing_context = None
2022-04-08 21:35:41,639 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None
2022-04-08 21:35:41,639 - INFO - allennlp.common.params - data_loader.batch_sampler.type = bucket
2022-04-08 21:35:41,639 - INFO - allennlp.common.params - data_loader.batch_sampler.batch_size = 2
2022-04-08 21:35:41,639 - INFO - allennlp.common.params - data_loader.batch_sampler.sorting_keys = None
2022-04-08 21:35:41,639 - INFO - allennlp.common.params - data_loader.batch_sampler.padding_noise = 0.1
2022-04-08 21:35:41,639 - INFO - allennlp.common.params - data_loader.batch_sampler.drop_last = False
2022-04-08 21:35:41,640 - INFO - allennlp.common.params - data_loader.type = pytorch_dataloader
2022-04-08 21:35:41,640 - INFO - allennlp.common.params - data_loader.batch_size = 1
2022-04-08 21:35:41,640 - INFO - allennlp.common.params - data_loader.shuffle = False
2022-04-08 21:35:41,641 - INFO - allennlp.common.params - data_loader.sampler = None
2022-04-08 21:35:41,641 - INFO - allennlp.common.params - data_loader.num_workers = 0
2022-04-08 21:35:41,641 - INFO - allennlp.common.params - data_loader.pin_memory = False
2022-04-08 21:35:41,641 - INFO - allennlp.common.params - data_loader.drop_last = False
2022-04-08 21:35:41,641 - INFO - allennlp.common.params - data_loader.timeout = 0
2022-04-08 21:35:41,641 - INFO - allennlp.common.params - data_loader.worker_init_fn = None
2022-04-08 21:35:41,641 - INFO - allennlp.common.params - data_loader.multiprocessing_context = None
2022-04-08 21:35:41,641 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None
2022-04-08 21:35:41,642 - INFO - allennlp.common.params - data_loader.batch_sampler.type = bucket
2022-04-08 21:35:41,642 - INFO - allennlp.common.params - data_loader.batch_sampler.batch_size = 2
2022-04-08 21:35:41,642 - INFO - allennlp.common.params - data_loader.batch_sampler.sorting_keys = None
2022-04-08 21:35:41,642 - INFO - allennlp.common.params - data_loader.batch_sampler.padding_noise = 0.1
2022-04-08 21:35:41,642 - INFO - allennlp.common.params - data_loader.batch_sampler.drop_last = False
2022-04-08 21:35:41,642 - INFO - allennlp.common.params - trainer.type = gradient_descent
2022-04-08 21:35:41,643 - INFO - allennlp.common.params - trainer.patience = None
2022-04-08 21:35:41,643 - INFO - allennlp.common.params - trainer.validation_metric = +accuracy
2022-04-08 21:35:41,643 - INFO - allennlp.common.params - trainer.num_epochs = 15
2022-04-08 21:35:41,643 - INFO - allennlp.common.params - trainer.cuda_device = 0
2022-04-08 21:35:41,643 - INFO - allennlp.common.params - trainer.grad_norm = None
2022-04-08 21:35:41,643 - INFO - allennlp.common.params - trainer.grad_clipping = None
2022-04-08 21:35:41,643 - INFO - allennlp.common.params - trainer.distributed = False
2022-04-08 21:35:41,643 - INFO - allennlp.common.params - trainer.world_size = 1
2022-04-08 21:35:41,644 - INFO - allennlp.common.params - trainer.num_gradient_accumulation_steps = 16
2022-04-08 21:35:41,644 - INFO - allennlp.common.params - trainer.use_amp = False
2022-04-08 21:35:41,644 - INFO - allennlp.common.params - trainer.no_grad = None
2022-04-08 21:35:41,644 - INFO - allennlp.common.params - trainer.momentum_scheduler = None
2022-04-08 21:35:41,644 - INFO - allennlp.common.params - trainer.tensorboard_writer = <allennlp.common.lazy.Lazy object at 0x7f57e5d89fd0>
2022-04-08 21:35:41,644 - INFO - allennlp.common.params - trainer.moving_average = None
2022-04-08 21:35:41,644 - INFO - allennlp.common.params - trainer.checkpointer = <allennlp.common.lazy.Lazy object at 0x7f57e5d96090>
2022-04-08 21:35:41,644 - INFO - allennlp.common.params - trainer.batch_callbacks = None
2022-04-08 21:35:41,645 - INFO - allennlp.common.params - trainer.epoch_callbacks = None
2022-04-08 21:35:41,645 - INFO - allennlp.common.params - trainer.end_callbacks = None
2022-04-08 21:35:41,645 - INFO - allennlp.common.params - trainer.trainer_callbacks = None
2022-04-08 21:35:44,372 - INFO - allennlp.common.params - trainer.optimizer.type = huggingface_adamw
2022-04-08 21:35:44,373 - INFO - allennlp.common.params - trainer.optimizer.parameter_groups = None
2022-04-08 21:35:44,373 - INFO - allennlp.common.params - trainer.optimizer.lr = 1e-05
2022-04-08 21:35:44,373 - INFO - allennlp.common.params - trainer.optimizer.betas = [0.9, 0.98]
2022-04-08 21:35:44,373 - INFO - allennlp.common.params - trainer.optimizer.eps = 1e-06
2022-04-08 21:35:44,373 - INFO - allennlp.common.params - trainer.optimizer.weight_decay = 0.1
2022-04-08 21:35:44,374 - INFO - allennlp.common.params - trainer.optimizer.correct_bias = True
2022-04-08 21:35:44,374 - INFO - allennlp.training.optimizers - Number of trainable parameters: 356411394
2022-04-08 21:35:44,376 - INFO - allennlp.common.util - The following parameters are Frozen (without gradient):
2022-04-08 21:35:44,378 - INFO - allennlp.common.util - The following parameters are Tunable (with gradient):
2022-04-08 21:35:44,378 - INFO - allennlp.common.util - _classifier.roberta.embeddings.word_embeddings.weight
2022-04-08 21:35:44,378 - INFO - allennlp.common.util - _classifier.roberta.embeddings.position_embeddings.weight
2022-04-08 21:35:44,378 - INFO - allennlp.common.util - _classifier.roberta.embeddings.token_type_embeddings.weight
2022-04-08 21:35:44,379 - INFO - allennlp.common.util - _classifier.roberta.embeddings.LayerNorm.weight
2022-04-08 21:35:44,379 - INFO - allennlp.common.util - _classifier.roberta.embeddings.LayerNorm.bias
2022-04-08 21:35:44,379 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.self.query.weight
2022-04-08 21:35:44,379 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.self.query.bias
2022-04-08 21:35:44,379 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.self.key.weight
2022-04-08 21:35:44,379 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.self.key.bias
2022-04-08 21:35:44,379 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.self.value.weight
2022-04-08 21:35:44,379 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.self.value.bias
2022-04-08 21:35:44,379 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.output.dense.weight
2022-04-08 21:35:44,379 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.output.dense.bias
2022-04-08 21:35:44,379 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.output.LayerNorm.weight
2022-04-08 21:35:44,380 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.output.LayerNorm.bias
2022-04-08 21:35:44,380 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.intermediate.dense.weight
2022-04-08 21:35:44,380 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.intermediate.dense.bias
2022-04-08 21:35:44,380 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.output.dense.weight
2022-04-08 21:35:44,380 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.output.dense.bias
2022-04-08 21:35:44,380 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.output.LayerNorm.weight
2022-04-08 21:35:44,380 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.output.LayerNorm.bias
2022-04-08 21:35:44,380 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.self.query.weight
2022-04-08 21:35:44,380 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.self.query.bias
2022-04-08 21:35:44,380 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.self.key.weight
2022-04-08 21:35:44,380 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.self.key.bias
2022-04-08 21:35:44,381 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.self.value.weight
2022-04-08 21:35:44,381 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.self.value.bias
2022-04-08 21:35:44,381 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.output.dense.weight
2022-04-08 21:35:44,381 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.output.dense.bias
2022-04-08 21:35:44,381 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.output.LayerNorm.weight
2022-04-08 21:35:44,381 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.output.LayerNorm.bias
2022-04-08 21:35:44,381 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.intermediate.dense.weight
2022-04-08 21:35:44,381 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.intermediate.dense.bias
2022-04-08 21:35:44,381 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.output.dense.weight
2022-04-08 21:35:44,381 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.output.dense.bias
2022-04-08 21:35:44,381 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.output.LayerNorm.weight
2022-04-08 21:35:44,381 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.output.LayerNorm.bias
2022-04-08 21:35:44,382 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.self.query.weight
2022-04-08 21:35:44,382 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.self.query.bias
2022-04-08 21:35:44,382 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.self.key.weight
2022-04-08 21:35:44,382 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.self.key.bias
2022-04-08 21:35:44,382 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.self.value.weight
2022-04-08 21:35:44,382 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.self.value.bias
2022-04-08 21:35:44,382 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.output.dense.weight
2022-04-08 21:35:44,382 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.output.dense.bias
2022-04-08 21:35:44,382 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.output.LayerNorm.weight
2022-04-08 21:35:44,382 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.output.LayerNorm.bias
2022-04-08 21:35:44,382 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.intermediate.dense.weight
2022-04-08 21:35:44,383 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.intermediate.dense.bias
2022-04-08 21:35:44,383 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.output.dense.weight
2022-04-08 21:35:44,383 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.output.dense.bias
2022-04-08 21:35:44,383 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.output.LayerNorm.weight
2022-04-08 21:35:44,383 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.output.LayerNorm.bias
2022-04-08 21:35:44,383 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.self.query.weight
2022-04-08 21:35:44,383 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.self.query.bias
2022-04-08 21:35:44,383 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.self.key.weight
2022-04-08 21:35:44,383 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.self.key.bias
2022-04-08 21:35:44,383 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.self.value.weight
2022-04-08 21:35:44,383 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.self.value.bias
2022-04-08 21:35:44,383 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.output.dense.weight
2022-04-08 21:35:44,384 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.output.dense.bias
2022-04-08 21:35:44,384 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.output.LayerNorm.weight
2022-04-08 21:35:44,384 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.output.LayerNorm.bias
2022-04-08 21:35:44,384 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.intermediate.dense.weight
2022-04-08 21:35:44,384 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.intermediate.dense.bias
2022-04-08 21:35:44,384 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.output.dense.weight
2022-04-08 21:35:44,384 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.output.dense.bias
2022-04-08 21:35:44,384 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.output.LayerNorm.weight
2022-04-08 21:35:44,384 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.output.LayerNorm.bias
2022-04-08 21:35:44,384 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.self.query.weight
2022-04-08 21:35:44,384 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.self.query.bias
2022-04-08 21:35:44,385 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.self.key.weight
2022-04-08 21:35:44,385 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.self.key.bias
2022-04-08 21:35:44,385 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.self.value.weight
2022-04-08 21:35:44,385 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.self.value.bias
2022-04-08 21:35:44,385 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.output.dense.weight
2022-04-08 21:35:44,385 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.output.dense.bias
2022-04-08 21:35:44,385 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.output.LayerNorm.weight
2022-04-08 21:35:44,385 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.output.LayerNorm.bias
2022-04-08 21:35:44,385 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.intermediate.dense.weight
2022-04-08 21:35:44,385 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.intermediate.dense.bias
2022-04-08 21:35:44,385 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.output.dense.weight
2022-04-08 21:35:44,385 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.output.dense.bias
2022-04-08 21:35:44,386 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.output.LayerNorm.weight
2022-04-08 21:35:44,386 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.output.LayerNorm.bias
2022-04-08 21:35:44,386 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.self.query.weight
2022-04-08 21:35:44,386 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.self.query.bias
2022-04-08 21:35:44,386 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.self.key.weight
2022-04-08 21:35:44,386 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.self.key.bias
2022-04-08 21:35:44,386 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.self.value.weight
2022-04-08 21:35:44,386 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.self.value.bias
2022-04-08 21:35:44,386 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.output.dense.weight
2022-04-08 21:35:44,386 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.output.dense.bias
2022-04-08 21:35:44,386 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.output.LayerNorm.weight
2022-04-08 21:35:44,386 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.output.LayerNorm.bias
2022-04-08 21:35:44,386 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.intermediate.dense.weight
2022-04-08 21:35:44,387 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.intermediate.dense.bias
2022-04-08 21:35:44,387 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.output.dense.weight
2022-04-08 21:35:44,387 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.output.dense.bias
2022-04-08 21:35:44,387 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.output.LayerNorm.weight
2022-04-08 21:35:44,387 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.output.LayerNorm.bias
2022-04-08 21:35:44,387 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.self.query.weight
2022-04-08 21:35:44,387 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.self.query.bias
2022-04-08 21:35:44,387 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.self.key.weight
2022-04-08 21:35:44,387 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.self.key.bias
2022-04-08 21:35:44,387 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.self.value.weight
2022-04-08 21:35:44,387 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.self.value.bias
2022-04-08 21:35:44,387 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.output.dense.weight
2022-04-08 21:35:44,388 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.output.dense.bias
2022-04-08 21:35:44,388 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.output.LayerNorm.weight
2022-04-08 21:35:44,388 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.output.LayerNorm.bias
2022-04-08 21:35:44,388 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.intermediate.dense.weight
2022-04-08 21:35:44,388 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.intermediate.dense.bias
2022-04-08 21:35:44,388 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.output.dense.weight
2022-04-08 21:35:44,388 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.output.dense.bias
2022-04-08 21:35:44,388 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.output.LayerNorm.weight
2022-04-08 21:35:44,388 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.output.LayerNorm.bias
2022-04-08 21:35:44,388 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.self.query.weight
2022-04-08 21:35:44,388 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.self.query.bias
2022-04-08 21:35:44,388 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.self.key.weight
2022-04-08 21:35:44,388 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.self.key.bias
2022-04-08 21:35:44,389 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.self.value.weight
2022-04-08 21:35:44,389 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.self.value.bias
2022-04-08 21:35:44,389 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.output.dense.weight
2022-04-08 21:35:44,389 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.output.dense.bias
2022-04-08 21:35:44,389 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.output.LayerNorm.weight
2022-04-08 21:35:44,389 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.output.LayerNorm.bias
2022-04-08 21:35:44,389 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.intermediate.dense.weight
2022-04-08 21:35:44,389 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.intermediate.dense.bias
2022-04-08 21:35:44,389 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.output.dense.weight
2022-04-08 21:35:44,389 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.output.dense.bias
2022-04-08 21:35:44,389 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.output.LayerNorm.weight
2022-04-08 21:35:44,390 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.output.LayerNorm.bias
2022-04-08 21:35:44,390 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.self.query.weight
2022-04-08 21:35:44,390 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.self.query.bias
2022-04-08 21:35:44,390 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.self.key.weight
2022-04-08 21:35:44,390 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.self.key.bias
2022-04-08 21:35:44,390 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.self.value.weight
2022-04-08 21:35:44,390 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.self.value.bias
2022-04-08 21:35:44,390 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.output.dense.weight
2022-04-08 21:35:44,390 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.output.dense.bias
2022-04-08 21:35:44,390 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.output.LayerNorm.weight
2022-04-08 21:35:44,390 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.output.LayerNorm.bias
2022-04-08 21:35:44,391 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.intermediate.dense.weight
2022-04-08 21:35:44,391 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.intermediate.dense.bias
2022-04-08 21:35:44,391 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.output.dense.weight
2022-04-08 21:35:44,391 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.output.dense.bias
2022-04-08 21:35:44,391 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.output.LayerNorm.weight
2022-04-08 21:35:44,391 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.output.LayerNorm.bias
2022-04-08 21:35:44,391 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.self.query.weight
2022-04-08 21:35:44,391 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.self.query.bias
2022-04-08 21:35:44,391 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.self.key.weight
2022-04-08 21:35:44,391 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.self.key.bias
2022-04-08 21:35:44,392 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.self.value.weight
2022-04-08 21:35:44,392 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.self.value.bias
2022-04-08 21:35:44,392 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.output.dense.weight
2022-04-08 21:35:44,392 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.output.dense.bias
2022-04-08 21:35:44,392 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.output.LayerNorm.weight
2022-04-08 21:35:44,392 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.output.LayerNorm.bias
2022-04-08 21:35:44,392 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.intermediate.dense.weight
2022-04-08 21:35:44,392 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.intermediate.dense.bias
2022-04-08 21:35:44,392 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.output.dense.weight
2022-04-08 21:35:44,392 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.output.dense.bias
2022-04-08 21:35:44,392 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.output.LayerNorm.weight
2022-04-08 21:35:44,392 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.output.LayerNorm.bias
2022-04-08 21:35:44,393 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.self.query.weight
2022-04-08 21:35:44,393 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.self.query.bias
2022-04-08 21:35:44,393 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.self.key.weight
2022-04-08 21:35:44,393 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.self.key.bias
2022-04-08 21:35:44,393 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.self.value.weight
2022-04-08 21:35:44,393 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.self.value.bias
2022-04-08 21:35:44,393 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.output.dense.weight
2022-04-08 21:35:44,393 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.output.dense.bias
2022-04-08 21:35:44,393 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.output.LayerNorm.weight
2022-04-08 21:35:44,393 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.output.LayerNorm.bias
2022-04-08 21:35:44,393 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.intermediate.dense.weight
2022-04-08 21:35:44,393 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.intermediate.dense.bias
2022-04-08 21:35:44,394 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.output.dense.weight
2022-04-08 21:35:44,394 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.output.dense.bias
2022-04-08 21:35:44,394 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.output.LayerNorm.weight
2022-04-08 21:35:44,394 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.output.LayerNorm.bias
2022-04-08 21:35:44,394 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.self.query.weight
2022-04-08 21:35:44,394 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.self.query.bias
2022-04-08 21:35:44,394 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.self.key.weight
2022-04-08 21:35:44,394 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.self.key.bias
2022-04-08 21:35:44,394 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.self.value.weight
2022-04-08 21:35:44,394 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.self.value.bias
2022-04-08 21:35:44,394 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.output.dense.weight
2022-04-08 21:35:44,395 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.output.dense.bias
2022-04-08 21:35:44,395 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.output.LayerNorm.weight
2022-04-08 21:35:44,395 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.output.LayerNorm.bias
2022-04-08 21:35:44,395 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.intermediate.dense.weight
2022-04-08 21:35:44,395 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.intermediate.dense.bias
2022-04-08 21:35:44,395 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.output.dense.weight
2022-04-08 21:35:44,395 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.output.dense.bias
2022-04-08 21:35:44,395 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.output.LayerNorm.weight
2022-04-08 21:35:44,395 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.output.LayerNorm.bias
2022-04-08 21:35:44,395 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.self.query.weight
2022-04-08 21:35:44,395 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.self.query.bias
2022-04-08 21:35:44,396 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.self.key.weight
2022-04-08 21:35:44,396 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.self.key.bias
2022-04-08 21:35:44,396 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.self.value.weight
2022-04-08 21:35:44,396 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.self.value.bias
2022-04-08 21:35:44,396 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.output.dense.weight
2022-04-08 21:35:44,396 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.output.dense.bias
2022-04-08 21:35:44,396 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.output.LayerNorm.weight
2022-04-08 21:35:44,396 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.output.LayerNorm.bias
2022-04-08 21:35:44,396 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.intermediate.dense.weight
2022-04-08 21:35:44,396 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.intermediate.dense.bias
2022-04-08 21:35:44,396 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.output.dense.weight
2022-04-08 21:35:44,396 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.output.dense.bias
2022-04-08 21:35:44,397 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.output.LayerNorm.weight
2022-04-08 21:35:44,397 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.output.LayerNorm.bias
2022-04-08 21:35:44,397 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.self.query.weight
2022-04-08 21:35:44,397 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.self.query.bias
2022-04-08 21:35:44,397 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.self.key.weight
2022-04-08 21:35:44,397 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.self.key.bias
2022-04-08 21:35:44,397 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.self.value.weight
2022-04-08 21:35:44,397 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.self.value.bias
2022-04-08 21:35:44,397 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.output.dense.weight
2022-04-08 21:35:44,397 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.output.dense.bias
2022-04-08 21:35:44,397 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.output.LayerNorm.weight
2022-04-08 21:35:44,397 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.output.LayerNorm.bias
2022-04-08 21:35:44,398 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.intermediate.dense.weight
2022-04-08 21:35:44,398 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.intermediate.dense.bias
2022-04-08 21:35:44,398 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.output.dense.weight
2022-04-08 21:35:44,398 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.output.dense.bias
2022-04-08 21:35:44,398 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.output.LayerNorm.weight
2022-04-08 21:35:44,398 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.output.LayerNorm.bias
2022-04-08 21:35:44,398 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.self.query.weight
2022-04-08 21:35:44,398 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.self.query.bias
2022-04-08 21:35:44,398 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.self.key.weight
2022-04-08 21:35:44,398 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.self.key.bias
2022-04-08 21:35:44,398 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.self.value.weight
2022-04-08 21:35:44,399 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.self.value.bias
2022-04-08 21:35:44,399 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.output.dense.weight
2022-04-08 21:35:44,399 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.output.dense.bias
2022-04-08 21:35:44,399 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.output.LayerNorm.weight
2022-04-08 21:35:44,399 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.output.LayerNorm.bias
2022-04-08 21:35:44,399 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.intermediate.dense.weight
2022-04-08 21:35:44,399 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.intermediate.dense.bias
2022-04-08 21:35:44,399 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.output.dense.weight
2022-04-08 21:35:44,399 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.output.dense.bias
2022-04-08 21:35:44,399 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.output.LayerNorm.weight
2022-04-08 21:35:44,399 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.output.LayerNorm.bias
2022-04-08 21:35:44,400 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.self.query.weight
2022-04-08 21:35:44,400 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.self.query.bias
2022-04-08 21:35:44,400 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.self.key.weight
2022-04-08 21:35:44,400 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.self.key.bias
2022-04-08 21:35:44,400 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.self.value.weight
2022-04-08 21:35:44,400 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.self.value.bias
2022-04-08 21:35:44,400 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.output.dense.weight
2022-04-08 21:35:44,400 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.output.dense.bias
2022-04-08 21:35:44,400 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.output.LayerNorm.weight
2022-04-08 21:35:44,400 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.output.LayerNorm.bias
2022-04-08 21:35:44,400 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.intermediate.dense.weight
2022-04-08 21:35:44,400 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.intermediate.dense.bias
2022-04-08 21:35:44,401 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.output.dense.weight
2022-04-08 21:35:44,401 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.output.dense.bias
2022-04-08 21:35:44,401 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.output.LayerNorm.weight
2022-04-08 21:35:44,401 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.output.LayerNorm.bias
2022-04-08 21:35:44,401 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.self.query.weight
2022-04-08 21:35:44,401 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.self.query.bias
2022-04-08 21:35:44,401 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.self.key.weight
2022-04-08 21:35:44,401 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.self.key.bias
2022-04-08 21:35:44,401 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.self.value.weight
2022-04-08 21:35:44,401 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.self.value.bias
2022-04-08 21:35:44,401 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.output.dense.weight
2022-04-08 21:35:44,401 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.output.dense.bias
2022-04-08 21:35:44,402 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.output.LayerNorm.weight
2022-04-08 21:35:44,402 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.output.LayerNorm.bias
2022-04-08 21:35:44,402 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.intermediate.dense.weight
2022-04-08 21:35:44,402 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.intermediate.dense.bias
2022-04-08 21:35:44,402 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.output.dense.weight
2022-04-08 21:35:44,402 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.output.dense.bias
2022-04-08 21:35:44,402 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.output.LayerNorm.weight
2022-04-08 21:35:44,402 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.output.LayerNorm.bias
2022-04-08 21:35:44,402 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.self.query.weight
2022-04-08 21:35:44,402 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.self.query.bias
2022-04-08 21:35:44,402 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.self.key.weight
2022-04-08 21:35:44,402 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.self.key.bias
2022-04-08 21:35:44,403 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.self.value.weight
2022-04-08 21:35:44,403 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.self.value.bias
2022-04-08 21:35:44,403 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.output.dense.weight
2022-04-08 21:35:44,403 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.output.dense.bias
2022-04-08 21:35:44,403 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.output.LayerNorm.weight
2022-04-08 21:35:44,403 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.output.LayerNorm.bias
2022-04-08 21:35:44,403 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.intermediate.dense.weight
2022-04-08 21:35:44,403 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.intermediate.dense.bias
2022-04-08 21:35:44,403 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.output.dense.weight
2022-04-08 21:35:44,403 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.output.dense.bias
2022-04-08 21:35:44,403 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.output.LayerNorm.weight
2022-04-08 21:35:44,404 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.output.LayerNorm.bias
2022-04-08 21:35:44,404 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.self.query.weight
2022-04-08 21:35:44,404 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.self.query.bias
2022-04-08 21:35:44,404 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.self.key.weight
2022-04-08 21:35:44,404 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.self.key.bias
2022-04-08 21:35:44,404 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.self.value.weight
2022-04-08 21:35:44,404 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.self.value.bias
2022-04-08 21:35:44,404 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.output.dense.weight
2022-04-08 21:35:44,404 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.output.dense.bias
2022-04-08 21:35:44,404 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.output.LayerNorm.weight
2022-04-08 21:35:44,404 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.output.LayerNorm.bias
2022-04-08 21:35:44,404 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.intermediate.dense.weight
2022-04-08 21:35:44,405 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.intermediate.dense.bias
2022-04-08 21:35:44,405 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.output.dense.weight
2022-04-08 21:35:44,405 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.output.dense.bias
2022-04-08 21:35:44,405 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.output.LayerNorm.weight
2022-04-08 21:35:44,405 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.output.LayerNorm.bias
2022-04-08 21:35:44,405 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.self.query.weight
2022-04-08 21:35:44,405 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.self.query.bias
2022-04-08 21:35:44,405 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.self.key.weight
2022-04-08 21:35:44,405 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.self.key.bias
2022-04-08 21:35:44,405 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.self.value.weight
2022-04-08 21:35:44,405 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.self.value.bias
2022-04-08 21:35:44,406 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.output.dense.weight
2022-04-08 21:35:44,406 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.output.dense.bias
2022-04-08 21:35:44,406 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.output.LayerNorm.weight
2022-04-08 21:35:44,406 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.output.LayerNorm.bias
2022-04-08 21:35:44,406 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.intermediate.dense.weight
2022-04-08 21:35:44,406 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.intermediate.dense.bias
2022-04-08 21:35:44,406 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.output.dense.weight
2022-04-08 21:35:44,406 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.output.dense.bias
2022-04-08 21:35:44,406 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.output.LayerNorm.weight
2022-04-08 21:35:44,406 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.output.LayerNorm.bias
2022-04-08 21:35:44,406 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.self.query.weight
2022-04-08 21:35:44,406 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.self.query.bias
2022-04-08 21:35:44,406 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.self.key.weight
2022-04-08 21:35:44,406 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.self.key.bias
2022-04-08 21:35:44,406 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.self.value.weight
2022-04-08 21:35:44,406 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.self.value.bias
2022-04-08 21:35:44,406 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.output.dense.weight
2022-04-08 21:35:44,406 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.output.dense.bias
2022-04-08 21:35:44,406 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.output.LayerNorm.weight
2022-04-08 21:35:44,407 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.output.LayerNorm.bias
2022-04-08 21:35:44,407 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.intermediate.dense.weight
2022-04-08 21:35:44,407 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.intermediate.dense.bias
2022-04-08 21:35:44,407 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.output.dense.weight
2022-04-08 21:35:44,407 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.output.dense.bias
2022-04-08 21:35:44,407 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.output.LayerNorm.weight
2022-04-08 21:35:44,407 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.output.LayerNorm.bias
2022-04-08 21:35:44,407 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.self.query.weight
2022-04-08 21:35:44,407 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.self.query.bias
2022-04-08 21:35:44,407 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.self.key.weight
2022-04-08 21:35:44,407 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.self.key.bias
2022-04-08 21:35:44,407 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.self.value.weight
2022-04-08 21:35:44,407 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.self.value.bias
2022-04-08 21:35:44,407 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.output.dense.weight
2022-04-08 21:35:44,407 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.output.dense.bias
2022-04-08 21:35:44,408 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.output.LayerNorm.weight
2022-04-08 21:35:44,408 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.output.LayerNorm.bias
2022-04-08 21:35:44,408 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.intermediate.dense.weight
2022-04-08 21:35:44,408 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.intermediate.dense.bias
2022-04-08 21:35:44,408 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.output.dense.weight
2022-04-08 21:35:44,408 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.output.dense.bias
2022-04-08 21:35:44,408 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.output.LayerNorm.weight
2022-04-08 21:35:44,408 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.output.LayerNorm.bias
2022-04-08 21:35:44,408 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.self.query.weight
2022-04-08 21:35:44,408 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.self.query.bias
2022-04-08 21:35:44,408 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.self.key.weight
2022-04-08 21:35:44,408 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.self.key.bias
2022-04-08 21:35:44,408 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.self.value.weight
2022-04-08 21:35:44,408 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.self.value.bias
2022-04-08 21:35:44,408 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.output.dense.weight
2022-04-08 21:35:44,408 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.output.dense.bias
2022-04-08 21:35:44,408 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.output.LayerNorm.weight
2022-04-08 21:35:44,408 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.output.LayerNorm.bias
2022-04-08 21:35:44,408 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.intermediate.dense.weight
2022-04-08 21:35:44,409 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.intermediate.dense.bias
2022-04-08 21:35:44,409 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.output.dense.weight
2022-04-08 21:35:44,409 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.output.dense.bias
2022-04-08 21:35:44,409 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.output.LayerNorm.weight
2022-04-08 21:35:44,409 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.output.LayerNorm.bias
2022-04-08 21:35:44,409 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.self.query.weight
2022-04-08 21:35:44,409 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.self.query.bias
2022-04-08 21:35:44,409 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.self.key.weight
2022-04-08 21:35:44,409 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.self.key.bias
2022-04-08 21:35:44,409 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.self.value.weight
2022-04-08 21:35:44,409 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.self.value.bias
2022-04-08 21:35:44,409 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.output.dense.weight
2022-04-08 21:35:44,409 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.output.dense.bias
2022-04-08 21:35:44,409 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.output.LayerNorm.weight
2022-04-08 21:35:44,409 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.output.LayerNorm.bias
2022-04-08 21:35:44,409 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.intermediate.dense.weight
2022-04-08 21:35:44,409 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.intermediate.dense.bias
2022-04-08 21:35:44,409 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.output.dense.weight
2022-04-08 21:35:44,409 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.output.dense.bias
2022-04-08 21:35:44,410 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.output.LayerNorm.weight
2022-04-08 21:35:44,410 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.output.LayerNorm.bias
2022-04-08 21:35:44,410 - INFO - allennlp.common.util - _classifier.roberta.pooler.dense.weight
2022-04-08 21:35:44,410 - INFO - allennlp.common.util - _classifier.roberta.pooler.dense.bias
2022-04-08 21:35:44,410 - INFO - allennlp.common.util - _classifier.classifier.dense.weight
2022-04-08 21:35:44,410 - INFO - allennlp.common.util - _classifier.classifier.dense.bias
2022-04-08 21:35:44,410 - INFO - allennlp.common.util - _classifier.classifier.out_proj.weight
2022-04-08 21:35:44,410 - INFO - allennlp.common.util - _classifier.classifier.out_proj.bias
2022-04-08 21:35:44,410 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.type = slanted_triangular
2022-04-08 21:35:44,410 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.cut_frac = 0.06
2022-04-08 21:35:44,410 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.ratio = 32
2022-04-08 21:35:44,410 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.last_epoch = -1
2022-04-08 21:35:44,411 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.gradual_unfreezing = False
2022-04-08 21:35:44,411 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.discriminative_fine_tuning = False
2022-04-08 21:35:44,411 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.decay_factor = 0.38
2022-04-08 21:35:44,411 - INFO - allennlp.common.params - type = default
2022-04-08 21:35:44,411 - INFO - allennlp.common.params - keep_serialized_model_every_num_seconds = None
2022-04-08 21:35:44,411 - INFO - allennlp.common.params - num_serialized_models_to_keep = 2
2022-04-08 21:35:44,411 - INFO - allennlp.common.params - model_save_interval = None
2022-04-08 21:35:44,411 - INFO - allennlp.common.params - summary_interval = 100
2022-04-08 21:35:44,411 - INFO - allennlp.common.params - histogram_interval = None
2022-04-08 21:35:44,411 - INFO - allennlp.common.params - batch_size_interval = None
2022-04-08 21:35:44,411 - INFO - allennlp.common.params - should_log_parameter_statistics = True
2022-04-08 21:35:44,411 - INFO - allennlp.common.params - should_log_learning_rate = False
2022-04-08 21:35:44,411 - INFO - allennlp.common.params - get_batch_num_total = None
2022-04-08 21:35:44,414 - WARNING - allennlp.training.trainer - You provided a validation dataset but patience was set to None, meaning that early stopping is disabled
2022-04-08 21:35:44,507 - INFO - allennlp.training.trainer - Beginning training.
2022-04-08 21:35:44,507 - INFO - allennlp.training.trainer - Epoch 0/14
2022-04-08 21:35:44,507 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-08 21:35:44,507 - INFO - allennlp.training.trainer - GPU 0 memory usage: 1.3G
2022-04-08 21:35:44,508 - INFO - allennlp.training.trainer - Training
2022-04-08 21:35:44,509 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-08 21:35:44,509 - INFO - allennlp.data.samplers.bucket_batch_sampler - No sorting keys given; trying to guess a good one
2022-04-08 21:35:44,509 - INFO - allennlp.data.samplers.bucket_batch_sampler - Using ['tokens'] as the sorting keys
2022-04-08 21:35:54,753 - INFO - tqdm - accuracy: 0.5312, batch_loss: 2.5930, loss: 2.5930 ||:   2%|1         | 1/65 [00:10<10:55, 10.24s/it]
2022-04-08 21:36:05,045 - INFO - tqdm - accuracy: 0.5469, batch_loss: 2.1898, loss: 2.3914 ||:   3%|3         | 2/65 [00:20<10:47, 10.27s/it]
2022-04-08 21:36:15,467 - INFO - tqdm - accuracy: 0.5000, batch_loss: 3.0479, loss: 2.6102 ||:   5%|4         | 3/65 [00:30<10:41, 10.34s/it]
2022-04-08 21:36:26,028 - INFO - tqdm - accuracy: 0.5156, batch_loss: 2.5260, loss: 2.5892 ||:   6%|6         | 4/65 [00:41<10:36, 10.43s/it]
2022-04-08 21:36:36,808 - INFO - tqdm - accuracy: 0.4938, batch_loss: 2.9313, loss: 2.6576 ||:   8%|7         | 5/65 [00:52<10:33, 10.55s/it]
2022-04-08 21:36:47,815 - INFO - tqdm - accuracy: 0.4740, batch_loss: 2.7583, loss: 2.6744 ||:   9%|9         | 6/65 [01:03<10:31, 10.71s/it]
2022-04-08 21:36:58,842 - INFO - tqdm - accuracy: 0.4688, batch_loss: 3.1544, loss: 2.7430 ||:  11%|#         | 7/65 [01:14<10:27, 10.81s/it]
2022-04-08 21:37:10,104 - INFO - tqdm - accuracy: 0.4531, batch_loss: 2.9200, loss: 2.7651 ||:  12%|#2        | 8/65 [01:25<10:24, 10.96s/it]
2022-04-08 21:37:21,631 - INFO - tqdm - accuracy: 0.4549, batch_loss: 2.2981, loss: 2.7132 ||:  14%|#3        | 9/65 [01:37<10:23, 11.13s/it]
2022-04-08 21:37:33,289 - INFO - tqdm - accuracy: 0.4594, batch_loss: 2.2473, loss: 2.6666 ||:  15%|#5        | 10/65 [01:48<10:21, 11.30s/it]
2022-04-08 21:37:45,334 - INFO - tqdm - accuracy: 0.4659, batch_loss: 2.2898, loss: 2.6323 ||:  17%|#6        | 11/65 [02:00<10:22, 11.53s/it]
2022-04-08 21:37:57,230 - INFO - tqdm - accuracy: 0.4688, batch_loss: 1.9590, loss: 2.5762 ||:  18%|#8        | 12/65 [02:12<10:16, 11.64s/it]
2022-04-08 21:38:08,857 - INFO - tqdm - accuracy: 0.4808, batch_loss: 1.4076, loss: 2.4863 ||:  20%|##        | 13/65 [02:24<10:05, 11.63s/it]
2022-04-08 21:38:20,249 - INFO - tqdm - accuracy: 0.4799, batch_loss: 2.2407, loss: 2.4688 ||:  22%|##1       | 14/65 [02:35<09:49, 11.56s/it]
2022-04-08 21:38:31,760 - INFO - tqdm - accuracy: 0.4854, batch_loss: 1.8817, loss: 2.4297 ||:  23%|##3       | 15/65 [02:47<09:37, 11.55s/it]
2022-04-08 21:38:43,411 - INFO - tqdm - accuracy: 0.4902, batch_loss: 1.6441, loss: 2.3806 ||:  25%|##4       | 16/65 [02:58<09:27, 11.58s/it]
2022-04-08 21:38:55,202 - INFO - tqdm - accuracy: 0.4963, batch_loss: 1.0376, loss: 2.3016 ||:  26%|##6       | 17/65 [03:10<09:18, 11.64s/it]
2022-04-08 21:39:06,973 - INFO - tqdm - accuracy: 0.4965, batch_loss: 1.7333, loss: 2.2700 ||:  28%|##7       | 18/65 [03:22<09:08, 11.68s/it]
2022-04-08 21:39:18,703 - INFO - tqdm - accuracy: 0.4951, batch_loss: 1.4116, loss: 2.2248 ||:  29%|##9       | 19/65 [03:34<08:57, 11.70s/it]
2022-04-08 21:39:30,106 - INFO - tqdm - accuracy: 0.5016, batch_loss: 0.7480, loss: 2.1510 ||:  31%|###       | 20/65 [03:45<08:42, 11.61s/it]
2022-04-08 21:39:41,767 - INFO - tqdm - accuracy: 0.5015, batch_loss: 1.3379, loss: 2.1123 ||:  32%|###2      | 21/65 [03:57<08:31, 11.62s/it]
2022-04-08 21:39:53,389 - INFO - tqdm - accuracy: 0.5043, batch_loss: 1.0952, loss: 2.0660 ||:  34%|###3      | 22/65 [04:08<08:19, 11.62s/it]
2022-04-08 21:40:05,173 - INFO - tqdm - accuracy: 0.5068, batch_loss: 1.0202, loss: 2.0206 ||:  35%|###5      | 23/65 [04:20<08:10, 11.67s/it]
2022-04-08 21:40:16,945 - INFO - tqdm - accuracy: 0.5052, batch_loss: 1.0735, loss: 1.9811 ||:  37%|###6      | 24/65 [04:32<07:59, 11.70s/it]
2022-04-08 21:40:28,670 - INFO - tqdm - accuracy: 0.5050, batch_loss: 1.1913, loss: 1.9495 ||:  38%|###8      | 25/65 [04:44<07:48, 11.71s/it]
2022-04-08 21:40:40,352 - INFO - tqdm - accuracy: 0.5012, batch_loss: 0.9178, loss: 1.9098 ||:  40%|####      | 26/65 [04:55<07:36, 11.70s/it]
2022-04-08 21:40:51,997 - INFO - tqdm - accuracy: 0.5035, batch_loss: 0.7837, loss: 1.8681 ||:  42%|####1     | 27/65 [05:07<07:23, 11.68s/it]
2022-04-08 21:41:03,671 - INFO - tqdm - accuracy: 0.5056, batch_loss: 0.7674, loss: 1.8288 ||:  43%|####3     | 28/65 [05:19<07:12, 11.68s/it]
2022-04-08 21:41:15,384 - INFO - tqdm - accuracy: 0.5022, batch_loss: 0.8468, loss: 1.7949 ||:  45%|####4     | 29/65 [05:30<07:00, 11.69s/it]
2022-04-08 21:41:27,101 - INFO - tqdm - accuracy: 0.5010, batch_loss: 0.9214, loss: 1.7658 ||:  46%|####6     | 30/65 [05:42<06:49, 11.70s/it]
2022-04-08 21:41:38,835 - INFO - tqdm - accuracy: 0.5081, batch_loss: 0.5659, loss: 1.7271 ||:  48%|####7     | 31/65 [05:54<06:38, 11.71s/it]
2022-04-08 21:41:50,580 - INFO - tqdm - accuracy: 0.5010, batch_loss: 0.9163, loss: 1.7018 ||:  49%|####9     | 32/65 [06:06<06:26, 11.72s/it]
2022-04-08 21:42:02,339 - INFO - tqdm - accuracy: 0.5019, batch_loss: 0.6859, loss: 1.6710 ||:  51%|#####     | 33/65 [06:17<06:15, 11.73s/it]
2022-04-08 21:42:14,084 - INFO - tqdm - accuracy: 0.5018, batch_loss: 0.6992, loss: 1.6424 ||:  52%|#####2    | 34/65 [06:29<06:03, 11.74s/it]
2022-04-08 21:42:25,845 - INFO - tqdm - accuracy: 0.4973, batch_loss: 0.8075, loss: 1.6186 ||:  54%|#####3    | 35/65 [06:41<05:52, 11.74s/it]
2022-04-08 21:42:37,656 - INFO - tqdm - accuracy: 0.4957, batch_loss: 0.7289, loss: 1.5938 ||:  55%|#####5    | 36/65 [06:53<05:41, 11.76s/it]
2022-04-08 21:42:49,421 - INFO - tqdm - accuracy: 0.4941, batch_loss: 0.6967, loss: 1.5696 ||:  57%|#####6    | 37/65 [07:04<05:29, 11.76s/it]
2022-04-08 21:43:01,180 - INFO - tqdm - accuracy: 0.4984, batch_loss: 0.6753, loss: 1.5461 ||:  58%|#####8    | 38/65 [07:16<05:17, 11.76s/it]
2022-04-08 21:43:12,889 - INFO - tqdm - accuracy: 0.4984, batch_loss: 0.6853, loss: 1.5240 ||:  60%|######    | 39/65 [07:28<05:05, 11.75s/it]
2022-04-08 21:43:24,585 - INFO - tqdm - accuracy: 0.4977, batch_loss: 0.6975, loss: 1.5033 ||:  62%|######1   | 40/65 [07:40<04:53, 11.73s/it]
2022-04-08 21:43:36,257 - INFO - tqdm - accuracy: 0.4962, batch_loss: 0.7334, loss: 1.4846 ||:  63%|######3   | 41/65 [07:51<04:41, 11.71s/it]
2022-04-08 21:43:47,923 - INFO - tqdm - accuracy: 0.5000, batch_loss: 0.6663, loss: 1.4651 ||:  65%|######4   | 42/65 [08:03<04:29, 11.70s/it]
2022-04-08 21:43:59,600 - INFO - tqdm - accuracy: 0.5000, batch_loss: 0.6902, loss: 1.4471 ||:  66%|######6   | 43/65 [08:15<04:17, 11.69s/it]
2022-04-08 21:44:11,269 - INFO - tqdm - accuracy: 0.5043, batch_loss: 0.6523, loss: 1.4290 ||:  68%|######7   | 44/65 [08:26<04:05, 11.69s/it]
2022-04-08 21:44:22,929 - INFO - tqdm - accuracy: 0.5049, batch_loss: 0.6850, loss: 1.4125 ||:  69%|######9   | 45/65 [08:38<03:53, 11.68s/it]
2022-04-08 21:44:34,589 - INFO - tqdm - accuracy: 0.5082, batch_loss: 0.6641, loss: 1.3962 ||:  71%|#######   | 46/65 [08:50<03:41, 11.67s/it]
2022-04-08 21:44:46,260 - INFO - tqdm - accuracy: 0.5086, batch_loss: 0.6788, loss: 1.3809 ||:  72%|#######2  | 47/65 [09:01<03:30, 11.67s/it]
2022-04-08 21:44:57,954 - INFO - tqdm - accuracy: 0.5091, batch_loss: 0.7294, loss: 1.3674 ||:  74%|#######3  | 48/65 [09:13<03:18, 11.68s/it]
2022-04-08 21:45:09,695 - INFO - tqdm - accuracy: 0.5121, batch_loss: 0.6749, loss: 1.3532 ||:  75%|#######5  | 49/65 [09:25<03:07, 11.70s/it]
2022-04-08 21:45:21,466 - INFO - tqdm - accuracy: 0.5131, batch_loss: 0.7057, loss: 1.3403 ||:  77%|#######6  | 50/65 [09:36<02:55, 11.72s/it]
2022-04-08 21:45:33,281 - INFO - tqdm - accuracy: 0.5129, batch_loss: 0.7268, loss: 1.3282 ||:  78%|#######8  | 51/65 [09:48<02:44, 11.75s/it]
2022-04-08 21:45:45,051 - INFO - tqdm - accuracy: 0.5168, batch_loss: 0.6251, loss: 1.3147 ||:  80%|########  | 52/65 [10:00<02:32, 11.75s/it]
2022-04-08 21:45:56,807 - INFO - tqdm - accuracy: 0.5165, batch_loss: 0.7108, loss: 1.3033 ||:  82%|########1 | 53/65 [10:12<02:21, 11.76s/it]
2022-04-08 21:46:08,160 - INFO - tqdm - accuracy: 0.5217, batch_loss: 0.5907, loss: 1.2901 ||:  83%|########3 | 54/65 [10:23<02:07, 11.63s/it]
2022-04-08 21:46:19,849 - INFO - tqdm - accuracy: 0.5196, batch_loss: 0.7671, loss: 1.2806 ||:  85%|########4 | 55/65 [10:35<01:56, 11.65s/it]
2022-04-08 21:46:31,530 - INFO - tqdm - accuracy: 0.5204, batch_loss: 0.6812, loss: 1.2699 ||:  86%|########6 | 56/65 [10:47<01:44, 11.66s/it]
2022-04-08 21:46:43,205 - INFO - tqdm - accuracy: 0.5162, batch_loss: 0.8354, loss: 1.2623 ||:  88%|########7 | 57/65 [10:58<01:33, 11.66s/it]
2022-04-08 21:46:54,894 - INFO - tqdm - accuracy: 0.5164, batch_loss: 0.6797, loss: 1.2523 ||:  89%|########9 | 58/65 [11:10<01:21, 11.67s/it]
2022-04-08 21:47:06,593 - INFO - tqdm - accuracy: 0.5162, batch_loss: 0.7146, loss: 1.2431 ||:  91%|######### | 59/65 [11:22<01:10, 11.68s/it]
2022-04-08 21:47:18,297 - INFO - tqdm - accuracy: 0.5175, batch_loss: 0.6754, loss: 1.2337 ||:  92%|#########2| 60/65 [11:33<00:58, 11.69s/it]
2022-04-08 21:47:29,977 - INFO - tqdm - accuracy: 0.5172, batch_loss: 0.6771, loss: 1.2246 ||:  94%|#########3| 61/65 [11:45<00:46, 11.68s/it]
2022-04-08 21:47:41,666 - INFO - tqdm - accuracy: 0.5174, batch_loss: 0.6948, loss: 1.2160 ||:  95%|#########5| 62/65 [11:57<00:35, 11.69s/it]
2022-04-08 21:47:53,413 - INFO - tqdm - accuracy: 0.5146, batch_loss: 0.7226, loss: 1.2082 ||:  97%|#########6| 63/65 [12:08<00:23, 11.70s/it]
2022-04-08 21:48:05,150 - INFO - tqdm - accuracy: 0.5154, batch_loss: 0.6992, loss: 1.2002 ||:  98%|#########8| 64/65 [12:20<00:11, 11.71s/it]
2022-04-08 21:48:10,360 - INFO - tqdm - accuracy: 0.5148, batch_loss: 0.7136, loss: 1.1927 ||: 100%|##########| 65/65 [12:25<00:00,  9.76s/it]
2022-04-08 21:48:10,360 - INFO - tqdm - accuracy: 0.5148, batch_loss: 0.7136, loss: 1.1927 ||: 100%|##########| 65/65 [12:25<00:00, 11.47s/it]
2022-04-08 21:48:13,240 - INFO - allennlp.training.trainer - Validating
2022-04-08 21:48:13,241 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-08 21:48:13,241 - INFO - allennlp.data.samplers.bucket_batch_sampler - No sorting keys given; trying to guess a good one
2022-04-08 21:48:13,242 - INFO - allennlp.data.samplers.bucket_batch_sampler - Using ['tokens'] as the sorting keys
2022-04-08 21:48:23,319 - INFO - tqdm - accuracy: 0.4872, batch_loss: 0.5949, loss: 0.7002 ||:  34%|###3      | 39/115 [00:10<00:19,  3.85it/s]
2022-04-08 21:48:33,417 - INFO - tqdm - accuracy: 0.4615, batch_loss: 0.7730, loss: 0.6987 ||:  68%|######7   | 78/115 [00:20<00:09,  3.97it/s]
2022-04-08 21:48:43,001 - INFO - tqdm - accuracy: 0.4629, batch_loss: 0.7472, loss: 0.6982 ||: 100%|##########| 115/115 [00:29<00:00,  3.80it/s]
2022-04-08 21:48:43,001 - INFO - tqdm - accuracy: 0.4629, batch_loss: 0.7472, loss: 0.6982 ||: 100%|##########| 115/115 [00:29<00:00,  3.86it/s]
2022-04-08 21:48:43,002 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-08 21:48:43,003 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.515  |     0.463
2022-04-08 21:48:43,004 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  1359.606  |       N/A
2022-04-08 21:48:43,004 - INFO - allennlp.training.tensorboard_writer - loss               |     1.193  |     0.698
2022-04-08 21:48:43,004 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5799.516  |       N/A
2022-04-08 21:48:48,827 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to './output_newboolq_ir-ora-d/best.th'.
2022-04-08 21:48:50,066 - INFO - allennlp.training.trainer - Epoch duration: 0:13:05.558964
2022-04-08 21:48:50,066 - INFO - allennlp.training.trainer - Estimated training time remaining: 3:03:17
2022-04-08 21:48:50,066 - INFO - allennlp.training.trainer - Epoch 1/14
2022-04-08 21:48:50,066 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-08 21:48:50,067 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-08 21:48:50,068 - INFO - allennlp.training.trainer - Training
2022-04-08 21:48:50,068 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-08 21:49:01,616 - INFO - tqdm - accuracy: 0.3125, batch_loss: 0.7282, loss: 0.7282 ||:   2%|1         | 1/65 [00:11<12:19, 11.55s/it]
2022-04-08 21:49:13,492 - INFO - tqdm - accuracy: 0.4219, batch_loss: 0.7030, loss: 0.7156 ||:   3%|3         | 2/65 [00:23<12:19, 11.74s/it]
2022-04-08 21:49:25,566 - INFO - tqdm - accuracy: 0.4375, batch_loss: 0.7271, loss: 0.7194 ||:   5%|4         | 3/65 [00:35<12:17, 11.89s/it]
2022-04-08 21:49:37,347 - INFO - tqdm - accuracy: 0.4844, batch_loss: 0.6753, loss: 0.7084 ||:   6%|6         | 4/65 [00:47<12:02, 11.85s/it]
2022-04-08 21:49:49,035 - INFO - tqdm - accuracy: 0.4750, batch_loss: 0.7060, loss: 0.7079 ||:   8%|7         | 5/65 [00:58<11:47, 11.79s/it]
2022-04-08 21:50:00,658 - INFO - tqdm - accuracy: 0.4896, batch_loss: 0.6890, loss: 0.7048 ||:   9%|9         | 6/65 [01:10<11:32, 11.73s/it]
2022-04-08 21:50:12,357 - INFO - tqdm - accuracy: 0.4732, batch_loss: 0.7183, loss: 0.7067 ||:  11%|#         | 7/65 [01:22<11:19, 11.72s/it]
2022-04-08 21:50:24,133 - INFO - tqdm - accuracy: 0.4766, batch_loss: 0.7083, loss: 0.7069 ||:  12%|#2        | 8/65 [01:34<11:09, 11.74s/it]
2022-04-08 21:50:35,961 - INFO - tqdm - accuracy: 0.4861, batch_loss: 0.6742, loss: 0.7033 ||:  14%|#3        | 9/65 [01:45<10:58, 11.77s/it]
2022-04-08 21:50:47,733 - INFO - tqdm - accuracy: 0.4906, batch_loss: 0.6734, loss: 0.7003 ||:  15%|#5        | 10/65 [01:57<10:47, 11.77s/it]
2022-04-08 21:50:59,435 - INFO - tqdm - accuracy: 0.4972, batch_loss: 0.6918, loss: 0.6995 ||:  17%|#6        | 11/65 [02:09<10:34, 11.75s/it]
2022-04-08 21:51:11,108 - INFO - tqdm - accuracy: 0.5000, batch_loss: 0.7037, loss: 0.6998 ||:  18%|#8        | 12/65 [02:21<10:21, 11.73s/it]
2022-04-08 21:51:22,801 - INFO - tqdm - accuracy: 0.5048, batch_loss: 0.6735, loss: 0.6978 ||:  20%|##        | 13/65 [02:32<10:09, 11.72s/it]
2022-04-08 21:51:34,424 - INFO - tqdm - accuracy: 0.5022, batch_loss: 0.7124, loss: 0.6989 ||:  22%|##1       | 14/65 [02:44<09:56, 11.69s/it]
2022-04-08 21:51:46,195 - INFO - tqdm - accuracy: 0.5083, batch_loss: 0.6776, loss: 0.6974 ||:  23%|##3       | 15/65 [02:56<09:45, 11.71s/it]
2022-04-08 21:51:57,991 - INFO - tqdm - accuracy: 0.5176, batch_loss: 0.6501, loss: 0.6945 ||:  25%|##4       | 16/65 [03:07<09:35, 11.74s/it]
2022-04-08 21:52:09,759 - INFO - tqdm - accuracy: 0.5129, batch_loss: 0.6804, loss: 0.6937 ||:  26%|##6       | 17/65 [03:19<09:23, 11.75s/it]
2022-04-08 21:52:21,528 - INFO - tqdm - accuracy: 0.5174, batch_loss: 0.6540, loss: 0.6915 ||:  28%|##7       | 18/65 [03:31<09:12, 11.75s/it]
2022-04-08 21:52:33,303 - INFO - tqdm - accuracy: 0.5164, batch_loss: 0.6979, loss: 0.6918 ||:  29%|##9       | 19/65 [03:43<09:00, 11.76s/it]
2022-04-08 21:52:45,060 - INFO - tqdm - accuracy: 0.5203, batch_loss: 0.6807, loss: 0.6912 ||:  31%|###       | 20/65 [03:54<08:49, 11.76s/it]
2022-04-08 21:52:56,817 - INFO - tqdm - accuracy: 0.5223, batch_loss: 0.6825, loss: 0.6908 ||:  32%|###2      | 21/65 [04:06<08:37, 11.76s/it]
2022-04-08 21:53:08,458 - INFO - tqdm - accuracy: 0.5213, batch_loss: 0.7028, loss: 0.6914 ||:  34%|###3      | 22/65 [04:18<08:24, 11.72s/it]
2022-04-08 21:53:20,222 - INFO - tqdm - accuracy: 0.5299, batch_loss: 0.6258, loss: 0.6885 ||:  35%|###5      | 23/65 [04:30<08:12, 11.74s/it]
2022-04-08 21:53:31,983 - INFO - tqdm - accuracy: 0.5260, batch_loss: 0.7420, loss: 0.6907 ||:  37%|###6      | 24/65 [04:41<08:01, 11.74s/it]
2022-04-08 21:53:43,739 - INFO - tqdm - accuracy: 0.5262, batch_loss: 0.6770, loss: 0.6902 ||:  38%|###8      | 25/65 [04:53<07:49, 11.75s/it]
2022-04-08 21:53:55,501 - INFO - tqdm - accuracy: 0.5276, batch_loss: 0.6813, loss: 0.6899 ||:  40%|####      | 26/65 [05:05<07:38, 11.75s/it]
2022-04-08 21:54:07,252 - INFO - tqdm - accuracy: 0.5266, batch_loss: 0.6940, loss: 0.6900 ||:  42%|####1     | 27/65 [05:17<07:26, 11.75s/it]
2022-04-08 21:54:19,018 - INFO - tqdm - accuracy: 0.5279, batch_loss: 0.6565, loss: 0.6888 ||:  43%|####3     | 28/65 [05:28<07:14, 11.76s/it]
2022-04-08 21:54:30,780 - INFO - tqdm - accuracy: 0.5269, batch_loss: 0.6980, loss: 0.6891 ||:  45%|####4     | 29/65 [05:40<07:03, 11.76s/it]
2022-04-08 21:54:42,539 - INFO - tqdm - accuracy: 0.5250, batch_loss: 0.7177, loss: 0.6901 ||:  46%|####6     | 30/65 [05:52<06:51, 11.76s/it]
2022-04-08 21:54:54,303 - INFO - tqdm - accuracy: 0.5232, batch_loss: 0.6942, loss: 0.6902 ||:  48%|####7     | 31/65 [06:04<06:39, 11.76s/it]
2022-04-08 21:55:06,070 - INFO - tqdm - accuracy: 0.5195, batch_loss: 0.6712, loss: 0.6896 ||:  49%|####9     | 32/65 [06:16<06:28, 11.76s/it]
2022-04-08 21:55:17,855 - INFO - tqdm - accuracy: 0.5161, batch_loss: 0.7100, loss: 0.6902 ||:  51%|#####     | 33/65 [06:27<06:16, 11.77s/it]
2022-04-08 21:55:29,621 - INFO - tqdm - accuracy: 0.5175, batch_loss: 0.6899, loss: 0.6902 ||:  52%|#####2    | 34/65 [06:39<06:04, 11.77s/it]
2022-04-08 21:55:41,778 - INFO - tqdm - accuracy: 0.5161, batch_loss: 0.7041, loss: 0.6906 ||:  54%|#####3    | 35/65 [06:51<05:56, 11.88s/it]
2022-04-08 21:55:53,490 - INFO - tqdm - accuracy: 0.5182, batch_loss: 0.6687, loss: 0.6900 ||:  55%|#####5    | 36/65 [07:03<05:43, 11.83s/it]
2022-04-08 21:56:05,268 - INFO - tqdm - accuracy: 0.5203, batch_loss: 0.6670, loss: 0.6894 ||:  57%|#####6    | 37/65 [07:15<05:30, 11.82s/it]
2022-04-08 21:56:17,061 - INFO - tqdm - accuracy: 0.5222, batch_loss: 0.6573, loss: 0.6885 ||:  58%|#####8    | 38/65 [07:26<05:18, 11.81s/it]
2022-04-08 21:56:28,859 - INFO - tqdm - accuracy: 0.5224, batch_loss: 0.7065, loss: 0.6890 ||:  60%|######    | 39/65 [07:38<05:06, 11.81s/it]
2022-04-08 21:56:40,651 - INFO - tqdm - accuracy: 0.5227, batch_loss: 0.6993, loss: 0.6893 ||:  62%|######1   | 40/65 [07:50<04:55, 11.80s/it]
2022-04-08 21:56:52,426 - INFO - tqdm - accuracy: 0.5244, batch_loss: 0.6776, loss: 0.6890 ||:  63%|######3   | 41/65 [08:02<04:43, 11.79s/it]
2022-04-08 21:57:04,190 - INFO - tqdm - accuracy: 0.5260, batch_loss: 0.6630, loss: 0.6884 ||:  65%|######4   | 42/65 [08:14<04:31, 11.78s/it]
2022-04-08 21:57:15,946 - INFO - tqdm - accuracy: 0.5233, batch_loss: 0.7056, loss: 0.6888 ||:  66%|######6   | 43/65 [08:25<04:19, 11.78s/it]
2022-04-08 21:57:27,692 - INFO - tqdm - accuracy: 0.5241, batch_loss: 0.6982, loss: 0.6890 ||:  68%|######7   | 44/65 [08:37<04:07, 11.77s/it]
2022-04-08 21:57:39,437 - INFO - tqdm - accuracy: 0.5243, batch_loss: 0.6725, loss: 0.6886 ||:  69%|######9   | 45/65 [08:49<03:55, 11.76s/it]
2022-04-08 21:57:51,147 - INFO - tqdm - accuracy: 0.5231, batch_loss: 0.7130, loss: 0.6891 ||:  71%|#######   | 46/65 [09:01<03:43, 11.75s/it]
2022-04-08 21:58:02,866 - INFO - tqdm - accuracy: 0.5219, batch_loss: 0.7057, loss: 0.6895 ||:  72%|#######2  | 47/65 [09:12<03:31, 11.74s/it]
2022-04-08 21:58:14,577 - INFO - tqdm - accuracy: 0.5228, batch_loss: 0.6972, loss: 0.6897 ||:  74%|#######3  | 48/65 [09:24<03:19, 11.73s/it]
2022-04-08 21:58:26,300 - INFO - tqdm - accuracy: 0.5198, batch_loss: 0.7129, loss: 0.6901 ||:  75%|#######5  | 49/65 [09:36<03:07, 11.73s/it]
2022-04-08 21:58:37,988 - INFO - tqdm - accuracy: 0.5219, batch_loss: 0.6818, loss: 0.6900 ||:  77%|#######6  | 50/65 [09:47<02:55, 11.72s/it]
2022-04-08 21:58:49,745 - INFO - tqdm - accuracy: 0.5202, batch_loss: 0.7131, loss: 0.6904 ||:  78%|#######8  | 51/65 [09:59<02:44, 11.73s/it]
2022-04-08 21:59:01,513 - INFO - tqdm - accuracy: 0.5210, batch_loss: 0.7046, loss: 0.6907 ||:  80%|########  | 52/65 [10:11<02:32, 11.74s/it]
2022-04-08 21:59:13,267 - INFO - tqdm - accuracy: 0.5218, batch_loss: 0.6647, loss: 0.6902 ||:  82%|########1 | 53/65 [10:23<02:20, 11.74s/it]
2022-04-08 21:59:25,061 - INFO - tqdm - accuracy: 0.5220, batch_loss: 0.7066, loss: 0.6905 ||:  83%|########3 | 54/65 [10:34<02:09, 11.76s/it]
2022-04-08 21:59:36,835 - INFO - tqdm - accuracy: 0.5239, batch_loss: 0.6538, loss: 0.6898 ||:  85%|########4 | 55/65 [10:46<01:57, 11.76s/it]
2022-04-08 21:59:48,554 - INFO - tqdm - accuracy: 0.5234, batch_loss: 0.7121, loss: 0.6902 ||:  86%|########6 | 56/65 [10:58<01:45, 11.75s/it]
2022-04-08 21:59:59,958 - INFO - tqdm - accuracy: 0.5252, batch_loss: 0.6863, loss: 0.6902 ||:  88%|########7 | 57/65 [11:09<01:33, 11.65s/it]
2022-04-08 22:00:11,639 - INFO - tqdm - accuracy: 0.5232, batch_loss: 0.7091, loss: 0.6905 ||:  89%|########9 | 58/65 [11:21<01:21, 11.66s/it]
2022-04-08 22:00:23,366 - INFO - tqdm - accuracy: 0.5244, batch_loss: 0.6711, loss: 0.6902 ||:  91%|######### | 59/65 [11:33<01:10, 11.68s/it]
2022-04-08 22:00:35,102 - INFO - tqdm - accuracy: 0.5229, batch_loss: 0.6880, loss: 0.6901 ||:  92%|#########2| 60/65 [11:45<00:58, 11.70s/it]
2022-04-08 22:00:46,842 - INFO - tqdm - accuracy: 0.5246, batch_loss: 0.6484, loss: 0.6894 ||:  94%|#########3| 61/65 [11:56<00:46, 11.71s/it]
2022-04-08 22:00:58,600 - INFO - tqdm - accuracy: 0.5262, batch_loss: 0.6387, loss: 0.6886 ||:  95%|#########5| 62/65 [12:08<00:35, 11.72s/it]
2022-04-08 22:01:10,391 - INFO - tqdm - accuracy: 0.5283, batch_loss: 0.6429, loss: 0.6879 ||:  97%|#########6| 63/65 [12:20<00:23, 11.74s/it]
2022-04-08 22:01:21,828 - INFO - tqdm - accuracy: 0.5281, batch_loss: 0.7098, loss: 0.6882 ||:  98%|#########8| 64/65 [12:31<00:11, 11.65s/it]
2022-04-08 22:01:27,048 - INFO - tqdm - accuracy: 0.5284, batch_loss: 0.6854, loss: 0.6882 ||: 100%|##########| 65/65 [12:36<00:00,  9.72s/it]
2022-04-08 22:01:27,048 - INFO - tqdm - accuracy: 0.5284, batch_loss: 0.6854, loss: 0.6882 ||: 100%|##########| 65/65 [12:36<00:00, 11.65s/it]
2022-04-08 22:01:29,929 - INFO - allennlp.training.trainer - Validating
2022-04-08 22:01:29,931 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-08 22:01:40,153 - INFO - tqdm - accuracy: 0.5250, batch_loss: 0.9521, loss: 0.7038 ||:  35%|###4      | 40/115 [00:10<00:19,  3.84it/s]
2022-04-08 22:01:50,317 - INFO - tqdm - accuracy: 0.5316, batch_loss: 0.4659, loss: 0.7135 ||:  69%|######8   | 79/115 [00:20<00:09,  3.82it/s]
2022-04-08 22:01:59,621 - INFO - tqdm - accuracy: 0.5371, batch_loss: 0.4938, loss: 0.7077 ||: 100%|##########| 115/115 [00:29<00:00,  3.83it/s]
2022-04-08 22:01:59,621 - INFO - tqdm - accuracy: 0.5371, batch_loss: 0.4938, loss: 0.7077 ||: 100%|##########| 115/115 [00:29<00:00,  3.87it/s]
2022-04-08 22:01:59,621 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-08 22:01:59,622 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.528  |     0.537
2022-04-08 22:01:59,623 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-08 22:01:59,623 - INFO - allennlp.training.tensorboard_writer - loss               |     0.688  |     0.708
2022-04-08 22:01:59,624 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5799.516  |       N/A
2022-04-08 22:02:05,529 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to './output_newboolq_ir-ora-d/best.th'.
2022-04-08 22:02:17,796 - INFO - allennlp.training.trainer - Epoch duration: 0:13:27.729300
2022-04-08 22:02:17,796 - INFO - allennlp.training.trainer - Estimated training time remaining: 2:52:36
2022-04-08 22:02:17,796 - INFO - allennlp.training.trainer - Epoch 2/14
2022-04-08 22:02:17,796 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-08 22:02:17,796 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-08 22:02:17,798 - INFO - allennlp.training.trainer - Training
2022-04-08 22:02:17,798 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-08 22:02:29,227 - INFO - tqdm - accuracy: 0.5938, batch_loss: 0.6851, loss: 0.6851 ||:   2%|1         | 1/65 [00:11<12:11, 11.43s/it]
2022-04-08 22:02:41,109 - INFO - tqdm - accuracy: 0.6562, batch_loss: 0.5824, loss: 0.6337 ||:   3%|3         | 2/65 [00:23<12:16, 11.70s/it]
2022-04-08 22:02:53,311 - INFO - tqdm - accuracy: 0.6042, batch_loss: 0.7044, loss: 0.6573 ||:   5%|4         | 3/65 [00:35<12:19, 11.93s/it]
2022-04-08 22:03:05,380 - INFO - tqdm - accuracy: 0.5781, batch_loss: 0.6703, loss: 0.6606 ||:   6%|6         | 4/65 [00:47<12:10, 11.98s/it]
2022-04-08 22:03:17,099 - INFO - tqdm - accuracy: 0.5813, batch_loss: 0.6412, loss: 0.6567 ||:   8%|7         | 5/65 [00:59<11:53, 11.89s/it]
2022-04-08 22:03:28,519 - INFO - tqdm - accuracy: 0.5885, batch_loss: 0.6399, loss: 0.6539 ||:   9%|9         | 6/65 [01:10<11:31, 11.73s/it]
2022-04-08 22:03:40,054 - INFO - tqdm - accuracy: 0.5580, batch_loss: 0.7415, loss: 0.6664 ||:  11%|#         | 7/65 [01:22<11:16, 11.67s/it]
2022-04-08 22:03:51,761 - INFO - tqdm - accuracy: 0.5625, batch_loss: 0.7098, loss: 0.6718 ||:  12%|#2        | 8/65 [01:33<11:05, 11.68s/it]
2022-04-08 22:04:03,606 - INFO - tqdm - accuracy: 0.5694, batch_loss: 0.6041, loss: 0.6643 ||:  14%|#3        | 9/65 [01:45<10:56, 11.73s/it]
2022-04-08 22:04:15,453 - INFO - tqdm - accuracy: 0.5844, batch_loss: 0.6308, loss: 0.6609 ||:  15%|#5        | 10/65 [01:57<10:47, 11.77s/it]
2022-04-08 22:04:27,208 - INFO - tqdm - accuracy: 0.5909, batch_loss: 0.6319, loss: 0.6583 ||:  17%|#6        | 11/65 [02:09<10:35, 11.76s/it]
2022-04-08 22:04:38,893 - INFO - tqdm - accuracy: 0.5885, batch_loss: 0.7074, loss: 0.6624 ||:  18%|#8        | 12/65 [02:21<10:22, 11.74s/it]
2022-04-08 22:04:50,200 - INFO - tqdm - accuracy: 0.5807, batch_loss: 0.6994, loss: 0.6652 ||:  20%|##        | 13/65 [02:32<10:03, 11.61s/it]
2022-04-08 22:05:01,927 - INFO - tqdm - accuracy: 0.5861, batch_loss: 0.6474, loss: 0.6640 ||:  22%|##1       | 14/65 [02:44<09:53, 11.64s/it]
2022-04-08 22:05:13,701 - INFO - tqdm - accuracy: 0.5825, batch_loss: 0.6576, loss: 0.6635 ||:  23%|##3       | 15/65 [02:55<09:44, 11.68s/it]
2022-04-08 22:05:25,513 - INFO - tqdm - accuracy: 0.5930, batch_loss: 0.5753, loss: 0.6580 ||:  25%|##4       | 16/65 [03:07<09:34, 11.72s/it]
2022-04-08 22:05:37,268 - INFO - tqdm - accuracy: 0.5912, batch_loss: 0.7160, loss: 0.6614 ||:  26%|##6       | 17/65 [03:19<09:23, 11.73s/it]
2022-04-08 22:05:48,986 - INFO - tqdm - accuracy: 0.5878, batch_loss: 0.7099, loss: 0.6641 ||:  28%|##7       | 18/65 [03:31<09:11, 11.73s/it]
2022-04-08 22:06:00,671 - INFO - tqdm - accuracy: 0.5898, batch_loss: 0.6606, loss: 0.6639 ||:  29%|##9       | 19/65 [03:42<08:58, 11.72s/it]
2022-04-08 22:06:12,347 - INFO - tqdm - accuracy: 0.5931, batch_loss: 0.6286, loss: 0.6622 ||:  31%|###       | 20/65 [03:54<08:46, 11.70s/it]
2022-04-08 22:06:24,098 - INFO - tqdm - accuracy: 0.5931, batch_loss: 0.6134, loss: 0.6598 ||:  32%|###2      | 21/65 [04:06<08:35, 11.72s/it]
2022-04-08 22:06:35,909 - INFO - tqdm - accuracy: 0.5960, batch_loss: 0.5769, loss: 0.6561 ||:  34%|###3      | 22/65 [04:18<08:25, 11.75s/it]
2022-04-08 22:06:47,703 - INFO - tqdm - accuracy: 0.6041, batch_loss: 0.5796, loss: 0.6528 ||:  35%|###5      | 23/65 [04:29<08:13, 11.76s/it]
2022-04-08 22:06:59,340 - INFO - tqdm - accuracy: 0.6023, batch_loss: 0.6254, loss: 0.6516 ||:  37%|###6      | 24/65 [04:41<08:00, 11.72s/it]
2022-04-08 22:07:11,014 - INFO - tqdm - accuracy: 0.6058, batch_loss: 0.6287, loss: 0.6507 ||:  38%|###8      | 25/65 [04:53<07:48, 11.71s/it]
2022-04-08 22:07:22,700 - INFO - tqdm - accuracy: 0.6065, batch_loss: 0.6938, loss: 0.6524 ||:  40%|####      | 26/65 [05:04<07:36, 11.70s/it]
2022-04-08 22:07:34,383 - INFO - tqdm - accuracy: 0.6083, batch_loss: 0.6736, loss: 0.6531 ||:  42%|####1     | 27/65 [05:16<07:24, 11.70s/it]
2022-04-08 22:07:46,132 - INFO - tqdm - accuracy: 0.6067, batch_loss: 0.6281, loss: 0.6522 ||:  43%|####3     | 28/65 [05:28<07:13, 11.71s/it]
2022-04-08 22:07:57,890 - INFO - tqdm - accuracy: 0.6073, batch_loss: 0.6151, loss: 0.6510 ||:  45%|####4     | 29/65 [05:40<07:02, 11.73s/it]
2022-04-08 22:08:09,673 - INFO - tqdm - accuracy: 0.6100, batch_loss: 0.6148, loss: 0.6498 ||:  46%|####6     | 30/65 [05:51<06:51, 11.74s/it]
2022-04-08 22:08:21,460 - INFO - tqdm - accuracy: 0.6125, batch_loss: 0.5624, loss: 0.6469 ||:  48%|####7     | 31/65 [06:03<06:39, 11.76s/it]
2022-04-08 22:08:33,229 - INFO - tqdm - accuracy: 0.6149, batch_loss: 0.6240, loss: 0.6462 ||:  49%|####9     | 32/65 [06:15<06:28, 11.76s/it]
2022-04-08 22:08:44,982 - INFO - tqdm - accuracy: 0.6114, batch_loss: 0.7869, loss: 0.6505 ||:  51%|#####     | 33/65 [06:27<06:16, 11.76s/it]
2022-04-08 22:08:56,726 - INFO - tqdm - accuracy: 0.6118, batch_loss: 0.6417, loss: 0.6502 ||:  52%|#####2    | 34/65 [06:38<06:04, 11.75s/it]
2022-04-08 22:09:08,471 - INFO - tqdm - accuracy: 0.6157, batch_loss: 0.5544, loss: 0.6475 ||:  54%|#####3    | 35/65 [06:50<05:52, 11.75s/it]
2022-04-08 22:09:20,170 - INFO - tqdm - accuracy: 0.6142, batch_loss: 0.6367, loss: 0.6472 ||:  55%|#####5    | 36/65 [07:02<05:40, 11.74s/it]
2022-04-08 22:09:31,860 - INFO - tqdm - accuracy: 0.6137, batch_loss: 0.6258, loss: 0.6466 ||:  57%|#####6    | 37/65 [07:14<05:28, 11.72s/it]
2022-04-08 22:09:43,543 - INFO - tqdm - accuracy: 0.6165, batch_loss: 0.5303, loss: 0.6436 ||:  58%|#####8    | 38/65 [07:25<05:16, 11.71s/it]
2022-04-08 22:09:55,247 - INFO - tqdm - accuracy: 0.6183, batch_loss: 0.5465, loss: 0.6411 ||:  60%|######    | 39/65 [07:37<05:04, 11.71s/it]
2022-04-08 22:10:06,970 - INFO - tqdm - accuracy: 0.6177, batch_loss: 0.7292, loss: 0.6433 ||:  62%|######1   | 40/65 [07:49<04:52, 11.71s/it]
2022-04-08 22:10:18,673 - INFO - tqdm - accuracy: 0.6171, batch_loss: 0.6483, loss: 0.6434 ||:  63%|######3   | 41/65 [08:00<04:41, 11.71s/it]
2022-04-08 22:10:30,270 - INFO - tqdm - accuracy: 0.6173, batch_loss: 0.6190, loss: 0.6428 ||:  65%|######4   | 42/65 [08:12<04:28, 11.68s/it]
2022-04-08 22:10:41,988 - INFO - tqdm - accuracy: 0.6189, batch_loss: 0.5857, loss: 0.6415 ||:  66%|######6   | 43/65 [08:24<04:17, 11.69s/it]
2022-04-08 22:10:53,697 - INFO - tqdm - accuracy: 0.6198, batch_loss: 0.5796, loss: 0.6401 ||:  68%|######7   | 44/65 [08:35<04:05, 11.69s/it]
2022-04-08 22:11:05,453 - INFO - tqdm - accuracy: 0.6213, batch_loss: 0.6584, loss: 0.6405 ||:  69%|######9   | 45/65 [08:47<03:54, 11.71s/it]
2022-04-08 22:11:17,194 - INFO - tqdm - accuracy: 0.6213, batch_loss: 0.5850, loss: 0.6393 ||:  71%|#######   | 46/65 [08:59<03:42, 11.72s/it]
2022-04-08 22:11:28,957 - INFO - tqdm - accuracy: 0.6194, batch_loss: 0.7382, loss: 0.6414 ||:  72%|#######2  | 47/65 [09:11<03:31, 11.73s/it]
2022-04-08 22:11:40,720 - INFO - tqdm - accuracy: 0.6215, batch_loss: 0.7114, loss: 0.6428 ||:  74%|#######3  | 48/65 [09:22<03:19, 11.74s/it]
2022-04-08 22:11:52,494 - INFO - tqdm - accuracy: 0.6197, batch_loss: 0.6480, loss: 0.6429 ||:  75%|#######5  | 49/65 [09:34<03:08, 11.75s/it]
2022-04-08 22:12:04,261 - INFO - tqdm - accuracy: 0.6185, batch_loss: 0.5638, loss: 0.6414 ||:  77%|#######6  | 50/65 [09:46<02:56, 11.76s/it]
2022-04-08 22:12:16,034 - INFO - tqdm - accuracy: 0.6199, batch_loss: 0.5850, loss: 0.6403 ||:  78%|#######8  | 51/65 [09:58<02:44, 11.76s/it]
2022-04-08 22:12:27,812 - INFO - tqdm - accuracy: 0.6206, batch_loss: 0.5969, loss: 0.6394 ||:  80%|########  | 52/65 [10:10<02:32, 11.77s/it]
2022-04-08 22:12:39,590 - INFO - tqdm - accuracy: 0.6230, batch_loss: 0.6324, loss: 0.6393 ||:  82%|########1 | 53/65 [10:21<02:21, 11.77s/it]
2022-04-08 22:12:51,332 - INFO - tqdm - accuracy: 0.6225, batch_loss: 0.6349, loss: 0.6392 ||:  83%|########3 | 54/65 [10:33<02:09, 11.76s/it]
2022-04-08 22:13:03,094 - INFO - tqdm - accuracy: 0.6254, batch_loss: 0.5820, loss: 0.6382 ||:  85%|########4 | 55/65 [10:45<01:57, 11.76s/it]
2022-04-08 22:13:14,799 - INFO - tqdm - accuracy: 0.6259, batch_loss: 0.5957, loss: 0.6374 ||:  86%|########6 | 56/65 [10:57<01:45, 11.74s/it]
2022-04-08 22:13:26,524 - INFO - tqdm - accuracy: 0.6226, batch_loss: 0.7239, loss: 0.6389 ||:  88%|########7 | 57/65 [11:08<01:33, 11.74s/it]
2022-04-08 22:13:37,967 - INFO - tqdm - accuracy: 0.6243, batch_loss: 0.5356, loss: 0.6371 ||:  89%|########9 | 58/65 [11:20<01:21, 11.65s/it]
2022-04-08 22:13:49,671 - INFO - tqdm - accuracy: 0.6259, batch_loss: 0.5649, loss: 0.6359 ||:  91%|######### | 59/65 [11:31<01:09, 11.67s/it]
2022-04-08 22:14:01,357 - INFO - tqdm - accuracy: 0.6274, batch_loss: 0.5731, loss: 0.6349 ||:  92%|#########2| 60/65 [11:43<00:58, 11.67s/it]
2022-04-08 22:14:13,050 - INFO - tqdm - accuracy: 0.6294, batch_loss: 0.4953, loss: 0.6326 ||:  94%|#########3| 61/65 [11:55<00:46, 11.68s/it]
2022-04-08 22:14:24,763 - INFO - tqdm - accuracy: 0.6273, batch_loss: 0.6803, loss: 0.6334 ||:  95%|#########5| 62/65 [12:06<00:35, 11.69s/it]
2022-04-08 22:14:36,476 - INFO - tqdm - accuracy: 0.6273, batch_loss: 0.6160, loss: 0.6331 ||:  97%|#########6| 63/65 [12:18<00:23, 11.70s/it]
2022-04-08 22:14:48,213 - INFO - tqdm - accuracy: 0.6263, batch_loss: 0.6849, loss: 0.6339 ||:  98%|#########8| 64/65 [12:30<00:11, 11.71s/it]
2022-04-08 22:14:53,413 - INFO - tqdm - accuracy: 0.6264, batch_loss: 0.7279, loss: 0.6353 ||: 100%|##########| 65/65 [12:35<00:00,  9.76s/it]
2022-04-08 22:14:53,413 - INFO - tqdm - accuracy: 0.6264, batch_loss: 0.7279, loss: 0.6353 ||: 100%|##########| 65/65 [12:35<00:00, 11.62s/it]
2022-04-08 22:14:56,330 - INFO - allennlp.training.trainer - Validating
2022-04-08 22:14:56,332 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-08 22:15:06,347 - INFO - tqdm - accuracy: 0.5641, batch_loss: 0.6706, loss: 0.7816 ||:  34%|###3      | 39/115 [00:10<00:19,  3.88it/s]
2022-04-08 22:15:16,558 - INFO - tqdm - accuracy: 0.5732, batch_loss: 0.1216, loss: 0.7209 ||:  69%|######8   | 79/115 [00:20<00:09,  3.95it/s]
2022-04-08 22:15:26,027 - INFO - tqdm - accuracy: 0.5852, batch_loss: 1.3576, loss: 0.7029 ||: 100%|##########| 115/115 [00:29<00:00,  3.81it/s]
2022-04-08 22:15:26,028 - INFO - tqdm - accuracy: 0.5852, batch_loss: 1.3576, loss: 0.7029 ||: 100%|##########| 115/115 [00:29<00:00,  3.87it/s]
2022-04-08 22:15:26,028 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-08 22:15:26,028 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.626  |     0.585
2022-04-08 22:15:26,029 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-08 22:15:26,030 - INFO - allennlp.training.tensorboard_writer - loss               |     0.635  |     0.703
2022-04-08 22:15:26,030 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5799.516  |       N/A
2022-04-08 22:15:31,637 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to './output_newboolq_ir-ora-d/best.th'.
2022-04-08 22:15:42,760 - INFO - allennlp.training.trainer - Epoch duration: 0:13:24.963916
2022-04-08 22:15:42,760 - INFO - allennlp.training.trainer - Estimated training time remaining: 2:39:53
2022-04-08 22:15:42,760 - INFO - allennlp.training.trainer - Epoch 3/14
2022-04-08 22:15:42,760 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-08 22:15:42,761 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-08 22:15:42,762 - INFO - allennlp.training.trainer - Training
2022-04-08 22:15:42,762 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-08 22:15:54,172 - INFO - tqdm - accuracy: 0.5625, batch_loss: 0.7166, loss: 0.7166 ||:   2%|1         | 1/65 [00:11<12:10, 11.41s/it]
2022-04-08 22:16:05,971 - INFO - tqdm - accuracy: 0.5938, batch_loss: 0.6350, loss: 0.6758 ||:   3%|3         | 2/65 [00:23<12:13, 11.64s/it]
2022-04-08 22:16:18,121 - INFO - tqdm - accuracy: 0.6562, batch_loss: 0.5564, loss: 0.6360 ||:   5%|4         | 3/65 [00:35<12:16, 11.87s/it]
2022-04-08 22:16:30,233 - INFO - tqdm - accuracy: 0.6875, batch_loss: 0.5038, loss: 0.6029 ||:   6%|6         | 4/65 [00:47<12:09, 11.97s/it]
2022-04-08 22:16:42,394 - INFO - tqdm - accuracy: 0.6875, batch_loss: 0.5754, loss: 0.5974 ||:   8%|7         | 5/65 [00:59<12:02, 12.04s/it]
2022-04-08 22:16:53,890 - INFO - tqdm - accuracy: 0.6927, batch_loss: 0.5457, loss: 0.5888 ||:   9%|9         | 6/65 [01:11<11:39, 11.85s/it]
2022-04-08 22:17:05,388 - INFO - tqdm - accuracy: 0.7098, batch_loss: 0.4931, loss: 0.5751 ||:  11%|#         | 7/65 [01:22<11:20, 11.74s/it]
2022-04-08 22:17:16,894 - INFO - tqdm - accuracy: 0.7109, batch_loss: 0.5539, loss: 0.5725 ||:  12%|#2        | 8/65 [01:34<11:04, 11.66s/it]
2022-04-08 22:17:28,709 - INFO - tqdm - accuracy: 0.7049, batch_loss: 0.5728, loss: 0.5725 ||:  14%|#3        | 9/65 [01:45<10:55, 11.71s/it]
2022-04-08 22:17:40,572 - INFO - tqdm - accuracy: 0.7219, batch_loss: 0.4224, loss: 0.5575 ||:  15%|#5        | 10/65 [01:57<10:46, 11.76s/it]
2022-04-08 22:17:52,218 - INFO - tqdm - accuracy: 0.7216, batch_loss: 0.4312, loss: 0.5460 ||:  17%|#6        | 11/65 [02:09<10:33, 11.72s/it]
2022-04-08 22:18:03,914 - INFO - tqdm - accuracy: 0.7292, batch_loss: 0.4322, loss: 0.5365 ||:  18%|#8        | 12/65 [02:21<10:20, 11.72s/it]
2022-04-08 22:18:15,571 - INFO - tqdm - accuracy: 0.7332, batch_loss: 0.4689, loss: 0.5313 ||:  20%|##        | 13/65 [02:32<10:08, 11.70s/it]
2022-04-08 22:18:27,309 - INFO - tqdm - accuracy: 0.7210, batch_loss: 0.6666, loss: 0.5410 ||:  22%|##1       | 14/65 [02:44<09:57, 11.71s/it]
2022-04-08 22:18:39,088 - INFO - tqdm - accuracy: 0.7250, batch_loss: 0.4674, loss: 0.5361 ||:  23%|##3       | 15/65 [02:56<09:46, 11.73s/it]
2022-04-08 22:18:50,868 - INFO - tqdm - accuracy: 0.7266, batch_loss: 0.5363, loss: 0.5361 ||:  25%|##4       | 16/65 [03:08<09:35, 11.75s/it]
2022-04-08 22:19:02,623 - INFO - tqdm - accuracy: 0.7243, batch_loss: 0.5453, loss: 0.5366 ||:  26%|##6       | 17/65 [03:19<09:23, 11.75s/it]
2022-04-08 22:19:14,338 - INFO - tqdm - accuracy: 0.7326, batch_loss: 0.3752, loss: 0.5277 ||:  28%|##7       | 18/65 [03:31<09:11, 11.74s/it]
2022-04-08 22:19:26,006 - INFO - tqdm - accuracy: 0.7303, batch_loss: 0.6518, loss: 0.5342 ||:  29%|##9       | 19/65 [03:43<08:58, 11.72s/it]
2022-04-08 22:19:37,670 - INFO - tqdm - accuracy: 0.7250, batch_loss: 0.5701, loss: 0.5360 ||:  31%|###       | 20/65 [03:54<08:46, 11.70s/it]
2022-04-08 22:19:49,402 - INFO - tqdm - accuracy: 0.7277, batch_loss: 0.4318, loss: 0.5310 ||:  32%|###2      | 21/65 [04:06<08:35, 11.71s/it]
2022-04-08 22:20:01,171 - INFO - tqdm - accuracy: 0.7344, batch_loss: 0.4468, loss: 0.5272 ||:  34%|###3      | 22/65 [04:18<08:24, 11.73s/it]
2022-04-08 22:20:12,959 - INFO - tqdm - accuracy: 0.7418, batch_loss: 0.3074, loss: 0.5177 ||:  35%|###5      | 23/65 [04:30<08:13, 11.75s/it]
2022-04-08 22:20:24,710 - INFO - tqdm - accuracy: 0.7435, batch_loss: 0.4081, loss: 0.5131 ||:  37%|###6      | 24/65 [04:41<08:01, 11.75s/it]
2022-04-08 22:20:36,449 - INFO - tqdm - accuracy: 0.7500, batch_loss: 0.3333, loss: 0.5059 ||:  38%|###8      | 25/65 [04:53<07:49, 11.74s/it]
2022-04-08 22:20:48,152 - INFO - tqdm - accuracy: 0.7536, batch_loss: 0.4453, loss: 0.5036 ||:  40%|####      | 26/65 [05:05<07:37, 11.73s/it]
2022-04-08 22:20:59,836 - INFO - tqdm - accuracy: 0.7535, batch_loss: 0.5790, loss: 0.5064 ||:  42%|####1     | 27/65 [05:17<07:25, 11.72s/it]
2022-04-08 22:21:11,528 - INFO - tqdm - accuracy: 0.7567, batch_loss: 0.3663, loss: 0.5014 ||:  43%|####3     | 28/65 [05:28<07:13, 11.71s/it]
2022-04-08 22:21:23,263 - INFO - tqdm - accuracy: 0.7586, batch_loss: 0.4793, loss: 0.5006 ||:  45%|####4     | 29/65 [05:40<07:01, 11.72s/it]
2022-04-08 22:21:35,029 - INFO - tqdm - accuracy: 0.7615, batch_loss: 0.4053, loss: 0.4974 ||:  46%|####6     | 30/65 [05:52<06:50, 11.73s/it]
2022-04-08 22:21:46,829 - INFO - tqdm - accuracy: 0.7641, batch_loss: 0.4503, loss: 0.4959 ||:  48%|####7     | 31/65 [06:04<06:39, 11.75s/it]
2022-04-08 22:21:58,510 - INFO - tqdm - accuracy: 0.7637, batch_loss: 0.4332, loss: 0.4939 ||:  49%|####9     | 32/65 [06:15<06:27, 11.73s/it]
2022-04-08 22:22:10,277 - INFO - tqdm - accuracy: 0.7614, batch_loss: 0.4943, loss: 0.4940 ||:  51%|#####     | 33/65 [06:27<06:15, 11.74s/it]
2022-04-08 22:22:22,002 - INFO - tqdm - accuracy: 0.7619, batch_loss: 0.4530, loss: 0.4928 ||:  52%|#####2    | 34/65 [06:39<06:03, 11.74s/it]
2022-04-08 22:22:33,674 - INFO - tqdm - accuracy: 0.7589, batch_loss: 0.6286, loss: 0.4966 ||:  54%|#####3    | 35/65 [06:50<05:51, 11.72s/it]
2022-04-08 22:22:45,387 - INFO - tqdm - accuracy: 0.7595, batch_loss: 0.5432, loss: 0.4979 ||:  55%|#####5    | 36/65 [07:02<05:39, 11.72s/it]
2022-04-08 22:22:57,124 - INFO - tqdm - accuracy: 0.7593, batch_loss: 0.6141, loss: 0.5011 ||:  57%|#####6    | 37/65 [07:14<05:28, 11.72s/it]
2022-04-08 22:23:08,876 - INFO - tqdm - accuracy: 0.7590, batch_loss: 0.5019, loss: 0.5011 ||:  58%|#####8    | 38/65 [07:26<05:16, 11.73s/it]
2022-04-08 22:23:20,622 - INFO - tqdm - accuracy: 0.7556, batch_loss: 0.6599, loss: 0.5052 ||:  60%|######    | 39/65 [07:37<05:05, 11.74s/it]
2022-04-08 22:23:32,401 - INFO - tqdm - accuracy: 0.7539, batch_loss: 0.4886, loss: 0.5048 ||:  62%|######1   | 40/65 [07:49<04:53, 11.75s/it]
2022-04-08 22:23:44,191 - INFO - tqdm - accuracy: 0.7538, batch_loss: 0.4603, loss: 0.5037 ||:  63%|######3   | 41/65 [08:01<04:42, 11.76s/it]
2022-04-08 22:23:55,994 - INFO - tqdm - accuracy: 0.7515, batch_loss: 0.5744, loss: 0.5054 ||:  65%|######4   | 42/65 [08:13<04:30, 11.77s/it]
2022-04-08 22:24:07,804 - INFO - tqdm - accuracy: 0.7529, batch_loss: 0.4868, loss: 0.5049 ||:  66%|######6   | 43/65 [08:25<04:19, 11.78s/it]
2022-04-08 22:24:19,583 - INFO - tqdm - accuracy: 0.7536, batch_loss: 0.5135, loss: 0.5051 ||:  68%|######7   | 44/65 [08:36<04:07, 11.78s/it]
2022-04-08 22:24:31,371 - INFO - tqdm - accuracy: 0.7521, batch_loss: 0.7228, loss: 0.5100 ||:  69%|######9   | 45/65 [08:48<03:55, 11.78s/it]
2022-04-08 22:24:43,126 - INFO - tqdm - accuracy: 0.7520, batch_loss: 0.4928, loss: 0.5096 ||:  71%|#######   | 46/65 [09:00<03:43, 11.78s/it]
2022-04-08 22:24:54,856 - INFO - tqdm - accuracy: 0.7527, batch_loss: 0.4434, loss: 0.5082 ||:  72%|#######2  | 47/65 [09:12<03:31, 11.76s/it]
2022-04-08 22:25:06,590 - INFO - tqdm - accuracy: 0.7546, batch_loss: 0.3949, loss: 0.5058 ||:  74%|#######3  | 48/65 [09:23<03:19, 11.75s/it]
2022-04-08 22:25:18,316 - INFO - tqdm - accuracy: 0.7532, batch_loss: 0.5464, loss: 0.5066 ||:  75%|#######5  | 49/65 [09:35<03:07, 11.75s/it]
2022-04-08 22:25:29,989 - INFO - tqdm - accuracy: 0.7519, batch_loss: 0.6324, loss: 0.5092 ||:  77%|#######6  | 50/65 [09:47<02:55, 11.72s/it]
2022-04-08 22:25:41,654 - INFO - tqdm - accuracy: 0.7500, batch_loss: 0.5846, loss: 0.5106 ||:  78%|#######8  | 51/65 [09:58<02:43, 11.71s/it]
2022-04-08 22:25:53,118 - INFO - tqdm - accuracy: 0.7488, batch_loss: 0.5172, loss: 0.5108 ||:  80%|########  | 52/65 [10:10<02:31, 11.63s/it]
2022-04-08 22:26:04,881 - INFO - tqdm - accuracy: 0.7476, batch_loss: 0.5688, loss: 0.5119 ||:  82%|########1 | 53/65 [10:22<02:20, 11.67s/it]
2022-04-08 22:26:16,670 - INFO - tqdm - accuracy: 0.7459, batch_loss: 0.6400, loss: 0.5142 ||:  83%|########3 | 54/65 [10:33<02:08, 11.71s/it]
2022-04-08 22:26:28,434 - INFO - tqdm - accuracy: 0.7483, batch_loss: 0.5010, loss: 0.5140 ||:  85%|########4 | 55/65 [10:45<01:57, 11.72s/it]
2022-04-08 22:26:40,185 - INFO - tqdm - accuracy: 0.7467, batch_loss: 0.5576, loss: 0.5148 ||:  86%|########6 | 56/65 [10:57<01:45, 11.73s/it]
2022-04-08 22:26:51,941 - INFO - tqdm - accuracy: 0.7445, batch_loss: 0.6051, loss: 0.5164 ||:  88%|########7 | 57/65 [11:09<01:33, 11.74s/it]
2022-04-08 22:27:03,659 - INFO - tqdm - accuracy: 0.7452, batch_loss: 0.4306, loss: 0.5149 ||:  89%|########9 | 58/65 [11:20<01:22, 11.73s/it]
2022-04-08 22:27:15,025 - INFO - tqdm - accuracy: 0.7430, batch_loss: 0.6012, loss: 0.5163 ||:  91%|######### | 59/65 [11:32<01:09, 11.62s/it]
2022-04-08 22:27:26,738 - INFO - tqdm - accuracy: 0.7436, batch_loss: 0.4931, loss: 0.5160 ||:  92%|#########2| 60/65 [11:43<00:58, 11.65s/it]
2022-04-08 22:27:38,442 - INFO - tqdm - accuracy: 0.7437, batch_loss: 0.4996, loss: 0.5157 ||:  94%|#########3| 61/65 [11:55<00:46, 11.67s/it]
2022-04-08 22:27:50,131 - INFO - tqdm - accuracy: 0.7443, batch_loss: 0.5084, loss: 0.5156 ||:  95%|#########5| 62/65 [12:07<00:35, 11.67s/it]
2022-04-08 22:28:01,800 - INFO - tqdm - accuracy: 0.7464, batch_loss: 0.5113, loss: 0.5155 ||:  97%|#########6| 63/65 [12:19<00:23, 11.67s/it]
2022-04-08 22:28:13,496 - INFO - tqdm - accuracy: 0.7474, batch_loss: 0.4678, loss: 0.5148 ||:  98%|#########8| 64/65 [12:30<00:11, 11.68s/it]
2022-04-08 22:28:18,698 - INFO - tqdm - accuracy: 0.7472, batch_loss: 0.5570, loss: 0.5154 ||: 100%|##########| 65/65 [12:35<00:00,  9.74s/it]
2022-04-08 22:28:18,699 - INFO - tqdm - accuracy: 0.7472, batch_loss: 0.5570, loss: 0.5154 ||: 100%|##########| 65/65 [12:35<00:00, 11.63s/it]
2022-04-08 22:28:21,532 - INFO - allennlp.training.trainer - Validating
2022-04-08 22:28:21,534 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-08 22:28:31,604 - INFO - tqdm - accuracy: 0.6026, batch_loss: 0.6127, loss: 0.8670 ||:  34%|###3      | 39/115 [00:10<00:19,  3.84it/s]
2022-04-08 22:28:41,682 - INFO - tqdm - accuracy: 0.6667, batch_loss: 0.8810, loss: 0.7452 ||:  68%|######7   | 78/115 [00:20<00:09,  3.82it/s]
2022-04-08 22:28:51,247 - INFO - tqdm - accuracy: 0.6550, batch_loss: 0.3316, loss: 0.7590 ||: 100%|##########| 115/115 [00:29<00:00,  3.88it/s]
2022-04-08 22:28:51,247 - INFO - tqdm - accuracy: 0.6550, batch_loss: 0.3316, loss: 0.7590 ||: 100%|##########| 115/115 [00:29<00:00,  3.87it/s]
2022-04-08 22:28:51,247 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-08 22:28:51,248 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.747  |     0.655
2022-04-08 22:28:51,248 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-08 22:28:51,249 - INFO - allennlp.training.tensorboard_writer - loss               |     0.515  |     0.759
2022-04-08 22:28:51,249 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5799.516  |       N/A
2022-04-08 22:28:56,883 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to './output_newboolq_ir-ora-d/best.th'.
2022-04-08 22:29:08,335 - INFO - allennlp.training.trainer - Epoch duration: 0:13:25.574753
2022-04-08 22:29:08,335 - INFO - allennlp.training.trainer - Estimated training time remaining: 2:26:50
2022-04-08 22:29:08,335 - INFO - allennlp.training.trainer - Epoch 4/14
2022-04-08 22:29:08,336 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-08 22:29:08,336 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-08 22:29:08,337 - INFO - allennlp.training.trainer - Training
2022-04-08 22:29:08,337 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-08 22:29:19,735 - INFO - tqdm - accuracy: 0.8750, batch_loss: 0.3966, loss: 0.3966 ||:   2%|1         | 1/65 [00:11<12:09, 11.40s/it]
2022-04-08 22:29:31,566 - INFO - tqdm - accuracy: 0.8906, batch_loss: 0.3287, loss: 0.3627 ||:   3%|3         | 2/65 [00:23<12:14, 11.65s/it]
2022-04-08 22:29:43,743 - INFO - tqdm - accuracy: 0.8854, batch_loss: 0.3574, loss: 0.3609 ||:   5%|4         | 3/65 [00:35<12:17, 11.89s/it]
2022-04-08 22:29:55,874 - INFO - tqdm - accuracy: 0.8672, batch_loss: 0.3952, loss: 0.3695 ||:   6%|6         | 4/65 [00:47<12:11, 11.99s/it]
2022-04-08 22:30:07,625 - INFO - tqdm - accuracy: 0.8375, batch_loss: 0.4936, loss: 0.3943 ||:   8%|7         | 5/65 [00:59<11:54, 11.90s/it]
2022-04-08 22:30:19,164 - INFO - tqdm - accuracy: 0.8438, batch_loss: 0.2836, loss: 0.3759 ||:   9%|9         | 6/65 [01:10<11:34, 11.78s/it]
2022-04-08 22:30:30,708 - INFO - tqdm - accuracy: 0.8527, batch_loss: 0.2802, loss: 0.3622 ||:  11%|#         | 7/65 [01:22<11:18, 11.70s/it]
2022-04-08 22:30:42,356 - INFO - tqdm - accuracy: 0.8594, batch_loss: 0.2627, loss: 0.3498 ||:  12%|#2        | 8/65 [01:34<11:06, 11.68s/it]
2022-04-08 22:30:54,160 - INFO - tqdm - accuracy: 0.8646, batch_loss: 0.2738, loss: 0.3413 ||:  14%|#3        | 9/65 [01:45<10:56, 11.72s/it]
2022-04-08 22:31:05,963 - INFO - tqdm - accuracy: 0.8656, batch_loss: 0.2306, loss: 0.3303 ||:  15%|#5        | 10/65 [01:57<10:46, 11.75s/it]
2022-04-08 22:31:17,715 - INFO - tqdm - accuracy: 0.8665, batch_loss: 0.2537, loss: 0.3233 ||:  17%|#6        | 11/65 [02:09<10:34, 11.75s/it]
2022-04-08 22:31:29,417 - INFO - tqdm - accuracy: 0.8568, batch_loss: 0.4616, loss: 0.3348 ||:  18%|#8        | 12/65 [02:21<10:21, 11.73s/it]
2022-04-08 22:31:41,181 - INFO - tqdm - accuracy: 0.8534, batch_loss: 0.4395, loss: 0.3429 ||:  20%|##        | 13/65 [02:32<10:10, 11.74s/it]
2022-04-08 22:31:52,945 - INFO - tqdm - accuracy: 0.8527, batch_loss: 0.4110, loss: 0.3477 ||:  22%|##1       | 14/65 [02:44<09:59, 11.75s/it]
2022-04-08 22:32:04,697 - INFO - tqdm - accuracy: 0.8521, batch_loss: 0.3382, loss: 0.3471 ||:  23%|##3       | 15/65 [02:56<09:47, 11.75s/it]
2022-04-08 22:32:16,455 - INFO - tqdm - accuracy: 0.8496, batch_loss: 0.2716, loss: 0.3424 ||:  25%|##4       | 16/65 [03:08<09:35, 11.75s/it]
2022-04-08 22:32:28,209 - INFO - tqdm - accuracy: 0.8511, batch_loss: 0.3884, loss: 0.3451 ||:  26%|##6       | 17/65 [03:19<09:24, 11.75s/it]
2022-04-08 22:32:39,965 - INFO - tqdm - accuracy: 0.8524, batch_loss: 0.3255, loss: 0.3440 ||:  28%|##7       | 18/65 [03:31<09:12, 11.75s/it]
2022-04-08 22:32:51,723 - INFO - tqdm - accuracy: 0.8503, batch_loss: 0.3226, loss: 0.3429 ||:  29%|##9       | 19/65 [03:43<09:00, 11.76s/it]
2022-04-08 22:33:03,482 - INFO - tqdm - accuracy: 0.8453, batch_loss: 0.4628, loss: 0.3489 ||:  31%|###       | 20/65 [03:55<08:49, 11.76s/it]
2022-04-08 22:33:15,229 - INFO - tqdm - accuracy: 0.8467, batch_loss: 0.3181, loss: 0.3474 ||:  32%|###2      | 21/65 [04:06<08:37, 11.75s/it]
2022-04-08 22:33:27,002 - INFO - tqdm - accuracy: 0.8466, batch_loss: 0.3210, loss: 0.3462 ||:  34%|###3      | 22/65 [04:18<08:25, 11.76s/it]
2022-04-08 22:33:38,758 - INFO - tqdm - accuracy: 0.8397, batch_loss: 0.5599, loss: 0.3555 ||:  35%|###5      | 23/65 [04:30<08:13, 11.76s/it]
2022-04-08 22:33:50,526 - INFO - tqdm - accuracy: 0.8372, batch_loss: 0.3178, loss: 0.3539 ||:  37%|###6      | 24/65 [04:42<08:02, 11.76s/it]
2022-04-08 22:34:02,303 - INFO - tqdm - accuracy: 0.8300, batch_loss: 0.6433, loss: 0.3655 ||:  38%|###8      | 25/65 [04:53<07:50, 11.77s/it]
2022-04-08 22:34:14,068 - INFO - tqdm - accuracy: 0.8329, batch_loss: 0.2705, loss: 0.3619 ||:  40%|####      | 26/65 [05:05<07:38, 11.77s/it]
2022-04-08 22:34:25,846 - INFO - tqdm - accuracy: 0.8380, batch_loss: 0.1949, loss: 0.3557 ||:  42%|####1     | 27/65 [05:17<07:27, 11.77s/it]
2022-04-08 22:34:37,620 - INFO - tqdm - accuracy: 0.8426, batch_loss: 0.1828, loss: 0.3495 ||:  43%|####3     | 28/65 [05:29<07:15, 11.77s/it]
