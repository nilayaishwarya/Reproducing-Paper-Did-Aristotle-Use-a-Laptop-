2022-04-08 21:34:59,038 - INFO - allennlp.common.params - random_seed = 42
2022-04-08 21:34:59,038 - INFO - allennlp.common.params - numpy_seed = 42
2022-04-08 21:34:59,039 - INFO - allennlp.common.params - pytorch_seed = 42
2022-04-08 21:34:59,715 - INFO - allennlp.common.checks - Pytorch version: 1.7.1
2022-04-08 21:34:59,715 - INFO - allennlp.common.params - type = default
2022-04-08 21:34:59,715 - INFO - allennlp.common.params - dataset_reader.type = strategy_qa_reader
2022-04-08 21:34:59,716 - INFO - allennlp.common.params - dataset_reader.lazy = False
2022-04-08 21:34:59,716 - INFO - allennlp.common.params - dataset_reader.cache_directory = None
2022-04-08 21:34:59,716 - INFO - allennlp.common.params - dataset_reader.max_instances = None
2022-04-08 21:34:59,716 - INFO - allennlp.common.params - dataset_reader.manual_distributed_sharding = False
2022-04-08 21:34:59,716 - INFO - allennlp.common.params - dataset_reader.manual_multi_process_sharding = False
2022-04-08 21:34:59,716 - INFO - allennlp.common.params - dataset_reader.tokenizer_wrapper.type = default
2022-04-08 21:34:59,716 - INFO - allennlp.common.params - dataset_reader.tokenizer_wrapper.pretrained_model = roberta-large
2022-04-08 21:34:59,716 - INFO - allennlp.common.params - dataset_reader.tokenizer_wrapper.call_kwargs.return_offsets_mapping = True
2022-04-08 21:34:59,716 - INFO - allennlp.common.params - dataset_reader.tokenizer_wrapper.call_kwargs.return_special_tokens_mask = True
2022-04-08 21:34:59,716 - INFO - allennlp.common.params - dataset_reader.tokenizer_wrapper.call_kwargs.truncation = only_first
2022-04-08 21:35:00,915 - INFO - allennlp.common.params - dataset_reader.save_tokenizer = True
2022-04-08 21:35:00,915 - INFO - allennlp.common.params - dataset_reader.is_training = True
2022-04-08 21:35:00,915 - INFO - allennlp.common.params - dataset_reader.pickle.action = None
2022-04-08 21:35:00,915 - INFO - allennlp.common.params - dataset_reader.pickle.file_name = None
2022-04-08 21:35:00,915 - INFO - allennlp.common.params - dataset_reader.pickle.path = ../pickle
2022-04-08 21:35:00,915 - INFO - allennlp.common.params - dataset_reader.pickle.save_even_when_max_instances = False
2022-04-08 21:35:00,915 - INFO - allennlp.common.params - dataset_reader.paragraphs_source = IR-ORA-D
2022-04-08 21:35:00,915 - INFO - allennlp.common.params - dataset_reader.answer_last_decomposition_step = False
2022-04-08 21:35:00,915 - INFO - allennlp.common.params - dataset_reader.skip_if_context_missing = True
2022-04-08 21:35:00,915 - INFO - allennlp.common.params - dataset_reader.paragraphs_limit = 10
2022-04-08 21:35:00,916 - INFO - allennlp.common.params - dataset_reader.generated_decompositions_paths = None
2022-04-08 21:35:00,916 - INFO - allennlp.common.params - dataset_reader.save_elasticsearch_cache = True
2022-04-08 21:35:01,370 - INFO - filelock - Lock 140015486777168 acquired on cache.lock
2022-04-08 21:35:02,217 - INFO - filelock - Lock 140015486777168 released on cache.lock
2022-04-08 21:35:02,217 - INFO - allennlp.common.params - train_data_path = data/strategyqa/train.json
2022-04-08 21:35:02,218 - INFO - allennlp.common.params - vocabulary = <allennlp.common.lazy.Lazy object at 0x7f57f01e2d10>
2022-04-08 21:35:02,218 - INFO - allennlp.common.params - datasets_for_vocab_creation = None
2022-04-08 21:35:02,218 - INFO - allennlp.common.params - validation_dataset_reader.type = strategy_qa_reader
2022-04-08 21:35:02,218 - INFO - allennlp.common.params - validation_dataset_reader.lazy = False
2022-04-08 21:35:02,218 - INFO - allennlp.common.params - validation_dataset_reader.cache_directory = None
2022-04-08 21:35:02,218 - INFO - allennlp.common.params - validation_dataset_reader.max_instances = None
2022-04-08 21:35:02,219 - INFO - allennlp.common.params - validation_dataset_reader.manual_distributed_sharding = False
2022-04-08 21:35:02,219 - INFO - allennlp.common.params - validation_dataset_reader.manual_multi_process_sharding = False
2022-04-08 21:35:02,219 - INFO - allennlp.common.params - validation_dataset_reader.tokenizer_wrapper.type = default
2022-04-08 21:35:02,219 - INFO - allennlp.common.params - validation_dataset_reader.tokenizer_wrapper.pretrained_model = roberta-large
2022-04-08 21:35:02,219 - INFO - allennlp.common.params - validation_dataset_reader.tokenizer_wrapper.call_kwargs.return_offsets_mapping = True
2022-04-08 21:35:02,219 - INFO - allennlp.common.params - validation_dataset_reader.tokenizer_wrapper.call_kwargs.return_special_tokens_mask = True
2022-04-08 21:35:02,219 - INFO - allennlp.common.params - validation_dataset_reader.tokenizer_wrapper.call_kwargs.truncation = only_first
2022-04-08 21:35:03,376 - INFO - allennlp.common.params - validation_dataset_reader.save_tokenizer = False
2022-04-08 21:35:03,377 - INFO - allennlp.common.params - validation_dataset_reader.is_training = False
2022-04-08 21:35:03,377 - INFO - allennlp.common.params - validation_dataset_reader.pickle.action = None
2022-04-08 21:35:03,377 - INFO - allennlp.common.params - validation_dataset_reader.pickle.file_name = None
2022-04-08 21:35:03,377 - INFO - allennlp.common.params - validation_dataset_reader.pickle.path = ../pickle
2022-04-08 21:35:03,377 - INFO - allennlp.common.params - validation_dataset_reader.pickle.save_even_when_max_instances = False
2022-04-08 21:35:03,377 - INFO - allennlp.common.params - validation_dataset_reader.paragraphs_source = IR-ORA-D
2022-04-08 21:35:03,377 - INFO - allennlp.common.params - validation_dataset_reader.answer_last_decomposition_step = False
2022-04-08 21:35:03,377 - INFO - allennlp.common.params - validation_dataset_reader.skip_if_context_missing = True
2022-04-08 21:35:03,377 - INFO - allennlp.common.params - validation_dataset_reader.paragraphs_limit = 10
2022-04-08 21:35:03,377 - INFO - allennlp.common.params - validation_dataset_reader.generated_decompositions_paths = None
2022-04-08 21:35:03,377 - INFO - allennlp.common.params - validation_dataset_reader.save_elasticsearch_cache = True
2022-04-08 21:35:03,377 - INFO - filelock - Lock 140015489015440 acquired on cache.lock
2022-04-08 21:35:04,204 - INFO - filelock - Lock 140015489015440 released on cache.lock
2022-04-08 21:35:04,205 - INFO - allennlp.common.params - validation_data_path = data/strategyqa/dev.json
2022-04-08 21:35:04,205 - INFO - allennlp.common.params - validation_data_loader = None
2022-04-08 21:35:04,205 - INFO - allennlp.common.params - test_data_path = None
2022-04-08 21:35:04,205 - INFO - allennlp.common.params - evaluate_on_test = True
2022-04-08 21:35:04,205 - INFO - allennlp.common.params - batch_weight_key = 
2022-04-08 21:35:04,205 - INFO - allennlp.training.util - Reading training data from data/strategyqa/train.json
2022-04-08 21:35:04,205 - INFO - tqdm - reading instances: 0it [00:00, ?it/s]
2022-04-08 21:35:04,206 - INFO - src.data.dataset_readers.base.base_dataset_reader - Reading the dataset from scratch
2022-04-08 21:35:04,206 - INFO - src.data.dataset_readers.strategy_qa_reader - Reading the dataset:
2022-04-08 21:35:04,206 - INFO - src.data.dataset_readers.strategy_qa_reader - Reading file at data/strategyqa/train.json
2022-04-08 21:35:11,374 - INFO - filelock - Lock 140014828086416 acquired on cache.lock
2022-04-08 21:35:12,150 - INFO - filelock - Lock 140014828086416 released on cache.lock
2022-04-08 21:35:12,186 - INFO - allennlp.training.util - Reading validation data from data/strategyqa/dev.json
2022-04-08 21:35:12,186 - INFO - tqdm - reading instances: 0it [00:00, ?it/s]
2022-04-08 21:35:12,186 - INFO - src.data.dataset_readers.base.base_dataset_reader - Reading the dataset from scratch
2022-04-08 21:35:12,186 - INFO - src.data.dataset_readers.strategy_qa_reader - Reading the dataset:
2022-04-08 21:35:12,186 - INFO - src.data.dataset_readers.strategy_qa_reader - Reading file at data/strategyqa/dev.json
2022-04-08 21:35:12,616 - INFO - src.data.dataset_readers.utils.elasticsearch_utils - Empty results from Elasticsearch for: actions commercial pilots take concerning engine noise arriving departing Orange County, California?
2022-04-08 21:35:12,941 - INFO - filelock - Lock 140014835985424 acquired on cache.lock
2022-04-08 21:35:13,695 - INFO - filelock - Lock 140014835985424 released on cache.lock
2022-04-08 21:35:13,729 - INFO - allennlp.common.params - type = from_instances
2022-04-08 21:35:13,730 - INFO - allennlp.common.params - min_count = None
2022-04-08 21:35:13,730 - INFO - allennlp.common.params - max_vocab_size = None
2022-04-08 21:35:13,730 - INFO - allennlp.common.params - non_padded_namespaces = ('*tags', '*labels')
2022-04-08 21:35:13,730 - INFO - allennlp.common.params - pretrained_files = None
2022-04-08 21:35:13,730 - INFO - allennlp.common.params - only_include_pretrained_words = False
2022-04-08 21:35:13,731 - INFO - allennlp.common.params - tokens_to_add = None
2022-04-08 21:35:13,731 - INFO - allennlp.common.params - min_pretrained_embeddings = None
2022-04-08 21:35:13,731 - INFO - allennlp.common.params - padding_token = @@PADDING@@
2022-04-08 21:35:13,731 - INFO - allennlp.common.params - oov_token = @@UNKNOWN@@
2022-04-08 21:35:13,731 - INFO - allennlp.data.vocabulary - Fitting token dictionary from dataset.
2022-04-08 21:35:13,731 - INFO - tqdm - building vocab: 0it [00:00, ?it/s]
2022-04-08 21:35:13,737 - INFO - allennlp.common.params - model.type = hf_classifier
2022-04-08 21:35:13,737 - INFO - allennlp.common.params - model.regularizer = None
2022-04-08 21:35:13,737 - INFO - allennlp.common.params - model.pretrained_model = roberta-large
2022-04-08 21:35:13,738 - INFO - allennlp.common.params - model.tokenizer_wrapper.type = default
2022-04-08 21:35:13,738 - INFO - allennlp.common.params - model.tokenizer_wrapper.pretrained_model = roberta-large
2022-04-08 21:35:13,738 - INFO - allennlp.common.params - model.tokenizer_wrapper.call_kwargs.return_offsets_mapping = True
2022-04-08 21:35:13,738 - INFO - allennlp.common.params - model.tokenizer_wrapper.call_kwargs.return_special_tokens_mask = True
2022-04-08 21:35:13,738 - INFO - allennlp.common.params - model.tokenizer_wrapper.call_kwargs.truncation = only_first
2022-04-08 21:35:14,924 - INFO - allennlp.common.params - model.num_labels = 2
2022-04-08 21:35:14,924 - INFO - allennlp.common.params - model.label_namespace = labels
2022-04-08 21:35:14,924 - INFO - allennlp.common.params - model.transformer_weights_path = None
2022-04-08 21:35:14,925 - INFO - allennlp.common.params - model.initializer.regexes.0.1.type = pretrained
2022-04-08 21:35:14,925 - INFO - allennlp.common.params - model.initializer.regexes.0.1.weights_file_path = output_newboolq/model.tar.gz
2022-04-08 21:35:14,926 - INFO - allennlp.models.archival - extracting archive file output_newboolq/model.tar.gz to temp dir /tmp/tmprkd1uknq
2022-04-08 21:35:26,460 - INFO - allennlp.models.archival - removing temporary unarchived model dir at /tmp/tmprkd1uknq
2022-04-08 21:35:26,629 - INFO - allennlp.common.params - model.initializer.prevent_regexes = None
2022-04-08 21:35:40,973 - INFO - allennlp.nn.initializers - Initializing parameters
2022-04-08 21:35:40,973 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.embeddings.word_embeddings.weight using .* initializer
2022-04-08 21:35:40,982 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.embeddings.position_embeddings.weight using .* initializer
2022-04-08 21:35:40,983 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.embeddings.token_type_embeddings.weight using .* initializer
2022-04-08 21:35:40,983 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.embeddings.LayerNorm.weight using .* initializer
2022-04-08 21:35:40,983 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.embeddings.LayerNorm.bias using .* initializer
2022-04-08 21:35:40,983 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.self.query.weight using .* initializer
2022-04-08 21:35:40,984 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.self.query.bias using .* initializer
2022-04-08 21:35:40,984 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.self.key.weight using .* initializer
2022-04-08 21:35:40,984 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.self.key.bias using .* initializer
2022-04-08 21:35:40,984 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.self.value.weight using .* initializer
2022-04-08 21:35:40,985 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.self.value.bias using .* initializer
2022-04-08 21:35:40,985 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.output.dense.weight using .* initializer
2022-04-08 21:35:40,985 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.output.dense.bias using .* initializer
2022-04-08 21:35:40,985 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.output.LayerNorm.weight using .* initializer
2022-04-08 21:35:40,986 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.output.LayerNorm.bias using .* initializer
2022-04-08 21:35:40,986 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.intermediate.dense.weight using .* initializer
2022-04-08 21:35:40,987 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.intermediate.dense.bias using .* initializer
2022-04-08 21:35:40,987 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.output.dense.weight using .* initializer
2022-04-08 21:35:40,988 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.output.dense.bias using .* initializer
2022-04-08 21:35:40,988 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.output.LayerNorm.weight using .* initializer
2022-04-08 21:35:40,988 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.output.LayerNorm.bias using .* initializer
2022-04-08 21:35:40,988 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.self.query.weight using .* initializer
2022-04-08 21:35:40,988 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.self.query.bias using .* initializer
2022-04-08 21:35:40,988 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.self.key.weight using .* initializer
2022-04-08 21:35:40,989 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.self.key.bias using .* initializer
2022-04-08 21:35:40,989 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.self.value.weight using .* initializer
2022-04-08 21:35:40,989 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.self.value.bias using .* initializer
2022-04-08 21:35:40,989 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.output.dense.weight using .* initializer
2022-04-08 21:35:40,990 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.output.dense.bias using .* initializer
2022-04-08 21:35:40,990 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.output.LayerNorm.weight using .* initializer
2022-04-08 21:35:40,990 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.output.LayerNorm.bias using .* initializer
2022-04-08 21:35:40,990 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.intermediate.dense.weight using .* initializer
2022-04-08 21:35:40,991 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.intermediate.dense.bias using .* initializer
2022-04-08 21:35:40,991 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.output.dense.weight using .* initializer
2022-04-08 21:35:40,992 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.output.dense.bias using .* initializer
2022-04-08 21:35:40,992 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.output.LayerNorm.weight using .* initializer
2022-04-08 21:35:40,992 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.output.LayerNorm.bias using .* initializer
2022-04-08 21:35:40,993 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.self.query.weight using .* initializer
2022-04-08 21:35:40,993 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.self.query.bias using .* initializer
2022-04-08 21:35:40,993 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.self.key.weight using .* initializer
2022-04-08 21:35:40,993 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.self.key.bias using .* initializer
2022-04-08 21:35:40,993 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.self.value.weight using .* initializer
2022-04-08 21:35:40,994 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.self.value.bias using .* initializer
2022-04-08 21:35:40,994 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.output.dense.weight using .* initializer
2022-04-08 21:35:40,994 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.output.dense.bias using .* initializer
2022-04-08 21:35:40,994 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.output.LayerNorm.weight using .* initializer
2022-04-08 21:35:40,995 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.output.LayerNorm.bias using .* initializer
2022-04-08 21:35:40,995 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.intermediate.dense.weight using .* initializer
2022-04-08 21:35:40,996 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.intermediate.dense.bias using .* initializer
2022-04-08 21:35:40,996 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.output.dense.weight using .* initializer
2022-04-08 21:35:40,997 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.output.dense.bias using .* initializer
2022-04-08 21:35:40,997 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.output.LayerNorm.weight using .* initializer
2022-04-08 21:35:40,997 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.output.LayerNorm.bias using .* initializer
2022-04-08 21:35:40,997 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.self.query.weight using .* initializer
2022-04-08 21:35:40,997 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.self.query.bias using .* initializer
2022-04-08 21:35:40,997 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.self.key.weight using .* initializer
2022-04-08 21:35:40,998 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.self.key.bias using .* initializer
2022-04-08 21:35:40,998 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.self.value.weight using .* initializer
2022-04-08 21:35:40,998 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.self.value.bias using .* initializer
2022-04-08 21:35:40,998 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.output.dense.weight using .* initializer
2022-04-08 21:35:40,999 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.output.dense.bias using .* initializer
2022-04-08 21:35:40,999 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.output.LayerNorm.weight using .* initializer
2022-04-08 21:35:40,999 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.output.LayerNorm.bias using .* initializer
2022-04-08 21:35:40,999 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.intermediate.dense.weight using .* initializer
2022-04-08 21:35:41,000 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.intermediate.dense.bias using .* initializer
2022-04-08 21:35:41,000 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.output.dense.weight using .* initializer
2022-04-08 21:35:41,001 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.output.dense.bias using .* initializer
2022-04-08 21:35:41,001 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.output.LayerNorm.weight using .* initializer
2022-04-08 21:35:41,001 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.output.LayerNorm.bias using .* initializer
2022-04-08 21:35:41,001 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.self.query.weight using .* initializer
2022-04-08 21:35:41,002 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.self.query.bias using .* initializer
2022-04-08 21:35:41,002 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.self.key.weight using .* initializer
2022-04-08 21:35:41,002 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.self.key.bias using .* initializer
2022-04-08 21:35:41,002 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.self.value.weight using .* initializer
2022-04-08 21:35:41,003 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.self.value.bias using .* initializer
2022-04-08 21:35:41,003 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.output.dense.weight using .* initializer
2022-04-08 21:35:41,003 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.output.dense.bias using .* initializer
2022-04-08 21:35:41,003 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.output.LayerNorm.weight using .* initializer
2022-04-08 21:35:41,003 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.output.LayerNorm.bias using .* initializer
2022-04-08 21:35:41,004 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.intermediate.dense.weight using .* initializer
2022-04-08 21:35:41,004 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.intermediate.dense.bias using .* initializer
2022-04-08 21:35:41,005 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.output.dense.weight using .* initializer
2022-04-08 21:35:41,005 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.output.dense.bias using .* initializer
2022-04-08 21:35:41,006 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.output.LayerNorm.weight using .* initializer
2022-04-08 21:35:41,006 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.output.LayerNorm.bias using .* initializer
2022-04-08 21:35:41,006 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.self.query.weight using .* initializer
2022-04-08 21:35:41,006 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.self.query.bias using .* initializer
2022-04-08 21:35:41,006 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.self.key.weight using .* initializer
2022-04-08 21:35:41,007 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.self.key.bias using .* initializer
2022-04-08 21:35:41,007 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.self.value.weight using .* initializer
2022-04-08 21:35:41,007 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.self.value.bias using .* initializer
2022-04-08 21:35:41,007 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.output.dense.weight using .* initializer
2022-04-08 21:35:41,008 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.output.dense.bias using .* initializer
2022-04-08 21:35:41,008 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.output.LayerNorm.weight using .* initializer
2022-04-08 21:35:41,008 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.output.LayerNorm.bias using .* initializer
2022-04-08 21:35:41,008 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.intermediate.dense.weight using .* initializer
2022-04-08 21:35:41,009 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.intermediate.dense.bias using .* initializer
2022-04-08 21:35:41,009 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.output.dense.weight using .* initializer
2022-04-08 21:35:41,010 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.output.dense.bias using .* initializer
2022-04-08 21:35:41,010 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.output.LayerNorm.weight using .* initializer
2022-04-08 21:35:41,010 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.output.LayerNorm.bias using .* initializer
2022-04-08 21:35:41,010 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.self.query.weight using .* initializer
2022-04-08 21:35:41,011 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.self.query.bias using .* initializer
2022-04-08 21:35:41,011 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.self.key.weight using .* initializer
2022-04-08 21:35:41,011 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.self.key.bias using .* initializer
2022-04-08 21:35:41,011 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.self.value.weight using .* initializer
2022-04-08 21:35:41,012 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.self.value.bias using .* initializer
2022-04-08 21:35:41,012 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.output.dense.weight using .* initializer
2022-04-08 21:35:41,012 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.output.dense.bias using .* initializer
2022-04-08 21:35:41,012 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.output.LayerNorm.weight using .* initializer
2022-04-08 21:35:41,012 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.output.LayerNorm.bias using .* initializer
2022-04-08 21:35:41,013 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.intermediate.dense.weight using .* initializer
2022-04-08 21:35:41,013 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.intermediate.dense.bias using .* initializer
2022-04-08 21:35:41,014 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.output.dense.weight using .* initializer
2022-04-08 21:35:41,014 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.output.dense.bias using .* initializer
2022-04-08 21:35:41,015 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.output.LayerNorm.weight using .* initializer
2022-04-08 21:35:41,015 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.output.LayerNorm.bias using .* initializer
2022-04-08 21:35:41,015 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.self.query.weight using .* initializer
2022-04-08 21:35:41,015 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.self.query.bias using .* initializer
2022-04-08 21:35:41,015 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.self.key.weight using .* initializer
2022-04-08 21:35:41,016 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.self.key.bias using .* initializer
2022-04-08 21:35:41,016 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.self.value.weight using .* initializer
2022-04-08 21:35:41,016 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.self.value.bias using .* initializer
2022-04-08 21:35:41,016 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.output.dense.weight using .* initializer
2022-04-08 21:35:41,017 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.output.dense.bias using .* initializer
2022-04-08 21:35:41,017 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.output.LayerNorm.weight using .* initializer
2022-04-08 21:35:41,017 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.output.LayerNorm.bias using .* initializer
2022-04-08 21:35:41,017 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.intermediate.dense.weight using .* initializer
2022-04-08 21:35:41,018 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.intermediate.dense.bias using .* initializer
2022-04-08 21:35:41,018 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.output.dense.weight using .* initializer
2022-04-08 21:35:41,019 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.output.dense.bias using .* initializer
2022-04-08 21:35:41,019 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.output.LayerNorm.weight using .* initializer
2022-04-08 21:35:41,019 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.output.LayerNorm.bias using .* initializer
2022-04-08 21:35:41,019 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.self.query.weight using .* initializer
2022-04-08 21:35:41,020 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.self.query.bias using .* initializer
2022-04-08 21:35:41,020 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.self.key.weight using .* initializer
2022-04-08 21:35:41,020 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.self.key.bias using .* initializer
2022-04-08 21:35:41,020 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.self.value.weight using .* initializer
2022-04-08 21:35:41,020 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.self.value.bias using .* initializer
2022-04-08 21:35:41,021 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.output.dense.weight using .* initializer
2022-04-08 21:35:41,021 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.output.dense.bias using .* initializer
2022-04-08 21:35:41,021 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.output.LayerNorm.weight using .* initializer
2022-04-08 21:35:41,021 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.output.LayerNorm.bias using .* initializer
2022-04-08 21:35:41,021 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.intermediate.dense.weight using .* initializer
2022-04-08 21:35:41,022 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.intermediate.dense.bias using .* initializer
2022-04-08 21:35:41,022 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.output.dense.weight using .* initializer
2022-04-08 21:35:41,023 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.output.dense.bias using .* initializer
2022-04-08 21:35:41,023 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.output.LayerNorm.weight using .* initializer
2022-04-08 21:35:41,024 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.output.LayerNorm.bias using .* initializer
2022-04-08 21:35:41,024 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.self.query.weight using .* initializer
2022-04-08 21:35:41,024 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.self.query.bias using .* initializer
2022-04-08 21:35:41,024 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.self.key.weight using .* initializer
2022-04-08 21:35:41,024 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.self.key.bias using .* initializer
2022-04-08 21:35:41,025 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.self.value.weight using .* initializer
2022-04-08 21:35:41,025 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.self.value.bias using .* initializer
2022-04-08 21:35:41,025 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.output.dense.weight using .* initializer
2022-04-08 21:35:41,025 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.output.dense.bias using .* initializer
2022-04-08 21:35:41,026 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.output.LayerNorm.weight using .* initializer
2022-04-08 21:35:41,026 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.output.LayerNorm.bias using .* initializer
2022-04-08 21:35:41,026 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.intermediate.dense.weight using .* initializer
2022-04-08 21:35:41,027 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.intermediate.dense.bias using .* initializer
2022-04-08 21:35:41,027 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.output.dense.weight using .* initializer
2022-04-08 21:35:41,028 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.output.dense.bias using .* initializer
2022-04-08 21:35:41,028 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.output.LayerNorm.weight using .* initializer
2022-04-08 21:35:41,028 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.output.LayerNorm.bias using .* initializer
2022-04-08 21:35:41,028 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.self.query.weight using .* initializer
2022-04-08 21:35:41,028 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.self.query.bias using .* initializer
2022-04-08 21:35:41,029 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.self.key.weight using .* initializer
2022-04-08 21:35:41,029 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.self.key.bias using .* initializer
2022-04-08 21:35:41,029 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.self.value.weight using .* initializer
2022-04-08 21:35:41,029 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.self.value.bias using .* initializer
2022-04-08 21:35:41,030 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.output.dense.weight using .* initializer
2022-04-08 21:35:41,030 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.output.dense.bias using .* initializer
2022-04-08 21:35:41,030 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.output.LayerNorm.weight using .* initializer
2022-04-08 21:35:41,030 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.output.LayerNorm.bias using .* initializer
2022-04-08 21:35:41,030 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.intermediate.dense.weight using .* initializer
2022-04-08 21:35:41,031 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.intermediate.dense.bias using .* initializer
2022-04-08 21:35:41,031 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.output.dense.weight using .* initializer
2022-04-08 21:35:41,032 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.output.dense.bias using .* initializer
2022-04-08 21:35:41,032 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.output.LayerNorm.weight using .* initializer
2022-04-08 21:35:41,032 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.output.LayerNorm.bias using .* initializer
2022-04-08 21:35:41,033 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.self.query.weight using .* initializer
2022-04-08 21:35:41,033 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.self.query.bias using .* initializer
2022-04-08 21:35:41,033 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.self.key.weight using .* initializer
2022-04-08 21:35:41,033 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.self.key.bias using .* initializer
2022-04-08 21:35:41,034 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.self.value.weight using .* initializer
2022-04-08 21:35:41,034 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.self.value.bias using .* initializer
2022-04-08 21:35:41,034 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.output.dense.weight using .* initializer
2022-04-08 21:35:41,034 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.output.dense.bias using .* initializer
2022-04-08 21:35:41,034 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.output.LayerNorm.weight using .* initializer
2022-04-08 21:35:41,035 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.output.LayerNorm.bias using .* initializer
2022-04-08 21:35:41,035 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.intermediate.dense.weight using .* initializer
2022-04-08 21:35:41,036 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.intermediate.dense.bias using .* initializer
2022-04-08 21:35:41,036 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.output.dense.weight using .* initializer
2022-04-08 21:35:41,037 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.output.dense.bias using .* initializer
2022-04-08 21:35:41,037 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.output.LayerNorm.weight using .* initializer
2022-04-08 21:35:41,037 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.output.LayerNorm.bias using .* initializer
2022-04-08 21:35:41,037 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.self.query.weight using .* initializer
2022-04-08 21:35:41,037 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.self.query.bias using .* initializer
2022-04-08 21:35:41,037 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.self.key.weight using .* initializer
2022-04-08 21:35:41,038 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.self.key.bias using .* initializer
2022-04-08 21:35:41,038 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.self.value.weight using .* initializer
2022-04-08 21:35:41,038 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.self.value.bias using .* initializer
2022-04-08 21:35:41,038 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.output.dense.weight using .* initializer
2022-04-08 21:35:41,039 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.output.dense.bias using .* initializer
2022-04-08 21:35:41,039 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.output.LayerNorm.weight using .* initializer
2022-04-08 21:35:41,039 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.output.LayerNorm.bias using .* initializer
2022-04-08 21:35:41,039 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.intermediate.dense.weight using .* initializer
2022-04-08 21:35:41,040 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.intermediate.dense.bias using .* initializer
2022-04-08 21:35:41,040 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.output.dense.weight using .* initializer
2022-04-08 21:35:41,041 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.output.dense.bias using .* initializer
2022-04-08 21:35:41,041 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.output.LayerNorm.weight using .* initializer
2022-04-08 21:35:41,041 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.output.LayerNorm.bias using .* initializer
2022-04-08 21:35:41,041 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.self.query.weight using .* initializer
2022-04-08 21:35:41,042 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.self.query.bias using .* initializer
2022-04-08 21:35:41,042 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.self.key.weight using .* initializer
2022-04-08 21:35:41,042 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.self.key.bias using .* initializer
2022-04-08 21:35:41,042 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.self.value.weight using .* initializer
2022-04-08 21:35:41,043 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.self.value.bias using .* initializer
2022-04-08 21:35:41,043 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.output.dense.weight using .* initializer
2022-04-08 21:35:41,043 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.output.dense.bias using .* initializer
2022-04-08 21:35:41,043 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.output.LayerNorm.weight using .* initializer
2022-04-08 21:35:41,044 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.output.LayerNorm.bias using .* initializer
2022-04-08 21:35:41,044 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.intermediate.dense.weight using .* initializer
2022-04-08 21:35:41,045 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.intermediate.dense.bias using .* initializer
2022-04-08 21:35:41,045 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.output.dense.weight using .* initializer
2022-04-08 21:35:41,046 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.output.dense.bias using .* initializer
2022-04-08 21:35:41,046 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.output.LayerNorm.weight using .* initializer
2022-04-08 21:35:41,046 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.output.LayerNorm.bias using .* initializer
2022-04-08 21:35:41,046 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.self.query.weight using .* initializer
2022-04-08 21:35:41,047 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.self.query.bias using .* initializer
2022-04-08 21:35:41,047 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.self.key.weight using .* initializer
2022-04-08 21:35:41,047 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.self.key.bias using .* initializer
2022-04-08 21:35:41,047 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.self.value.weight using .* initializer
2022-04-08 21:35:41,048 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.self.value.bias using .* initializer
2022-04-08 21:35:41,048 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.output.dense.weight using .* initializer
2022-04-08 21:35:41,048 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.output.dense.bias using .* initializer
2022-04-08 21:35:41,048 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.output.LayerNorm.weight using .* initializer
2022-04-08 21:35:41,048 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.output.LayerNorm.bias using .* initializer
2022-04-08 21:35:41,049 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.intermediate.dense.weight using .* initializer
2022-04-08 21:35:41,049 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.intermediate.dense.bias using .* initializer
2022-04-08 21:35:41,050 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.output.dense.weight using .* initializer
2022-04-08 21:35:41,050 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.output.dense.bias using .* initializer
2022-04-08 21:35:41,051 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.output.LayerNorm.weight using .* initializer
2022-04-08 21:35:41,051 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.output.LayerNorm.bias using .* initializer
2022-04-08 21:35:41,051 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.self.query.weight using .* initializer
2022-04-08 21:35:41,051 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.self.query.bias using .* initializer
2022-04-08 21:35:41,051 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.self.key.weight using .* initializer
2022-04-08 21:35:41,052 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.self.key.bias using .* initializer
2022-04-08 21:35:41,052 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.self.value.weight using .* initializer
2022-04-08 21:35:41,052 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.self.value.bias using .* initializer
2022-04-08 21:35:41,052 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.output.dense.weight using .* initializer
2022-04-08 21:35:41,053 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.output.dense.bias using .* initializer
2022-04-08 21:35:41,053 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.output.LayerNorm.weight using .* initializer
2022-04-08 21:35:41,053 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.output.LayerNorm.bias using .* initializer
2022-04-08 21:35:41,053 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.intermediate.dense.weight using .* initializer
2022-04-08 21:35:41,054 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.intermediate.dense.bias using .* initializer
2022-04-08 21:35:41,054 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.output.dense.weight using .* initializer
2022-04-08 21:35:41,055 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.output.dense.bias using .* initializer
2022-04-08 21:35:41,055 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.output.LayerNorm.weight using .* initializer
2022-04-08 21:35:41,055 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.output.LayerNorm.bias using .* initializer
2022-04-08 21:35:41,055 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.self.query.weight using .* initializer
2022-04-08 21:35:41,056 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.self.query.bias using .* initializer
2022-04-08 21:35:41,056 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.self.key.weight using .* initializer
2022-04-08 21:35:41,056 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.self.key.bias using .* initializer
2022-04-08 21:35:41,056 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.self.value.weight using .* initializer
2022-04-08 21:35:41,057 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.self.value.bias using .* initializer
2022-04-08 21:35:41,057 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.output.dense.weight using .* initializer
2022-04-08 21:35:41,057 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.output.dense.bias using .* initializer
2022-04-08 21:35:41,057 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.output.LayerNorm.weight using .* initializer
2022-04-08 21:35:41,057 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.output.LayerNorm.bias using .* initializer
2022-04-08 21:35:41,058 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.intermediate.dense.weight using .* initializer
2022-04-08 21:35:41,058 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.intermediate.dense.bias using .* initializer
2022-04-08 21:35:41,059 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.output.dense.weight using .* initializer
2022-04-08 21:35:41,059 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.output.dense.bias using .* initializer
2022-04-08 21:35:41,060 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.output.LayerNorm.weight using .* initializer
2022-04-08 21:35:41,060 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.output.LayerNorm.bias using .* initializer
2022-04-08 21:35:41,060 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.self.query.weight using .* initializer
2022-04-08 21:35:41,060 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.self.query.bias using .* initializer
2022-04-08 21:35:41,060 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.self.key.weight using .* initializer
2022-04-08 21:35:41,061 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.self.key.bias using .* initializer
2022-04-08 21:35:41,061 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.self.value.weight using .* initializer
2022-04-08 21:35:41,061 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.self.value.bias using .* initializer
2022-04-08 21:35:41,061 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.output.dense.weight using .* initializer
2022-04-08 21:35:41,062 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.output.dense.bias using .* initializer
2022-04-08 21:35:41,062 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.output.LayerNorm.weight using .* initializer
2022-04-08 21:35:41,062 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.output.LayerNorm.bias using .* initializer
2022-04-08 21:35:41,062 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.intermediate.dense.weight using .* initializer
2022-04-08 21:35:41,063 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.intermediate.dense.bias using .* initializer
2022-04-08 21:35:41,063 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.output.dense.weight using .* initializer
2022-04-08 21:35:41,064 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.output.dense.bias using .* initializer
2022-04-08 21:35:41,064 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.output.LayerNorm.weight using .* initializer
2022-04-08 21:35:41,064 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.output.LayerNorm.bias using .* initializer
2022-04-08 21:35:41,064 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.self.query.weight using .* initializer
2022-04-08 21:35:41,065 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.self.query.bias using .* initializer
2022-04-08 21:35:41,065 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.self.key.weight using .* initializer
2022-04-08 21:35:41,065 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.self.key.bias using .* initializer
2022-04-08 21:35:41,065 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.self.value.weight using .* initializer
2022-04-08 21:35:41,066 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.self.value.bias using .* initializer
2022-04-08 21:35:41,066 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.output.dense.weight using .* initializer
2022-04-08 21:35:41,066 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.output.dense.bias using .* initializer
2022-04-08 21:35:41,066 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.output.LayerNorm.weight using .* initializer
2022-04-08 21:35:41,066 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.output.LayerNorm.bias using .* initializer
2022-04-08 21:35:41,067 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.intermediate.dense.weight using .* initializer
2022-04-08 21:35:41,067 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.intermediate.dense.bias using .* initializer
2022-04-08 21:35:41,068 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.output.dense.weight using .* initializer
2022-04-08 21:35:41,068 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.output.dense.bias using .* initializer
2022-04-08 21:35:41,069 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.output.LayerNorm.weight using .* initializer
2022-04-08 21:35:41,069 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.output.LayerNorm.bias using .* initializer
2022-04-08 21:35:41,069 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.self.query.weight using .* initializer
2022-04-08 21:35:41,069 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.self.query.bias using .* initializer
2022-04-08 21:35:41,069 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.self.key.weight using .* initializer
2022-04-08 21:35:41,070 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.self.key.bias using .* initializer
2022-04-08 21:35:41,070 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.self.value.weight using .* initializer
2022-04-08 21:35:41,070 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.self.value.bias using .* initializer
2022-04-08 21:35:41,070 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.output.dense.weight using .* initializer
2022-04-08 21:35:41,071 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.output.dense.bias using .* initializer
2022-04-08 21:35:41,071 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.output.LayerNorm.weight using .* initializer
2022-04-08 21:35:41,071 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.output.LayerNorm.bias using .* initializer
2022-04-08 21:35:41,071 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.intermediate.dense.weight using .* initializer
2022-04-08 21:35:41,072 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.intermediate.dense.bias using .* initializer
2022-04-08 21:35:41,072 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.output.dense.weight using .* initializer
2022-04-08 21:35:41,073 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.output.dense.bias using .* initializer
2022-04-08 21:35:41,073 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.output.LayerNorm.weight using .* initializer
2022-04-08 21:35:41,073 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.output.LayerNorm.bias using .* initializer
2022-04-08 21:35:41,073 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.self.query.weight using .* initializer
2022-04-08 21:35:41,074 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.self.query.bias using .* initializer
2022-04-08 21:35:41,074 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.self.key.weight using .* initializer
2022-04-08 21:35:41,074 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.self.key.bias using .* initializer
2022-04-08 21:35:41,074 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.self.value.weight using .* initializer
2022-04-08 21:35:41,075 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.self.value.bias using .* initializer
2022-04-08 21:35:41,075 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.output.dense.weight using .* initializer
2022-04-08 21:35:41,075 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.output.dense.bias using .* initializer
2022-04-08 21:35:41,075 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.output.LayerNorm.weight using .* initializer
2022-04-08 21:35:41,075 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.output.LayerNorm.bias using .* initializer
2022-04-08 21:35:41,076 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.intermediate.dense.weight using .* initializer
2022-04-08 21:35:41,076 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.intermediate.dense.bias using .* initializer
2022-04-08 21:35:41,077 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.output.dense.weight using .* initializer
2022-04-08 21:35:41,077 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.output.dense.bias using .* initializer
2022-04-08 21:35:41,078 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.output.LayerNorm.weight using .* initializer
2022-04-08 21:35:41,078 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.output.LayerNorm.bias using .* initializer
2022-04-08 21:35:41,078 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.self.query.weight using .* initializer
2022-04-08 21:35:41,078 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.self.query.bias using .* initializer
2022-04-08 21:35:41,078 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.self.key.weight using .* initializer
2022-04-08 21:35:41,079 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.self.key.bias using .* initializer
2022-04-08 21:35:41,079 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.self.value.weight using .* initializer
2022-04-08 21:35:41,079 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.self.value.bias using .* initializer
2022-04-08 21:35:41,079 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.output.dense.weight using .* initializer
2022-04-08 21:35:41,080 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.output.dense.bias using .* initializer
2022-04-08 21:35:41,080 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.output.LayerNorm.weight using .* initializer
2022-04-08 21:35:41,080 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.output.LayerNorm.bias using .* initializer
2022-04-08 21:35:41,080 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.intermediate.dense.weight using .* initializer
2022-04-08 21:35:41,081 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.intermediate.dense.bias using .* initializer
2022-04-08 21:35:41,081 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.output.dense.weight using .* initializer
2022-04-08 21:35:41,082 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.output.dense.bias using .* initializer
2022-04-08 21:35:41,082 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.output.LayerNorm.weight using .* initializer
2022-04-08 21:35:41,082 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.output.LayerNorm.bias using .* initializer
2022-04-08 21:35:41,082 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.self.query.weight using .* initializer
2022-04-08 21:35:41,083 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.self.query.bias using .* initializer
2022-04-08 21:35:41,083 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.self.key.weight using .* initializer
2022-04-08 21:35:41,083 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.self.key.bias using .* initializer
2022-04-08 21:35:41,083 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.self.value.weight using .* initializer
2022-04-08 21:35:41,084 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.self.value.bias using .* initializer
2022-04-08 21:35:41,084 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.output.dense.weight using .* initializer
2022-04-08 21:35:41,084 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.output.dense.bias using .* initializer
2022-04-08 21:35:41,084 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.output.LayerNorm.weight using .* initializer
2022-04-08 21:35:41,084 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.output.LayerNorm.bias using .* initializer
2022-04-08 21:35:41,084 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.intermediate.dense.weight using .* initializer
2022-04-08 21:35:41,085 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.intermediate.dense.bias using .* initializer
2022-04-08 21:35:41,085 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.output.dense.weight using .* initializer
2022-04-08 21:35:41,086 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.output.dense.bias using .* initializer
2022-04-08 21:35:41,086 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.output.LayerNorm.weight using .* initializer
2022-04-08 21:35:41,087 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.output.LayerNorm.bias using .* initializer
2022-04-08 21:35:41,087 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.self.query.weight using .* initializer
2022-04-08 21:35:41,087 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.self.query.bias using .* initializer
2022-04-08 21:35:41,087 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.self.key.weight using .* initializer
2022-04-08 21:35:41,088 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.self.key.bias using .* initializer
2022-04-08 21:35:41,088 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.self.value.weight using .* initializer
2022-04-08 21:35:41,088 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.self.value.bias using .* initializer
2022-04-08 21:35:41,088 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.output.dense.weight using .* initializer
2022-04-08 21:35:41,088 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.output.dense.bias using .* initializer
2022-04-08 21:35:41,089 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.output.LayerNorm.weight using .* initializer
2022-04-08 21:35:41,089 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.output.LayerNorm.bias using .* initializer
2022-04-08 21:35:41,089 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.intermediate.dense.weight using .* initializer
2022-04-08 21:35:41,090 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.intermediate.dense.bias using .* initializer
2022-04-08 21:35:41,090 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.output.dense.weight using .* initializer
2022-04-08 21:35:41,091 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.output.dense.bias using .* initializer
2022-04-08 21:35:41,091 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.output.LayerNorm.weight using .* initializer
2022-04-08 21:35:41,091 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.output.LayerNorm.bias using .* initializer
2022-04-08 21:35:41,091 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.pooler.dense.weight using .* initializer
2022-04-08 21:35:41,092 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.pooler.dense.bias using .* initializer
2022-04-08 21:35:41,092 - INFO - allennlp.nn.initializers - Initializing _classifier.classifier.dense.weight using .* initializer
2022-04-08 21:35:41,092 - INFO - allennlp.nn.initializers - Initializing _classifier.classifier.dense.bias using .* initializer
2022-04-08 21:35:41,092 - INFO - allennlp.nn.initializers - Initializing _classifier.classifier.out_proj.weight using .* initializer
2022-04-08 21:35:41,092 - INFO - allennlp.nn.initializers - Initializing _classifier.classifier.out_proj.bias using .* initializer
2022-04-08 21:35:41,092 - INFO - allennlp.nn.initializers - Done initializing parameters; the following parameters are using their default initialization from their code
2022-04-08 21:35:41,637 - INFO - filelock - Lock 140014828086352 acquired on ./output_newboolq_ir-ora-d/vocabulary/.lock
2022-04-08 21:35:41,637 - INFO - filelock - Lock 140014828086352 released on ./output_newboolq_ir-ora-d/vocabulary/.lock
2022-04-08 21:35:41,637 - INFO - allennlp.common.params - data_loader.type = pytorch_dataloader
2022-04-08 21:35:41,638 - INFO - allennlp.common.params - data_loader.batch_size = 1
2022-04-08 21:35:41,638 - INFO - allennlp.common.params - data_loader.shuffle = False
2022-04-08 21:35:41,638 - INFO - allennlp.common.params - data_loader.sampler = None
2022-04-08 21:35:41,638 - INFO - allennlp.common.params - data_loader.num_workers = 0
2022-04-08 21:35:41,638 - INFO - allennlp.common.params - data_loader.pin_memory = False
2022-04-08 21:35:41,638 - INFO - allennlp.common.params - data_loader.drop_last = False
2022-04-08 21:35:41,638 - INFO - allennlp.common.params - data_loader.timeout = 0
2022-04-08 21:35:41,639 - INFO - allennlp.common.params - data_loader.worker_init_fn = None
2022-04-08 21:35:41,639 - INFO - allennlp.common.params - data_loader.multiprocessing_context = None
2022-04-08 21:35:41,639 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None
2022-04-08 21:35:41,639 - INFO - allennlp.common.params - data_loader.batch_sampler.type = bucket
2022-04-08 21:35:41,639 - INFO - allennlp.common.params - data_loader.batch_sampler.batch_size = 2
2022-04-08 21:35:41,639 - INFO - allennlp.common.params - data_loader.batch_sampler.sorting_keys = None
2022-04-08 21:35:41,639 - INFO - allennlp.common.params - data_loader.batch_sampler.padding_noise = 0.1
2022-04-08 21:35:41,639 - INFO - allennlp.common.params - data_loader.batch_sampler.drop_last = False
2022-04-08 21:35:41,640 - INFO - allennlp.common.params - data_loader.type = pytorch_dataloader
2022-04-08 21:35:41,640 - INFO - allennlp.common.params - data_loader.batch_size = 1
2022-04-08 21:35:41,640 - INFO - allennlp.common.params - data_loader.shuffle = False
2022-04-08 21:35:41,641 - INFO - allennlp.common.params - data_loader.sampler = None
2022-04-08 21:35:41,641 - INFO - allennlp.common.params - data_loader.num_workers = 0
2022-04-08 21:35:41,641 - INFO - allennlp.common.params - data_loader.pin_memory = False
2022-04-08 21:35:41,641 - INFO - allennlp.common.params - data_loader.drop_last = False
2022-04-08 21:35:41,641 - INFO - allennlp.common.params - data_loader.timeout = 0
2022-04-08 21:35:41,641 - INFO - allennlp.common.params - data_loader.worker_init_fn = None
2022-04-08 21:35:41,641 - INFO - allennlp.common.params - data_loader.multiprocessing_context = None
2022-04-08 21:35:41,641 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None
2022-04-08 21:35:41,642 - INFO - allennlp.common.params - data_loader.batch_sampler.type = bucket
2022-04-08 21:35:41,642 - INFO - allennlp.common.params - data_loader.batch_sampler.batch_size = 2
2022-04-08 21:35:41,642 - INFO - allennlp.common.params - data_loader.batch_sampler.sorting_keys = None
2022-04-08 21:35:41,642 - INFO - allennlp.common.params - data_loader.batch_sampler.padding_noise = 0.1
2022-04-08 21:35:41,642 - INFO - allennlp.common.params - data_loader.batch_sampler.drop_last = False
2022-04-08 21:35:41,642 - INFO - allennlp.common.params - trainer.type = gradient_descent
2022-04-08 21:35:41,643 - INFO - allennlp.common.params - trainer.patience = None
2022-04-08 21:35:41,643 - INFO - allennlp.common.params - trainer.validation_metric = +accuracy
2022-04-08 21:35:41,643 - INFO - allennlp.common.params - trainer.num_epochs = 15
2022-04-08 21:35:41,643 - INFO - allennlp.common.params - trainer.cuda_device = 0
2022-04-08 21:35:41,643 - INFO - allennlp.common.params - trainer.grad_norm = None
2022-04-08 21:35:41,643 - INFO - allennlp.common.params - trainer.grad_clipping = None
2022-04-08 21:35:41,643 - INFO - allennlp.common.params - trainer.distributed = False
2022-04-08 21:35:41,643 - INFO - allennlp.common.params - trainer.world_size = 1
2022-04-08 21:35:41,644 - INFO - allennlp.common.params - trainer.num_gradient_accumulation_steps = 16
2022-04-08 21:35:41,644 - INFO - allennlp.common.params - trainer.use_amp = False
2022-04-08 21:35:41,644 - INFO - allennlp.common.params - trainer.no_grad = None
2022-04-08 21:35:41,644 - INFO - allennlp.common.params - trainer.momentum_scheduler = None
2022-04-08 21:35:41,644 - INFO - allennlp.common.params - trainer.tensorboard_writer = <allennlp.common.lazy.Lazy object at 0x7f57e5d89fd0>
2022-04-08 21:35:41,644 - INFO - allennlp.common.params - trainer.moving_average = None
2022-04-08 21:35:41,644 - INFO - allennlp.common.params - trainer.checkpointer = <allennlp.common.lazy.Lazy object at 0x7f57e5d96090>
2022-04-08 21:35:41,644 - INFO - allennlp.common.params - trainer.batch_callbacks = None
2022-04-08 21:35:41,645 - INFO - allennlp.common.params - trainer.epoch_callbacks = None
2022-04-08 21:35:41,645 - INFO - allennlp.common.params - trainer.end_callbacks = None
2022-04-08 21:35:41,645 - INFO - allennlp.common.params - trainer.trainer_callbacks = None
2022-04-08 21:35:44,372 - INFO - allennlp.common.params - trainer.optimizer.type = huggingface_adamw
2022-04-08 21:35:44,373 - INFO - allennlp.common.params - trainer.optimizer.parameter_groups = None
2022-04-08 21:35:44,373 - INFO - allennlp.common.params - trainer.optimizer.lr = 1e-05
2022-04-08 21:35:44,373 - INFO - allennlp.common.params - trainer.optimizer.betas = [0.9, 0.98]
2022-04-08 21:35:44,373 - INFO - allennlp.common.params - trainer.optimizer.eps = 1e-06
2022-04-08 21:35:44,373 - INFO - allennlp.common.params - trainer.optimizer.weight_decay = 0.1
2022-04-08 21:35:44,374 - INFO - allennlp.common.params - trainer.optimizer.correct_bias = True
2022-04-08 21:35:44,374 - INFO - allennlp.training.optimizers - Number of trainable parameters: 356411394
2022-04-08 21:35:44,376 - INFO - allennlp.common.util - The following parameters are Frozen (without gradient):
2022-04-08 21:35:44,378 - INFO - allennlp.common.util - The following parameters are Tunable (with gradient):
2022-04-08 21:35:44,378 - INFO - allennlp.common.util - _classifier.roberta.embeddings.word_embeddings.weight
2022-04-08 21:35:44,378 - INFO - allennlp.common.util - _classifier.roberta.embeddings.position_embeddings.weight
2022-04-08 21:35:44,378 - INFO - allennlp.common.util - _classifier.roberta.embeddings.token_type_embeddings.weight
2022-04-08 21:35:44,379 - INFO - allennlp.common.util - _classifier.roberta.embeddings.LayerNorm.weight
2022-04-08 21:35:44,379 - INFO - allennlp.common.util - _classifier.roberta.embeddings.LayerNorm.bias
2022-04-08 21:35:44,379 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.self.query.weight
2022-04-08 21:35:44,379 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.self.query.bias
2022-04-08 21:35:44,379 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.self.key.weight
2022-04-08 21:35:44,379 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.self.key.bias
2022-04-08 21:35:44,379 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.self.value.weight
2022-04-08 21:35:44,379 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.self.value.bias
2022-04-08 21:35:44,379 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.output.dense.weight
2022-04-08 21:35:44,379 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.output.dense.bias
2022-04-08 21:35:44,379 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.output.LayerNorm.weight
2022-04-08 21:35:44,380 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.output.LayerNorm.bias
2022-04-08 21:35:44,380 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.intermediate.dense.weight
2022-04-08 21:35:44,380 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.intermediate.dense.bias
2022-04-08 21:35:44,380 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.output.dense.weight
2022-04-08 21:35:44,380 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.output.dense.bias
2022-04-08 21:35:44,380 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.output.LayerNorm.weight
2022-04-08 21:35:44,380 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.output.LayerNorm.bias
2022-04-08 21:35:44,380 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.self.query.weight
2022-04-08 21:35:44,380 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.self.query.bias
2022-04-08 21:35:44,380 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.self.key.weight
2022-04-08 21:35:44,380 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.self.key.bias
2022-04-08 21:35:44,381 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.self.value.weight
2022-04-08 21:35:44,381 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.self.value.bias
2022-04-08 21:35:44,381 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.output.dense.weight
2022-04-08 21:35:44,381 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.output.dense.bias
2022-04-08 21:35:44,381 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.output.LayerNorm.weight
2022-04-08 21:35:44,381 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.output.LayerNorm.bias
2022-04-08 21:35:44,381 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.intermediate.dense.weight
2022-04-08 21:35:44,381 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.intermediate.dense.bias
2022-04-08 21:35:44,381 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.output.dense.weight
2022-04-08 21:35:44,381 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.output.dense.bias
2022-04-08 21:35:44,381 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.output.LayerNorm.weight
2022-04-08 21:35:44,381 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.output.LayerNorm.bias
2022-04-08 21:35:44,382 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.self.query.weight
2022-04-08 21:35:44,382 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.self.query.bias
2022-04-08 21:35:44,382 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.self.key.weight
2022-04-08 21:35:44,382 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.self.key.bias
2022-04-08 21:35:44,382 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.self.value.weight
2022-04-08 21:35:44,382 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.self.value.bias
2022-04-08 21:35:44,382 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.output.dense.weight
2022-04-08 21:35:44,382 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.output.dense.bias
2022-04-08 21:35:44,382 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.output.LayerNorm.weight
2022-04-08 21:35:44,382 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.output.LayerNorm.bias
2022-04-08 21:35:44,382 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.intermediate.dense.weight
2022-04-08 21:35:44,383 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.intermediate.dense.bias
2022-04-08 21:35:44,383 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.output.dense.weight
2022-04-08 21:35:44,383 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.output.dense.bias
2022-04-08 21:35:44,383 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.output.LayerNorm.weight
2022-04-08 21:35:44,383 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.output.LayerNorm.bias
2022-04-08 21:35:44,383 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.self.query.weight
2022-04-08 21:35:44,383 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.self.query.bias
2022-04-08 21:35:44,383 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.self.key.weight
2022-04-08 21:35:44,383 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.self.key.bias
2022-04-08 21:35:44,383 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.self.value.weight
2022-04-08 21:35:44,383 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.self.value.bias
2022-04-08 21:35:44,383 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.output.dense.weight
2022-04-08 21:35:44,384 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.output.dense.bias
2022-04-08 21:35:44,384 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.output.LayerNorm.weight
2022-04-08 21:35:44,384 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.output.LayerNorm.bias
2022-04-08 21:35:44,384 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.intermediate.dense.weight
2022-04-08 21:35:44,384 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.intermediate.dense.bias
2022-04-08 21:35:44,384 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.output.dense.weight
2022-04-08 21:35:44,384 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.output.dense.bias
2022-04-08 21:35:44,384 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.output.LayerNorm.weight
2022-04-08 21:35:44,384 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.output.LayerNorm.bias
2022-04-08 21:35:44,384 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.self.query.weight
2022-04-08 21:35:44,384 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.self.query.bias
2022-04-08 21:35:44,385 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.self.key.weight
2022-04-08 21:35:44,385 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.self.key.bias
2022-04-08 21:35:44,385 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.self.value.weight
2022-04-08 21:35:44,385 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.self.value.bias
2022-04-08 21:35:44,385 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.output.dense.weight
2022-04-08 21:35:44,385 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.output.dense.bias
2022-04-08 21:35:44,385 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.output.LayerNorm.weight
2022-04-08 21:35:44,385 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.output.LayerNorm.bias
2022-04-08 21:35:44,385 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.intermediate.dense.weight
2022-04-08 21:35:44,385 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.intermediate.dense.bias
2022-04-08 21:35:44,385 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.output.dense.weight
2022-04-08 21:35:44,385 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.output.dense.bias
2022-04-08 21:35:44,386 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.output.LayerNorm.weight
2022-04-08 21:35:44,386 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.output.LayerNorm.bias
2022-04-08 21:35:44,386 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.self.query.weight
2022-04-08 21:35:44,386 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.self.query.bias
2022-04-08 21:35:44,386 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.self.key.weight
2022-04-08 21:35:44,386 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.self.key.bias
2022-04-08 21:35:44,386 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.self.value.weight
2022-04-08 21:35:44,386 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.self.value.bias
2022-04-08 21:35:44,386 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.output.dense.weight
2022-04-08 21:35:44,386 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.output.dense.bias
2022-04-08 21:35:44,386 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.output.LayerNorm.weight
2022-04-08 21:35:44,386 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.output.LayerNorm.bias
2022-04-08 21:35:44,386 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.intermediate.dense.weight
2022-04-08 21:35:44,387 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.intermediate.dense.bias
2022-04-08 21:35:44,387 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.output.dense.weight
2022-04-08 21:35:44,387 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.output.dense.bias
2022-04-08 21:35:44,387 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.output.LayerNorm.weight
2022-04-08 21:35:44,387 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.output.LayerNorm.bias
2022-04-08 21:35:44,387 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.self.query.weight
2022-04-08 21:35:44,387 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.self.query.bias
2022-04-08 21:35:44,387 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.self.key.weight
2022-04-08 21:35:44,387 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.self.key.bias
2022-04-08 21:35:44,387 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.self.value.weight
2022-04-08 21:35:44,387 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.self.value.bias
2022-04-08 21:35:44,387 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.output.dense.weight
2022-04-08 21:35:44,388 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.output.dense.bias
2022-04-08 21:35:44,388 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.output.LayerNorm.weight
2022-04-08 21:35:44,388 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.output.LayerNorm.bias
2022-04-08 21:35:44,388 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.intermediate.dense.weight
2022-04-08 21:35:44,388 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.intermediate.dense.bias
2022-04-08 21:35:44,388 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.output.dense.weight
2022-04-08 21:35:44,388 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.output.dense.bias
2022-04-08 21:35:44,388 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.output.LayerNorm.weight
2022-04-08 21:35:44,388 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.output.LayerNorm.bias
2022-04-08 21:35:44,388 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.self.query.weight
2022-04-08 21:35:44,388 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.self.query.bias
2022-04-08 21:35:44,388 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.self.key.weight
2022-04-08 21:35:44,388 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.self.key.bias
2022-04-08 21:35:44,389 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.self.value.weight
2022-04-08 21:35:44,389 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.self.value.bias
2022-04-08 21:35:44,389 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.output.dense.weight
2022-04-08 21:35:44,389 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.output.dense.bias
2022-04-08 21:35:44,389 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.output.LayerNorm.weight
2022-04-08 21:35:44,389 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.output.LayerNorm.bias
2022-04-08 21:35:44,389 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.intermediate.dense.weight
2022-04-08 21:35:44,389 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.intermediate.dense.bias
2022-04-08 21:35:44,389 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.output.dense.weight
2022-04-08 21:35:44,389 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.output.dense.bias
2022-04-08 21:35:44,389 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.output.LayerNorm.weight
2022-04-08 21:35:44,390 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.output.LayerNorm.bias
2022-04-08 21:35:44,390 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.self.query.weight
2022-04-08 21:35:44,390 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.self.query.bias
2022-04-08 21:35:44,390 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.self.key.weight
2022-04-08 21:35:44,390 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.self.key.bias
2022-04-08 21:35:44,390 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.self.value.weight
2022-04-08 21:35:44,390 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.self.value.bias
2022-04-08 21:35:44,390 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.output.dense.weight
2022-04-08 21:35:44,390 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.output.dense.bias
2022-04-08 21:35:44,390 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.output.LayerNorm.weight
2022-04-08 21:35:44,390 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.output.LayerNorm.bias
2022-04-08 21:35:44,391 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.intermediate.dense.weight
2022-04-08 21:35:44,391 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.intermediate.dense.bias
2022-04-08 21:35:44,391 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.output.dense.weight
2022-04-08 21:35:44,391 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.output.dense.bias
2022-04-08 21:35:44,391 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.output.LayerNorm.weight
2022-04-08 21:35:44,391 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.output.LayerNorm.bias
2022-04-08 21:35:44,391 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.self.query.weight
2022-04-08 21:35:44,391 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.self.query.bias
2022-04-08 21:35:44,391 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.self.key.weight
2022-04-08 21:35:44,391 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.self.key.bias
2022-04-08 21:35:44,392 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.self.value.weight
2022-04-08 21:35:44,392 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.self.value.bias
2022-04-08 21:35:44,392 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.output.dense.weight
2022-04-08 21:35:44,392 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.output.dense.bias
2022-04-08 21:35:44,392 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.output.LayerNorm.weight
2022-04-08 21:35:44,392 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.output.LayerNorm.bias
2022-04-08 21:35:44,392 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.intermediate.dense.weight
2022-04-08 21:35:44,392 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.intermediate.dense.bias
2022-04-08 21:35:44,392 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.output.dense.weight
2022-04-08 21:35:44,392 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.output.dense.bias
2022-04-08 21:35:44,392 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.output.LayerNorm.weight
2022-04-08 21:35:44,392 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.output.LayerNorm.bias
2022-04-08 21:35:44,393 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.self.query.weight
2022-04-08 21:35:44,393 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.self.query.bias
2022-04-08 21:35:44,393 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.self.key.weight
2022-04-08 21:35:44,393 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.self.key.bias
2022-04-08 21:35:44,393 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.self.value.weight
2022-04-08 21:35:44,393 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.self.value.bias
2022-04-08 21:35:44,393 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.output.dense.weight
2022-04-08 21:35:44,393 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.output.dense.bias
2022-04-08 21:35:44,393 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.output.LayerNorm.weight
2022-04-08 21:35:44,393 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.output.LayerNorm.bias
2022-04-08 21:35:44,393 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.intermediate.dense.weight
2022-04-08 21:35:44,393 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.intermediate.dense.bias
2022-04-08 21:35:44,394 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.output.dense.weight
2022-04-08 21:35:44,394 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.output.dense.bias
2022-04-08 21:35:44,394 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.output.LayerNorm.weight
2022-04-08 21:35:44,394 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.output.LayerNorm.bias
2022-04-08 21:35:44,394 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.self.query.weight
2022-04-08 21:35:44,394 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.self.query.bias
2022-04-08 21:35:44,394 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.self.key.weight
2022-04-08 21:35:44,394 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.self.key.bias
2022-04-08 21:35:44,394 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.self.value.weight
2022-04-08 21:35:44,394 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.self.value.bias
2022-04-08 21:35:44,394 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.output.dense.weight
2022-04-08 21:35:44,395 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.output.dense.bias
2022-04-08 21:35:44,395 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.output.LayerNorm.weight
2022-04-08 21:35:44,395 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.output.LayerNorm.bias
2022-04-08 21:35:44,395 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.intermediate.dense.weight
2022-04-08 21:35:44,395 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.intermediate.dense.bias
2022-04-08 21:35:44,395 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.output.dense.weight
2022-04-08 21:35:44,395 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.output.dense.bias
2022-04-08 21:35:44,395 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.output.LayerNorm.weight
2022-04-08 21:35:44,395 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.output.LayerNorm.bias
2022-04-08 21:35:44,395 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.self.query.weight
2022-04-08 21:35:44,395 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.self.query.bias
2022-04-08 21:35:44,396 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.self.key.weight
2022-04-08 21:35:44,396 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.self.key.bias
2022-04-08 21:35:44,396 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.self.value.weight
2022-04-08 21:35:44,396 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.self.value.bias
2022-04-08 21:35:44,396 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.output.dense.weight
2022-04-08 21:35:44,396 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.output.dense.bias
2022-04-08 21:35:44,396 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.output.LayerNorm.weight
2022-04-08 21:35:44,396 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.output.LayerNorm.bias
2022-04-08 21:35:44,396 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.intermediate.dense.weight
2022-04-08 21:35:44,396 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.intermediate.dense.bias
2022-04-08 21:35:44,396 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.output.dense.weight
2022-04-08 21:35:44,396 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.output.dense.bias
2022-04-08 21:35:44,397 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.output.LayerNorm.weight
2022-04-08 21:35:44,397 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.output.LayerNorm.bias
2022-04-08 21:35:44,397 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.self.query.weight
2022-04-08 21:35:44,397 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.self.query.bias
2022-04-08 21:35:44,397 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.self.key.weight
2022-04-08 21:35:44,397 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.self.key.bias
2022-04-08 21:35:44,397 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.self.value.weight
2022-04-08 21:35:44,397 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.self.value.bias
2022-04-08 21:35:44,397 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.output.dense.weight
2022-04-08 21:35:44,397 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.output.dense.bias
2022-04-08 21:35:44,397 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.output.LayerNorm.weight
2022-04-08 21:35:44,397 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.output.LayerNorm.bias
2022-04-08 21:35:44,398 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.intermediate.dense.weight
2022-04-08 21:35:44,398 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.intermediate.dense.bias
2022-04-08 21:35:44,398 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.output.dense.weight
2022-04-08 21:35:44,398 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.output.dense.bias
2022-04-08 21:35:44,398 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.output.LayerNorm.weight
2022-04-08 21:35:44,398 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.output.LayerNorm.bias
2022-04-08 21:35:44,398 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.self.query.weight
2022-04-08 21:35:44,398 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.self.query.bias
2022-04-08 21:35:44,398 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.self.key.weight
2022-04-08 21:35:44,398 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.self.key.bias
2022-04-08 21:35:44,398 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.self.value.weight
2022-04-08 21:35:44,399 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.self.value.bias
2022-04-08 21:35:44,399 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.output.dense.weight
2022-04-08 21:35:44,399 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.output.dense.bias
2022-04-08 21:35:44,399 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.output.LayerNorm.weight
2022-04-08 21:35:44,399 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.output.LayerNorm.bias
2022-04-08 21:35:44,399 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.intermediate.dense.weight
2022-04-08 21:35:44,399 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.intermediate.dense.bias
2022-04-08 21:35:44,399 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.output.dense.weight
2022-04-08 21:35:44,399 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.output.dense.bias
2022-04-08 21:35:44,399 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.output.LayerNorm.weight
2022-04-08 21:35:44,399 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.output.LayerNorm.bias
2022-04-08 21:35:44,400 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.self.query.weight
2022-04-08 21:35:44,400 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.self.query.bias
2022-04-08 21:35:44,400 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.self.key.weight
2022-04-08 21:35:44,400 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.self.key.bias
2022-04-08 21:35:44,400 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.self.value.weight
2022-04-08 21:35:44,400 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.self.value.bias
2022-04-08 21:35:44,400 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.output.dense.weight
2022-04-08 21:35:44,400 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.output.dense.bias
2022-04-08 21:35:44,400 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.output.LayerNorm.weight
2022-04-08 21:35:44,400 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.output.LayerNorm.bias
2022-04-08 21:35:44,400 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.intermediate.dense.weight
2022-04-08 21:35:44,400 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.intermediate.dense.bias
2022-04-08 21:35:44,401 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.output.dense.weight
2022-04-08 21:35:44,401 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.output.dense.bias
2022-04-08 21:35:44,401 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.output.LayerNorm.weight
2022-04-08 21:35:44,401 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.output.LayerNorm.bias
2022-04-08 21:35:44,401 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.self.query.weight
2022-04-08 21:35:44,401 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.self.query.bias
2022-04-08 21:35:44,401 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.self.key.weight
2022-04-08 21:35:44,401 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.self.key.bias
2022-04-08 21:35:44,401 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.self.value.weight
2022-04-08 21:35:44,401 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.self.value.bias
2022-04-08 21:35:44,401 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.output.dense.weight
2022-04-08 21:35:44,401 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.output.dense.bias
2022-04-08 21:35:44,402 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.output.LayerNorm.weight
2022-04-08 21:35:44,402 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.output.LayerNorm.bias
2022-04-08 21:35:44,402 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.intermediate.dense.weight
2022-04-08 21:35:44,402 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.intermediate.dense.bias
2022-04-08 21:35:44,402 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.output.dense.weight
2022-04-08 21:35:44,402 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.output.dense.bias
2022-04-08 21:35:44,402 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.output.LayerNorm.weight
2022-04-08 21:35:44,402 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.output.LayerNorm.bias
2022-04-08 21:35:44,402 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.self.query.weight
2022-04-08 21:35:44,402 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.self.query.bias
2022-04-08 21:35:44,402 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.self.key.weight
2022-04-08 21:35:44,402 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.self.key.bias
2022-04-08 21:35:44,403 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.self.value.weight
2022-04-08 21:35:44,403 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.self.value.bias
2022-04-08 21:35:44,403 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.output.dense.weight
2022-04-08 21:35:44,403 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.output.dense.bias
2022-04-08 21:35:44,403 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.output.LayerNorm.weight
2022-04-08 21:35:44,403 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.output.LayerNorm.bias
2022-04-08 21:35:44,403 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.intermediate.dense.weight
2022-04-08 21:35:44,403 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.intermediate.dense.bias
2022-04-08 21:35:44,403 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.output.dense.weight
2022-04-08 21:35:44,403 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.output.dense.bias
2022-04-08 21:35:44,403 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.output.LayerNorm.weight
2022-04-08 21:35:44,404 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.output.LayerNorm.bias
2022-04-08 21:35:44,404 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.self.query.weight
2022-04-08 21:35:44,404 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.self.query.bias
2022-04-08 21:35:44,404 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.self.key.weight
2022-04-08 21:35:44,404 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.self.key.bias
2022-04-08 21:35:44,404 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.self.value.weight
2022-04-08 21:35:44,404 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.self.value.bias
2022-04-08 21:35:44,404 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.output.dense.weight
2022-04-08 21:35:44,404 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.output.dense.bias
2022-04-08 21:35:44,404 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.output.LayerNorm.weight
2022-04-08 21:35:44,404 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.output.LayerNorm.bias
2022-04-08 21:35:44,404 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.intermediate.dense.weight
2022-04-08 21:35:44,405 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.intermediate.dense.bias
2022-04-08 21:35:44,405 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.output.dense.weight
2022-04-08 21:35:44,405 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.output.dense.bias
2022-04-08 21:35:44,405 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.output.LayerNorm.weight
2022-04-08 21:35:44,405 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.output.LayerNorm.bias
2022-04-08 21:35:44,405 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.self.query.weight
2022-04-08 21:35:44,405 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.self.query.bias
2022-04-08 21:35:44,405 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.self.key.weight
2022-04-08 21:35:44,405 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.self.key.bias
2022-04-08 21:35:44,405 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.self.value.weight
2022-04-08 21:35:44,405 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.self.value.bias
2022-04-08 21:35:44,406 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.output.dense.weight
2022-04-08 21:35:44,406 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.output.dense.bias
2022-04-08 21:35:44,406 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.output.LayerNorm.weight
2022-04-08 21:35:44,406 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.output.LayerNorm.bias
2022-04-08 21:35:44,406 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.intermediate.dense.weight
2022-04-08 21:35:44,406 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.intermediate.dense.bias
2022-04-08 21:35:44,406 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.output.dense.weight
2022-04-08 21:35:44,406 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.output.dense.bias
2022-04-08 21:35:44,406 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.output.LayerNorm.weight
2022-04-08 21:35:44,406 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.output.LayerNorm.bias
2022-04-08 21:35:44,406 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.self.query.weight
2022-04-08 21:35:44,406 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.self.query.bias
2022-04-08 21:35:44,406 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.self.key.weight
2022-04-08 21:35:44,406 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.self.key.bias
2022-04-08 21:35:44,406 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.self.value.weight
2022-04-08 21:35:44,406 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.self.value.bias
2022-04-08 21:35:44,406 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.output.dense.weight
2022-04-08 21:35:44,406 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.output.dense.bias
2022-04-08 21:35:44,406 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.output.LayerNorm.weight
2022-04-08 21:35:44,407 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.output.LayerNorm.bias
2022-04-08 21:35:44,407 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.intermediate.dense.weight
2022-04-08 21:35:44,407 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.intermediate.dense.bias
2022-04-08 21:35:44,407 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.output.dense.weight
2022-04-08 21:35:44,407 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.output.dense.bias
2022-04-08 21:35:44,407 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.output.LayerNorm.weight
2022-04-08 21:35:44,407 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.output.LayerNorm.bias
2022-04-08 21:35:44,407 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.self.query.weight
2022-04-08 21:35:44,407 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.self.query.bias
2022-04-08 21:35:44,407 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.self.key.weight
2022-04-08 21:35:44,407 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.self.key.bias
2022-04-08 21:35:44,407 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.self.value.weight
2022-04-08 21:35:44,407 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.self.value.bias
2022-04-08 21:35:44,407 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.output.dense.weight
2022-04-08 21:35:44,407 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.output.dense.bias
2022-04-08 21:35:44,408 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.output.LayerNorm.weight
2022-04-08 21:35:44,408 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.output.LayerNorm.bias
2022-04-08 21:35:44,408 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.intermediate.dense.weight
2022-04-08 21:35:44,408 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.intermediate.dense.bias
2022-04-08 21:35:44,408 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.output.dense.weight
2022-04-08 21:35:44,408 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.output.dense.bias
2022-04-08 21:35:44,408 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.output.LayerNorm.weight
2022-04-08 21:35:44,408 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.output.LayerNorm.bias
2022-04-08 21:35:44,408 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.self.query.weight
2022-04-08 21:35:44,408 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.self.query.bias
2022-04-08 21:35:44,408 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.self.key.weight
2022-04-08 21:35:44,408 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.self.key.bias
2022-04-08 21:35:44,408 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.self.value.weight
2022-04-08 21:35:44,408 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.self.value.bias
2022-04-08 21:35:44,408 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.output.dense.weight
2022-04-08 21:35:44,408 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.output.dense.bias
2022-04-08 21:35:44,408 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.output.LayerNorm.weight
2022-04-08 21:35:44,408 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.output.LayerNorm.bias
2022-04-08 21:35:44,408 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.intermediate.dense.weight
2022-04-08 21:35:44,409 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.intermediate.dense.bias
2022-04-08 21:35:44,409 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.output.dense.weight
2022-04-08 21:35:44,409 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.output.dense.bias
2022-04-08 21:35:44,409 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.output.LayerNorm.weight
2022-04-08 21:35:44,409 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.output.LayerNorm.bias
2022-04-08 21:35:44,409 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.self.query.weight
2022-04-08 21:35:44,409 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.self.query.bias
2022-04-08 21:35:44,409 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.self.key.weight
2022-04-08 21:35:44,409 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.self.key.bias
2022-04-08 21:35:44,409 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.self.value.weight
2022-04-08 21:35:44,409 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.self.value.bias
2022-04-08 21:35:44,409 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.output.dense.weight
2022-04-08 21:35:44,409 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.output.dense.bias
2022-04-08 21:35:44,409 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.output.LayerNorm.weight
2022-04-08 21:35:44,409 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.output.LayerNorm.bias
2022-04-08 21:35:44,409 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.intermediate.dense.weight
2022-04-08 21:35:44,409 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.intermediate.dense.bias
2022-04-08 21:35:44,409 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.output.dense.weight
2022-04-08 21:35:44,409 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.output.dense.bias
2022-04-08 21:35:44,410 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.output.LayerNorm.weight
2022-04-08 21:35:44,410 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.output.LayerNorm.bias
2022-04-08 21:35:44,410 - INFO - allennlp.common.util - _classifier.roberta.pooler.dense.weight
2022-04-08 21:35:44,410 - INFO - allennlp.common.util - _classifier.roberta.pooler.dense.bias
2022-04-08 21:35:44,410 - INFO - allennlp.common.util - _classifier.classifier.dense.weight
2022-04-08 21:35:44,410 - INFO - allennlp.common.util - _classifier.classifier.dense.bias
2022-04-08 21:35:44,410 - INFO - allennlp.common.util - _classifier.classifier.out_proj.weight
2022-04-08 21:35:44,410 - INFO - allennlp.common.util - _classifier.classifier.out_proj.bias
2022-04-08 21:35:44,410 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.type = slanted_triangular
2022-04-08 21:35:44,410 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.cut_frac = 0.06
2022-04-08 21:35:44,410 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.ratio = 32
2022-04-08 21:35:44,410 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.last_epoch = -1
2022-04-08 21:35:44,411 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.gradual_unfreezing = False
2022-04-08 21:35:44,411 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.discriminative_fine_tuning = False
2022-04-08 21:35:44,411 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.decay_factor = 0.38
2022-04-08 21:35:44,411 - INFO - allennlp.common.params - type = default
2022-04-08 21:35:44,411 - INFO - allennlp.common.params - keep_serialized_model_every_num_seconds = None
2022-04-08 21:35:44,411 - INFO - allennlp.common.params - num_serialized_models_to_keep = 2
2022-04-08 21:35:44,411 - INFO - allennlp.common.params - model_save_interval = None
2022-04-08 21:35:44,411 - INFO - allennlp.common.params - summary_interval = 100
2022-04-08 21:35:44,411 - INFO - allennlp.common.params - histogram_interval = None
2022-04-08 21:35:44,411 - INFO - allennlp.common.params - batch_size_interval = None
2022-04-08 21:35:44,411 - INFO - allennlp.common.params - should_log_parameter_statistics = True
2022-04-08 21:35:44,411 - INFO - allennlp.common.params - should_log_learning_rate = False
2022-04-08 21:35:44,411 - INFO - allennlp.common.params - get_batch_num_total = None
2022-04-08 21:35:44,414 - WARNING - allennlp.training.trainer - You provided a validation dataset but patience was set to None, meaning that early stopping is disabled
2022-04-08 21:35:44,507 - INFO - allennlp.training.trainer - Beginning training.
2022-04-08 21:35:44,507 - INFO - allennlp.training.trainer - Epoch 0/14
2022-04-08 21:35:44,507 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-08 21:35:44,507 - INFO - allennlp.training.trainer - GPU 0 memory usage: 1.3G
2022-04-08 21:35:44,508 - INFO - allennlp.training.trainer - Training
2022-04-08 21:35:44,509 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-08 21:35:44,509 - INFO - allennlp.data.samplers.bucket_batch_sampler - No sorting keys given; trying to guess a good one
2022-04-08 21:35:44,509 - INFO - allennlp.data.samplers.bucket_batch_sampler - Using ['tokens'] as the sorting keys
2022-04-08 21:35:54,753 - INFO - tqdm - accuracy: 0.5312, batch_loss: 2.5930, loss: 2.5930 ||:   2%|1         | 1/65 [00:10<10:55, 10.24s/it]
2022-04-08 21:36:05,045 - INFO - tqdm - accuracy: 0.5469, batch_loss: 2.1898, loss: 2.3914 ||:   3%|3         | 2/65 [00:20<10:47, 10.27s/it]
2022-04-08 21:36:15,467 - INFO - tqdm - accuracy: 0.5000, batch_loss: 3.0479, loss: 2.6102 ||:   5%|4         | 3/65 [00:30<10:41, 10.34s/it]
2022-04-08 21:36:26,028 - INFO - tqdm - accuracy: 0.5156, batch_loss: 2.5260, loss: 2.5892 ||:   6%|6         | 4/65 [00:41<10:36, 10.43s/it]
2022-04-08 21:36:36,808 - INFO - tqdm - accuracy: 0.4938, batch_loss: 2.9313, loss: 2.6576 ||:   8%|7         | 5/65 [00:52<10:33, 10.55s/it]
2022-04-08 21:36:47,815 - INFO - tqdm - accuracy: 0.4740, batch_loss: 2.7583, loss: 2.6744 ||:   9%|9         | 6/65 [01:03<10:31, 10.71s/it]
2022-04-08 21:36:58,842 - INFO - tqdm - accuracy: 0.4688, batch_loss: 3.1544, loss: 2.7430 ||:  11%|#         | 7/65 [01:14<10:27, 10.81s/it]
2022-04-08 21:37:10,104 - INFO - tqdm - accuracy: 0.4531, batch_loss: 2.9200, loss: 2.7651 ||:  12%|#2        | 8/65 [01:25<10:24, 10.96s/it]
2022-04-08 21:37:21,631 - INFO - tqdm - accuracy: 0.4549, batch_loss: 2.2981, loss: 2.7132 ||:  14%|#3        | 9/65 [01:37<10:23, 11.13s/it]
2022-04-08 21:37:33,289 - INFO - tqdm - accuracy: 0.4594, batch_loss: 2.2473, loss: 2.6666 ||:  15%|#5        | 10/65 [01:48<10:21, 11.30s/it]
2022-04-08 21:37:45,334 - INFO - tqdm - accuracy: 0.4659, batch_loss: 2.2898, loss: 2.6323 ||:  17%|#6        | 11/65 [02:00<10:22, 11.53s/it]
2022-04-08 21:37:57,230 - INFO - tqdm - accuracy: 0.4688, batch_loss: 1.9590, loss: 2.5762 ||:  18%|#8        | 12/65 [02:12<10:16, 11.64s/it]
2022-04-08 21:38:08,857 - INFO - tqdm - accuracy: 0.4808, batch_loss: 1.4076, loss: 2.4863 ||:  20%|##        | 13/65 [02:24<10:05, 11.63s/it]
2022-04-08 21:38:20,249 - INFO - tqdm - accuracy: 0.4799, batch_loss: 2.2407, loss: 2.4688 ||:  22%|##1       | 14/65 [02:35<09:49, 11.56s/it]
2022-04-08 21:38:31,760 - INFO - tqdm - accuracy: 0.4854, batch_loss: 1.8817, loss: 2.4297 ||:  23%|##3       | 15/65 [02:47<09:37, 11.55s/it]
2022-04-08 21:38:43,411 - INFO - tqdm - accuracy: 0.4902, batch_loss: 1.6441, loss: 2.3806 ||:  25%|##4       | 16/65 [02:58<09:27, 11.58s/it]
2022-04-08 21:38:55,202 - INFO - tqdm - accuracy: 0.4963, batch_loss: 1.0376, loss: 2.3016 ||:  26%|##6       | 17/65 [03:10<09:18, 11.64s/it]
2022-04-08 21:39:06,973 - INFO - tqdm - accuracy: 0.4965, batch_loss: 1.7333, loss: 2.2700 ||:  28%|##7       | 18/65 [03:22<09:08, 11.68s/it]
2022-04-08 21:39:18,703 - INFO - tqdm - accuracy: 0.4951, batch_loss: 1.4116, loss: 2.2248 ||:  29%|##9       | 19/65 [03:34<08:57, 11.70s/it]
2022-04-08 21:39:30,106 - INFO - tqdm - accuracy: 0.5016, batch_loss: 0.7480, loss: 2.1510 ||:  31%|###       | 20/65 [03:45<08:42, 11.61s/it]
2022-04-08 21:39:41,767 - INFO - tqdm - accuracy: 0.5015, batch_loss: 1.3379, loss: 2.1123 ||:  32%|###2      | 21/65 [03:57<08:31, 11.62s/it]
2022-04-08 21:39:53,389 - INFO - tqdm - accuracy: 0.5043, batch_loss: 1.0952, loss: 2.0660 ||:  34%|###3      | 22/65 [04:08<08:19, 11.62s/it]
2022-04-08 21:40:05,173 - INFO - tqdm - accuracy: 0.5068, batch_loss: 1.0202, loss: 2.0206 ||:  35%|###5      | 23/65 [04:20<08:10, 11.67s/it]
2022-04-08 21:40:16,945 - INFO - tqdm - accuracy: 0.5052, batch_loss: 1.0735, loss: 1.9811 ||:  37%|###6      | 24/65 [04:32<07:59, 11.70s/it]
2022-04-08 21:40:28,670 - INFO - tqdm - accuracy: 0.5050, batch_loss: 1.1913, loss: 1.9495 ||:  38%|###8      | 25/65 [04:44<07:48, 11.71s/it]
2022-04-08 21:40:40,352 - INFO - tqdm - accuracy: 0.5012, batch_loss: 0.9178, loss: 1.9098 ||:  40%|####      | 26/65 [04:55<07:36, 11.70s/it]
2022-04-08 21:40:51,997 - INFO - tqdm - accuracy: 0.5035, batch_loss: 0.7837, loss: 1.8681 ||:  42%|####1     | 27/65 [05:07<07:23, 11.68s/it]
2022-04-08 21:41:03,671 - INFO - tqdm - accuracy: 0.5056, batch_loss: 0.7674, loss: 1.8288 ||:  43%|####3     | 28/65 [05:19<07:12, 11.68s/it]
2022-04-08 21:41:15,384 - INFO - tqdm - accuracy: 0.5022, batch_loss: 0.8468, loss: 1.7949 ||:  45%|####4     | 29/65 [05:30<07:00, 11.69s/it]
2022-04-08 21:41:27,101 - INFO - tqdm - accuracy: 0.5010, batch_loss: 0.9214, loss: 1.7658 ||:  46%|####6     | 30/65 [05:42<06:49, 11.70s/it]
2022-04-08 21:41:38,835 - INFO - tqdm - accuracy: 0.5081, batch_loss: 0.5659, loss: 1.7271 ||:  48%|####7     | 31/65 [05:54<06:38, 11.71s/it]
2022-04-08 21:41:50,580 - INFO - tqdm - accuracy: 0.5010, batch_loss: 0.9163, loss: 1.7018 ||:  49%|####9     | 32/65 [06:06<06:26, 11.72s/it]
2022-04-08 21:42:02,339 - INFO - tqdm - accuracy: 0.5019, batch_loss: 0.6859, loss: 1.6710 ||:  51%|#####     | 33/65 [06:17<06:15, 11.73s/it]
2022-04-08 21:42:14,084 - INFO - tqdm - accuracy: 0.5018, batch_loss: 0.6992, loss: 1.6424 ||:  52%|#####2    | 34/65 [06:29<06:03, 11.74s/it]
2022-04-08 21:42:25,845 - INFO - tqdm - accuracy: 0.4973, batch_loss: 0.8075, loss: 1.6186 ||:  54%|#####3    | 35/65 [06:41<05:52, 11.74s/it]
2022-04-08 21:42:37,656 - INFO - tqdm - accuracy: 0.4957, batch_loss: 0.7289, loss: 1.5938 ||:  55%|#####5    | 36/65 [06:53<05:41, 11.76s/it]
2022-04-08 21:42:49,421 - INFO - tqdm - accuracy: 0.4941, batch_loss: 0.6967, loss: 1.5696 ||:  57%|#####6    | 37/65 [07:04<05:29, 11.76s/it]
2022-04-08 21:43:01,180 - INFO - tqdm - accuracy: 0.4984, batch_loss: 0.6753, loss: 1.5461 ||:  58%|#####8    | 38/65 [07:16<05:17, 11.76s/it]
2022-04-08 21:43:12,889 - INFO - tqdm - accuracy: 0.4984, batch_loss: 0.6853, loss: 1.5240 ||:  60%|######    | 39/65 [07:28<05:05, 11.75s/it]
2022-04-08 21:43:24,585 - INFO - tqdm - accuracy: 0.4977, batch_loss: 0.6975, loss: 1.5033 ||:  62%|######1   | 40/65 [07:40<04:53, 11.73s/it]
2022-04-08 21:43:36,257 - INFO - tqdm - accuracy: 0.4962, batch_loss: 0.7334, loss: 1.4846 ||:  63%|######3   | 41/65 [07:51<04:41, 11.71s/it]
2022-04-08 21:43:47,923 - INFO - tqdm - accuracy: 0.5000, batch_loss: 0.6663, loss: 1.4651 ||:  65%|######4   | 42/65 [08:03<04:29, 11.70s/it]
2022-04-08 21:43:59,600 - INFO - tqdm - accuracy: 0.5000, batch_loss: 0.6902, loss: 1.4471 ||:  66%|######6   | 43/65 [08:15<04:17, 11.69s/it]
2022-04-08 21:44:11,269 - INFO - tqdm - accuracy: 0.5043, batch_loss: 0.6523, loss: 1.4290 ||:  68%|######7   | 44/65 [08:26<04:05, 11.69s/it]
2022-04-08 21:44:22,929 - INFO - tqdm - accuracy: 0.5049, batch_loss: 0.6850, loss: 1.4125 ||:  69%|######9   | 45/65 [08:38<03:53, 11.68s/it]
2022-04-08 21:44:34,589 - INFO - tqdm - accuracy: 0.5082, batch_loss: 0.6641, loss: 1.3962 ||:  71%|#######   | 46/65 [08:50<03:41, 11.67s/it]
2022-04-08 21:44:46,260 - INFO - tqdm - accuracy: 0.5086, batch_loss: 0.6788, loss: 1.3809 ||:  72%|#######2  | 47/65 [09:01<03:30, 11.67s/it]
2022-04-08 21:44:57,954 - INFO - tqdm - accuracy: 0.5091, batch_loss: 0.7294, loss: 1.3674 ||:  74%|#######3  | 48/65 [09:13<03:18, 11.68s/it]
2022-04-08 21:45:09,695 - INFO - tqdm - accuracy: 0.5121, batch_loss: 0.6749, loss: 1.3532 ||:  75%|#######5  | 49/65 [09:25<03:07, 11.70s/it]
2022-04-08 21:45:21,466 - INFO - tqdm - accuracy: 0.5131, batch_loss: 0.7057, loss: 1.3403 ||:  77%|#######6  | 50/65 [09:36<02:55, 11.72s/it]
2022-04-08 21:45:33,281 - INFO - tqdm - accuracy: 0.5129, batch_loss: 0.7268, loss: 1.3282 ||:  78%|#######8  | 51/65 [09:48<02:44, 11.75s/it]
2022-04-08 21:45:45,051 - INFO - tqdm - accuracy: 0.5168, batch_loss: 0.6251, loss: 1.3147 ||:  80%|########  | 52/65 [10:00<02:32, 11.75s/it]
2022-04-08 21:45:56,807 - INFO - tqdm - accuracy: 0.5165, batch_loss: 0.7108, loss: 1.3033 ||:  82%|########1 | 53/65 [10:12<02:21, 11.76s/it]
2022-04-08 21:46:08,160 - INFO - tqdm - accuracy: 0.5217, batch_loss: 0.5907, loss: 1.2901 ||:  83%|########3 | 54/65 [10:23<02:07, 11.63s/it]
2022-04-08 21:46:19,849 - INFO - tqdm - accuracy: 0.5196, batch_loss: 0.7671, loss: 1.2806 ||:  85%|########4 | 55/65 [10:35<01:56, 11.65s/it]
2022-04-08 21:46:31,530 - INFO - tqdm - accuracy: 0.5204, batch_loss: 0.6812, loss: 1.2699 ||:  86%|########6 | 56/65 [10:47<01:44, 11.66s/it]
2022-04-08 21:46:43,205 - INFO - tqdm - accuracy: 0.5162, batch_loss: 0.8354, loss: 1.2623 ||:  88%|########7 | 57/65 [10:58<01:33, 11.66s/it]
2022-04-08 21:46:54,894 - INFO - tqdm - accuracy: 0.5164, batch_loss: 0.6797, loss: 1.2523 ||:  89%|########9 | 58/65 [11:10<01:21, 11.67s/it]
2022-04-08 21:47:06,593 - INFO - tqdm - accuracy: 0.5162, batch_loss: 0.7146, loss: 1.2431 ||:  91%|######### | 59/65 [11:22<01:10, 11.68s/it]
2022-04-08 21:47:18,297 - INFO - tqdm - accuracy: 0.5175, batch_loss: 0.6754, loss: 1.2337 ||:  92%|#########2| 60/65 [11:33<00:58, 11.69s/it]
2022-04-08 21:47:29,977 - INFO - tqdm - accuracy: 0.5172, batch_loss: 0.6771, loss: 1.2246 ||:  94%|#########3| 61/65 [11:45<00:46, 11.68s/it]
2022-04-08 21:47:41,666 - INFO - tqdm - accuracy: 0.5174, batch_loss: 0.6948, loss: 1.2160 ||:  95%|#########5| 62/65 [11:57<00:35, 11.69s/it]
2022-04-08 21:47:53,413 - INFO - tqdm - accuracy: 0.5146, batch_loss: 0.7226, loss: 1.2082 ||:  97%|#########6| 63/65 [12:08<00:23, 11.70s/it]
2022-04-08 21:48:05,150 - INFO - tqdm - accuracy: 0.5154, batch_loss: 0.6992, loss: 1.2002 ||:  98%|#########8| 64/65 [12:20<00:11, 11.71s/it]
2022-04-08 21:48:10,360 - INFO - tqdm - accuracy: 0.5148, batch_loss: 0.7136, loss: 1.1927 ||: 100%|##########| 65/65 [12:25<00:00,  9.76s/it]
2022-04-08 21:48:10,360 - INFO - tqdm - accuracy: 0.5148, batch_loss: 0.7136, loss: 1.1927 ||: 100%|##########| 65/65 [12:25<00:00, 11.47s/it]
2022-04-08 21:48:13,240 - INFO - allennlp.training.trainer - Validating
2022-04-08 21:48:13,241 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-08 21:48:13,241 - INFO - allennlp.data.samplers.bucket_batch_sampler - No sorting keys given; trying to guess a good one
2022-04-08 21:48:13,242 - INFO - allennlp.data.samplers.bucket_batch_sampler - Using ['tokens'] as the sorting keys
2022-04-08 21:48:23,319 - INFO - tqdm - accuracy: 0.4872, batch_loss: 0.5949, loss: 0.7002 ||:  34%|###3      | 39/115 [00:10<00:19,  3.85it/s]
2022-04-08 21:48:33,417 - INFO - tqdm - accuracy: 0.4615, batch_loss: 0.7730, loss: 0.6987 ||:  68%|######7   | 78/115 [00:20<00:09,  3.97it/s]
2022-04-08 21:48:43,001 - INFO - tqdm - accuracy: 0.4629, batch_loss: 0.7472, loss: 0.6982 ||: 100%|##########| 115/115 [00:29<00:00,  3.80it/s]
2022-04-08 21:48:43,001 - INFO - tqdm - accuracy: 0.4629, batch_loss: 0.7472, loss: 0.6982 ||: 100%|##########| 115/115 [00:29<00:00,  3.86it/s]
2022-04-08 21:48:43,002 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-08 21:48:43,003 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.515  |     0.463
2022-04-08 21:48:43,004 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  1359.606  |       N/A
2022-04-08 21:48:43,004 - INFO - allennlp.training.tensorboard_writer - loss               |     1.193  |     0.698
2022-04-08 21:48:43,004 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5799.516  |       N/A
2022-04-08 21:48:48,827 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to './output_newboolq_ir-ora-d/best.th'.
2022-04-08 21:48:50,066 - INFO - allennlp.training.trainer - Epoch duration: 0:13:05.558964
2022-04-08 21:48:50,066 - INFO - allennlp.training.trainer - Estimated training time remaining: 3:03:17
2022-04-08 21:48:50,066 - INFO - allennlp.training.trainer - Epoch 1/14
2022-04-08 21:48:50,066 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-08 21:48:50,067 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-08 21:48:50,068 - INFO - allennlp.training.trainer - Training
2022-04-08 21:48:50,068 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-08 21:49:01,616 - INFO - tqdm - accuracy: 0.3125, batch_loss: 0.7282, loss: 0.7282 ||:   2%|1         | 1/65 [00:11<12:19, 11.55s/it]
2022-04-08 21:49:13,492 - INFO - tqdm - accuracy: 0.4219, batch_loss: 0.7030, loss: 0.7156 ||:   3%|3         | 2/65 [00:23<12:19, 11.74s/it]
2022-04-08 21:49:25,566 - INFO - tqdm - accuracy: 0.4375, batch_loss: 0.7271, loss: 0.7194 ||:   5%|4         | 3/65 [00:35<12:17, 11.89s/it]
2022-04-08 21:49:37,347 - INFO - tqdm - accuracy: 0.4844, batch_loss: 0.6753, loss: 0.7084 ||:   6%|6         | 4/65 [00:47<12:02, 11.85s/it]
2022-04-08 21:49:49,035 - INFO - tqdm - accuracy: 0.4750, batch_loss: 0.7060, loss: 0.7079 ||:   8%|7         | 5/65 [00:58<11:47, 11.79s/it]
2022-04-08 21:50:00,658 - INFO - tqdm - accuracy: 0.4896, batch_loss: 0.6890, loss: 0.7048 ||:   9%|9         | 6/65 [01:10<11:32, 11.73s/it]
2022-04-08 21:50:12,357 - INFO - tqdm - accuracy: 0.4732, batch_loss: 0.7183, loss: 0.7067 ||:  11%|#         | 7/65 [01:22<11:19, 11.72s/it]
2022-04-08 21:50:24,133 - INFO - tqdm - accuracy: 0.4766, batch_loss: 0.7083, loss: 0.7069 ||:  12%|#2        | 8/65 [01:34<11:09, 11.74s/it]
2022-04-08 21:50:35,961 - INFO - tqdm - accuracy: 0.4861, batch_loss: 0.6742, loss: 0.7033 ||:  14%|#3        | 9/65 [01:45<10:58, 11.77s/it]
2022-04-08 21:50:47,733 - INFO - tqdm - accuracy: 0.4906, batch_loss: 0.6734, loss: 0.7003 ||:  15%|#5        | 10/65 [01:57<10:47, 11.77s/it]
2022-04-08 21:50:59,435 - INFO - tqdm - accuracy: 0.4972, batch_loss: 0.6918, loss: 0.6995 ||:  17%|#6        | 11/65 [02:09<10:34, 11.75s/it]
2022-04-08 21:51:11,108 - INFO - tqdm - accuracy: 0.5000, batch_loss: 0.7037, loss: 0.6998 ||:  18%|#8        | 12/65 [02:21<10:21, 11.73s/it]
2022-04-08 21:51:22,801 - INFO - tqdm - accuracy: 0.5048, batch_loss: 0.6735, loss: 0.6978 ||:  20%|##        | 13/65 [02:32<10:09, 11.72s/it]
2022-04-08 21:51:34,424 - INFO - tqdm - accuracy: 0.5022, batch_loss: 0.7124, loss: 0.6989 ||:  22%|##1       | 14/65 [02:44<09:56, 11.69s/it]
2022-04-08 21:51:46,195 - INFO - tqdm - accuracy: 0.5083, batch_loss: 0.6776, loss: 0.6974 ||:  23%|##3       | 15/65 [02:56<09:45, 11.71s/it]
2022-04-08 21:51:57,991 - INFO - tqdm - accuracy: 0.5176, batch_loss: 0.6501, loss: 0.6945 ||:  25%|##4       | 16/65 [03:07<09:35, 11.74s/it]
2022-04-08 21:52:09,759 - INFO - tqdm - accuracy: 0.5129, batch_loss: 0.6804, loss: 0.6937 ||:  26%|##6       | 17/65 [03:19<09:23, 11.75s/it]
2022-04-08 21:52:21,528 - INFO - tqdm - accuracy: 0.5174, batch_loss: 0.6540, loss: 0.6915 ||:  28%|##7       | 18/65 [03:31<09:12, 11.75s/it]
2022-04-08 21:52:33,303 - INFO - tqdm - accuracy: 0.5164, batch_loss: 0.6979, loss: 0.6918 ||:  29%|##9       | 19/65 [03:43<09:00, 11.76s/it]
2022-04-08 21:52:45,060 - INFO - tqdm - accuracy: 0.5203, batch_loss: 0.6807, loss: 0.6912 ||:  31%|###       | 20/65 [03:54<08:49, 11.76s/it]
2022-04-08 21:52:56,817 - INFO - tqdm - accuracy: 0.5223, batch_loss: 0.6825, loss: 0.6908 ||:  32%|###2      | 21/65 [04:06<08:37, 11.76s/it]
2022-04-08 21:53:08,458 - INFO - tqdm - accuracy: 0.5213, batch_loss: 0.7028, loss: 0.6914 ||:  34%|###3      | 22/65 [04:18<08:24, 11.72s/it]
2022-04-08 21:53:20,222 - INFO - tqdm - accuracy: 0.5299, batch_loss: 0.6258, loss: 0.6885 ||:  35%|###5      | 23/65 [04:30<08:12, 11.74s/it]
2022-04-08 21:53:31,983 - INFO - tqdm - accuracy: 0.5260, batch_loss: 0.7420, loss: 0.6907 ||:  37%|###6      | 24/65 [04:41<08:01, 11.74s/it]
2022-04-08 21:53:43,739 - INFO - tqdm - accuracy: 0.5262, batch_loss: 0.6770, loss: 0.6902 ||:  38%|###8      | 25/65 [04:53<07:49, 11.75s/it]
2022-04-08 21:53:55,501 - INFO - tqdm - accuracy: 0.5276, batch_loss: 0.6813, loss: 0.6899 ||:  40%|####      | 26/65 [05:05<07:38, 11.75s/it]
2022-04-08 21:54:07,252 - INFO - tqdm - accuracy: 0.5266, batch_loss: 0.6940, loss: 0.6900 ||:  42%|####1     | 27/65 [05:17<07:26, 11.75s/it]
2022-04-08 21:54:19,018 - INFO - tqdm - accuracy: 0.5279, batch_loss: 0.6565, loss: 0.6888 ||:  43%|####3     | 28/65 [05:28<07:14, 11.76s/it]
2022-04-08 21:54:30,780 - INFO - tqdm - accuracy: 0.5269, batch_loss: 0.6980, loss: 0.6891 ||:  45%|####4     | 29/65 [05:40<07:03, 11.76s/it]
2022-04-08 21:54:42,539 - INFO - tqdm - accuracy: 0.5250, batch_loss: 0.7177, loss: 0.6901 ||:  46%|####6     | 30/65 [05:52<06:51, 11.76s/it]
2022-04-08 21:54:54,303 - INFO - tqdm - accuracy: 0.5232, batch_loss: 0.6942, loss: 0.6902 ||:  48%|####7     | 31/65 [06:04<06:39, 11.76s/it]
2022-04-08 21:55:06,070 - INFO - tqdm - accuracy: 0.5195, batch_loss: 0.6712, loss: 0.6896 ||:  49%|####9     | 32/65 [06:16<06:28, 11.76s/it]
2022-04-08 21:55:17,855 - INFO - tqdm - accuracy: 0.5161, batch_loss: 0.7100, loss: 0.6902 ||:  51%|#####     | 33/65 [06:27<06:16, 11.77s/it]
2022-04-08 21:55:29,621 - INFO - tqdm - accuracy: 0.5175, batch_loss: 0.6899, loss: 0.6902 ||:  52%|#####2    | 34/65 [06:39<06:04, 11.77s/it]
2022-04-08 21:55:41,778 - INFO - tqdm - accuracy: 0.5161, batch_loss: 0.7041, loss: 0.6906 ||:  54%|#####3    | 35/65 [06:51<05:56, 11.88s/it]
2022-04-08 21:55:53,490 - INFO - tqdm - accuracy: 0.5182, batch_loss: 0.6687, loss: 0.6900 ||:  55%|#####5    | 36/65 [07:03<05:43, 11.83s/it]
2022-04-08 21:56:05,268 - INFO - tqdm - accuracy: 0.5203, batch_loss: 0.6670, loss: 0.6894 ||:  57%|#####6    | 37/65 [07:15<05:30, 11.82s/it]
2022-04-08 21:56:17,061 - INFO - tqdm - accuracy: 0.5222, batch_loss: 0.6573, loss: 0.6885 ||:  58%|#####8    | 38/65 [07:26<05:18, 11.81s/it]
2022-04-08 21:56:28,859 - INFO - tqdm - accuracy: 0.5224, batch_loss: 0.7065, loss: 0.6890 ||:  60%|######    | 39/65 [07:38<05:06, 11.81s/it]
2022-04-08 21:56:40,651 - INFO - tqdm - accuracy: 0.5227, batch_loss: 0.6993, loss: 0.6893 ||:  62%|######1   | 40/65 [07:50<04:55, 11.80s/it]
2022-04-08 21:56:52,426 - INFO - tqdm - accuracy: 0.5244, batch_loss: 0.6776, loss: 0.6890 ||:  63%|######3   | 41/65 [08:02<04:43, 11.79s/it]
2022-04-08 21:57:04,190 - INFO - tqdm - accuracy: 0.5260, batch_loss: 0.6630, loss: 0.6884 ||:  65%|######4   | 42/65 [08:14<04:31, 11.78s/it]
2022-04-08 21:57:15,946 - INFO - tqdm - accuracy: 0.5233, batch_loss: 0.7056, loss: 0.6888 ||:  66%|######6   | 43/65 [08:25<04:19, 11.78s/it]
2022-04-08 21:57:27,692 - INFO - tqdm - accuracy: 0.5241, batch_loss: 0.6982, loss: 0.6890 ||:  68%|######7   | 44/65 [08:37<04:07, 11.77s/it]
2022-04-08 21:57:39,437 - INFO - tqdm - accuracy: 0.5243, batch_loss: 0.6725, loss: 0.6886 ||:  69%|######9   | 45/65 [08:49<03:55, 11.76s/it]
2022-04-08 21:57:51,147 - INFO - tqdm - accuracy: 0.5231, batch_loss: 0.7130, loss: 0.6891 ||:  71%|#######   | 46/65 [09:01<03:43, 11.75s/it]
2022-04-08 21:58:02,866 - INFO - tqdm - accuracy: 0.5219, batch_loss: 0.7057, loss: 0.6895 ||:  72%|#######2  | 47/65 [09:12<03:31, 11.74s/it]
2022-04-08 21:58:14,577 - INFO - tqdm - accuracy: 0.5228, batch_loss: 0.6972, loss: 0.6897 ||:  74%|#######3  | 48/65 [09:24<03:19, 11.73s/it]
2022-04-08 21:58:26,300 - INFO - tqdm - accuracy: 0.5198, batch_loss: 0.7129, loss: 0.6901 ||:  75%|#######5  | 49/65 [09:36<03:07, 11.73s/it]
2022-04-08 21:58:37,988 - INFO - tqdm - accuracy: 0.5219, batch_loss: 0.6818, loss: 0.6900 ||:  77%|#######6  | 50/65 [09:47<02:55, 11.72s/it]
2022-04-08 21:58:49,745 - INFO - tqdm - accuracy: 0.5202, batch_loss: 0.7131, loss: 0.6904 ||:  78%|#######8  | 51/65 [09:59<02:44, 11.73s/it]
2022-04-08 21:59:01,513 - INFO - tqdm - accuracy: 0.5210, batch_loss: 0.7046, loss: 0.6907 ||:  80%|########  | 52/65 [10:11<02:32, 11.74s/it]
2022-04-08 21:59:13,267 - INFO - tqdm - accuracy: 0.5218, batch_loss: 0.6647, loss: 0.6902 ||:  82%|########1 | 53/65 [10:23<02:20, 11.74s/it]
2022-04-08 21:59:25,061 - INFO - tqdm - accuracy: 0.5220, batch_loss: 0.7066, loss: 0.6905 ||:  83%|########3 | 54/65 [10:34<02:09, 11.76s/it]
2022-04-08 21:59:36,835 - INFO - tqdm - accuracy: 0.5239, batch_loss: 0.6538, loss: 0.6898 ||:  85%|########4 | 55/65 [10:46<01:57, 11.76s/it]
2022-04-08 21:59:48,554 - INFO - tqdm - accuracy: 0.5234, batch_loss: 0.7121, loss: 0.6902 ||:  86%|########6 | 56/65 [10:58<01:45, 11.75s/it]
2022-04-08 21:59:59,958 - INFO - tqdm - accuracy: 0.5252, batch_loss: 0.6863, loss: 0.6902 ||:  88%|########7 | 57/65 [11:09<01:33, 11.65s/it]
2022-04-08 22:00:11,639 - INFO - tqdm - accuracy: 0.5232, batch_loss: 0.7091, loss: 0.6905 ||:  89%|########9 | 58/65 [11:21<01:21, 11.66s/it]
2022-04-08 22:00:23,366 - INFO - tqdm - accuracy: 0.5244, batch_loss: 0.6711, loss: 0.6902 ||:  91%|######### | 59/65 [11:33<01:10, 11.68s/it]
2022-04-08 22:00:35,102 - INFO - tqdm - accuracy: 0.5229, batch_loss: 0.6880, loss: 0.6901 ||:  92%|#########2| 60/65 [11:45<00:58, 11.70s/it]
2022-04-08 22:00:46,842 - INFO - tqdm - accuracy: 0.5246, batch_loss: 0.6484, loss: 0.6894 ||:  94%|#########3| 61/65 [11:56<00:46, 11.71s/it]
2022-04-08 22:00:58,600 - INFO - tqdm - accuracy: 0.5262, batch_loss: 0.6387, loss: 0.6886 ||:  95%|#########5| 62/65 [12:08<00:35, 11.72s/it]
2022-04-08 22:01:10,391 - INFO - tqdm - accuracy: 0.5283, batch_loss: 0.6429, loss: 0.6879 ||:  97%|#########6| 63/65 [12:20<00:23, 11.74s/it]
2022-04-08 22:01:21,828 - INFO - tqdm - accuracy: 0.5281, batch_loss: 0.7098, loss: 0.6882 ||:  98%|#########8| 64/65 [12:31<00:11, 11.65s/it]
2022-04-08 22:01:27,048 - INFO - tqdm - accuracy: 0.5284, batch_loss: 0.6854, loss: 0.6882 ||: 100%|##########| 65/65 [12:36<00:00,  9.72s/it]
2022-04-08 22:01:27,048 - INFO - tqdm - accuracy: 0.5284, batch_loss: 0.6854, loss: 0.6882 ||: 100%|##########| 65/65 [12:36<00:00, 11.65s/it]
2022-04-08 22:01:29,929 - INFO - allennlp.training.trainer - Validating
2022-04-08 22:01:29,931 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-08 22:01:40,153 - INFO - tqdm - accuracy: 0.5250, batch_loss: 0.9521, loss: 0.7038 ||:  35%|###4      | 40/115 [00:10<00:19,  3.84it/s]
2022-04-08 22:01:50,317 - INFO - tqdm - accuracy: 0.5316, batch_loss: 0.4659, loss: 0.7135 ||:  69%|######8   | 79/115 [00:20<00:09,  3.82it/s]
2022-04-08 22:01:59,621 - INFO - tqdm - accuracy: 0.5371, batch_loss: 0.4938, loss: 0.7077 ||: 100%|##########| 115/115 [00:29<00:00,  3.83it/s]
2022-04-08 22:01:59,621 - INFO - tqdm - accuracy: 0.5371, batch_loss: 0.4938, loss: 0.7077 ||: 100%|##########| 115/115 [00:29<00:00,  3.87it/s]
2022-04-08 22:01:59,621 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-08 22:01:59,622 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.528  |     0.537
2022-04-08 22:01:59,623 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-08 22:01:59,623 - INFO - allennlp.training.tensorboard_writer - loss               |     0.688  |     0.708
2022-04-08 22:01:59,624 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5799.516  |       N/A
2022-04-08 22:02:05,529 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to './output_newboolq_ir-ora-d/best.th'.
2022-04-08 22:02:17,796 - INFO - allennlp.training.trainer - Epoch duration: 0:13:27.729300
2022-04-08 22:02:17,796 - INFO - allennlp.training.trainer - Estimated training time remaining: 2:52:36
2022-04-08 22:02:17,796 - INFO - allennlp.training.trainer - Epoch 2/14
2022-04-08 22:02:17,796 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-08 22:02:17,796 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-08 22:02:17,798 - INFO - allennlp.training.trainer - Training
2022-04-08 22:02:17,798 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-08 22:02:29,227 - INFO - tqdm - accuracy: 0.5938, batch_loss: 0.6851, loss: 0.6851 ||:   2%|1         | 1/65 [00:11<12:11, 11.43s/it]
2022-04-08 22:02:41,109 - INFO - tqdm - accuracy: 0.6562, batch_loss: 0.5824, loss: 0.6337 ||:   3%|3         | 2/65 [00:23<12:16, 11.70s/it]
2022-04-08 22:02:53,311 - INFO - tqdm - accuracy: 0.6042, batch_loss: 0.7044, loss: 0.6573 ||:   5%|4         | 3/65 [00:35<12:19, 11.93s/it]
2022-04-08 22:03:05,380 - INFO - tqdm - accuracy: 0.5781, batch_loss: 0.6703, loss: 0.6606 ||:   6%|6         | 4/65 [00:47<12:10, 11.98s/it]
2022-04-08 22:03:17,099 - INFO - tqdm - accuracy: 0.5813, batch_loss: 0.6412, loss: 0.6567 ||:   8%|7         | 5/65 [00:59<11:53, 11.89s/it]
2022-04-08 22:03:28,519 - INFO - tqdm - accuracy: 0.5885, batch_loss: 0.6399, loss: 0.6539 ||:   9%|9         | 6/65 [01:10<11:31, 11.73s/it]
2022-04-08 22:03:40,054 - INFO - tqdm - accuracy: 0.5580, batch_loss: 0.7415, loss: 0.6664 ||:  11%|#         | 7/65 [01:22<11:16, 11.67s/it]
2022-04-08 22:03:51,761 - INFO - tqdm - accuracy: 0.5625, batch_loss: 0.7098, loss: 0.6718 ||:  12%|#2        | 8/65 [01:33<11:05, 11.68s/it]
2022-04-08 22:04:03,606 - INFO - tqdm - accuracy: 0.5694, batch_loss: 0.6041, loss: 0.6643 ||:  14%|#3        | 9/65 [01:45<10:56, 11.73s/it]
2022-04-08 22:04:15,453 - INFO - tqdm - accuracy: 0.5844, batch_loss: 0.6308, loss: 0.6609 ||:  15%|#5        | 10/65 [01:57<10:47, 11.77s/it]
2022-04-08 22:04:27,208 - INFO - tqdm - accuracy: 0.5909, batch_loss: 0.6319, loss: 0.6583 ||:  17%|#6        | 11/65 [02:09<10:35, 11.76s/it]
2022-04-08 22:04:38,893 - INFO - tqdm - accuracy: 0.5885, batch_loss: 0.7074, loss: 0.6624 ||:  18%|#8        | 12/65 [02:21<10:22, 11.74s/it]
2022-04-08 22:04:50,200 - INFO - tqdm - accuracy: 0.5807, batch_loss: 0.6994, loss: 0.6652 ||:  20%|##        | 13/65 [02:32<10:03, 11.61s/it]
2022-04-08 22:05:01,927 - INFO - tqdm - accuracy: 0.5861, batch_loss: 0.6474, loss: 0.6640 ||:  22%|##1       | 14/65 [02:44<09:53, 11.64s/it]
2022-04-08 22:05:13,701 - INFO - tqdm - accuracy: 0.5825, batch_loss: 0.6576, loss: 0.6635 ||:  23%|##3       | 15/65 [02:55<09:44, 11.68s/it]
2022-04-08 22:05:25,513 - INFO - tqdm - accuracy: 0.5930, batch_loss: 0.5753, loss: 0.6580 ||:  25%|##4       | 16/65 [03:07<09:34, 11.72s/it]
2022-04-08 22:05:37,268 - INFO - tqdm - accuracy: 0.5912, batch_loss: 0.7160, loss: 0.6614 ||:  26%|##6       | 17/65 [03:19<09:23, 11.73s/it]
2022-04-08 22:05:48,986 - INFO - tqdm - accuracy: 0.5878, batch_loss: 0.7099, loss: 0.6641 ||:  28%|##7       | 18/65 [03:31<09:11, 11.73s/it]
2022-04-08 22:06:00,671 - INFO - tqdm - accuracy: 0.5898, batch_loss: 0.6606, loss: 0.6639 ||:  29%|##9       | 19/65 [03:42<08:58, 11.72s/it]
2022-04-08 22:06:12,347 - INFO - tqdm - accuracy: 0.5931, batch_loss: 0.6286, loss: 0.6622 ||:  31%|###       | 20/65 [03:54<08:46, 11.70s/it]
2022-04-08 22:06:24,098 - INFO - tqdm - accuracy: 0.5931, batch_loss: 0.6134, loss: 0.6598 ||:  32%|###2      | 21/65 [04:06<08:35, 11.72s/it]
2022-04-08 22:06:35,909 - INFO - tqdm - accuracy: 0.5960, batch_loss: 0.5769, loss: 0.6561 ||:  34%|###3      | 22/65 [04:18<08:25, 11.75s/it]
2022-04-08 22:06:47,703 - INFO - tqdm - accuracy: 0.6041, batch_loss: 0.5796, loss: 0.6528 ||:  35%|###5      | 23/65 [04:29<08:13, 11.76s/it]
2022-04-08 22:06:59,340 - INFO - tqdm - accuracy: 0.6023, batch_loss: 0.6254, loss: 0.6516 ||:  37%|###6      | 24/65 [04:41<08:00, 11.72s/it]
2022-04-08 22:07:11,014 - INFO - tqdm - accuracy: 0.6058, batch_loss: 0.6287, loss: 0.6507 ||:  38%|###8      | 25/65 [04:53<07:48, 11.71s/it]
2022-04-08 22:07:22,700 - INFO - tqdm - accuracy: 0.6065, batch_loss: 0.6938, loss: 0.6524 ||:  40%|####      | 26/65 [05:04<07:36, 11.70s/it]
2022-04-08 22:07:34,383 - INFO - tqdm - accuracy: 0.6083, batch_loss: 0.6736, loss: 0.6531 ||:  42%|####1     | 27/65 [05:16<07:24, 11.70s/it]
2022-04-08 22:07:46,132 - INFO - tqdm - accuracy: 0.6067, batch_loss: 0.6281, loss: 0.6522 ||:  43%|####3     | 28/65 [05:28<07:13, 11.71s/it]
2022-04-08 22:07:57,890 - INFO - tqdm - accuracy: 0.6073, batch_loss: 0.6151, loss: 0.6510 ||:  45%|####4     | 29/65 [05:40<07:02, 11.73s/it]
2022-04-08 22:08:09,673 - INFO - tqdm - accuracy: 0.6100, batch_loss: 0.6148, loss: 0.6498 ||:  46%|####6     | 30/65 [05:51<06:51, 11.74s/it]
2022-04-08 22:08:21,460 - INFO - tqdm - accuracy: 0.6125, batch_loss: 0.5624, loss: 0.6469 ||:  48%|####7     | 31/65 [06:03<06:39, 11.76s/it]
2022-04-08 22:08:33,229 - INFO - tqdm - accuracy: 0.6149, batch_loss: 0.6240, loss: 0.6462 ||:  49%|####9     | 32/65 [06:15<06:28, 11.76s/it]
2022-04-08 22:08:44,982 - INFO - tqdm - accuracy: 0.6114, batch_loss: 0.7869, loss: 0.6505 ||:  51%|#####     | 33/65 [06:27<06:16, 11.76s/it]
2022-04-08 22:08:56,726 - INFO - tqdm - accuracy: 0.6118, batch_loss: 0.6417, loss: 0.6502 ||:  52%|#####2    | 34/65 [06:38<06:04, 11.75s/it]
2022-04-08 22:09:08,471 - INFO - tqdm - accuracy: 0.6157, batch_loss: 0.5544, loss: 0.6475 ||:  54%|#####3    | 35/65 [06:50<05:52, 11.75s/it]
2022-04-08 22:09:20,170 - INFO - tqdm - accuracy: 0.6142, batch_loss: 0.6367, loss: 0.6472 ||:  55%|#####5    | 36/65 [07:02<05:40, 11.74s/it]
2022-04-08 22:09:31,860 - INFO - tqdm - accuracy: 0.6137, batch_loss: 0.6258, loss: 0.6466 ||:  57%|#####6    | 37/65 [07:14<05:28, 11.72s/it]
2022-04-08 22:09:43,543 - INFO - tqdm - accuracy: 0.6165, batch_loss: 0.5303, loss: 0.6436 ||:  58%|#####8    | 38/65 [07:25<05:16, 11.71s/it]
2022-04-08 22:09:55,247 - INFO - tqdm - accuracy: 0.6183, batch_loss: 0.5465, loss: 0.6411 ||:  60%|######    | 39/65 [07:37<05:04, 11.71s/it]
2022-04-08 22:10:06,970 - INFO - tqdm - accuracy: 0.6177, batch_loss: 0.7292, loss: 0.6433 ||:  62%|######1   | 40/65 [07:49<04:52, 11.71s/it]
2022-04-08 22:10:18,673 - INFO - tqdm - accuracy: 0.6171, batch_loss: 0.6483, loss: 0.6434 ||:  63%|######3   | 41/65 [08:00<04:41, 11.71s/it]
2022-04-08 22:10:30,270 - INFO - tqdm - accuracy: 0.6173, batch_loss: 0.6190, loss: 0.6428 ||:  65%|######4   | 42/65 [08:12<04:28, 11.68s/it]
2022-04-08 22:10:41,988 - INFO - tqdm - accuracy: 0.6189, batch_loss: 0.5857, loss: 0.6415 ||:  66%|######6   | 43/65 [08:24<04:17, 11.69s/it]
2022-04-08 22:10:53,697 - INFO - tqdm - accuracy: 0.6198, batch_loss: 0.5796, loss: 0.6401 ||:  68%|######7   | 44/65 [08:35<04:05, 11.69s/it]
2022-04-08 22:11:05,453 - INFO - tqdm - accuracy: 0.6213, batch_loss: 0.6584, loss: 0.6405 ||:  69%|######9   | 45/65 [08:47<03:54, 11.71s/it]
2022-04-08 22:11:17,194 - INFO - tqdm - accuracy: 0.6213, batch_loss: 0.5850, loss: 0.6393 ||:  71%|#######   | 46/65 [08:59<03:42, 11.72s/it]
2022-04-08 22:11:28,957 - INFO - tqdm - accuracy: 0.6194, batch_loss: 0.7382, loss: 0.6414 ||:  72%|#######2  | 47/65 [09:11<03:31, 11.73s/it]
2022-04-08 22:11:40,720 - INFO - tqdm - accuracy: 0.6215, batch_loss: 0.7114, loss: 0.6428 ||:  74%|#######3  | 48/65 [09:22<03:19, 11.74s/it]
2022-04-08 22:11:52,494 - INFO - tqdm - accuracy: 0.6197, batch_loss: 0.6480, loss: 0.6429 ||:  75%|#######5  | 49/65 [09:34<03:08, 11.75s/it]
2022-04-08 22:12:04,261 - INFO - tqdm - accuracy: 0.6185, batch_loss: 0.5638, loss: 0.6414 ||:  77%|#######6  | 50/65 [09:46<02:56, 11.76s/it]
2022-04-08 22:12:16,034 - INFO - tqdm - accuracy: 0.6199, batch_loss: 0.5850, loss: 0.6403 ||:  78%|#######8  | 51/65 [09:58<02:44, 11.76s/it]
2022-04-08 22:12:27,812 - INFO - tqdm - accuracy: 0.6206, batch_loss: 0.5969, loss: 0.6394 ||:  80%|########  | 52/65 [10:10<02:32, 11.77s/it]
2022-04-08 22:12:39,590 - INFO - tqdm - accuracy: 0.6230, batch_loss: 0.6324, loss: 0.6393 ||:  82%|########1 | 53/65 [10:21<02:21, 11.77s/it]
2022-04-08 22:12:51,332 - INFO - tqdm - accuracy: 0.6225, batch_loss: 0.6349, loss: 0.6392 ||:  83%|########3 | 54/65 [10:33<02:09, 11.76s/it]
2022-04-08 22:13:03,094 - INFO - tqdm - accuracy: 0.6254, batch_loss: 0.5820, loss: 0.6382 ||:  85%|########4 | 55/65 [10:45<01:57, 11.76s/it]
2022-04-08 22:13:14,799 - INFO - tqdm - accuracy: 0.6259, batch_loss: 0.5957, loss: 0.6374 ||:  86%|########6 | 56/65 [10:57<01:45, 11.74s/it]
2022-04-08 22:13:26,524 - INFO - tqdm - accuracy: 0.6226, batch_loss: 0.7239, loss: 0.6389 ||:  88%|########7 | 57/65 [11:08<01:33, 11.74s/it]
2022-04-08 22:13:37,967 - INFO - tqdm - accuracy: 0.6243, batch_loss: 0.5356, loss: 0.6371 ||:  89%|########9 | 58/65 [11:20<01:21, 11.65s/it]
2022-04-08 22:13:49,671 - INFO - tqdm - accuracy: 0.6259, batch_loss: 0.5649, loss: 0.6359 ||:  91%|######### | 59/65 [11:31<01:09, 11.67s/it]
2022-04-08 22:14:01,357 - INFO - tqdm - accuracy: 0.6274, batch_loss: 0.5731, loss: 0.6349 ||:  92%|#########2| 60/65 [11:43<00:58, 11.67s/it]
2022-04-08 22:14:13,050 - INFO - tqdm - accuracy: 0.6294, batch_loss: 0.4953, loss: 0.6326 ||:  94%|#########3| 61/65 [11:55<00:46, 11.68s/it]
2022-04-08 22:14:24,763 - INFO - tqdm - accuracy: 0.6273, batch_loss: 0.6803, loss: 0.6334 ||:  95%|#########5| 62/65 [12:06<00:35, 11.69s/it]
2022-04-08 22:14:36,476 - INFO - tqdm - accuracy: 0.6273, batch_loss: 0.6160, loss: 0.6331 ||:  97%|#########6| 63/65 [12:18<00:23, 11.70s/it]
2022-04-08 22:14:48,213 - INFO - tqdm - accuracy: 0.6263, batch_loss: 0.6849, loss: 0.6339 ||:  98%|#########8| 64/65 [12:30<00:11, 11.71s/it]
2022-04-08 22:14:53,413 - INFO - tqdm - accuracy: 0.6264, batch_loss: 0.7279, loss: 0.6353 ||: 100%|##########| 65/65 [12:35<00:00,  9.76s/it]
2022-04-08 22:14:53,413 - INFO - tqdm - accuracy: 0.6264, batch_loss: 0.7279, loss: 0.6353 ||: 100%|##########| 65/65 [12:35<00:00, 11.62s/it]
2022-04-08 22:14:56,330 - INFO - allennlp.training.trainer - Validating
2022-04-08 22:14:56,332 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-08 22:15:06,347 - INFO - tqdm - accuracy: 0.5641, batch_loss: 0.6706, loss: 0.7816 ||:  34%|###3      | 39/115 [00:10<00:19,  3.88it/s]
2022-04-08 22:15:16,558 - INFO - tqdm - accuracy: 0.5732, batch_loss: 0.1216, loss: 0.7209 ||:  69%|######8   | 79/115 [00:20<00:09,  3.95it/s]
2022-04-08 22:15:26,027 - INFO - tqdm - accuracy: 0.5852, batch_loss: 1.3576, loss: 0.7029 ||: 100%|##########| 115/115 [00:29<00:00,  3.81it/s]
2022-04-08 22:15:26,028 - INFO - tqdm - accuracy: 0.5852, batch_loss: 1.3576, loss: 0.7029 ||: 100%|##########| 115/115 [00:29<00:00,  3.87it/s]
2022-04-08 22:15:26,028 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-08 22:15:26,028 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.626  |     0.585
2022-04-08 22:15:26,029 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-08 22:15:26,030 - INFO - allennlp.training.tensorboard_writer - loss               |     0.635  |     0.703
2022-04-08 22:15:26,030 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5799.516  |       N/A
2022-04-08 22:15:31,637 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to './output_newboolq_ir-ora-d/best.th'.
2022-04-08 22:15:42,760 - INFO - allennlp.training.trainer - Epoch duration: 0:13:24.963916
2022-04-08 22:15:42,760 - INFO - allennlp.training.trainer - Estimated training time remaining: 2:39:53
2022-04-08 22:15:42,760 - INFO - allennlp.training.trainer - Epoch 3/14
2022-04-08 22:15:42,760 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-08 22:15:42,761 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-08 22:15:42,762 - INFO - allennlp.training.trainer - Training
2022-04-08 22:15:42,762 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-08 22:15:54,172 - INFO - tqdm - accuracy: 0.5625, batch_loss: 0.7166, loss: 0.7166 ||:   2%|1         | 1/65 [00:11<12:10, 11.41s/it]
2022-04-08 22:16:05,971 - INFO - tqdm - accuracy: 0.5938, batch_loss: 0.6350, loss: 0.6758 ||:   3%|3         | 2/65 [00:23<12:13, 11.64s/it]
2022-04-08 22:16:18,121 - INFO - tqdm - accuracy: 0.6562, batch_loss: 0.5564, loss: 0.6360 ||:   5%|4         | 3/65 [00:35<12:16, 11.87s/it]
2022-04-08 22:16:30,233 - INFO - tqdm - accuracy: 0.6875, batch_loss: 0.5038, loss: 0.6029 ||:   6%|6         | 4/65 [00:47<12:09, 11.97s/it]
2022-04-08 22:16:42,394 - INFO - tqdm - accuracy: 0.6875, batch_loss: 0.5754, loss: 0.5974 ||:   8%|7         | 5/65 [00:59<12:02, 12.04s/it]
2022-04-08 22:16:53,890 - INFO - tqdm - accuracy: 0.6927, batch_loss: 0.5457, loss: 0.5888 ||:   9%|9         | 6/65 [01:11<11:39, 11.85s/it]
2022-04-08 22:17:05,388 - INFO - tqdm - accuracy: 0.7098, batch_loss: 0.4931, loss: 0.5751 ||:  11%|#         | 7/65 [01:22<11:20, 11.74s/it]
2022-04-08 22:17:16,894 - INFO - tqdm - accuracy: 0.7109, batch_loss: 0.5539, loss: 0.5725 ||:  12%|#2        | 8/65 [01:34<11:04, 11.66s/it]
2022-04-08 22:17:28,709 - INFO - tqdm - accuracy: 0.7049, batch_loss: 0.5728, loss: 0.5725 ||:  14%|#3        | 9/65 [01:45<10:55, 11.71s/it]
2022-04-08 22:17:40,572 - INFO - tqdm - accuracy: 0.7219, batch_loss: 0.4224, loss: 0.5575 ||:  15%|#5        | 10/65 [01:57<10:46, 11.76s/it]
2022-04-08 22:17:52,218 - INFO - tqdm - accuracy: 0.7216, batch_loss: 0.4312, loss: 0.5460 ||:  17%|#6        | 11/65 [02:09<10:33, 11.72s/it]
2022-04-08 22:18:03,914 - INFO - tqdm - accuracy: 0.7292, batch_loss: 0.4322, loss: 0.5365 ||:  18%|#8        | 12/65 [02:21<10:20, 11.72s/it]
2022-04-08 22:18:15,571 - INFO - tqdm - accuracy: 0.7332, batch_loss: 0.4689, loss: 0.5313 ||:  20%|##        | 13/65 [02:32<10:08, 11.70s/it]
2022-04-08 22:18:27,309 - INFO - tqdm - accuracy: 0.7210, batch_loss: 0.6666, loss: 0.5410 ||:  22%|##1       | 14/65 [02:44<09:57, 11.71s/it]
2022-04-08 22:18:39,088 - INFO - tqdm - accuracy: 0.7250, batch_loss: 0.4674, loss: 0.5361 ||:  23%|##3       | 15/65 [02:56<09:46, 11.73s/it]
2022-04-08 22:18:50,868 - INFO - tqdm - accuracy: 0.7266, batch_loss: 0.5363, loss: 0.5361 ||:  25%|##4       | 16/65 [03:08<09:35, 11.75s/it]
2022-04-08 22:19:02,623 - INFO - tqdm - accuracy: 0.7243, batch_loss: 0.5453, loss: 0.5366 ||:  26%|##6       | 17/65 [03:19<09:23, 11.75s/it]
2022-04-08 22:19:14,338 - INFO - tqdm - accuracy: 0.7326, batch_loss: 0.3752, loss: 0.5277 ||:  28%|##7       | 18/65 [03:31<09:11, 11.74s/it]
2022-04-08 22:19:26,006 - INFO - tqdm - accuracy: 0.7303, batch_loss: 0.6518, loss: 0.5342 ||:  29%|##9       | 19/65 [03:43<08:58, 11.72s/it]
2022-04-08 22:19:37,670 - INFO - tqdm - accuracy: 0.7250, batch_loss: 0.5701, loss: 0.5360 ||:  31%|###       | 20/65 [03:54<08:46, 11.70s/it]
2022-04-08 22:19:49,402 - INFO - tqdm - accuracy: 0.7277, batch_loss: 0.4318, loss: 0.5310 ||:  32%|###2      | 21/65 [04:06<08:35, 11.71s/it]
2022-04-08 22:20:01,171 - INFO - tqdm - accuracy: 0.7344, batch_loss: 0.4468, loss: 0.5272 ||:  34%|###3      | 22/65 [04:18<08:24, 11.73s/it]
2022-04-08 22:20:12,959 - INFO - tqdm - accuracy: 0.7418, batch_loss: 0.3074, loss: 0.5177 ||:  35%|###5      | 23/65 [04:30<08:13, 11.75s/it]
2022-04-08 22:20:24,710 - INFO - tqdm - accuracy: 0.7435, batch_loss: 0.4081, loss: 0.5131 ||:  37%|###6      | 24/65 [04:41<08:01, 11.75s/it]
2022-04-08 22:20:36,449 - INFO - tqdm - accuracy: 0.7500, batch_loss: 0.3333, loss: 0.5059 ||:  38%|###8      | 25/65 [04:53<07:49, 11.74s/it]
2022-04-08 22:20:48,152 - INFO - tqdm - accuracy: 0.7536, batch_loss: 0.4453, loss: 0.5036 ||:  40%|####      | 26/65 [05:05<07:37, 11.73s/it]
2022-04-08 22:20:59,836 - INFO - tqdm - accuracy: 0.7535, batch_loss: 0.5790, loss: 0.5064 ||:  42%|####1     | 27/65 [05:17<07:25, 11.72s/it]
2022-04-08 22:21:11,528 - INFO - tqdm - accuracy: 0.7567, batch_loss: 0.3663, loss: 0.5014 ||:  43%|####3     | 28/65 [05:28<07:13, 11.71s/it]
2022-04-08 22:21:23,263 - INFO - tqdm - accuracy: 0.7586, batch_loss: 0.4793, loss: 0.5006 ||:  45%|####4     | 29/65 [05:40<07:01, 11.72s/it]
2022-04-08 22:21:35,029 - INFO - tqdm - accuracy: 0.7615, batch_loss: 0.4053, loss: 0.4974 ||:  46%|####6     | 30/65 [05:52<06:50, 11.73s/it]
2022-04-08 22:21:46,829 - INFO - tqdm - accuracy: 0.7641, batch_loss: 0.4503, loss: 0.4959 ||:  48%|####7     | 31/65 [06:04<06:39, 11.75s/it]
2022-04-08 22:21:58,510 - INFO - tqdm - accuracy: 0.7637, batch_loss: 0.4332, loss: 0.4939 ||:  49%|####9     | 32/65 [06:15<06:27, 11.73s/it]
2022-04-08 22:22:10,277 - INFO - tqdm - accuracy: 0.7614, batch_loss: 0.4943, loss: 0.4940 ||:  51%|#####     | 33/65 [06:27<06:15, 11.74s/it]
2022-04-08 22:22:22,002 - INFO - tqdm - accuracy: 0.7619, batch_loss: 0.4530, loss: 0.4928 ||:  52%|#####2    | 34/65 [06:39<06:03, 11.74s/it]
2022-04-08 22:22:33,674 - INFO - tqdm - accuracy: 0.7589, batch_loss: 0.6286, loss: 0.4966 ||:  54%|#####3    | 35/65 [06:50<05:51, 11.72s/it]
2022-04-08 22:22:45,387 - INFO - tqdm - accuracy: 0.7595, batch_loss: 0.5432, loss: 0.4979 ||:  55%|#####5    | 36/65 [07:02<05:39, 11.72s/it]
2022-04-08 22:22:57,124 - INFO - tqdm - accuracy: 0.7593, batch_loss: 0.6141, loss: 0.5011 ||:  57%|#####6    | 37/65 [07:14<05:28, 11.72s/it]
2022-04-08 22:23:08,876 - INFO - tqdm - accuracy: 0.7590, batch_loss: 0.5019, loss: 0.5011 ||:  58%|#####8    | 38/65 [07:26<05:16, 11.73s/it]
2022-04-08 22:23:20,622 - INFO - tqdm - accuracy: 0.7556, batch_loss: 0.6599, loss: 0.5052 ||:  60%|######    | 39/65 [07:37<05:05, 11.74s/it]
2022-04-08 22:23:32,401 - INFO - tqdm - accuracy: 0.7539, batch_loss: 0.4886, loss: 0.5048 ||:  62%|######1   | 40/65 [07:49<04:53, 11.75s/it]
2022-04-08 22:23:44,191 - INFO - tqdm - accuracy: 0.7538, batch_loss: 0.4603, loss: 0.5037 ||:  63%|######3   | 41/65 [08:01<04:42, 11.76s/it]
2022-04-08 22:23:55,994 - INFO - tqdm - accuracy: 0.7515, batch_loss: 0.5744, loss: 0.5054 ||:  65%|######4   | 42/65 [08:13<04:30, 11.77s/it]
2022-04-08 22:24:07,804 - INFO - tqdm - accuracy: 0.7529, batch_loss: 0.4868, loss: 0.5049 ||:  66%|######6   | 43/65 [08:25<04:19, 11.78s/it]
2022-04-08 22:24:19,583 - INFO - tqdm - accuracy: 0.7536, batch_loss: 0.5135, loss: 0.5051 ||:  68%|######7   | 44/65 [08:36<04:07, 11.78s/it]
2022-04-08 22:24:31,371 - INFO - tqdm - accuracy: 0.7521, batch_loss: 0.7228, loss: 0.5100 ||:  69%|######9   | 45/65 [08:48<03:55, 11.78s/it]
2022-04-08 22:24:43,126 - INFO - tqdm - accuracy: 0.7520, batch_loss: 0.4928, loss: 0.5096 ||:  71%|#######   | 46/65 [09:00<03:43, 11.78s/it]
2022-04-08 22:24:54,856 - INFO - tqdm - accuracy: 0.7527, batch_loss: 0.4434, loss: 0.5082 ||:  72%|#######2  | 47/65 [09:12<03:31, 11.76s/it]
2022-04-08 22:25:06,590 - INFO - tqdm - accuracy: 0.7546, batch_loss: 0.3949, loss: 0.5058 ||:  74%|#######3  | 48/65 [09:23<03:19, 11.75s/it]
2022-04-08 22:25:18,316 - INFO - tqdm - accuracy: 0.7532, batch_loss: 0.5464, loss: 0.5066 ||:  75%|#######5  | 49/65 [09:35<03:07, 11.75s/it]
2022-04-08 22:25:29,989 - INFO - tqdm - accuracy: 0.7519, batch_loss: 0.6324, loss: 0.5092 ||:  77%|#######6  | 50/65 [09:47<02:55, 11.72s/it]
2022-04-08 22:25:41,654 - INFO - tqdm - accuracy: 0.7500, batch_loss: 0.5846, loss: 0.5106 ||:  78%|#######8  | 51/65 [09:58<02:43, 11.71s/it]
2022-04-08 22:25:53,118 - INFO - tqdm - accuracy: 0.7488, batch_loss: 0.5172, loss: 0.5108 ||:  80%|########  | 52/65 [10:10<02:31, 11.63s/it]
2022-04-08 22:26:04,881 - INFO - tqdm - accuracy: 0.7476, batch_loss: 0.5688, loss: 0.5119 ||:  82%|########1 | 53/65 [10:22<02:20, 11.67s/it]
2022-04-08 22:26:16,670 - INFO - tqdm - accuracy: 0.7459, batch_loss: 0.6400, loss: 0.5142 ||:  83%|########3 | 54/65 [10:33<02:08, 11.71s/it]
2022-04-08 22:26:28,434 - INFO - tqdm - accuracy: 0.7483, batch_loss: 0.5010, loss: 0.5140 ||:  85%|########4 | 55/65 [10:45<01:57, 11.72s/it]
2022-04-08 22:26:40,185 - INFO - tqdm - accuracy: 0.7467, batch_loss: 0.5576, loss: 0.5148 ||:  86%|########6 | 56/65 [10:57<01:45, 11.73s/it]
2022-04-08 22:26:51,941 - INFO - tqdm - accuracy: 0.7445, batch_loss: 0.6051, loss: 0.5164 ||:  88%|########7 | 57/65 [11:09<01:33, 11.74s/it]
2022-04-08 22:27:03,659 - INFO - tqdm - accuracy: 0.7452, batch_loss: 0.4306, loss: 0.5149 ||:  89%|########9 | 58/65 [11:20<01:22, 11.73s/it]
2022-04-08 22:27:15,025 - INFO - tqdm - accuracy: 0.7430, batch_loss: 0.6012, loss: 0.5163 ||:  91%|######### | 59/65 [11:32<01:09, 11.62s/it]
2022-04-08 22:27:26,738 - INFO - tqdm - accuracy: 0.7436, batch_loss: 0.4931, loss: 0.5160 ||:  92%|#########2| 60/65 [11:43<00:58, 11.65s/it]
2022-04-08 22:27:38,442 - INFO - tqdm - accuracy: 0.7437, batch_loss: 0.4996, loss: 0.5157 ||:  94%|#########3| 61/65 [11:55<00:46, 11.67s/it]
2022-04-08 22:27:50,131 - INFO - tqdm - accuracy: 0.7443, batch_loss: 0.5084, loss: 0.5156 ||:  95%|#########5| 62/65 [12:07<00:35, 11.67s/it]
2022-04-08 22:28:01,800 - INFO - tqdm - accuracy: 0.7464, batch_loss: 0.5113, loss: 0.5155 ||:  97%|#########6| 63/65 [12:19<00:23, 11.67s/it]
2022-04-08 22:28:13,496 - INFO - tqdm - accuracy: 0.7474, batch_loss: 0.4678, loss: 0.5148 ||:  98%|#########8| 64/65 [12:30<00:11, 11.68s/it]
2022-04-08 22:28:18,698 - INFO - tqdm - accuracy: 0.7472, batch_loss: 0.5570, loss: 0.5154 ||: 100%|##########| 65/65 [12:35<00:00,  9.74s/it]
2022-04-08 22:28:18,699 - INFO - tqdm - accuracy: 0.7472, batch_loss: 0.5570, loss: 0.5154 ||: 100%|##########| 65/65 [12:35<00:00, 11.63s/it]
2022-04-08 22:28:21,532 - INFO - allennlp.training.trainer - Validating
2022-04-08 22:28:21,534 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-08 22:28:31,604 - INFO - tqdm - accuracy: 0.6026, batch_loss: 0.6127, loss: 0.8670 ||:  34%|###3      | 39/115 [00:10<00:19,  3.84it/s]
2022-04-08 22:28:41,682 - INFO - tqdm - accuracy: 0.6667, batch_loss: 0.8810, loss: 0.7452 ||:  68%|######7   | 78/115 [00:20<00:09,  3.82it/s]
2022-04-08 22:28:51,247 - INFO - tqdm - accuracy: 0.6550, batch_loss: 0.3316, loss: 0.7590 ||: 100%|##########| 115/115 [00:29<00:00,  3.88it/s]
2022-04-08 22:28:51,247 - INFO - tqdm - accuracy: 0.6550, batch_loss: 0.3316, loss: 0.7590 ||: 100%|##########| 115/115 [00:29<00:00,  3.87it/s]
2022-04-08 22:28:51,247 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-08 22:28:51,248 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.747  |     0.655
2022-04-08 22:28:51,248 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-08 22:28:51,249 - INFO - allennlp.training.tensorboard_writer - loss               |     0.515  |     0.759
2022-04-08 22:28:51,249 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5799.516  |       N/A
2022-04-08 22:28:56,883 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to './output_newboolq_ir-ora-d/best.th'.
2022-04-08 22:29:08,335 - INFO - allennlp.training.trainer - Epoch duration: 0:13:25.574753
2022-04-08 22:29:08,335 - INFO - allennlp.training.trainer - Estimated training time remaining: 2:26:50
2022-04-08 22:29:08,335 - INFO - allennlp.training.trainer - Epoch 4/14
2022-04-08 22:29:08,336 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-08 22:29:08,336 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-08 22:29:08,337 - INFO - allennlp.training.trainer - Training
2022-04-08 22:29:08,337 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-08 22:29:19,735 - INFO - tqdm - accuracy: 0.8750, batch_loss: 0.3966, loss: 0.3966 ||:   2%|1         | 1/65 [00:11<12:09, 11.40s/it]
2022-04-08 22:29:31,566 - INFO - tqdm - accuracy: 0.8906, batch_loss: 0.3287, loss: 0.3627 ||:   3%|3         | 2/65 [00:23<12:14, 11.65s/it]
2022-04-08 22:29:43,743 - INFO - tqdm - accuracy: 0.8854, batch_loss: 0.3574, loss: 0.3609 ||:   5%|4         | 3/65 [00:35<12:17, 11.89s/it]
2022-04-08 22:29:55,874 - INFO - tqdm - accuracy: 0.8672, batch_loss: 0.3952, loss: 0.3695 ||:   6%|6         | 4/65 [00:47<12:11, 11.99s/it]
2022-04-08 22:30:07,625 - INFO - tqdm - accuracy: 0.8375, batch_loss: 0.4936, loss: 0.3943 ||:   8%|7         | 5/65 [00:59<11:54, 11.90s/it]
2022-04-08 22:30:19,164 - INFO - tqdm - accuracy: 0.8438, batch_loss: 0.2836, loss: 0.3759 ||:   9%|9         | 6/65 [01:10<11:34, 11.78s/it]
2022-04-08 22:30:30,708 - INFO - tqdm - accuracy: 0.8527, batch_loss: 0.2802, loss: 0.3622 ||:  11%|#         | 7/65 [01:22<11:18, 11.70s/it]
2022-04-08 22:30:42,356 - INFO - tqdm - accuracy: 0.8594, batch_loss: 0.2627, loss: 0.3498 ||:  12%|#2        | 8/65 [01:34<11:06, 11.68s/it]
2022-04-08 22:30:54,160 - INFO - tqdm - accuracy: 0.8646, batch_loss: 0.2738, loss: 0.3413 ||:  14%|#3        | 9/65 [01:45<10:56, 11.72s/it]
2022-04-08 22:31:05,963 - INFO - tqdm - accuracy: 0.8656, batch_loss: 0.2306, loss: 0.3303 ||:  15%|#5        | 10/65 [01:57<10:46, 11.75s/it]
2022-04-08 22:31:17,715 - INFO - tqdm - accuracy: 0.8665, batch_loss: 0.2537, loss: 0.3233 ||:  17%|#6        | 11/65 [02:09<10:34, 11.75s/it]
2022-04-08 22:31:29,417 - INFO - tqdm - accuracy: 0.8568, batch_loss: 0.4616, loss: 0.3348 ||:  18%|#8        | 12/65 [02:21<10:21, 11.73s/it]
2022-04-08 22:31:41,181 - INFO - tqdm - accuracy: 0.8534, batch_loss: 0.4395, loss: 0.3429 ||:  20%|##        | 13/65 [02:32<10:10, 11.74s/it]
2022-04-08 22:31:52,945 - INFO - tqdm - accuracy: 0.8527, batch_loss: 0.4110, loss: 0.3477 ||:  22%|##1       | 14/65 [02:44<09:59, 11.75s/it]
2022-04-08 22:32:04,697 - INFO - tqdm - accuracy: 0.8521, batch_loss: 0.3382, loss: 0.3471 ||:  23%|##3       | 15/65 [02:56<09:47, 11.75s/it]
2022-04-08 22:32:16,455 - INFO - tqdm - accuracy: 0.8496, batch_loss: 0.2716, loss: 0.3424 ||:  25%|##4       | 16/65 [03:08<09:35, 11.75s/it]
2022-04-08 22:32:28,209 - INFO - tqdm - accuracy: 0.8511, batch_loss: 0.3884, loss: 0.3451 ||:  26%|##6       | 17/65 [03:19<09:24, 11.75s/it]
2022-04-08 22:32:39,965 - INFO - tqdm - accuracy: 0.8524, batch_loss: 0.3255, loss: 0.3440 ||:  28%|##7       | 18/65 [03:31<09:12, 11.75s/it]
2022-04-08 22:32:51,723 - INFO - tqdm - accuracy: 0.8503, batch_loss: 0.3226, loss: 0.3429 ||:  29%|##9       | 19/65 [03:43<09:00, 11.76s/it]
2022-04-08 22:33:03,482 - INFO - tqdm - accuracy: 0.8453, batch_loss: 0.4628, loss: 0.3489 ||:  31%|###       | 20/65 [03:55<08:49, 11.76s/it]
2022-04-08 22:33:15,229 - INFO - tqdm - accuracy: 0.8467, batch_loss: 0.3181, loss: 0.3474 ||:  32%|###2      | 21/65 [04:06<08:37, 11.75s/it]
2022-04-08 22:33:27,002 - INFO - tqdm - accuracy: 0.8466, batch_loss: 0.3210, loss: 0.3462 ||:  34%|###3      | 22/65 [04:18<08:25, 11.76s/it]
2022-04-08 22:33:38,758 - INFO - tqdm - accuracy: 0.8397, batch_loss: 0.5599, loss: 0.3555 ||:  35%|###5      | 23/65 [04:30<08:13, 11.76s/it]
2022-04-08 22:33:50,526 - INFO - tqdm - accuracy: 0.8372, batch_loss: 0.3178, loss: 0.3539 ||:  37%|###6      | 24/65 [04:42<08:02, 11.76s/it]
2022-04-08 22:34:02,303 - INFO - tqdm - accuracy: 0.8300, batch_loss: 0.6433, loss: 0.3655 ||:  38%|###8      | 25/65 [04:53<07:50, 11.77s/it]
2022-04-08 22:34:14,068 - INFO - tqdm - accuracy: 0.8329, batch_loss: 0.2705, loss: 0.3619 ||:  40%|####      | 26/65 [05:05<07:38, 11.77s/it]
2022-04-08 22:34:25,846 - INFO - tqdm - accuracy: 0.8380, batch_loss: 0.1949, loss: 0.3557 ||:  42%|####1     | 27/65 [05:17<07:27, 11.77s/it]
2022-04-08 22:34:37,620 - INFO - tqdm - accuracy: 0.8426, batch_loss: 0.1828, loss: 0.3495 ||:  43%|####3     | 28/65 [05:29<07:15, 11.77s/it]
2022-04-08 22:34:49,396 - INFO - tqdm - accuracy: 0.8438, batch_loss: 0.2794, loss: 0.3471 ||:  45%|####4     | 29/65 [05:41<07:03, 11.77s/it]
2022-04-08 22:35:01,036 - INFO - tqdm - accuracy: 0.8427, batch_loss: 0.3130, loss: 0.3459 ||:  46%|####6     | 30/65 [05:52<06:50, 11.73s/it]
2022-04-08 22:35:12,828 - INFO - tqdm - accuracy: 0.8417, batch_loss: 0.3486, loss: 0.3460 ||:  48%|####7     | 31/65 [06:04<06:39, 11.75s/it]
2022-04-08 22:35:24,592 - INFO - tqdm - accuracy: 0.8428, batch_loss: 0.2505, loss: 0.3430 ||:  49%|####9     | 32/65 [06:16<06:27, 11.75s/it]
2022-04-08 22:35:36,373 - INFO - tqdm - accuracy: 0.8447, batch_loss: 0.3082, loss: 0.3420 ||:  51%|#####     | 33/65 [06:28<06:16, 11.76s/it]
2022-04-08 22:35:48,128 - INFO - tqdm - accuracy: 0.8456, batch_loss: 0.3363, loss: 0.3418 ||:  52%|#####2    | 34/65 [06:39<06:04, 11.76s/it]
2022-04-08 22:35:59,898 - INFO - tqdm - accuracy: 0.8464, batch_loss: 0.2944, loss: 0.3405 ||:  54%|#####3    | 35/65 [06:51<05:52, 11.76s/it]
2022-04-08 22:36:11,682 - INFO - tqdm - accuracy: 0.8464, batch_loss: 0.3594, loss: 0.3410 ||:  55%|#####5    | 36/65 [07:03<05:41, 11.77s/it]
2022-04-08 22:36:23,108 - INFO - tqdm - accuracy: 0.8428, batch_loss: 0.6068, loss: 0.3482 ||:  57%|#####6    | 37/65 [07:14<05:26, 11.67s/it]
2022-04-08 22:36:34,878 - INFO - tqdm - accuracy: 0.8461, batch_loss: 0.1122, loss: 0.3420 ||:  58%|#####8    | 38/65 [07:26<05:15, 11.70s/it]
2022-04-08 22:36:46,660 - INFO - tqdm - accuracy: 0.8476, batch_loss: 0.2732, loss: 0.3402 ||:  60%|######    | 39/65 [07:38<05:04, 11.72s/it]
2022-04-08 22:36:58,821 - INFO - tqdm - accuracy: 0.8483, batch_loss: 0.2848, loss: 0.3388 ||:  62%|######1   | 40/65 [07:50<04:56, 11.85s/it]
2022-04-08 22:37:10,549 - INFO - tqdm - accuracy: 0.8490, batch_loss: 0.2943, loss: 0.3377 ||:  63%|######3   | 41/65 [08:02<04:43, 11.82s/it]
2022-04-08 22:37:22,319 - INFO - tqdm - accuracy: 0.8496, batch_loss: 0.3326, loss: 0.3376 ||:  65%|######4   | 42/65 [08:13<04:31, 11.80s/it]
2022-04-08 22:37:34,090 - INFO - tqdm - accuracy: 0.8495, batch_loss: 0.3146, loss: 0.3371 ||:  66%|######6   | 43/65 [08:25<04:19, 11.79s/it]
2022-04-08 22:37:45,850 - INFO - tqdm - accuracy: 0.8500, batch_loss: 0.2336, loss: 0.3347 ||:  68%|######7   | 44/65 [08:37<04:07, 11.78s/it]
2022-04-08 22:37:57,480 - INFO - tqdm - accuracy: 0.8492, batch_loss: 0.3355, loss: 0.3347 ||:  69%|######9   | 45/65 [08:49<03:54, 11.74s/it]
2022-04-08 22:38:09,241 - INFO - tqdm - accuracy: 0.8470, batch_loss: 0.3667, loss: 0.3354 ||:  71%|#######   | 46/65 [09:00<03:43, 11.74s/it]
2022-04-08 22:38:21,010 - INFO - tqdm - accuracy: 0.8476, batch_loss: 0.3698, loss: 0.3362 ||:  72%|#######2  | 47/65 [09:12<03:31, 11.75s/it]
2022-04-08 22:38:32,780 - INFO - tqdm - accuracy: 0.8456, batch_loss: 0.4142, loss: 0.3378 ||:  74%|#######3  | 48/65 [09:24<03:19, 11.76s/it]
2022-04-08 22:38:44,531 - INFO - tqdm - accuracy: 0.8449, batch_loss: 0.4736, loss: 0.3406 ||:  75%|#######5  | 49/65 [09:36<03:08, 11.76s/it]
2022-04-08 22:38:56,289 - INFO - tqdm - accuracy: 0.8455, batch_loss: 0.2539, loss: 0.3388 ||:  77%|#######6  | 50/65 [09:47<02:56, 11.76s/it]
2022-04-08 22:39:08,045 - INFO - tqdm - accuracy: 0.8461, batch_loss: 0.2578, loss: 0.3372 ||:  78%|#######8  | 51/65 [09:59<02:44, 11.76s/it]
2022-04-08 22:39:19,812 - INFO - tqdm - accuracy: 0.8473, batch_loss: 0.2719, loss: 0.3360 ||:  80%|########  | 52/65 [10:11<02:32, 11.76s/it]
2022-04-08 22:39:31,553 - INFO - tqdm - accuracy: 0.8472, batch_loss: 0.3531, loss: 0.3363 ||:  82%|########1 | 53/65 [10:23<02:21, 11.75s/it]
2022-04-08 22:39:43,311 - INFO - tqdm - accuracy: 0.8477, batch_loss: 0.3137, loss: 0.3359 ||:  83%|########3 | 54/65 [10:34<02:09, 11.76s/it]
2022-04-08 22:39:55,062 - INFO - tqdm - accuracy: 0.8482, batch_loss: 0.3056, loss: 0.3353 ||:  85%|########4 | 55/65 [10:46<01:57, 11.75s/it]
2022-04-08 22:40:06,803 - INFO - tqdm - accuracy: 0.8481, batch_loss: 0.3283, loss: 0.3352 ||:  86%|########6 | 56/65 [10:58<01:45, 11.75s/it]
2022-04-08 22:40:18,559 - INFO - tqdm - accuracy: 0.8459, batch_loss: 0.4807, loss: 0.3378 ||:  88%|########7 | 57/65 [11:10<01:34, 11.75s/it]
2022-04-08 22:40:30,307 - INFO - tqdm - accuracy: 0.8469, batch_loss: 0.2247, loss: 0.3358 ||:  89%|########9 | 58/65 [11:21<01:22, 11.75s/it]
2022-04-08 22:40:42,049 - INFO - tqdm - accuracy: 0.8490, batch_loss: 0.1890, loss: 0.3333 ||:  91%|######### | 59/65 [11:33<01:10, 11.75s/it]
2022-04-08 22:40:53,776 - INFO - tqdm - accuracy: 0.8489, batch_loss: 0.3341, loss: 0.3333 ||:  92%|#########2| 60/65 [11:45<00:58, 11.74s/it]
2022-04-08 22:41:05,494 - INFO - tqdm - accuracy: 0.8473, batch_loss: 0.3965, loss: 0.3344 ||:  94%|#########3| 61/65 [11:57<00:46, 11.73s/it]
2022-04-08 22:41:16,964 - INFO - tqdm - accuracy: 0.8482, batch_loss: 0.3047, loss: 0.3339 ||:  95%|#########5| 62/65 [12:08<00:34, 11.66s/it]
2022-04-08 22:41:28,685 - INFO - tqdm - accuracy: 0.8486, batch_loss: 0.2588, loss: 0.3327 ||:  97%|#########6| 63/65 [12:20<00:23, 11.67s/it]
2022-04-08 22:41:40,403 - INFO - tqdm - accuracy: 0.8481, batch_loss: 0.3517, loss: 0.3330 ||:  98%|#########8| 64/65 [12:32<00:11, 11.69s/it]
2022-04-08 22:41:45,599 - INFO - tqdm - accuracy: 0.8481, batch_loss: 0.3586, loss: 0.3334 ||: 100%|##########| 65/65 [12:37<00:00,  9.74s/it]
2022-04-08 22:41:45,600 - INFO - tqdm - accuracy: 0.8481, batch_loss: 0.3586, loss: 0.3334 ||: 100%|##########| 65/65 [12:37<00:00, 11.65s/it]
2022-04-08 22:41:48,459 - INFO - allennlp.training.trainer - Validating
2022-04-08 22:41:48,460 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-08 22:41:58,472 - INFO - tqdm - accuracy: 0.6154, batch_loss: 1.2157, loss: 1.2096 ||:  34%|###3      | 39/115 [00:10<00:19,  3.86it/s]
2022-04-08 22:42:08,598 - INFO - tqdm - accuracy: 0.6218, batch_loss: 0.8466, loss: 1.1852 ||:  68%|######7   | 78/115 [00:20<00:09,  3.84it/s]
2022-04-08 22:42:18,038 - INFO - tqdm - accuracy: 0.6157, batch_loss: 0.0024, loss: 1.1555 ||: 100%|##########| 115/115 [00:29<00:00,  3.83it/s]
2022-04-08 22:42:18,038 - INFO - tqdm - accuracy: 0.6157, batch_loss: 0.0024, loss: 1.1555 ||: 100%|##########| 115/115 [00:29<00:00,  3.89it/s]
2022-04-08 22:42:18,038 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-08 22:42:18,039 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.848  |     0.616
2022-04-08 22:42:18,040 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-08 22:42:18,040 - INFO - allennlp.training.tensorboard_writer - loss               |     0.333  |     1.156
2022-04-08 22:42:18,040 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5799.516  |       N/A
2022-04-08 22:42:24,168 - INFO - allennlp.training.trainer - Epoch duration: 0:13:15.832405
2022-04-08 22:42:24,168 - INFO - allennlp.training.trainer - Estimated training time remaining: 2:13:19
2022-04-08 22:42:24,168 - INFO - allennlp.training.trainer - Epoch 5/14
2022-04-08 22:42:24,168 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-08 22:42:24,168 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-08 22:42:24,170 - INFO - allennlp.training.trainer - Training
2022-04-08 22:42:24,170 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-08 22:42:35,678 - INFO - tqdm - accuracy: 0.9062, batch_loss: 0.2210, loss: 0.2210 ||:   2%|1         | 1/65 [00:11<12:16, 11.51s/it]
2022-04-08 22:42:47,607 - INFO - tqdm - accuracy: 0.9531, batch_loss: 0.1701, loss: 0.1955 ||:   3%|3         | 2/65 [00:23<12:20, 11.76s/it]
2022-04-08 22:42:59,577 - INFO - tqdm - accuracy: 0.9583, batch_loss: 0.2005, loss: 0.1972 ||:   5%|4         | 3/65 [00:35<12:14, 11.85s/it]
2022-04-08 22:43:11,343 - INFO - tqdm - accuracy: 0.9531, batch_loss: 0.1720, loss: 0.1909 ||:   6%|6         | 4/65 [00:47<12:00, 11.82s/it]
2022-04-08 22:43:23,023 - INFO - tqdm - accuracy: 0.9500, batch_loss: 0.1415, loss: 0.1810 ||:   8%|7         | 5/65 [00:58<11:46, 11.77s/it]
2022-04-08 22:43:34,679 - INFO - tqdm - accuracy: 0.9583, batch_loss: 0.0871, loss: 0.1654 ||:   9%|9         | 6/65 [01:10<11:32, 11.73s/it]
2022-04-08 22:43:46,257 - INFO - tqdm - accuracy: 0.9464, batch_loss: 0.2378, loss: 0.1757 ||:  11%|#         | 7/65 [01:22<11:17, 11.68s/it]
2022-04-08 22:43:58,000 - INFO - tqdm - accuracy: 0.9492, batch_loss: 0.0839, loss: 0.1642 ||:  12%|#2        | 8/65 [01:33<11:06, 11.70s/it]
2022-04-08 22:44:09,787 - INFO - tqdm - accuracy: 0.9549, batch_loss: 0.0689, loss: 0.1536 ||:  14%|#3        | 9/65 [01:45<10:56, 11.73s/it]
2022-04-08 22:44:21,570 - INFO - tqdm - accuracy: 0.9531, batch_loss: 0.1582, loss: 0.1541 ||:  15%|#5        | 10/65 [01:57<10:45, 11.74s/it]
2022-04-08 22:44:33,352 - INFO - tqdm - accuracy: 0.9545, batch_loss: 0.1220, loss: 0.1512 ||:  17%|#6        | 11/65 [02:09<10:34, 11.76s/it]
2022-04-08 22:44:45,115 - INFO - tqdm - accuracy: 0.9557, batch_loss: 0.1228, loss: 0.1488 ||:  18%|#8        | 12/65 [02:20<10:23, 11.76s/it]
2022-04-08 22:44:56,876 - INFO - tqdm - accuracy: 0.9519, batch_loss: 0.2308, loss: 0.1551 ||:  20%|##        | 13/65 [02:32<10:11, 11.76s/it]
2022-04-08 22:45:08,645 - INFO - tqdm - accuracy: 0.9487, batch_loss: 0.2196, loss: 0.1597 ||:  22%|##1       | 14/65 [02:44<09:59, 11.76s/it]
2022-04-08 22:45:20,405 - INFO - tqdm - accuracy: 0.9500, batch_loss: 0.0964, loss: 0.1555 ||:  23%|##3       | 15/65 [02:56<09:48, 11.76s/it]
2022-04-08 22:45:31,797 - INFO - tqdm - accuracy: 0.9472, batch_loss: 0.1132, loss: 0.1529 ||:  25%|##4       | 16/65 [03:07<09:30, 11.65s/it]
2022-04-08 22:45:43,520 - INFO - tqdm - accuracy: 0.9392, batch_loss: 0.2999, loss: 0.1615 ||:  26%|##6       | 17/65 [03:19<09:20, 11.67s/it]
2022-04-08 22:45:55,226 - INFO - tqdm - accuracy: 0.9409, batch_loss: 0.1579, loss: 0.1613 ||:  28%|##7       | 18/65 [03:31<09:09, 11.68s/it]
2022-04-08 22:46:06,902 - INFO - tqdm - accuracy: 0.9423, batch_loss: 0.0912, loss: 0.1576 ||:  29%|##9       | 19/65 [03:42<08:57, 11.68s/it]
2022-04-08 22:46:18,608 - INFO - tqdm - accuracy: 0.9437, batch_loss: 0.0943, loss: 0.1544 ||:  31%|###       | 20/65 [03:54<08:45, 11.69s/it]
2022-04-08 22:46:30,343 - INFO - tqdm - accuracy: 0.9419, batch_loss: 0.1406, loss: 0.1538 ||:  32%|###2      | 21/65 [04:06<08:34, 11.70s/it]
2022-04-08 22:46:42,137 - INFO - tqdm - accuracy: 0.9403, batch_loss: 0.2901, loss: 0.1600 ||:  34%|###3      | 22/65 [04:17<08:24, 11.73s/it]
2022-04-08 22:46:53,939 - INFO - tqdm - accuracy: 0.9388, batch_loss: 0.3383, loss: 0.1677 ||:  35%|###5      | 23/65 [04:29<08:13, 11.75s/it]
2022-04-08 22:47:05,691 - INFO - tqdm - accuracy: 0.9387, batch_loss: 0.1432, loss: 0.1667 ||:  37%|###6      | 24/65 [04:41<08:01, 11.75s/it]
2022-04-08 22:47:17,413 - INFO - tqdm - accuracy: 0.9362, batch_loss: 0.2272, loss: 0.1691 ||:  38%|###8      | 25/65 [04:53<07:49, 11.74s/it]
2022-04-08 22:47:29,111 - INFO - tqdm - accuracy: 0.9362, batch_loss: 0.0965, loss: 0.1663 ||:  40%|####      | 26/65 [05:04<07:37, 11.73s/it]
2022-04-08 22:47:40,804 - INFO - tqdm - accuracy: 0.9374, batch_loss: 0.1147, loss: 0.1644 ||:  42%|####1     | 27/65 [05:16<07:25, 11.72s/it]
2022-04-08 22:47:52,463 - INFO - tqdm - accuracy: 0.9385, batch_loss: 0.0543, loss: 0.1605 ||:  43%|####3     | 28/65 [05:28<07:12, 11.70s/it]
2022-04-08 22:48:04,143 - INFO - tqdm - accuracy: 0.9364, batch_loss: 0.2548, loss: 0.1638 ||:  45%|####4     | 29/65 [05:39<07:01, 11.69s/it]
2022-04-08 22:48:15,823 - INFO - tqdm - accuracy: 0.9333, batch_loss: 0.3747, loss: 0.1708 ||:  46%|####6     | 30/65 [05:51<06:49, 11.69s/it]
2022-04-08 22:48:27,546 - INFO - tqdm - accuracy: 0.9354, batch_loss: 0.0505, loss: 0.1669 ||:  48%|####7     | 31/65 [06:03<06:37, 11.70s/it]
2022-04-08 22:48:39,311 - INFO - tqdm - accuracy: 0.9335, batch_loss: 0.1868, loss: 0.1675 ||:  49%|####9     | 32/65 [06:15<06:26, 11.72s/it]
2022-04-08 22:48:51,106 - INFO - tqdm - accuracy: 0.9318, batch_loss: 0.3034, loss: 0.1716 ||:  51%|#####     | 33/65 [06:26<06:15, 11.74s/it]
2022-04-08 22:49:02,920 - INFO - tqdm - accuracy: 0.9301, batch_loss: 0.3829, loss: 0.1779 ||:  52%|#####2    | 34/65 [06:38<06:04, 11.76s/it]
2022-04-08 22:49:14,677 - INFO - tqdm - accuracy: 0.9303, batch_loss: 0.1622, loss: 0.1774 ||:  54%|#####3    | 35/65 [06:50<05:52, 11.76s/it]
2022-04-08 22:49:26,437 - INFO - tqdm - accuracy: 0.9314, batch_loss: 0.0594, loss: 0.1741 ||:  55%|#####5    | 36/65 [07:02<05:41, 11.76s/it]
2022-04-08 22:49:38,151 - INFO - tqdm - accuracy: 0.9298, batch_loss: 0.3276, loss: 0.1783 ||:  57%|#####6    | 37/65 [07:13<05:28, 11.75s/it]
2022-04-08 22:49:49,845 - INFO - tqdm - accuracy: 0.9309, batch_loss: 0.1632, loss: 0.1779 ||:  58%|#####8    | 38/65 [07:25<05:16, 11.73s/it]
2022-04-08 22:50:01,520 - INFO - tqdm - accuracy: 0.9310, batch_loss: 0.1513, loss: 0.1772 ||:  60%|######    | 39/65 [07:37<05:04, 11.71s/it]
2022-04-08 22:50:13,201 - INFO - tqdm - accuracy: 0.9296, batch_loss: 0.1858, loss: 0.1774 ||:  62%|######1   | 40/65 [07:49<04:52, 11.70s/it]
2022-04-08 22:50:24,912 - INFO - tqdm - accuracy: 0.9298, batch_loss: 0.1344, loss: 0.1764 ||:  63%|######3   | 41/65 [08:00<04:40, 11.71s/it]
2022-04-08 22:50:36,656 - INFO - tqdm - accuracy: 0.9293, batch_loss: 0.2322, loss: 0.1777 ||:  65%|######4   | 42/65 [08:12<04:29, 11.72s/it]
2022-04-08 22:50:48,417 - INFO - tqdm - accuracy: 0.9273, batch_loss: 0.3339, loss: 0.1813 ||:  66%|######6   | 43/65 [08:24<04:18, 11.73s/it]
2022-04-08 22:51:00,205 - INFO - tqdm - accuracy: 0.9261, batch_loss: 0.2289, loss: 0.1824 ||:  68%|######7   | 44/65 [08:36<04:06, 11.75s/it]
2022-04-08 22:51:11,727 - INFO - tqdm - accuracy: 0.9270, batch_loss: 0.1261, loss: 0.1812 ||:  69%|######9   | 45/65 [08:47<03:53, 11.68s/it]
2022-04-08 22:51:23,501 - INFO - tqdm - accuracy: 0.9259, batch_loss: 0.2251, loss: 0.1821 ||:  71%|#######   | 46/65 [08:59<03:42, 11.71s/it]
2022-04-08 22:51:35,262 - INFO - tqdm - accuracy: 0.9261, batch_loss: 0.1436, loss: 0.1813 ||:  72%|#######2  | 47/65 [09:11<03:31, 11.72s/it]
2022-04-08 22:51:47,023 - INFO - tqdm - accuracy: 0.9264, batch_loss: 0.1733, loss: 0.1811 ||:  74%|#######3  | 48/65 [09:22<03:19, 11.74s/it]
2022-04-08 22:51:58,745 - INFO - tqdm - accuracy: 0.9279, batch_loss: 0.0620, loss: 0.1787 ||:  75%|#######5  | 49/65 [09:34<03:07, 11.73s/it]
2022-04-08 22:52:10,460 - INFO - tqdm - accuracy: 0.9256, batch_loss: 0.3796, loss: 0.1827 ||:  77%|#######6  | 50/65 [09:46<02:55, 11.73s/it]
2022-04-08 22:52:22,145 - INFO - tqdm - accuracy: 0.9264, batch_loss: 0.1069, loss: 0.1812 ||:  78%|#######8  | 51/65 [09:57<02:43, 11.71s/it]
2022-04-08 22:52:33,830 - INFO - tqdm - accuracy: 0.9272, batch_loss: 0.1138, loss: 0.1799 ||:  80%|########  | 52/65 [10:09<02:32, 11.71s/it]
2022-04-08 22:52:45,518 - INFO - tqdm - accuracy: 0.9268, batch_loss: 0.2021, loss: 0.1803 ||:  82%|########1 | 53/65 [10:21<02:20, 11.70s/it]
2022-04-08 22:52:57,218 - INFO - tqdm - accuracy: 0.9276, batch_loss: 0.1274, loss: 0.1794 ||:  83%|########3 | 54/65 [10:33<02:08, 11.70s/it]
2022-04-08 22:53:08,936 - INFO - tqdm - accuracy: 0.9267, batch_loss: 0.2929, loss: 0.1814 ||:  85%|########4 | 55/65 [10:44<01:57, 11.71s/it]
2022-04-08 22:53:20,655 - INFO - tqdm - accuracy: 0.9269, batch_loss: 0.1682, loss: 0.1812 ||:  86%|########6 | 56/65 [10:56<01:45, 11.71s/it]
2022-04-08 22:53:32,358 - INFO - tqdm - accuracy: 0.9270, batch_loss: 0.1300, loss: 0.1803 ||:  88%|########7 | 57/65 [11:08<01:33, 11.71s/it]
2022-04-08 22:53:44,063 - INFO - tqdm - accuracy: 0.9278, batch_loss: 0.1031, loss: 0.1790 ||:  89%|########9 | 58/65 [11:19<01:21, 11.71s/it]
2022-04-08 22:53:55,638 - INFO - tqdm - accuracy: 0.9274, batch_loss: 0.1715, loss: 0.1788 ||:  91%|######### | 59/65 [11:31<01:10, 11.67s/it]
2022-04-08 22:54:07,339 - INFO - tqdm - accuracy: 0.9270, batch_loss: 0.2191, loss: 0.1795 ||:  92%|#########2| 60/65 [11:43<00:58, 11.68s/it]
2022-04-08 22:54:19,007 - INFO - tqdm - accuracy: 0.9277, batch_loss: 0.0780, loss: 0.1778 ||:  94%|#########3| 61/65 [11:54<00:46, 11.67s/it]
2022-04-08 22:54:30,697 - INFO - tqdm - accuracy: 0.9279, batch_loss: 0.1289, loss: 0.1771 ||:  95%|#########5| 62/65 [12:06<00:35, 11.68s/it]
2022-04-08 22:54:42,426 - INFO - tqdm - accuracy: 0.9290, batch_loss: 0.0973, loss: 0.1758 ||:  97%|#########6| 63/65 [12:18<00:23, 11.69s/it]
2022-04-08 22:54:54,204 - INFO - tqdm - accuracy: 0.9272, batch_loss: 0.2712, loss: 0.1773 ||:  98%|#########8| 64/65 [12:30<00:11, 11.72s/it]
2022-04-08 22:54:59,435 - INFO - tqdm - accuracy: 0.9272, batch_loss: 0.2498, loss: 0.1784 ||: 100%|##########| 65/65 [12:35<00:00,  9.77s/it]
2022-04-08 22:54:59,435 - INFO - tqdm - accuracy: 0.9272, batch_loss: 0.2498, loss: 0.1784 ||: 100%|##########| 65/65 [12:35<00:00, 11.62s/it]
2022-04-08 22:55:02,430 - INFO - allennlp.training.trainer - Validating
2022-04-08 22:55:02,431 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-08 22:55:12,488 - INFO - tqdm - accuracy: 0.5769, batch_loss: 0.0035, loss: 1.7623 ||:  34%|###3      | 39/115 [00:10<00:19,  3.87it/s]
2022-04-08 22:55:22,665 - INFO - tqdm - accuracy: 0.5987, batch_loss: 2.9362, loss: 1.6369 ||:  69%|######8   | 79/115 [00:20<00:08,  4.14it/s]
2022-04-08 22:55:32,022 - INFO - tqdm - accuracy: 0.6245, batch_loss: 0.0146, loss: 1.5390 ||: 100%|##########| 115/115 [00:29<00:00,  3.86it/s]
2022-04-08 22:55:32,022 - INFO - tqdm - accuracy: 0.6245, batch_loss: 0.0146, loss: 1.5390 ||: 100%|##########| 115/115 [00:29<00:00,  3.89it/s]
2022-04-08 22:55:32,023 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-08 22:55:32,023 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.927  |     0.624
2022-04-08 22:55:32,024 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-08 22:55:32,025 - INFO - allennlp.training.tensorboard_writer - loss               |     0.178  |     1.539
2022-04-08 22:55:32,025 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5799.516  |       N/A
2022-04-08 22:55:38,096 - INFO - allennlp.training.trainer - Epoch duration: 0:13:13.927596
2022-04-08 22:55:38,096 - INFO - allennlp.training.trainer - Estimated training time remaining: 1:59:50
2022-04-08 22:55:38,096 - INFO - allennlp.training.trainer - Epoch 6/14
2022-04-08 22:55:38,096 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-08 22:55:38,097 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-08 22:55:38,098 - INFO - allennlp.training.trainer - Training
2022-04-08 22:55:38,098 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-08 22:55:49,660 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.0478, loss: 0.0478 ||:   2%|1         | 1/65 [00:11<12:19, 11.56s/it]
2022-04-08 22:56:01,594 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.1797, loss: 0.1138 ||:   3%|3         | 2/65 [00:23<12:22, 11.78s/it]
2022-04-08 22:56:13,621 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.1105, loss: 0.1127 ||:   5%|4         | 3/65 [00:35<12:17, 11.89s/it]
2022-04-08 22:56:25,390 - INFO - tqdm - accuracy: 0.9609, batch_loss: 0.1199, loss: 0.1145 ||:   6%|6         | 4/65 [00:47<12:02, 11.84s/it]
2022-04-08 22:56:37,045 - INFO - tqdm - accuracy: 0.9625, batch_loss: 0.0903, loss: 0.1096 ||:   8%|7         | 5/65 [00:58<11:46, 11.78s/it]
2022-04-08 22:56:48,612 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.0472, loss: 0.0992 ||:   9%|9         | 6/65 [01:10<11:30, 11.70s/it]
2022-04-08 22:57:00,216 - INFO - tqdm - accuracy: 0.9554, batch_loss: 0.2343, loss: 0.1185 ||:  11%|#         | 7/65 [01:22<11:16, 11.67s/it]
2022-04-08 22:57:11,947 - INFO - tqdm - accuracy: 0.9609, batch_loss: 0.0249, loss: 0.1068 ||:  12%|#2        | 8/65 [01:33<11:06, 11.69s/it]
2022-04-08 22:57:23,777 - INFO - tqdm - accuracy: 0.9653, batch_loss: 0.0740, loss: 0.1032 ||:  14%|#3        | 9/65 [01:45<10:57, 11.73s/it]
2022-04-08 22:57:35,997 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.0398, loss: 0.0969 ||:  15%|#5        | 10/65 [01:57<10:53, 11.88s/it]
2022-04-08 22:57:47,700 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.1059, loss: 0.0977 ||:  17%|#6        | 11/65 [02:09<10:38, 11.83s/it]
2022-04-08 22:57:59,371 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.0855, loss: 0.0967 ||:  18%|#8        | 12/65 [02:21<10:24, 11.78s/it]
2022-04-08 22:58:11,023 - INFO - tqdm - accuracy: 0.9712, batch_loss: 0.0461, loss: 0.0928 ||:  20%|##        | 13/65 [02:32<10:10, 11.74s/it]
2022-04-08 22:58:22,707 - INFO - tqdm - accuracy: 0.9643, batch_loss: 0.2103, loss: 0.1012 ||:  22%|##1       | 14/65 [02:44<09:57, 11.72s/it]
2022-04-08 22:58:34,451 - INFO - tqdm - accuracy: 0.9646, batch_loss: 0.1178, loss: 0.1023 ||:  23%|##3       | 15/65 [02:56<09:46, 11.73s/it]
2022-04-08 22:58:46,199 - INFO - tqdm - accuracy: 0.9629, batch_loss: 0.1279, loss: 0.1039 ||:  25%|##4       | 16/65 [03:08<09:35, 11.74s/it]
2022-04-08 22:58:57,964 - INFO - tqdm - accuracy: 0.9632, batch_loss: 0.1036, loss: 0.1039 ||:  26%|##6       | 17/65 [03:19<09:23, 11.74s/it]
2022-04-08 22:59:09,730 - INFO - tqdm - accuracy: 0.9601, batch_loss: 0.1372, loss: 0.1057 ||:  28%|##7       | 18/65 [03:31<09:12, 11.75s/it]
2022-04-08 22:59:21,516 - INFO - tqdm - accuracy: 0.9589, batch_loss: 0.1091, loss: 0.1059 ||:  29%|##9       | 19/65 [03:43<09:01, 11.76s/it]
2022-04-08 22:59:33,286 - INFO - tqdm - accuracy: 0.9578, batch_loss: 0.0782, loss: 0.1045 ||:  31%|###       | 20/65 [03:55<08:49, 11.76s/it]
2022-04-08 22:59:45,047 - INFO - tqdm - accuracy: 0.9568, batch_loss: 0.0774, loss: 0.1032 ||:  32%|###2      | 21/65 [04:06<08:37, 11.76s/it]
2022-04-08 22:59:56,804 - INFO - tqdm - accuracy: 0.9560, batch_loss: 0.1614, loss: 0.1059 ||:  34%|###3      | 22/65 [04:18<08:25, 11.76s/it]
2022-04-08 23:00:08,580 - INFO - tqdm - accuracy: 0.9565, batch_loss: 0.0708, loss: 0.1043 ||:  35%|###5      | 23/65 [04:30<08:14, 11.77s/it]
2022-04-08 23:00:20,356 - INFO - tqdm - accuracy: 0.9557, batch_loss: 0.1687, loss: 0.1070 ||:  37%|###6      | 24/65 [04:42<08:02, 11.77s/it]
2022-04-08 23:00:32,136 - INFO - tqdm - accuracy: 0.9537, batch_loss: 0.1720, loss: 0.1096 ||:  38%|###8      | 25/65 [04:54<07:50, 11.77s/it]
2022-04-08 23:00:43,904 - INFO - tqdm - accuracy: 0.9531, batch_loss: 0.1185, loss: 0.1100 ||:  40%|####      | 26/65 [05:05<07:39, 11.77s/it]
2022-04-08 23:00:55,647 - INFO - tqdm - accuracy: 0.9549, batch_loss: 0.0183, loss: 0.1066 ||:  42%|####1     | 27/65 [05:17<07:26, 11.76s/it]
2022-04-08 23:01:07,339 - INFO - tqdm - accuracy: 0.9542, batch_loss: 0.0948, loss: 0.1061 ||:  43%|####3     | 28/65 [05:29<07:14, 11.74s/it]
2022-04-08 23:01:19,044 - INFO - tqdm - accuracy: 0.9558, batch_loss: 0.0321, loss: 0.1036 ||:  45%|####4     | 29/65 [05:40<07:02, 11.73s/it]
2022-04-08 23:01:30,631 - INFO - tqdm - accuracy: 0.9573, batch_loss: 0.0479, loss: 0.1017 ||:  46%|####6     | 30/65 [05:52<06:49, 11.69s/it]
2022-04-08 23:01:42,383 - INFO - tqdm - accuracy: 0.9567, batch_loss: 0.1239, loss: 0.1024 ||:  48%|####7     | 31/65 [06:04<06:38, 11.71s/it]
2022-04-08 23:01:54,157 - INFO - tqdm - accuracy: 0.9580, batch_loss: 0.0110, loss: 0.0996 ||:  49%|####9     | 32/65 [06:16<06:26, 11.73s/it]
2022-04-08 23:02:05,912 - INFO - tqdm - accuracy: 0.9564, batch_loss: 0.1051, loss: 0.0998 ||:  51%|#####     | 33/65 [06:27<06:15, 11.74s/it]
2022-04-08 23:02:17,698 - INFO - tqdm - accuracy: 0.9577, batch_loss: 0.0603, loss: 0.0986 ||:  52%|#####2    | 34/65 [06:39<06:04, 11.75s/it]
2022-04-08 23:02:29,476 - INFO - tqdm - accuracy: 0.9580, batch_loss: 0.1255, loss: 0.0994 ||:  54%|#####3    | 35/65 [06:51<05:52, 11.76s/it]
2022-04-08 23:02:41,285 - INFO - tqdm - accuracy: 0.9592, batch_loss: 0.0302, loss: 0.0974 ||:  55%|#####5    | 36/65 [07:03<05:41, 11.77s/it]
2022-04-08 23:02:53,070 - INFO - tqdm - accuracy: 0.9603, batch_loss: 0.0659, loss: 0.0966 ||:  57%|#####6    | 37/65 [07:14<05:29, 11.78s/it]
2022-04-08 23:03:04,889 - INFO - tqdm - accuracy: 0.9613, batch_loss: 0.0607, loss: 0.0956 ||:  58%|#####8    | 38/65 [07:26<05:18, 11.79s/it]
2022-04-08 23:03:16,689 - INFO - tqdm - accuracy: 0.9615, batch_loss: 0.0978, loss: 0.0957 ||:  60%|######    | 39/65 [07:38<05:06, 11.79s/it]
2022-04-08 23:03:28,493 - INFO - tqdm - accuracy: 0.9625, batch_loss: 0.0542, loss: 0.0947 ||:  62%|######1   | 40/65 [07:50<04:54, 11.80s/it]
2022-04-08 23:03:39,933 - INFO - tqdm - accuracy: 0.9626, batch_loss: 0.0837, loss: 0.0944 ||:  63%|######3   | 41/65 [08:01<04:40, 11.69s/it]
2022-04-08 23:03:51,684 - INFO - tqdm - accuracy: 0.9635, batch_loss: 0.0320, loss: 0.0929 ||:  65%|######4   | 42/65 [08:13<04:29, 11.71s/it]
2022-04-08 23:04:03,193 - INFO - tqdm - accuracy: 0.9629, batch_loss: 0.1492, loss: 0.0942 ||:  66%|######6   | 43/65 [08:25<04:16, 11.65s/it]
2022-04-08 23:04:14,954 - INFO - tqdm - accuracy: 0.9638, batch_loss: 0.0261, loss: 0.0927 ||:  68%|######7   | 44/65 [08:36<04:05, 11.68s/it]
2022-04-08 23:04:26,725 - INFO - tqdm - accuracy: 0.9632, batch_loss: 0.1386, loss: 0.0937 ||:  69%|######9   | 45/65 [08:48<03:54, 11.71s/it]
2022-04-08 23:04:38,473 - INFO - tqdm - accuracy: 0.9633, batch_loss: 0.1653, loss: 0.0952 ||:  71%|#######   | 46/65 [09:00<03:42, 11.72s/it]
2022-04-08 23:04:50,203 - INFO - tqdm - accuracy: 0.9627, batch_loss: 0.0990, loss: 0.0953 ||:  72%|#######2  | 47/65 [09:12<03:31, 11.72s/it]
2022-04-08 23:05:01,961 - INFO - tqdm - accuracy: 0.9635, batch_loss: 0.0092, loss: 0.0935 ||:  74%|#######3  | 48/65 [09:23<03:19, 11.73s/it]
2022-04-08 23:05:13,694 - INFO - tqdm - accuracy: 0.9643, batch_loss: 0.0178, loss: 0.0920 ||:  75%|#######5  | 49/65 [09:35<03:07, 11.73s/it]
2022-04-08 23:05:25,419 - INFO - tqdm - accuracy: 0.9650, batch_loss: 0.0135, loss: 0.0904 ||:  77%|#######6  | 50/65 [09:47<02:55, 11.73s/it]
2022-04-08 23:05:37,183 - INFO - tqdm - accuracy: 0.9644, batch_loss: 0.0871, loss: 0.0904 ||:  78%|#######8  | 51/65 [09:59<02:44, 11.74s/it]
2022-04-08 23:05:48,800 - INFO - tqdm - accuracy: 0.9651, batch_loss: 0.0331, loss: 0.0893 ||:  80%|########  | 52/65 [10:10<02:32, 11.70s/it]
2022-04-08 23:06:00,551 - INFO - tqdm - accuracy: 0.9658, batch_loss: 0.0094, loss: 0.0877 ||:  82%|########1 | 53/65 [10:22<02:20, 11.72s/it]
2022-04-08 23:06:12,302 - INFO - tqdm - accuracy: 0.9658, batch_loss: 0.0681, loss: 0.0874 ||:  83%|########3 | 54/65 [10:34<02:09, 11.73s/it]
2022-04-08 23:06:24,046 - INFO - tqdm - accuracy: 0.9665, batch_loss: 0.0339, loss: 0.0864 ||:  85%|########4 | 55/65 [10:45<01:57, 11.73s/it]
2022-04-08 23:06:35,687 - INFO - tqdm - accuracy: 0.9671, batch_loss: 0.0071, loss: 0.0850 ||:  86%|########6 | 56/65 [10:57<01:45, 11.71s/it]
2022-04-08 23:06:47,452 - INFO - tqdm - accuracy: 0.9654, batch_loss: 0.3024, loss: 0.0888 ||:  88%|########7 | 57/65 [11:09<01:33, 11.72s/it]
2022-04-08 23:06:59,226 - INFO - tqdm - accuracy: 0.9650, batch_loss: 0.0557, loss: 0.0882 ||:  89%|########9 | 58/65 [11:21<01:22, 11.74s/it]
2022-04-08 23:07:10,998 - INFO - tqdm - accuracy: 0.9656, batch_loss: 0.0048, loss: 0.0868 ||:  91%|######### | 59/65 [11:32<01:10, 11.75s/it]
2022-04-08 23:07:22,781 - INFO - tqdm - accuracy: 0.9656, batch_loss: 0.0448, loss: 0.0861 ||:  92%|#########2| 60/65 [11:44<00:58, 11.76s/it]
2022-04-08 23:07:34,576 - INFO - tqdm - accuracy: 0.9651, batch_loss: 0.0899, loss: 0.0862 ||:  94%|#########3| 61/65 [11:56<00:47, 11.77s/it]
2022-04-08 23:07:46,378 - INFO - tqdm - accuracy: 0.9642, batch_loss: 0.1680, loss: 0.0875 ||:  95%|#########5| 62/65 [12:08<00:35, 11.78s/it]
2022-04-08 23:07:58,167 - INFO - tqdm - accuracy: 0.9648, batch_loss: 0.0442, loss: 0.0868 ||:  97%|#########6| 63/65 [12:20<00:23, 11.78s/it]
2022-04-08 23:08:09,942 - INFO - tqdm - accuracy: 0.9653, batch_loss: 0.0303, loss: 0.0859 ||:  98%|#########8| 64/65 [12:31<00:11, 11.78s/it]
2022-04-08 23:08:15,152 - INFO - tqdm - accuracy: 0.9656, batch_loss: 0.0040, loss: 0.0847 ||: 100%|##########| 65/65 [12:37<00:00,  9.81s/it]
2022-04-08 23:08:15,153 - INFO - tqdm - accuracy: 0.9656, batch_loss: 0.0040, loss: 0.0847 ||: 100%|##########| 65/65 [12:37<00:00, 11.65s/it]
2022-04-08 23:08:18,027 - INFO - allennlp.training.trainer - Validating
2022-04-08 23:08:18,029 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-08 23:08:28,222 - INFO - tqdm - accuracy: 0.6329, batch_loss: 3.6308, loss: 1.7256 ||:  35%|###4      | 40/115 [00:10<00:19,  3.86it/s]
2022-04-08 23:08:38,407 - INFO - tqdm - accuracy: 0.6306, batch_loss: 0.0063, loss: 1.7867 ||:  69%|######8   | 79/115 [00:20<00:09,  3.81it/s]
2022-04-08 23:08:47,757 - INFO - tqdm - accuracy: 0.6245, batch_loss: 1.8441, loss: 1.8532 ||: 100%|##########| 115/115 [00:29<00:00,  3.81it/s]
2022-04-08 23:08:47,757 - INFO - tqdm - accuracy: 0.6245, batch_loss: 1.8441, loss: 1.8532 ||: 100%|##########| 115/115 [00:29<00:00,  3.87it/s]
2022-04-08 23:08:47,757 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-08 23:08:47,758 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.966  |     0.624
2022-04-08 23:08:47,759 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-08 23:08:47,759 - INFO - allennlp.training.tensorboard_writer - loss               |     0.085  |     1.853
2022-04-08 23:08:47,760 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5799.516  |       N/A
2022-04-08 23:08:53,657 - INFO - allennlp.training.trainer - Epoch duration: 0:13:15.560661
2022-04-08 23:08:53,657 - INFO - allennlp.training.trainer - Estimated training time remaining: 1:46:27
2022-04-08 23:08:53,657 - INFO - allennlp.training.trainer - Epoch 7/14
2022-04-08 23:08:53,657 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-08 23:08:53,658 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-08 23:08:53,659 - INFO - allennlp.training.trainer - Training
2022-04-08 23:08:53,659 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-08 23:09:05,263 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0342, loss: 0.0342 ||:   2%|1         | 1/65 [00:11<12:22, 11.60s/it]
2022-04-08 23:09:17,195 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0723, loss: 0.0533 ||:   3%|3         | 2/65 [00:23<12:23, 11.80s/it]
2022-04-08 23:09:29,261 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.0150, loss: 0.0405 ||:   5%|4         | 3/65 [00:35<12:19, 11.92s/it]
2022-04-08 23:09:41,012 - INFO - tqdm - accuracy: 0.9922, batch_loss: 0.0155, loss: 0.0343 ||:   6%|6         | 4/65 [00:47<12:03, 11.85s/it]
2022-04-08 23:09:52,667 - INFO - tqdm - accuracy: 0.9938, batch_loss: 0.0389, loss: 0.0352 ||:   8%|7         | 5/65 [00:59<11:46, 11.78s/it]
2022-04-08 23:10:04,295 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.1735, loss: 0.0582 ||:   9%|9         | 6/65 [01:10<11:32, 11.73s/it]
2022-04-08 23:10:16,019 - INFO - tqdm - accuracy: 0.9866, batch_loss: 0.0131, loss: 0.0518 ||:  11%|#         | 7/65 [01:22<11:20, 11.73s/it]
2022-04-08 23:10:27,848 - INFO - tqdm - accuracy: 0.9883, batch_loss: 0.0098, loss: 0.0465 ||:  12%|#2        | 8/65 [01:34<11:10, 11.76s/it]
2022-04-08 23:10:39,709 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.0089, loss: 0.0424 ||:  14%|#3        | 9/65 [01:46<11:00, 11.79s/it]
2022-04-08 23:10:51,506 - INFO - tqdm - accuracy: 0.9875, batch_loss: 0.0937, loss: 0.0475 ||:  15%|#5        | 10/65 [01:57<10:48, 11.79s/it]
2022-04-08 23:11:03,238 - INFO - tqdm - accuracy: 0.9886, batch_loss: 0.0112, loss: 0.0442 ||:  17%|#6        | 11/65 [02:09<10:35, 11.77s/it]
2022-04-08 23:11:14,925 - INFO - tqdm - accuracy: 0.9870, batch_loss: 0.0344, loss: 0.0434 ||:  18%|#8        | 12/65 [02:21<10:22, 11.75s/it]
2022-04-08 23:11:26,646 - INFO - tqdm - accuracy: 0.9856, batch_loss: 0.0539, loss: 0.0442 ||:  20%|##        | 13/65 [02:32<10:10, 11.74s/it]
2022-04-08 23:11:38,412 - INFO - tqdm - accuracy: 0.9821, batch_loss: 0.2038, loss: 0.0556 ||:  22%|##1       | 14/65 [02:44<09:59, 11.75s/it]
2022-04-08 23:11:50,074 - INFO - tqdm - accuracy: 0.9833, batch_loss: 0.0166, loss: 0.0530 ||:  23%|##3       | 15/65 [02:56<09:46, 11.72s/it]
2022-04-08 23:12:01,897 - INFO - tqdm - accuracy: 0.9824, batch_loss: 0.0723, loss: 0.0542 ||:  25%|##4       | 16/65 [03:08<09:35, 11.75s/it]
2022-04-08 23:12:13,528 - INFO - tqdm - accuracy: 0.9835, batch_loss: 0.0273, loss: 0.0526 ||:  26%|##6       | 17/65 [03:19<09:22, 11.72s/it]
2022-04-08 23:12:25,264 - INFO - tqdm - accuracy: 0.9826, batch_loss: 0.1053, loss: 0.0555 ||:  28%|##7       | 18/65 [03:31<09:10, 11.72s/it]
2022-04-08 23:12:36,945 - INFO - tqdm - accuracy: 0.9836, batch_loss: 0.0364, loss: 0.0545 ||:  29%|##9       | 19/65 [03:43<08:58, 11.71s/it]
2022-04-08 23:12:48,525 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0252, loss: 0.0531 ||:  31%|###       | 20/65 [03:54<08:45, 11.67s/it]
2022-04-08 23:13:00,243 - INFO - tqdm - accuracy: 0.9836, batch_loss: 0.0519, loss: 0.0530 ||:  32%|###2      | 21/65 [04:06<08:34, 11.68s/it]
2022-04-08 23:13:12,006 - INFO - tqdm - accuracy: 0.9830, batch_loss: 0.0326, loss: 0.0521 ||:  34%|###3      | 22/65 [04:18<08:23, 11.71s/it]
2022-04-08 23:13:23,769 - INFO - tqdm - accuracy: 0.9810, batch_loss: 0.0903, loss: 0.0537 ||:  35%|###5      | 23/65 [04:30<08:12, 11.72s/it]
2022-04-08 23:13:35,557 - INFO - tqdm - accuracy: 0.9818, batch_loss: 0.0130, loss: 0.0520 ||:  37%|###6      | 24/65 [04:41<08:01, 11.74s/it]
2022-04-08 23:13:47,342 - INFO - tqdm - accuracy: 0.9812, batch_loss: 0.0422, loss: 0.0517 ||:  38%|###8      | 25/65 [04:53<07:50, 11.76s/it]
2022-04-08 23:13:59,131 - INFO - tqdm - accuracy: 0.9820, batch_loss: 0.0170, loss: 0.0503 ||:  40%|####      | 26/65 [05:05<07:38, 11.77s/it]
2022-04-08 23:14:10,907 - INFO - tqdm - accuracy: 0.9815, batch_loss: 0.0909, loss: 0.0518 ||:  42%|####1     | 27/65 [05:17<07:27, 11.77s/it]
2022-04-08 23:14:22,668 - INFO - tqdm - accuracy: 0.9821, batch_loss: 0.0109, loss: 0.0504 ||:  43%|####3     | 28/65 [05:29<07:15, 11.77s/it]
2022-04-08 23:14:34,430 - INFO - tqdm - accuracy: 0.9828, batch_loss: 0.0306, loss: 0.0497 ||:  45%|####4     | 29/65 [05:40<07:03, 11.77s/it]
2022-04-08 23:14:46,163 - INFO - tqdm - accuracy: 0.9833, batch_loss: 0.0302, loss: 0.0490 ||:  46%|####6     | 30/65 [05:52<06:51, 11.76s/it]
2022-04-08 23:14:57,957 - INFO - tqdm - accuracy: 0.9829, batch_loss: 0.0560, loss: 0.0493 ||:  48%|####7     | 31/65 [06:04<06:40, 11.77s/it]
2022-04-08 23:15:09,772 - INFO - tqdm - accuracy: 0.9834, batch_loss: 0.0340, loss: 0.0488 ||:  49%|####9     | 32/65 [06:16<06:28, 11.78s/it]
2022-04-08 23:15:21,561 - INFO - tqdm - accuracy: 0.9839, batch_loss: 0.0050, loss: 0.0475 ||:  51%|#####     | 33/65 [06:27<06:17, 11.78s/it]
2022-04-08 23:15:33,313 - INFO - tqdm - accuracy: 0.9835, batch_loss: 0.0478, loss: 0.0475 ||:  52%|#####2    | 34/65 [06:39<06:04, 11.77s/it]
2022-04-08 23:15:45,037 - INFO - tqdm - accuracy: 0.9830, batch_loss: 0.0921, loss: 0.0487 ||:  54%|#####3    | 35/65 [06:51<05:52, 11.76s/it]
2022-04-08 23:15:56,730 - INFO - tqdm - accuracy: 0.9835, batch_loss: 0.0210, loss: 0.0480 ||:  55%|#####5    | 36/65 [07:03<05:40, 11.74s/it]
2022-04-08 23:16:08,469 - INFO - tqdm - accuracy: 0.9831, batch_loss: 0.1476, loss: 0.0507 ||:  57%|#####6    | 37/65 [07:14<05:28, 11.74s/it]
2022-04-08 23:16:20,199 - INFO - tqdm - accuracy: 0.9827, batch_loss: 0.0410, loss: 0.0504 ||:  58%|#####8    | 38/65 [07:26<05:16, 11.74s/it]
2022-04-08 23:16:31,962 - INFO - tqdm - accuracy: 0.9824, batch_loss: 0.0607, loss: 0.0507 ||:  60%|######    | 39/65 [07:38<05:05, 11.74s/it]
2022-04-08 23:16:43,723 - INFO - tqdm - accuracy: 0.9820, batch_loss: 0.0927, loss: 0.0517 ||:  62%|######1   | 40/65 [07:50<04:53, 11.75s/it]
2022-04-08 23:16:55,493 - INFO - tqdm - accuracy: 0.9825, batch_loss: 0.0360, loss: 0.0513 ||:  63%|######3   | 41/65 [08:01<04:42, 11.76s/it]
2022-04-08 23:17:07,267 - INFO - tqdm - accuracy: 0.9829, batch_loss: 0.0165, loss: 0.0505 ||:  65%|######4   | 42/65 [08:13<04:30, 11.76s/it]
2022-04-08 23:17:19,035 - INFO - tqdm - accuracy: 0.9826, batch_loss: 0.0438, loss: 0.0503 ||:  66%|######6   | 43/65 [08:25<04:18, 11.76s/it]
2022-04-08 23:17:30,822 - INFO - tqdm - accuracy: 0.9830, batch_loss: 0.0219, loss: 0.0497 ||:  68%|######7   | 44/65 [08:37<04:07, 11.77s/it]
2022-04-08 23:17:43,022 - INFO - tqdm - accuracy: 0.9833, batch_loss: 0.0025, loss: 0.0487 ||:  69%|######9   | 45/65 [08:49<03:57, 11.90s/it]
2022-04-08 23:17:54,775 - INFO - tqdm - accuracy: 0.9837, batch_loss: 0.0429, loss: 0.0485 ||:  71%|#######   | 46/65 [09:01<03:45, 11.86s/it]
2022-04-08 23:18:06,562 - INFO - tqdm - accuracy: 0.9827, batch_loss: 0.1071, loss: 0.0498 ||:  72%|#######2  | 47/65 [09:12<03:33, 11.83s/it]
2022-04-08 23:18:18,341 - INFO - tqdm - accuracy: 0.9824, batch_loss: 0.0688, loss: 0.0502 ||:  74%|#######3  | 48/65 [09:24<03:20, 11.82s/it]
2022-04-08 23:18:30,142 - INFO - tqdm - accuracy: 0.9821, batch_loss: 0.0431, loss: 0.0500 ||:  75%|#######5  | 49/65 [09:36<03:09, 11.81s/it]
2022-04-08 23:18:41,931 - INFO - tqdm - accuracy: 0.9825, batch_loss: 0.0222, loss: 0.0495 ||:  77%|#######6  | 50/65 [09:48<02:57, 11.81s/it]
2022-04-08 23:18:53,727 - INFO - tqdm - accuracy: 0.9828, batch_loss: 0.0115, loss: 0.0487 ||:  78%|#######8  | 51/65 [10:00<02:45, 11.80s/it]
2022-04-08 23:19:05,531 - INFO - tqdm - accuracy: 0.9832, batch_loss: 0.0383, loss: 0.0485 ||:  80%|########  | 52/65 [10:11<02:33, 11.80s/it]
2022-04-08 23:19:17,347 - INFO - tqdm - accuracy: 0.9829, batch_loss: 0.0716, loss: 0.0490 ||:  82%|########1 | 53/65 [10:23<02:21, 11.81s/it]
2022-04-08 23:19:29,115 - INFO - tqdm - accuracy: 0.9826, batch_loss: 0.0916, loss: 0.0498 ||:  83%|########3 | 54/65 [10:35<02:09, 11.80s/it]
2022-04-08 23:19:40,880 - INFO - tqdm - accuracy: 0.9824, batch_loss: 0.1430, loss: 0.0514 ||:  85%|########4 | 55/65 [10:47<01:57, 11.79s/it]
2022-04-08 23:19:52,644 - INFO - tqdm - accuracy: 0.9827, batch_loss: 0.0350, loss: 0.0512 ||:  86%|########6 | 56/65 [10:58<01:46, 11.78s/it]
2022-04-08 23:20:04,408 - INFO - tqdm - accuracy: 0.9830, batch_loss: 0.0249, loss: 0.0507 ||:  88%|########7 | 57/65 [11:10<01:34, 11.77s/it]
2022-04-08 23:20:16,163 - INFO - tqdm - accuracy: 0.9828, batch_loss: 0.1194, loss: 0.0519 ||:  89%|########9 | 58/65 [11:22<01:22, 11.77s/it]
2022-04-08 23:20:27,265 - INFO - tqdm - accuracy: 0.9820, batch_loss: 0.0956, loss: 0.0526 ||:  91%|######### | 59/65 [11:33<01:09, 11.57s/it]
2022-04-08 23:20:39,007 - INFO - tqdm - accuracy: 0.9823, batch_loss: 0.0059, loss: 0.0518 ||:  92%|#########2| 60/65 [11:45<00:58, 11.62s/it]
2022-04-08 23:20:50,749 - INFO - tqdm - accuracy: 0.9810, batch_loss: 0.1429, loss: 0.0533 ||:  94%|#########3| 61/65 [11:57<00:46, 11.66s/it]
2022-04-08 23:21:02,481 - INFO - tqdm - accuracy: 0.9808, batch_loss: 0.1214, loss: 0.0544 ||:  95%|#########5| 62/65 [12:08<00:35, 11.68s/it]
2022-04-08 23:21:14,208 - INFO - tqdm - accuracy: 0.9811, batch_loss: 0.0400, loss: 0.0542 ||:  97%|#########6| 63/65 [12:20<00:23, 11.69s/it]
2022-04-08 23:21:25,952 - INFO - tqdm - accuracy: 0.9809, batch_loss: 0.0316, loss: 0.0538 ||:  98%|#########8| 64/65 [12:32<00:11, 11.71s/it]
2022-04-08 23:21:31,165 - INFO - tqdm - accuracy: 0.9811, batch_loss: 0.0063, loss: 0.0531 ||: 100%|##########| 65/65 [12:37<00:00,  9.76s/it]
2022-04-08 23:21:31,165 - INFO - tqdm - accuracy: 0.9811, batch_loss: 0.0063, loss: 0.0531 ||: 100%|##########| 65/65 [12:37<00:00, 11.65s/it]
2022-04-08 23:21:34,032 - INFO - allennlp.training.trainer - Validating
2022-04-08 23:21:34,034 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-08 23:21:44,244 - INFO - tqdm - accuracy: 0.6750, batch_loss: 0.0009, loss: 1.6941 ||:  35%|###4      | 40/115 [00:10<00:19,  3.92it/s]
2022-04-08 23:21:54,293 - INFO - tqdm - accuracy: 0.6242, batch_loss: 3.4927, loss: 2.0252 ||:  69%|######8   | 79/115 [00:20<00:09,  3.81it/s]
2022-04-08 23:22:03,768 - INFO - tqdm - accuracy: 0.6114, batch_loss: 6.1210, loss: 2.1021 ||: 100%|##########| 115/115 [00:29<00:00,  3.80it/s]
2022-04-08 23:22:03,768 - INFO - tqdm - accuracy: 0.6114, batch_loss: 6.1210, loss: 2.1021 ||: 100%|##########| 115/115 [00:29<00:00,  3.87it/s]
2022-04-08 23:22:03,768 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-08 23:22:03,769 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.981  |     0.611
2022-04-08 23:22:03,770 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-08 23:22:03,771 - INFO - allennlp.training.tensorboard_writer - loss               |     0.053  |     2.102
2022-04-08 23:22:03,771 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5799.516  |       N/A
2022-04-08 23:22:09,971 - INFO - allennlp.training.trainer - Epoch duration: 0:13:16.313399
2022-04-08 23:22:09,971 - INFO - allennlp.training.trainer - Estimated training time remaining: 1:33:07
2022-04-08 23:22:09,971 - INFO - allennlp.training.trainer - Epoch 8/14
2022-04-08 23:22:09,971 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-08 23:22:09,972 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-08 23:22:09,973 - INFO - allennlp.training.trainer - Training
2022-04-08 23:22:09,974 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-08 23:22:21,573 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0097, loss: 0.0097 ||:   2%|1         | 1/65 [00:11<12:22, 11.60s/it]
2022-04-08 23:22:33,491 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0036, loss: 0.0066 ||:   3%|3         | 2/65 [00:23<12:22, 11.79s/it]
2022-04-08 23:22:45,518 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0043, loss: 0.0059 ||:   5%|4         | 3/65 [00:35<12:17, 11.90s/it]
2022-04-08 23:22:57,245 - INFO - tqdm - accuracy: 0.9922, batch_loss: 0.1462, loss: 0.0409 ||:   6%|6         | 4/65 [00:47<12:01, 11.83s/it]
2022-04-08 23:23:08,920 - INFO - tqdm - accuracy: 0.9750, batch_loss: 0.1100, loss: 0.0548 ||:   8%|7         | 5/65 [00:58<11:46, 11.77s/it]
2022-04-08 23:23:20,577 - INFO - tqdm - accuracy: 0.9740, batch_loss: 0.0514, loss: 0.0542 ||:   9%|9         | 6/65 [01:10<11:32, 11.73s/it]
2022-04-08 23:23:32,283 - INFO - tqdm - accuracy: 0.9777, batch_loss: 0.0198, loss: 0.0493 ||:  11%|#         | 7/65 [01:22<11:20, 11.72s/it]
2022-04-08 23:23:44,048 - INFO - tqdm - accuracy: 0.9766, batch_loss: 0.1175, loss: 0.0578 ||:  12%|#2        | 8/65 [01:34<11:09, 11.74s/it]
2022-04-08 23:23:55,852 - INFO - tqdm - accuracy: 0.9757, batch_loss: 0.1193, loss: 0.0647 ||:  14%|#3        | 9/65 [01:45<10:58, 11.76s/it]
2022-04-08 23:24:07,628 - INFO - tqdm - accuracy: 0.9781, batch_loss: 0.0091, loss: 0.0591 ||:  15%|#5        | 10/65 [01:57<10:47, 11.76s/it]
2022-04-08 23:24:19,393 - INFO - tqdm - accuracy: 0.9773, batch_loss: 0.0396, loss: 0.0573 ||:  17%|#6        | 11/65 [02:09<10:35, 11.76s/it]
2022-04-08 23:24:31,136 - INFO - tqdm - accuracy: 0.9740, batch_loss: 0.2316, loss: 0.0718 ||:  18%|#8        | 12/65 [02:21<10:23, 11.76s/it]
2022-04-08 23:24:42,898 - INFO - tqdm - accuracy: 0.9760, batch_loss: 0.0127, loss: 0.0673 ||:  20%|##        | 13/65 [02:32<10:11, 11.76s/it]
2022-04-08 23:24:54,651 - INFO - tqdm - accuracy: 0.9777, batch_loss: 0.0248, loss: 0.0643 ||:  22%|##1       | 14/65 [02:44<09:59, 11.76s/it]
2022-04-08 23:25:06,371 - INFO - tqdm - accuracy: 0.9792, batch_loss: 0.0193, loss: 0.0613 ||:  23%|##3       | 15/65 [02:56<09:47, 11.75s/it]
2022-04-08 23:25:18,075 - INFO - tqdm - accuracy: 0.9805, batch_loss: 0.0278, loss: 0.0592 ||:  25%|##4       | 16/65 [03:08<09:34, 11.73s/it]
2022-04-08 23:25:29,766 - INFO - tqdm - accuracy: 0.9798, batch_loss: 0.0759, loss: 0.0602 ||:  26%|##6       | 17/65 [03:19<09:22, 11.72s/it]
2022-04-08 23:25:41,451 - INFO - tqdm - accuracy: 0.9809, batch_loss: 0.0330, loss: 0.0587 ||:  28%|##7       | 18/65 [03:31<09:10, 11.71s/it]
2022-04-08 23:25:53,162 - INFO - tqdm - accuracy: 0.9819, batch_loss: 0.0092, loss: 0.0561 ||:  29%|##9       | 19/65 [03:43<08:58, 11.71s/it]
2022-04-08 23:26:04,881 - INFO - tqdm - accuracy: 0.9828, batch_loss: 0.0135, loss: 0.0539 ||:  31%|###       | 20/65 [03:54<08:47, 11.71s/it]
2022-04-08 23:26:16,644 - INFO - tqdm - accuracy: 0.9836, batch_loss: 0.0252, loss: 0.0526 ||:  32%|###2      | 21/65 [04:06<08:36, 11.73s/it]
2022-04-08 23:26:28,435 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0050, loss: 0.0504 ||:  34%|###3      | 22/65 [04:18<08:25, 11.75s/it]
2022-04-08 23:26:40,232 - INFO - tqdm - accuracy: 0.9837, batch_loss: 0.0474, loss: 0.0503 ||:  35%|###5      | 23/65 [04:30<08:13, 11.76s/it]
2022-04-08 23:26:52,001 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0489, loss: 0.0502 ||:  37%|###6      | 24/65 [04:42<08:02, 11.76s/it]
2022-04-08 23:27:03,765 - INFO - tqdm - accuracy: 0.9838, batch_loss: 0.0501, loss: 0.0502 ||:  38%|###8      | 25/65 [04:53<07:50, 11.76s/it]
2022-04-08 23:27:15,497 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0076, loss: 0.0486 ||:  40%|####      | 26/65 [05:05<07:38, 11.75s/it]
2022-04-08 23:27:26,944 - INFO - tqdm - accuracy: 0.9826, batch_loss: 0.0971, loss: 0.0504 ||:  42%|####1     | 27/65 [05:16<07:23, 11.66s/it]
2022-04-08 23:27:38,635 - INFO - tqdm - accuracy: 0.9833, batch_loss: 0.0021, loss: 0.0486 ||:  43%|####3     | 28/65 [05:28<07:11, 11.67s/it]
2022-04-08 23:27:50,353 - INFO - tqdm - accuracy: 0.9838, batch_loss: 0.0040, loss: 0.0471 ||:  45%|####4     | 29/65 [05:40<07:00, 11.69s/it]
2022-04-08 23:28:02,065 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0163, loss: 0.0461 ||:  46%|####6     | 30/65 [05:52<06:49, 11.69s/it]
2022-04-08 23:28:13,749 - INFO - tqdm - accuracy: 0.9839, batch_loss: 0.0901, loss: 0.0475 ||:  48%|####7     | 31/65 [06:03<06:37, 11.69s/it]
2022-04-08 23:28:25,439 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0046, loss: 0.0462 ||:  49%|####9     | 32/65 [06:15<06:25, 11.69s/it]
2022-04-08 23:28:37,142 - INFO - tqdm - accuracy: 0.9839, batch_loss: 0.0455, loss: 0.0461 ||:  51%|#####     | 33/65 [06:27<06:14, 11.69s/it]
2022-04-08 23:28:48,851 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0260, loss: 0.0455 ||:  52%|#####2    | 34/65 [06:38<06:02, 11.70s/it]
2022-04-08 23:29:00,580 - INFO - tqdm - accuracy: 0.9848, batch_loss: 0.0158, loss: 0.0447 ||:  54%|#####3    | 35/65 [06:50<05:51, 11.71s/it]
2022-04-08 23:29:12,319 - INFO - tqdm - accuracy: 0.9852, batch_loss: 0.0178, loss: 0.0439 ||:  55%|#####5    | 36/65 [07:02<05:39, 11.72s/it]
2022-04-08 23:29:24,066 - INFO - tqdm - accuracy: 0.9856, batch_loss: 0.0229, loss: 0.0434 ||:  57%|#####6    | 37/65 [07:14<05:28, 11.73s/it]
2022-04-08 23:29:35,827 - INFO - tqdm - accuracy: 0.9860, batch_loss: 0.0139, loss: 0.0426 ||:  58%|#####8    | 38/65 [07:25<05:16, 11.74s/it]
2022-04-08 23:29:47,350 - INFO - tqdm - accuracy: 0.9864, batch_loss: 0.0153, loss: 0.0419 ||:  60%|######    | 39/65 [07:37<05:03, 11.67s/it]
2022-04-08 23:29:59,158 - INFO - tqdm - accuracy: 0.9859, batch_loss: 0.0334, loss: 0.0417 ||:  62%|######1   | 40/65 [07:49<04:52, 11.71s/it]
2022-04-08 23:30:10,964 - INFO - tqdm - accuracy: 0.9855, batch_loss: 0.0679, loss: 0.0423 ||:  63%|######3   | 41/65 [08:00<04:41, 11.74s/it]
2022-04-08 23:30:22,778 - INFO - tqdm - accuracy: 0.9851, batch_loss: 0.0353, loss: 0.0422 ||:  65%|######4   | 42/65 [08:12<04:30, 11.76s/it]
2022-04-08 23:30:34,555 - INFO - tqdm - accuracy: 0.9855, batch_loss: 0.0093, loss: 0.0414 ||:  66%|######6   | 43/65 [08:24<04:18, 11.77s/it]
2022-04-08 23:30:46,315 - INFO - tqdm - accuracy: 0.9851, batch_loss: 0.1362, loss: 0.0435 ||:  68%|######7   | 44/65 [08:36<04:07, 11.77s/it]
2022-04-08 23:30:58,078 - INFO - tqdm - accuracy: 0.9854, batch_loss: 0.0041, loss: 0.0427 ||:  69%|######9   | 45/65 [08:48<03:55, 11.76s/it]
2022-04-08 23:31:09,845 - INFO - tqdm - accuracy: 0.9857, batch_loss: 0.0046, loss: 0.0418 ||:  71%|#######   | 46/65 [08:59<03:43, 11.76s/it]
2022-04-08 23:31:21,574 - INFO - tqdm - accuracy: 0.9860, batch_loss: 0.0064, loss: 0.0411 ||:  72%|#######2  | 47/65 [09:11<03:31, 11.75s/it]
2022-04-08 23:31:33,283 - INFO - tqdm - accuracy: 0.9863, batch_loss: 0.0165, loss: 0.0406 ||:  74%|#######3  | 48/65 [09:23<03:19, 11.74s/it]
2022-04-08 23:31:44,979 - INFO - tqdm - accuracy: 0.9866, batch_loss: 0.0042, loss: 0.0398 ||:  75%|#######5  | 49/65 [09:35<03:07, 11.73s/it]
2022-04-08 23:31:56,724 - INFO - tqdm - accuracy: 0.9869, batch_loss: 0.0051, loss: 0.0391 ||:  77%|#######6  | 50/65 [09:46<02:55, 11.73s/it]
2022-04-08 23:32:08,546 - INFO - tqdm - accuracy: 0.9865, batch_loss: 0.0279, loss: 0.0389 ||:  78%|#######8  | 51/65 [09:58<02:44, 11.76s/it]
2022-04-08 23:32:20,348 - INFO - tqdm - accuracy: 0.9868, batch_loss: 0.0113, loss: 0.0384 ||:  80%|########  | 52/65 [10:10<02:33, 11.77s/it]
2022-04-08 23:32:32,135 - INFO - tqdm - accuracy: 0.9870, batch_loss: 0.0073, loss: 0.0378 ||:  82%|########1 | 53/65 [10:22<02:21, 11.78s/it]
2022-04-08 23:32:43,898 - INFO - tqdm - accuracy: 0.9867, batch_loss: 0.0707, loss: 0.0384 ||:  83%|########3 | 54/65 [10:33<02:09, 11.77s/it]
2022-04-08 23:32:55,671 - INFO - tqdm - accuracy: 0.9869, batch_loss: 0.0230, loss: 0.0381 ||:  85%|########4 | 55/65 [10:45<01:57, 11.77s/it]
2022-04-08 23:33:07,399 - INFO - tqdm - accuracy: 0.9872, batch_loss: 0.0137, loss: 0.0377 ||:  86%|########6 | 56/65 [10:57<01:45, 11.76s/it]
2022-04-08 23:33:19,135 - INFO - tqdm - accuracy: 0.9874, batch_loss: 0.0016, loss: 0.0371 ||:  88%|########7 | 57/65 [11:09<01:34, 11.75s/it]
2022-04-08 23:33:30,864 - INFO - tqdm - accuracy: 0.9876, batch_loss: 0.0050, loss: 0.0365 ||:  89%|########9 | 58/65 [11:20<01:22, 11.75s/it]
2022-04-08 23:33:42,587 - INFO - tqdm - accuracy: 0.9878, batch_loss: 0.0147, loss: 0.0361 ||:  91%|######### | 59/65 [11:32<01:10, 11.74s/it]
2022-04-08 23:33:53,981 - INFO - tqdm - accuracy: 0.9880, batch_loss: 0.0203, loss: 0.0359 ||:  92%|#########2| 60/65 [11:44<00:58, 11.64s/it]
2022-04-08 23:34:05,732 - INFO - tqdm - accuracy: 0.9882, batch_loss: 0.0176, loss: 0.0356 ||:  94%|#########3| 61/65 [11:55<00:46, 11.67s/it]
2022-04-08 23:34:17,479 - INFO - tqdm - accuracy: 0.9884, batch_loss: 0.0183, loss: 0.0353 ||:  95%|#########5| 62/65 [12:07<00:35, 11.69s/it]
2022-04-08 23:34:29,237 - INFO - tqdm - accuracy: 0.9881, batch_loss: 0.0470, loss: 0.0355 ||:  97%|#########6| 63/65 [12:19<00:23, 11.71s/it]
2022-04-08 23:34:40,971 - INFO - tqdm - accuracy: 0.9878, batch_loss: 0.0338, loss: 0.0355 ||:  98%|#########8| 64/65 [12:30<00:11, 11.72s/it]
2022-04-08 23:34:46,049 - INFO - tqdm - accuracy: 0.9874, batch_loss: 0.0841, loss: 0.0362 ||: 100%|##########| 65/65 [12:36<00:00,  9.73s/it]
2022-04-08 23:34:46,049 - INFO - tqdm - accuracy: 0.9874, batch_loss: 0.0841, loss: 0.0362 ||: 100%|##########| 65/65 [12:36<00:00, 11.63s/it]
2022-04-08 23:34:48,912 - INFO - allennlp.training.trainer - Validating
2022-04-08 23:34:48,913 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-08 23:34:58,941 - INFO - tqdm - accuracy: 0.5641, batch_loss: 8.3370, loss: 2.4955 ||:  34%|###3      | 39/115 [00:10<00:19,  3.88it/s]
2022-04-08 23:35:09,078 - INFO - tqdm - accuracy: 0.5769, batch_loss: 0.0696, loss: 2.3736 ||:  68%|######7   | 78/115 [00:20<00:09,  3.82it/s]
2022-04-08 23:35:18,567 - INFO - tqdm - accuracy: 0.6070, batch_loss: 0.1734, loss: 2.1949 ||: 100%|##########| 115/115 [00:29<00:00,  3.89it/s]
2022-04-08 23:35:18,567 - INFO - tqdm - accuracy: 0.6070, batch_loss: 0.1734, loss: 2.1949 ||: 100%|##########| 115/115 [00:29<00:00,  3.88it/s]
2022-04-08 23:35:18,568 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-08 23:35:18,568 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.987  |     0.607
2022-04-08 23:35:18,569 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-08 23:35:18,570 - INFO - allennlp.training.tensorboard_writer - loss               |     0.036  |     2.195
2022-04-08 23:35:18,570 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5799.516  |       N/A
2022-04-08 23:35:24,366 - INFO - allennlp.training.trainer - Epoch duration: 0:13:14.394494
2022-04-08 23:35:24,366 - INFO - allennlp.training.trainer - Estimated training time remaining: 1:19:46
2022-04-08 23:35:24,366 - INFO - allennlp.training.trainer - Epoch 9/14
2022-04-08 23:35:24,366 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-08 23:35:24,366 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-08 23:35:24,368 - INFO - allennlp.training.trainer - Training
2022-04-08 23:35:24,368 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-08 23:35:35,971 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0030, loss: 0.0030 ||:   2%|1         | 1/65 [00:11<12:22, 11.60s/it]
2022-04-08 23:35:47,875 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0108, loss: 0.0069 ||:   3%|3         | 2/65 [00:23<12:22, 11.78s/it]
2022-04-08 23:35:59,924 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0136, loss: 0.0091 ||:   5%|4         | 3/65 [00:35<12:17, 11.90s/it]
2022-04-08 23:36:11,525 - INFO - tqdm - accuracy: 0.9922, batch_loss: 0.0948, loss: 0.0306 ||:   6%|6         | 4/65 [00:47<11:58, 11.78s/it]
2022-04-08 23:36:23,190 - INFO - tqdm - accuracy: 0.9938, batch_loss: 0.0158, loss: 0.0276 ||:   8%|7         | 5/65 [00:58<11:44, 11.74s/it]
2022-04-08 23:36:34,825 - INFO - tqdm - accuracy: 0.9948, batch_loss: 0.0035, loss: 0.0236 ||:   9%|9         | 6/65 [01:10<11:30, 11.70s/it]
2022-04-08 23:36:46,528 - INFO - tqdm - accuracy: 0.9955, batch_loss: 0.0048, loss: 0.0209 ||:  11%|#         | 7/65 [01:22<11:18, 11.70s/it]
2022-04-08 23:36:58,309 - INFO - tqdm - accuracy: 0.9961, batch_loss: 0.0032, loss: 0.0187 ||:  12%|#2        | 8/65 [01:33<11:08, 11.73s/it]
2022-04-08 23:37:10,138 - INFO - tqdm - accuracy: 0.9965, batch_loss: 0.0015, loss: 0.0168 ||:  14%|#3        | 9/65 [01:45<10:58, 11.76s/it]
2022-04-08 23:37:21,913 - INFO - tqdm - accuracy: 0.9969, batch_loss: 0.0046, loss: 0.0156 ||:  15%|#5        | 10/65 [01:57<10:47, 11.76s/it]
2022-04-08 23:37:33,638 - INFO - tqdm - accuracy: 0.9943, batch_loss: 0.0418, loss: 0.0179 ||:  17%|#6        | 11/65 [02:09<10:34, 11.75s/it]
2022-04-08 23:37:45,328 - INFO - tqdm - accuracy: 0.9948, batch_loss: 0.0085, loss: 0.0172 ||:  18%|#8        | 12/65 [02:20<10:21, 11.73s/it]
2022-04-08 23:37:57,065 - INFO - tqdm - accuracy: 0.9904, batch_loss: 0.0654, loss: 0.0209 ||:  20%|##        | 13/65 [02:32<10:10, 11.73s/it]
2022-04-08 23:38:08,802 - INFO - tqdm - accuracy: 0.9888, batch_loss: 0.0334, loss: 0.0218 ||:  22%|##1       | 14/65 [02:44<09:58, 11.74s/it]
2022-04-08 23:38:20,965 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.0075, loss: 0.0208 ||:  23%|##3       | 15/65 [02:56<09:53, 11.86s/it]
2022-04-08 23:38:32,679 - INFO - tqdm - accuracy: 0.9902, batch_loss: 0.0063, loss: 0.0199 ||:  25%|##4       | 16/65 [03:08<09:39, 11.82s/it]
2022-04-08 23:38:44,471 - INFO - tqdm - accuracy: 0.9908, batch_loss: 0.0022, loss: 0.0189 ||:  26%|##6       | 17/65 [03:20<09:26, 11.81s/it]
2022-04-08 23:38:56,277 - INFO - tqdm - accuracy: 0.9878, batch_loss: 0.1925, loss: 0.0285 ||:  28%|##7       | 18/65 [03:31<09:15, 11.81s/it]
2022-04-08 23:39:08,100 - INFO - tqdm - accuracy: 0.9885, batch_loss: 0.0117, loss: 0.0276 ||:  29%|##9       | 19/65 [03:43<09:03, 11.81s/it]
2022-04-08 23:39:19,823 - INFO - tqdm - accuracy: 0.9891, batch_loss: 0.0068, loss: 0.0266 ||:  31%|###       | 20/65 [03:55<08:50, 11.79s/it]
2022-04-08 23:39:31,494 - INFO - tqdm - accuracy: 0.9881, batch_loss: 0.0454, loss: 0.0275 ||:  32%|###2      | 21/65 [04:07<08:37, 11.75s/it]
2022-04-08 23:39:43,216 - INFO - tqdm - accuracy: 0.9886, batch_loss: 0.0024, loss: 0.0264 ||:  34%|###3      | 22/65 [04:18<08:24, 11.74s/it]
2022-04-08 23:39:55,014 - INFO - tqdm - accuracy: 0.9891, batch_loss: 0.0128, loss: 0.0258 ||:  35%|###5      | 23/65 [04:30<08:13, 11.76s/it]
2022-04-08 23:40:06,849 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.0194, loss: 0.0255 ||:  37%|###6      | 24/65 [04:42<08:03, 11.78s/it]
2022-04-08 23:40:18,653 - INFO - tqdm - accuracy: 0.9888, batch_loss: 0.0541, loss: 0.0266 ||:  38%|###8      | 25/65 [04:54<07:51, 11.79s/it]
2022-04-08 23:40:30,414 - INFO - tqdm - accuracy: 0.9892, batch_loss: 0.0106, loss: 0.0260 ||:  40%|####      | 26/65 [05:06<07:39, 11.78s/it]
2022-04-08 23:40:42,165 - INFO - tqdm - accuracy: 0.9884, batch_loss: 0.0479, loss: 0.0268 ||:  42%|####1     | 27/65 [05:17<07:27, 11.77s/it]
2022-04-08 23:40:53,911 - INFO - tqdm - accuracy: 0.9888, batch_loss: 0.0083, loss: 0.0262 ||:  43%|####3     | 28/65 [05:29<07:15, 11.76s/it]
2022-04-08 23:41:05,634 - INFO - tqdm - accuracy: 0.9892, batch_loss: 0.0051, loss: 0.0254 ||:  45%|####4     | 29/65 [05:41<07:03, 11.75s/it]
2022-04-08 23:41:17,218 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.0020, loss: 0.0247 ||:  46%|####6     | 30/65 [05:52<06:49, 11.70s/it]
2022-04-08 23:41:28,914 - INFO - tqdm - accuracy: 0.9899, batch_loss: 0.0243, loss: 0.0247 ||:  48%|####7     | 31/65 [06:04<06:37, 11.70s/it]
2022-04-08 23:41:40,626 - INFO - tqdm - accuracy: 0.9902, batch_loss: 0.0066, loss: 0.0241 ||:  49%|####9     | 32/65 [06:16<06:26, 11.70s/it]
2022-04-08 23:41:52,355 - INFO - tqdm - accuracy: 0.9905, batch_loss: 0.0047, loss: 0.0235 ||:  51%|#####     | 33/65 [06:27<06:14, 11.71s/it]
2022-04-08 23:42:04,101 - INFO - tqdm - accuracy: 0.9908, batch_loss: 0.0021, loss: 0.0229 ||:  52%|#####2    | 34/65 [06:39<06:03, 11.72s/it]
2022-04-08 23:42:15,556 - INFO - tqdm - accuracy: 0.9911, batch_loss: 0.0134, loss: 0.0226 ||:  54%|#####3    | 35/65 [06:51<05:49, 11.64s/it]
2022-04-08 23:42:27,330 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.2110, loss: 0.0278 ||:  55%|#####5    | 36/65 [07:02<05:38, 11.68s/it]
2022-04-08 23:42:39,048 - INFO - tqdm - accuracy: 0.9899, batch_loss: 0.0218, loss: 0.0277 ||:  57%|#####6    | 37/65 [07:14<05:27, 11.69s/it]
2022-04-08 23:42:50,764 - INFO - tqdm - accuracy: 0.9901, batch_loss: 0.0007, loss: 0.0270 ||:  58%|#####8    | 38/65 [07:26<05:15, 11.70s/it]
2022-04-08 23:43:02,465 - INFO - tqdm - accuracy: 0.9904, batch_loss: 0.0020, loss: 0.0263 ||:  60%|######    | 39/65 [07:38<05:04, 11.70s/it]
2022-04-08 23:43:14,205 - INFO - tqdm - accuracy: 0.9906, batch_loss: 0.0190, loss: 0.0261 ||:  62%|######1   | 40/65 [07:49<04:52, 11.71s/it]
2022-04-08 23:43:25,956 - INFO - tqdm - accuracy: 0.9908, batch_loss: 0.0028, loss: 0.0256 ||:  63%|######3   | 41/65 [08:01<04:41, 11.72s/it]
2022-04-08 23:43:37,715 - INFO - tqdm - accuracy: 0.9911, batch_loss: 0.0112, loss: 0.0252 ||:  65%|######4   | 42/65 [08:13<04:29, 11.73s/it]
2022-04-08 23:43:49,491 - INFO - tqdm - accuracy: 0.9913, batch_loss: 0.0009, loss: 0.0247 ||:  66%|######6   | 43/65 [08:25<04:18, 11.75s/it]
2022-04-08 23:44:01,287 - INFO - tqdm - accuracy: 0.9908, batch_loss: 0.0790, loss: 0.0259 ||:  68%|######7   | 44/65 [08:36<04:06, 11.76s/it]
2022-04-08 23:44:13,080 - INFO - tqdm - accuracy: 0.9903, batch_loss: 0.0429, loss: 0.0263 ||:  69%|######9   | 45/65 [08:48<03:55, 11.77s/it]
2022-04-08 23:44:24,863 - INFO - tqdm - accuracy: 0.9898, batch_loss: 0.0872, loss: 0.0276 ||:  71%|#######   | 46/65 [09:00<03:43, 11.77s/it]
2022-04-08 23:44:36,672 - INFO - tqdm - accuracy: 0.9900, batch_loss: 0.0030, loss: 0.0271 ||:  72%|#######2  | 47/65 [09:12<03:32, 11.78s/it]
2022-04-08 23:44:48,486 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.0322, loss: 0.0272 ||:  74%|#######3  | 48/65 [09:24<03:20, 11.79s/it]
2022-04-08 23:45:00,269 - INFO - tqdm - accuracy: 0.9898, batch_loss: 0.0384, loss: 0.0274 ||:  75%|#######5  | 49/65 [09:35<03:08, 11.79s/it]
2022-04-08 23:45:11,755 - INFO - tqdm - accuracy: 0.9900, batch_loss: 0.0125, loss: 0.0271 ||:  77%|#######6  | 50/65 [09:47<02:55, 11.70s/it]
2022-04-08 23:45:23,486 - INFO - tqdm - accuracy: 0.9902, batch_loss: 0.0260, loss: 0.0271 ||:  78%|#######8  | 51/65 [09:59<02:43, 11.71s/it]
2022-04-08 23:45:35,167 - INFO - tqdm - accuracy: 0.9898, batch_loss: 0.0406, loss: 0.0273 ||:  80%|########  | 52/65 [10:10<02:32, 11.70s/it]
2022-04-08 23:45:46,884 - INFO - tqdm - accuracy: 0.9894, batch_loss: 0.0416, loss: 0.0276 ||:  82%|########1 | 53/65 [10:22<02:20, 11.71s/it]
2022-04-08 23:45:58,621 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.0157, loss: 0.0274 ||:  83%|########3 | 54/65 [10:34<02:08, 11.71s/it]
2022-04-08 23:46:10,389 - INFO - tqdm - accuracy: 0.9898, batch_loss: 0.0108, loss: 0.0271 ||:  85%|########4 | 55/65 [10:46<01:57, 11.73s/it]
2022-04-08 23:46:22,171 - INFO - tqdm - accuracy: 0.9899, batch_loss: 0.0033, loss: 0.0267 ||:  86%|########6 | 56/65 [10:57<01:45, 11.75s/it]
2022-04-08 23:46:33,981 - INFO - tqdm - accuracy: 0.9901, batch_loss: 0.0081, loss: 0.0263 ||:  88%|########7 | 57/65 [11:09<01:34, 11.77s/it]
2022-04-08 23:46:45,807 - INFO - tqdm - accuracy: 0.9903, batch_loss: 0.0022, loss: 0.0259 ||:  89%|########9 | 58/65 [11:21<01:22, 11.78s/it]
2022-04-08 23:46:57,627 - INFO - tqdm - accuracy: 0.9905, batch_loss: 0.0099, loss: 0.0257 ||:  91%|######### | 59/65 [11:33<01:10, 11.79s/it]
2022-04-08 23:47:09,468 - INFO - tqdm - accuracy: 0.9901, batch_loss: 0.0448, loss: 0.0260 ||:  92%|#########2| 60/65 [11:45<00:59, 11.81s/it]
2022-04-08 23:47:21,250 - INFO - tqdm - accuracy: 0.9903, batch_loss: 0.0018, loss: 0.0256 ||:  94%|#########3| 61/65 [11:56<00:47, 11.80s/it]
2022-04-08 23:47:32,980 - INFO - tqdm - accuracy: 0.9899, batch_loss: 0.0346, loss: 0.0257 ||:  95%|#########5| 62/65 [12:08<00:35, 11.78s/it]
2022-04-08 23:47:44,668 - INFO - tqdm - accuracy: 0.9901, batch_loss: 0.0011, loss: 0.0253 ||:  97%|#########6| 63/65 [12:20<00:23, 11.75s/it]
2022-04-08 23:47:56,416 - INFO - tqdm - accuracy: 0.9897, batch_loss: 0.0285, loss: 0.0254 ||:  98%|#########8| 64/65 [12:32<00:11, 11.75s/it]
2022-04-08 23:48:01,649 - INFO - tqdm - accuracy: 0.9898, batch_loss: 0.0097, loss: 0.0251 ||: 100%|##########| 65/65 [12:37<00:00,  9.80s/it]
2022-04-08 23:48:01,649 - INFO - tqdm - accuracy: 0.9898, batch_loss: 0.0097, loss: 0.0251 ||: 100%|##########| 65/65 [12:37<00:00, 11.65s/it]
2022-04-08 23:48:04,577 - INFO - allennlp.training.trainer - Validating
2022-04-08 23:48:04,578 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-08 23:48:14,747 - INFO - tqdm - accuracy: 0.5823, batch_loss: 1.9144, loss: 2.7391 ||:  35%|###4      | 40/115 [00:10<00:19,  3.83it/s]
2022-04-08 23:48:24,982 - INFO - tqdm - accuracy: 0.6051, batch_loss: 0.0121, loss: 2.3756 ||:  69%|######8   | 79/115 [00:20<00:09,  3.81it/s]
2022-04-08 23:48:34,420 - INFO - tqdm - accuracy: 0.6070, batch_loss: 1.8606, loss: 2.3700 ||: 100%|##########| 115/115 [00:29<00:00,  3.84it/s]
2022-04-08 23:48:34,420 - INFO - tqdm - accuracy: 0.6070, batch_loss: 1.8606, loss: 2.3700 ||: 100%|##########| 115/115 [00:29<00:00,  3.85it/s]
2022-04-08 23:48:34,420 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-08 23:48:34,421 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.990  |     0.607
2022-04-08 23:48:34,421 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-08 23:48:34,422 - INFO - allennlp.training.tensorboard_writer - loss               |     0.025  |     2.370
2022-04-08 23:48:34,422 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5799.516  |       N/A
2022-04-08 23:48:40,524 - INFO - allennlp.training.trainer - Epoch duration: 0:13:16.158227
2022-04-08 23:48:40,525 - INFO - allennlp.training.trainer - Estimated training time remaining: 1:06:28
2022-04-08 23:48:40,525 - INFO - allennlp.training.trainer - Epoch 10/14
2022-04-08 23:48:40,525 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-08 23:48:40,525 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-08 23:48:40,526 - INFO - allennlp.training.trainer - Training
2022-04-08 23:48:40,527 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-08 23:48:52,092 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0059, loss: 0.0059 ||:   2%|1         | 1/65 [00:11<12:20, 11.57s/it]
2022-04-08 23:49:03,725 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0032, loss: 0.0046 ||:   3%|3         | 2/65 [00:23<12:11, 11.61s/it]
2022-04-08 23:49:15,758 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0018, loss: 0.0036 ||:   5%|4         | 3/65 [00:35<12:11, 11.80s/it]
2022-04-08 23:49:27,544 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0028, loss: 0.0034 ||:   6%|6         | 4/65 [00:47<11:59, 11.79s/it]
2022-04-08 23:49:39,125 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0010, loss: 0.0029 ||:   8%|7         | 5/65 [00:58<11:43, 11.72s/it]
2022-04-08 23:49:50,770 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0010, loss: 0.0026 ||:   9%|9         | 6/65 [01:10<11:29, 11.69s/it]
2022-04-08 23:50:02,494 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0018, loss: 0.0025 ||:  11%|#         | 7/65 [01:21<11:18, 11.70s/it]
2022-04-08 23:50:14,241 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0135, loss: 0.0039 ||:  12%|#2        | 8/65 [01:33<11:07, 11.72s/it]
2022-04-08 23:50:26,034 - INFO - tqdm - accuracy: 0.9931, batch_loss: 0.0739, loss: 0.0117 ||:  14%|#3        | 9/65 [01:45<10:57, 11.74s/it]
2022-04-08 23:50:37,812 - INFO - tqdm - accuracy: 0.9938, batch_loss: 0.0015, loss: 0.0106 ||:  15%|#5        | 10/65 [01:57<10:46, 11.75s/it]
2022-04-08 23:50:49,540 - INFO - tqdm - accuracy: 0.9943, batch_loss: 0.0025, loss: 0.0099 ||:  17%|#6        | 11/65 [02:09<10:34, 11.74s/it]
2022-04-08 23:51:01,241 - INFO - tqdm - accuracy: 0.9948, batch_loss: 0.0021, loss: 0.0093 ||:  18%|#8        | 12/65 [02:20<10:21, 11.73s/it]
2022-04-08 23:51:12,933 - INFO - tqdm - accuracy: 0.9952, batch_loss: 0.0177, loss: 0.0099 ||:  20%|##        | 13/65 [02:32<10:09, 11.72s/it]
2022-04-08 23:51:24,667 - INFO - tqdm - accuracy: 0.9955, batch_loss: 0.0053, loss: 0.0096 ||:  22%|##1       | 14/65 [02:44<09:57, 11.72s/it]
2022-04-08 23:51:36,426 - INFO - tqdm - accuracy: 0.9958, batch_loss: 0.0044, loss: 0.0092 ||:  23%|##3       | 15/65 [02:55<09:46, 11.73s/it]
2022-04-08 23:51:48,211 - INFO - tqdm - accuracy: 0.9961, batch_loss: 0.0023, loss: 0.0088 ||:  25%|##4       | 16/65 [03:07<09:35, 11.75s/it]
2022-04-08 23:52:00,024 - INFO - tqdm - accuracy: 0.9945, batch_loss: 0.0388, loss: 0.0106 ||:  26%|##6       | 17/65 [03:19<09:24, 11.77s/it]
2022-04-08 23:52:11,767 - INFO - tqdm - accuracy: 0.9948, batch_loss: 0.0022, loss: 0.0101 ||:  28%|##7       | 18/65 [03:31<09:12, 11.76s/it]
2022-04-08 23:52:23,434 - INFO - tqdm - accuracy: 0.9951, batch_loss: 0.0059, loss: 0.0099 ||:  29%|##9       | 19/65 [03:42<08:59, 11.73s/it]
2022-04-08 23:52:35,108 - INFO - tqdm - accuracy: 0.9938, batch_loss: 0.0549, loss: 0.0121 ||:  31%|###       | 20/65 [03:54<08:47, 11.72s/it]
2022-04-08 23:52:46,838 - INFO - tqdm - accuracy: 0.9940, batch_loss: 0.0027, loss: 0.0117 ||:  32%|###2      | 21/65 [04:06<08:35, 11.72s/it]
2022-04-08 23:52:58,612 - INFO - tqdm - accuracy: 0.9915, batch_loss: 0.1168, loss: 0.0165 ||:  34%|###3      | 22/65 [04:18<08:24, 11.74s/it]
2022-04-08 23:53:10,051 - INFO - tqdm - accuracy: 0.9918, batch_loss: 0.0010, loss: 0.0158 ||:  35%|###5      | 23/65 [04:29<08:09, 11.65s/it]
2022-04-08 23:53:21,810 - INFO - tqdm - accuracy: 0.9909, batch_loss: 0.0398, loss: 0.0168 ||:  37%|###6      | 24/65 [04:41<07:58, 11.68s/it]
2022-04-08 23:53:33,585 - INFO - tqdm - accuracy: 0.9912, batch_loss: 0.0023, loss: 0.0162 ||:  38%|###8      | 25/65 [04:53<07:48, 11.71s/it]
2022-04-08 23:53:45,206 - INFO - tqdm - accuracy: 0.9916, batch_loss: 0.0109, loss: 0.0160 ||:  40%|####      | 26/65 [05:04<07:35, 11.68s/it]
2022-04-08 23:53:56,948 - INFO - tqdm - accuracy: 0.9919, batch_loss: 0.0192, loss: 0.0161 ||:  42%|####1     | 27/65 [05:16<07:24, 11.70s/it]
2022-04-08 23:54:08,674 - INFO - tqdm - accuracy: 0.9922, batch_loss: 0.0054, loss: 0.0157 ||:  43%|####3     | 28/65 [05:28<07:13, 11.71s/it]
2022-04-08 23:54:20,386 - INFO - tqdm - accuracy: 0.9924, batch_loss: 0.0032, loss: 0.0153 ||:  45%|####4     | 29/65 [05:39<07:01, 11.71s/it]
2022-04-08 23:54:32,100 - INFO - tqdm - accuracy: 0.9927, batch_loss: 0.0097, loss: 0.0151 ||:  46%|####6     | 30/65 [05:51<06:49, 11.71s/it]
2022-04-08 23:54:43,820 - INFO - tqdm - accuracy: 0.9909, batch_loss: 0.0950, loss: 0.0177 ||:  48%|####7     | 31/65 [06:03<06:38, 11.71s/it]
2022-04-08 23:54:55,545 - INFO - tqdm - accuracy: 0.9912, batch_loss: 0.0090, loss: 0.0174 ||:  49%|####9     | 32/65 [06:15<06:26, 11.72s/it]
2022-04-08 23:55:07,279 - INFO - tqdm - accuracy: 0.9915, batch_loss: 0.0057, loss: 0.0171 ||:  51%|#####     | 33/65 [06:26<06:15, 11.72s/it]
2022-04-08 23:55:19,030 - INFO - tqdm - accuracy: 0.9899, batch_loss: 0.1420, loss: 0.0207 ||:  52%|#####2    | 34/65 [06:38<06:03, 11.73s/it]
2022-04-08 23:55:30,765 - INFO - tqdm - accuracy: 0.9902, batch_loss: 0.0139, loss: 0.0205 ||:  54%|#####3    | 35/65 [06:50<05:51, 11.73s/it]
2022-04-08 23:55:42,492 - INFO - tqdm - accuracy: 0.9904, batch_loss: 0.0009, loss: 0.0200 ||:  55%|#####5    | 36/65 [07:01<05:40, 11.73s/it]
2022-04-08 23:55:54,229 - INFO - tqdm - accuracy: 0.9907, batch_loss: 0.0018, loss: 0.0195 ||:  57%|#####6    | 37/65 [07:13<05:28, 11.73s/it]
2022-04-08 23:56:05,949 - INFO - tqdm - accuracy: 0.9909, batch_loss: 0.0110, loss: 0.0193 ||:  58%|#####8    | 38/65 [07:25<05:16, 11.73s/it]
2022-04-08 23:56:17,676 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.1250, loss: 0.0220 ||:  60%|######    | 39/65 [07:37<05:04, 11.73s/it]
2022-04-08 23:56:29,411 - INFO - tqdm - accuracy: 0.9898, batch_loss: 0.0130, loss: 0.0218 ||:  62%|######1   | 40/65 [07:48<04:53, 11.73s/it]
2022-04-08 23:56:41,158 - INFO - tqdm - accuracy: 0.9901, batch_loss: 0.0022, loss: 0.0213 ||:  63%|######3   | 41/65 [08:00<04:41, 11.74s/it]
2022-04-08 23:56:52,899 - INFO - tqdm - accuracy: 0.9903, batch_loss: 0.0054, loss: 0.0209 ||:  65%|######4   | 42/65 [08:12<04:29, 11.74s/it]
2022-04-08 23:57:04,636 - INFO - tqdm - accuracy: 0.9905, batch_loss: 0.0030, loss: 0.0205 ||:  66%|######6   | 43/65 [08:24<04:18, 11.74s/it]
2022-04-08 23:57:16,367 - INFO - tqdm - accuracy: 0.9908, batch_loss: 0.0021, loss: 0.0201 ||:  68%|######7   | 44/65 [08:35<04:06, 11.74s/it]
2022-04-08 23:57:28,096 - INFO - tqdm - accuracy: 0.9910, batch_loss: 0.0106, loss: 0.0199 ||:  69%|######9   | 45/65 [08:47<03:54, 11.73s/it]
2022-04-08 23:57:39,818 - INFO - tqdm - accuracy: 0.9912, batch_loss: 0.0014, loss: 0.0195 ||:  71%|#######   | 46/65 [08:59<03:42, 11.73s/it]
2022-04-08 23:57:51,542 - INFO - tqdm - accuracy: 0.9914, batch_loss: 0.0176, loss: 0.0194 ||:  72%|#######2  | 47/65 [09:11<03:31, 11.73s/it]
2022-04-08 23:58:03,276 - INFO - tqdm - accuracy: 0.9909, batch_loss: 0.0757, loss: 0.0206 ||:  74%|#######3  | 48/65 [09:22<03:19, 11.73s/it]
2022-04-08 23:58:15,011 - INFO - tqdm - accuracy: 0.9911, batch_loss: 0.0009, loss: 0.0202 ||:  75%|#######5  | 49/65 [09:34<03:07, 11.73s/it]
2022-04-08 23:58:27,129 - INFO - tqdm - accuracy: 0.9912, batch_loss: 0.0028, loss: 0.0199 ||:  77%|#######6  | 50/65 [09:46<02:57, 11.85s/it]
2022-04-08 23:58:38,817 - INFO - tqdm - accuracy: 0.9914, batch_loss: 0.0033, loss: 0.0195 ||:  78%|#######8  | 51/65 [09:58<02:45, 11.80s/it]
2022-04-08 23:58:50,579 - INFO - tqdm - accuracy: 0.9916, batch_loss: 0.0209, loss: 0.0196 ||:  80%|########  | 52/65 [10:10<02:33, 11.79s/it]
2022-04-08 23:59:02,365 - INFO - tqdm - accuracy: 0.9912, batch_loss: 0.0443, loss: 0.0200 ||:  82%|########1 | 53/65 [10:21<02:21, 11.79s/it]
2022-04-08 23:59:14,145 - INFO - tqdm - accuracy: 0.9913, batch_loss: 0.0023, loss: 0.0197 ||:  83%|########3 | 54/65 [10:33<02:09, 11.79s/it]
2022-04-08 23:59:25,938 - INFO - tqdm - accuracy: 0.9915, batch_loss: 0.0056, loss: 0.0194 ||:  85%|########4 | 55/65 [10:45<01:57, 11.79s/it]
2022-04-08 23:59:37,729 - INFO - tqdm - accuracy: 0.9916, batch_loss: 0.0023, loss: 0.0191 ||:  86%|########6 | 56/65 [10:57<01:46, 11.79s/it]
2022-04-08 23:59:49,493 - INFO - tqdm - accuracy: 0.9918, batch_loss: 0.0025, loss: 0.0188 ||:  88%|########7 | 57/65 [11:08<01:34, 11.78s/it]
2022-04-09 00:00:01,267 - INFO - tqdm - accuracy: 0.9914, batch_loss: 0.0858, loss: 0.0200 ||:  89%|########9 | 58/65 [11:20<01:22, 11.78s/it]
2022-04-09 00:00:13,011 - INFO - tqdm - accuracy: 0.9915, batch_loss: 0.0231, loss: 0.0200 ||:  91%|######### | 59/65 [11:32<01:10, 11.77s/it]
2022-04-09 00:00:24,716 - INFO - tqdm - accuracy: 0.9911, batch_loss: 0.0577, loss: 0.0207 ||:  92%|#########2| 60/65 [11:44<00:58, 11.75s/it]
2022-04-09 00:00:36,440 - INFO - tqdm - accuracy: 0.9913, batch_loss: 0.0016, loss: 0.0204 ||:  94%|#########3| 61/65 [11:55<00:46, 11.74s/it]
2022-04-09 00:00:48,149 - INFO - tqdm - accuracy: 0.9914, batch_loss: 0.0029, loss: 0.0201 ||:  95%|#########5| 62/65 [12:07<00:35, 11.73s/it]
2022-04-09 00:00:59,848 - INFO - tqdm - accuracy: 0.9911, batch_loss: 0.0378, loss: 0.0204 ||:  97%|#########6| 63/65 [12:19<00:23, 11.72s/it]
2022-04-09 00:01:11,538 - INFO - tqdm - accuracy: 0.9912, batch_loss: 0.0281, loss: 0.0205 ||:  98%|#########8| 64/65 [12:31<00:11, 11.71s/it]
2022-04-09 00:01:16,720 - INFO - tqdm - accuracy: 0.9913, batch_loss: 0.0011, loss: 0.0202 ||: 100%|##########| 65/65 [12:36<00:00,  9.75s/it]
2022-04-09 00:01:16,721 - INFO - tqdm - accuracy: 0.9913, batch_loss: 0.0011, loss: 0.0202 ||: 100%|##########| 65/65 [12:36<00:00, 11.63s/it]
2022-04-09 00:01:19,598 - INFO - allennlp.training.trainer - Validating
2022-04-09 00:01:19,600 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-09 00:01:29,611 - INFO - tqdm - accuracy: 0.6282, batch_loss: 8.9218, loss: 2.1257 ||:  34%|###3      | 39/115 [00:10<00:19,  3.89it/s]
2022-04-09 00:01:39,801 - INFO - tqdm - accuracy: 0.6051, batch_loss: 2.3843, loss: 2.3330 ||:  69%|######8   | 79/115 [00:20<00:09,  3.85it/s]
2022-04-09 00:01:49,311 - INFO - tqdm - accuracy: 0.6157, batch_loss: 4.6565, loss: 2.4740 ||: 100%|##########| 115/115 [00:29<00:00,  3.77it/s]
2022-04-09 00:01:49,311 - INFO - tqdm - accuracy: 0.6157, batch_loss: 4.6565, loss: 2.4740 ||: 100%|##########| 115/115 [00:29<00:00,  3.87it/s]
2022-04-09 00:01:49,311 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-09 00:01:49,312 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.991  |     0.616
2022-04-09 00:01:49,312 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-09 00:01:49,313 - INFO - allennlp.training.tensorboard_writer - loss               |     0.020  |     2.474
2022-04-09 00:01:49,314 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5799.516  |       N/A
2022-04-09 00:01:55,158 - INFO - allennlp.training.trainer - Epoch duration: 0:13:14.632972
2022-04-09 00:01:55,158 - INFO - allennlp.training.trainer - Estimated training time remaining: 0:53:09
2022-04-09 00:01:55,158 - INFO - allennlp.training.trainer - Epoch 11/14
2022-04-09 00:01:55,158 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-09 00:01:55,158 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-09 00:01:55,160 - INFO - allennlp.training.trainer - Training
2022-04-09 00:01:55,160 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-09 00:02:06,739 - INFO - tqdm - accuracy: 0.9375, batch_loss: 0.3117, loss: 0.3117 ||:   2%|1         | 1/65 [00:11<12:21, 11.58s/it]
2022-04-09 00:02:18,550 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.0275, loss: 0.1696 ||:   3%|3         | 2/65 [00:23<12:18, 11.72s/it]
2022-04-09 00:02:30,543 - INFO - tqdm - accuracy: 0.9792, batch_loss: 0.0143, loss: 0.1178 ||:   5%|4         | 3/65 [00:35<12:14, 11.84s/it]
2022-04-09 00:02:42,292 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0105, loss: 0.0910 ||:   6%|6         | 4/65 [00:47<12:00, 11.81s/it]
2022-04-09 00:02:53,980 - INFO - tqdm - accuracy: 0.9750, batch_loss: 0.2187, loss: 0.1165 ||:   8%|7         | 5/65 [00:58<11:45, 11.76s/it]
2022-04-09 00:03:05,642 - INFO - tqdm - accuracy: 0.9792, batch_loss: 0.0010, loss: 0.0973 ||:   9%|9         | 6/65 [01:10<11:31, 11.73s/it]
2022-04-09 00:03:17,343 - INFO - tqdm - accuracy: 0.9821, batch_loss: 0.0064, loss: 0.0843 ||:  11%|#         | 7/65 [01:22<11:19, 11.72s/it]
2022-04-09 00:03:29,074 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0145, loss: 0.0756 ||:  12%|#2        | 8/65 [01:33<11:08, 11.72s/it]
2022-04-09 00:03:40,867 - INFO - tqdm - accuracy: 0.9861, batch_loss: 0.0058, loss: 0.0678 ||:  14%|#3        | 9/65 [01:45<10:57, 11.75s/it]
2022-04-09 00:03:52,667 - INFO - tqdm - accuracy: 0.9875, batch_loss: 0.0008, loss: 0.0611 ||:  15%|#5        | 10/65 [01:57<10:46, 11.76s/it]
2022-04-09 00:04:04,433 - INFO - tqdm - accuracy: 0.9830, batch_loss: 0.1129, loss: 0.0658 ||:  17%|#6        | 11/65 [02:09<10:35, 11.76s/it]
2022-04-09 00:04:16,190 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0016, loss: 0.0605 ||:  18%|#8        | 12/65 [02:21<10:23, 11.76s/it]
2022-04-09 00:04:27,941 - INFO - tqdm - accuracy: 0.9856, batch_loss: 0.0037, loss: 0.0561 ||:  20%|##        | 13/65 [02:32<10:11, 11.76s/it]
2022-04-09 00:04:39,725 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0712, loss: 0.0572 ||:  22%|##1       | 14/65 [02:44<10:00, 11.77s/it]
2022-04-09 00:04:51,504 - INFO - tqdm - accuracy: 0.9854, batch_loss: 0.0013, loss: 0.0535 ||:  23%|##3       | 15/65 [02:56<09:48, 11.77s/it]
2022-04-09 00:05:03,147 - INFO - tqdm - accuracy: 0.9863, batch_loss: 0.0017, loss: 0.0502 ||:  25%|##4       | 16/65 [03:07<09:34, 11.73s/it]
2022-04-09 00:05:14,914 - INFO - tqdm - accuracy: 0.9871, batch_loss: 0.0079, loss: 0.0477 ||:  26%|##6       | 17/65 [03:19<09:23, 11.74s/it]
2022-04-09 00:05:26,649 - INFO - tqdm - accuracy: 0.9878, batch_loss: 0.0161, loss: 0.0460 ||:  28%|##7       | 18/65 [03:31<09:11, 11.74s/it]
2022-04-09 00:05:38,388 - INFO - tqdm - accuracy: 0.9885, batch_loss: 0.0014, loss: 0.0436 ||:  29%|##9       | 19/65 [03:43<09:00, 11.74s/it]
2022-04-09 00:05:50,127 - INFO - tqdm - accuracy: 0.9891, batch_loss: 0.0009, loss: 0.0415 ||:  31%|###       | 20/65 [03:54<08:48, 11.74s/it]
2022-04-09 00:06:01,874 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.0175, loss: 0.0404 ||:  32%|###2      | 21/65 [04:06<08:36, 11.74s/it]
2022-04-09 00:06:13,583 - INFO - tqdm - accuracy: 0.9901, batch_loss: 0.0014, loss: 0.0386 ||:  34%|###3      | 22/65 [04:18<08:24, 11.73s/it]
2022-04-09 00:06:25,288 - INFO - tqdm - accuracy: 0.9905, batch_loss: 0.0016, loss: 0.0370 ||:  35%|###5      | 23/65 [04:30<08:12, 11.72s/it]
2022-04-09 00:06:36,991 - INFO - tqdm - accuracy: 0.9909, batch_loss: 0.0045, loss: 0.0356 ||:  37%|###6      | 24/65 [04:41<08:00, 11.72s/it]
2022-04-09 00:06:48,704 - INFO - tqdm - accuracy: 0.9912, batch_loss: 0.0188, loss: 0.0349 ||:  38%|###8      | 25/65 [04:53<07:48, 11.72s/it]
2022-04-09 00:07:00,432 - INFO - tqdm - accuracy: 0.9916, batch_loss: 0.0168, loss: 0.0342 ||:  40%|####      | 26/65 [05:05<07:37, 11.72s/it]
2022-04-09 00:07:12,162 - INFO - tqdm - accuracy: 0.9907, batch_loss: 0.0364, loss: 0.0343 ||:  42%|####1     | 27/65 [05:17<07:25, 11.72s/it]
2022-04-09 00:07:23,887 - INFO - tqdm - accuracy: 0.9911, batch_loss: 0.0200, loss: 0.0338 ||:  43%|####3     | 28/65 [05:28<07:13, 11.72s/it]
2022-04-09 00:07:35,648 - INFO - tqdm - accuracy: 0.9903, batch_loss: 0.0959, loss: 0.0360 ||:  45%|####4     | 29/65 [05:40<07:02, 11.73s/it]
2022-04-09 00:07:47,379 - INFO - tqdm - accuracy: 0.9906, batch_loss: 0.0124, loss: 0.0352 ||:  46%|####6     | 30/65 [05:52<06:50, 11.73s/it]
2022-04-09 00:07:59,123 - INFO - tqdm - accuracy: 0.9909, batch_loss: 0.0011, loss: 0.0341 ||:  48%|####7     | 31/65 [06:03<06:39, 11.74s/it]
2022-04-09 00:08:10,881 - INFO - tqdm - accuracy: 0.9912, batch_loss: 0.0070, loss: 0.0332 ||:  49%|####9     | 32/65 [06:15<06:27, 11.74s/it]
2022-04-09 00:08:22,647 - INFO - tqdm - accuracy: 0.9915, batch_loss: 0.0017, loss: 0.0323 ||:  51%|#####     | 33/65 [06:27<06:16, 11.75s/it]
2022-04-09 00:08:34,402 - INFO - tqdm - accuracy: 0.9917, batch_loss: 0.0024, loss: 0.0314 ||:  52%|#####2    | 34/65 [06:39<06:04, 11.75s/it]
2022-04-09 00:08:46,183 - INFO - tqdm - accuracy: 0.9920, batch_loss: 0.0162, loss: 0.0310 ||:  54%|#####3    | 35/65 [06:51<05:52, 11.76s/it]
2022-04-09 00:08:57,953 - INFO - tqdm - accuracy: 0.9922, batch_loss: 0.0179, loss: 0.0306 ||:  55%|#####5    | 36/65 [07:02<05:41, 11.76s/it]
2022-04-09 00:09:09,741 - INFO - tqdm - accuracy: 0.9924, batch_loss: 0.0015, loss: 0.0298 ||:  57%|#####6    | 37/65 [07:14<05:29, 11.77s/it]
2022-04-09 00:09:21,519 - INFO - tqdm - accuracy: 0.9926, batch_loss: 0.0053, loss: 0.0292 ||:  58%|#####8    | 38/65 [07:26<05:17, 11.77s/it]
2022-04-09 00:09:33,300 - INFO - tqdm - accuracy: 0.9928, batch_loss: 0.0060, loss: 0.0286 ||:  60%|######    | 39/65 [07:38<05:06, 11.78s/it]
2022-04-09 00:09:45,091 - INFO - tqdm - accuracy: 0.9930, batch_loss: 0.0085, loss: 0.0281 ||:  62%|######1   | 40/65 [07:49<04:54, 11.78s/it]
2022-04-09 00:09:56,876 - INFO - tqdm - accuracy: 0.9924, batch_loss: 0.0917, loss: 0.0296 ||:  63%|######3   | 41/65 [08:01<04:42, 11.78s/it]
2022-04-09 00:10:08,677 - INFO - tqdm - accuracy: 0.9926, batch_loss: 0.0004, loss: 0.0289 ||:  65%|######4   | 42/65 [08:13<04:31, 11.79s/it]
2022-04-09 00:10:20,336 - INFO - tqdm - accuracy: 0.9927, batch_loss: 0.0017, loss: 0.0283 ||:  66%|######6   | 43/65 [08:25<04:18, 11.75s/it]
2022-04-09 00:10:32,127 - INFO - tqdm - accuracy: 0.9929, batch_loss: 0.0033, loss: 0.0277 ||:  68%|######7   | 44/65 [08:36<04:06, 11.76s/it]
2022-04-09 00:10:43,571 - INFO - tqdm - accuracy: 0.9931, batch_loss: 0.0217, loss: 0.0276 ||:  69%|######9   | 45/65 [08:48<03:53, 11.67s/it]
2022-04-09 00:10:55,357 - INFO - tqdm - accuracy: 0.9932, batch_loss: 0.0262, loss: 0.0276 ||:  71%|#######   | 46/65 [09:00<03:42, 11.70s/it]
2022-04-09 00:11:07,129 - INFO - tqdm - accuracy: 0.9933, batch_loss: 0.0110, loss: 0.0272 ||:  72%|#######2  | 47/65 [09:11<03:31, 11.72s/it]
2022-04-09 00:11:18,882 - INFO - tqdm - accuracy: 0.9922, batch_loss: 0.0985, loss: 0.0287 ||:  74%|#######3  | 48/65 [09:23<03:19, 11.73s/it]
2022-04-09 00:11:30,618 - INFO - tqdm - accuracy: 0.9923, batch_loss: 0.0021, loss: 0.0281 ||:  75%|#######5  | 49/65 [09:35<03:07, 11.73s/it]
2022-04-09 00:11:42,337 - INFO - tqdm - accuracy: 0.9925, batch_loss: 0.0006, loss: 0.0276 ||:  77%|#######6  | 50/65 [09:47<02:55, 11.73s/it]
2022-04-09 00:11:54,025 - INFO - tqdm - accuracy: 0.9926, batch_loss: 0.0063, loss: 0.0272 ||:  78%|#######8  | 51/65 [09:58<02:44, 11.72s/it]
2022-04-09 00:12:05,741 - INFO - tqdm - accuracy: 0.9928, batch_loss: 0.0005, loss: 0.0267 ||:  80%|########  | 52/65 [10:10<02:32, 11.72s/it]
2022-04-09 00:12:17,423 - INFO - tqdm - accuracy: 0.9929, batch_loss: 0.0007, loss: 0.0262 ||:  82%|########1 | 53/65 [10:22<02:20, 11.71s/it]
2022-04-09 00:12:29,150 - INFO - tqdm - accuracy: 0.9931, batch_loss: 0.0007, loss: 0.0257 ||:  83%|########3 | 54/65 [10:33<02:08, 11.71s/it]
2022-04-09 00:12:40,883 - INFO - tqdm - accuracy: 0.9932, batch_loss: 0.0006, loss: 0.0252 ||:  85%|########4 | 55/65 [10:45<01:57, 11.72s/it]
2022-04-09 00:12:52,596 - INFO - tqdm - accuracy: 0.9933, batch_loss: 0.0041, loss: 0.0249 ||:  86%|########6 | 56/65 [10:57<01:45, 11.72s/it]
2022-04-09 00:13:04,030 - INFO - tqdm - accuracy: 0.9934, batch_loss: 0.0032, loss: 0.0245 ||:  88%|########7 | 57/65 [11:08<01:33, 11.63s/it]
2022-04-09 00:13:15,736 - INFO - tqdm - accuracy: 0.9935, batch_loss: 0.0035, loss: 0.0241 ||:  89%|########9 | 58/65 [11:20<01:21, 11.65s/it]
2022-04-09 00:13:27,452 - INFO - tqdm - accuracy: 0.9936, batch_loss: 0.0053, loss: 0.0238 ||:  91%|######### | 59/65 [11:32<01:10, 11.67s/it]
2022-04-09 00:13:39,197 - INFO - tqdm - accuracy: 0.9932, batch_loss: 0.0767, loss: 0.0247 ||:  92%|#########2| 60/65 [11:44<00:58, 11.69s/it]
2022-04-09 00:13:50,941 - INFO - tqdm - accuracy: 0.9933, batch_loss: 0.0012, loss: 0.0243 ||:  94%|#########3| 61/65 [11:55<00:46, 11.71s/it]
2022-04-09 00:14:02,665 - INFO - tqdm - accuracy: 0.9934, batch_loss: 0.0023, loss: 0.0240 ||:  95%|#########5| 62/65 [12:07<00:35, 11.71s/it]
2022-04-09 00:14:14,413 - INFO - tqdm - accuracy: 0.9935, batch_loss: 0.0041, loss: 0.0236 ||:  97%|#########6| 63/65 [12:19<00:23, 11.72s/it]
2022-04-09 00:14:26,155 - INFO - tqdm - accuracy: 0.9936, batch_loss: 0.0022, loss: 0.0233 ||:  98%|#########8| 64/65 [12:30<00:11, 11.73s/it]
2022-04-09 00:14:31,353 - INFO - tqdm - accuracy: 0.9937, batch_loss: 0.0019, loss: 0.0230 ||: 100%|##########| 65/65 [12:36<00:00,  9.77s/it]
2022-04-09 00:14:31,354 - INFO - tqdm - accuracy: 0.9937, batch_loss: 0.0019, loss: 0.0230 ||: 100%|##########| 65/65 [12:36<00:00, 11.63s/it]
2022-04-09 00:14:34,249 - INFO - allennlp.training.trainer - Validating
2022-04-09 00:14:34,251 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-09 00:14:44,319 - INFO - tqdm - accuracy: 0.5256, batch_loss: 0.0377, loss: 2.9611 ||:  34%|###3      | 39/115 [00:10<00:19,  3.87it/s]
2022-04-09 00:14:54,527 - INFO - tqdm - accuracy: 0.5833, batch_loss: 1.5538, loss: 2.4989 ||:  68%|######7   | 78/115 [00:20<00:09,  3.80it/s]
2022-04-09 00:15:04,030 - INFO - tqdm - accuracy: 0.6070, batch_loss: 0.0044, loss: 2.3336 ||: 100%|##########| 115/115 [00:29<00:00,  3.81it/s]
2022-04-09 00:15:04,030 - INFO - tqdm - accuracy: 0.6070, batch_loss: 0.0044, loss: 2.3336 ||: 100%|##########| 115/115 [00:29<00:00,  3.86it/s]
2022-04-09 00:15:04,030 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-09 00:15:04,031 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.994  |     0.607
2022-04-09 00:15:04,032 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-09 00:15:04,033 - INFO - allennlp.training.tensorboard_writer - loss               |     0.023  |     2.334
2022-04-09 00:15:04,033 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5799.516  |       N/A
2022-04-09 00:15:09,971 - INFO - allennlp.training.trainer - Epoch duration: 0:13:14.813143
2022-04-09 00:15:09,971 - INFO - allennlp.training.trainer - Estimated training time remaining: 0:39:51
2022-04-09 00:15:09,971 - INFO - allennlp.training.trainer - Epoch 12/14
2022-04-09 00:15:09,972 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-09 00:15:09,972 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-09 00:15:09,973 - INFO - allennlp.training.trainer - Training
2022-04-09 00:15:09,974 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-09 00:15:21,563 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0084, loss: 0.0084 ||:   2%|1         | 1/65 [00:11<12:21, 11.59s/it]
2022-04-09 00:15:33,450 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0067, loss: 0.0075 ||:   3%|3         | 2/65 [00:23<12:21, 11.76s/it]
2022-04-09 00:15:45,334 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0009, loss: 0.0053 ||:   5%|4         | 3/65 [00:35<12:12, 11.82s/it]
2022-04-09 00:15:57,022 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0233, loss: 0.0098 ||:   6%|6         | 4/65 [00:47<11:57, 11.77s/it]
2022-04-09 00:16:08,643 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0017, loss: 0.0082 ||:   8%|7         | 5/65 [00:58<11:42, 11.71s/it]
2022-04-09 00:16:20,332 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0113, loss: 0.0087 ||:   9%|9         | 6/65 [01:10<11:30, 11.71s/it]
2022-04-09 00:16:32,089 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0044, loss: 0.0081 ||:  11%|#         | 7/65 [01:22<11:19, 11.72s/it]
2022-04-09 00:16:43,910 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0017, loss: 0.0073 ||:  12%|#2        | 8/65 [01:33<11:09, 11.75s/it]
2022-04-09 00:16:55,684 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0011, loss: 0.0066 ||:  14%|#3        | 9/65 [01:45<10:58, 11.76s/it]
2022-04-09 00:17:07,441 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0007, loss: 0.0060 ||:  15%|#5        | 10/65 [01:57<10:46, 11.76s/it]
2022-04-09 00:17:19,155 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0018, loss: 0.0056 ||:  17%|#6        | 11/65 [02:09<10:34, 11.75s/it]
2022-04-09 00:17:30,845 - INFO - tqdm - accuracy: 0.9974, batch_loss: 0.0450, loss: 0.0089 ||:  18%|#8        | 12/65 [02:20<10:21, 11.73s/it]
2022-04-09 00:17:42,195 - INFO - tqdm - accuracy: 0.9976, batch_loss: 0.0007, loss: 0.0083 ||:  20%|##        | 13/65 [02:32<10:03, 11.61s/it]
2022-04-09 00:17:53,912 - INFO - tqdm - accuracy: 0.9978, batch_loss: 0.0060, loss: 0.0081 ||:  22%|##1       | 14/65 [02:43<09:53, 11.64s/it]
2022-04-09 00:18:05,638 - INFO - tqdm - accuracy: 0.9979, batch_loss: 0.0016, loss: 0.0077 ||:  23%|##3       | 15/65 [02:55<09:43, 11.67s/it]
2022-04-09 00:18:17,391 - INFO - tqdm - accuracy: 0.9980, batch_loss: 0.0011, loss: 0.0073 ||:  25%|##4       | 16/65 [03:07<09:33, 11.69s/it]
2022-04-09 00:18:29,147 - INFO - tqdm - accuracy: 0.9963, batch_loss: 0.0691, loss: 0.0109 ||:  26%|##6       | 17/65 [03:19<09:22, 11.71s/it]
2022-04-09 00:18:40,917 - INFO - tqdm - accuracy: 0.9965, batch_loss: 0.0005, loss: 0.0103 ||:  28%|##7       | 18/65 [03:30<09:11, 11.73s/it]
2022-04-09 00:18:52,693 - INFO - tqdm - accuracy: 0.9967, batch_loss: 0.0198, loss: 0.0108 ||:  29%|##9       | 19/65 [03:42<09:00, 11.74s/it]
2022-04-09 00:19:05,007 - INFO - tqdm - accuracy: 0.9969, batch_loss: 0.0031, loss: 0.0104 ||:  31%|###       | 20/65 [03:55<08:56, 11.91s/it]
2022-04-09 00:19:16,702 - INFO - tqdm - accuracy: 0.9970, batch_loss: 0.0015, loss: 0.0100 ||:  32%|###2      | 21/65 [04:06<08:41, 11.85s/it]
2022-04-09 00:19:28,455 - INFO - tqdm - accuracy: 0.9957, batch_loss: 0.0366, loss: 0.0112 ||:  34%|###3      | 22/65 [04:18<08:28, 11.82s/it]
2022-04-09 00:19:40,182 - INFO - tqdm - accuracy: 0.9959, batch_loss: 0.0116, loss: 0.0112 ||:  35%|###5      | 23/65 [04:30<08:15, 11.79s/it]
2022-04-09 00:19:51,890 - INFO - tqdm - accuracy: 0.9961, batch_loss: 0.0011, loss: 0.0108 ||:  37%|###6      | 24/65 [04:41<08:02, 11.77s/it]
2022-04-09 00:20:03,331 - INFO - tqdm - accuracy: 0.9962, batch_loss: 0.0059, loss: 0.0106 ||:  38%|###8      | 25/65 [04:53<07:46, 11.67s/it]
2022-04-09 00:20:15,021 - INFO - tqdm - accuracy: 0.9964, batch_loss: 0.0007, loss: 0.0102 ||:  40%|####      | 26/65 [05:05<07:35, 11.68s/it]
2022-04-09 00:20:26,710 - INFO - tqdm - accuracy: 0.9965, batch_loss: 0.0016, loss: 0.0099 ||:  42%|####1     | 27/65 [05:16<07:23, 11.68s/it]
2022-04-09 00:20:38,399 - INFO - tqdm - accuracy: 0.9966, batch_loss: 0.0018, loss: 0.0096 ||:  43%|####3     | 28/65 [05:28<07:12, 11.68s/it]
2022-04-09 00:20:50,101 - INFO - tqdm - accuracy: 0.9957, batch_loss: 0.0470, loss: 0.0109 ||:  45%|####4     | 29/65 [05:40<07:00, 11.69s/it]
2022-04-09 00:21:01,842 - INFO - tqdm - accuracy: 0.9958, batch_loss: 0.0014, loss: 0.0106 ||:  46%|####6     | 30/65 [05:51<06:49, 11.70s/it]
2022-04-09 00:21:13,613 - INFO - tqdm - accuracy: 0.9960, batch_loss: 0.0005, loss: 0.0103 ||:  48%|####7     | 31/65 [06:03<06:38, 11.72s/it]
2022-04-09 00:21:25,275 - INFO - tqdm - accuracy: 0.9961, batch_loss: 0.0240, loss: 0.0107 ||:  49%|####9     | 32/65 [06:15<06:26, 11.71s/it]
2022-04-09 00:21:37,074 - INFO - tqdm - accuracy: 0.9962, batch_loss: 0.0070, loss: 0.0106 ||:  51%|#####     | 33/65 [06:27<06:15, 11.73s/it]
2022-04-09 00:21:48,840 - INFO - tqdm - accuracy: 0.9963, batch_loss: 0.0010, loss: 0.0103 ||:  52%|#####2    | 34/65 [06:38<06:04, 11.74s/it]
2022-04-09 00:22:00,602 - INFO - tqdm - accuracy: 0.9964, batch_loss: 0.0056, loss: 0.0102 ||:  54%|#####3    | 35/65 [06:50<05:52, 11.75s/it]
2022-04-09 00:22:12,361 - INFO - tqdm - accuracy: 0.9965, batch_loss: 0.0242, loss: 0.0106 ||:  55%|#####5    | 36/65 [07:02<05:40, 11.75s/it]
2022-04-09 00:22:24,126 - INFO - tqdm - accuracy: 0.9966, batch_loss: 0.0024, loss: 0.0103 ||:  57%|#####6    | 37/65 [07:14<05:29, 11.76s/it]
2022-04-09 00:22:35,895 - INFO - tqdm - accuracy: 0.9959, batch_loss: 0.0426, loss: 0.0112 ||:  58%|#####8    | 38/65 [07:25<05:17, 11.76s/it]
2022-04-09 00:22:47,685 - INFO - tqdm - accuracy: 0.9960, batch_loss: 0.0050, loss: 0.0110 ||:  60%|######    | 39/65 [07:37<05:05, 11.77s/it]
2022-04-09 00:22:59,489 - INFO - tqdm - accuracy: 0.9961, batch_loss: 0.0186, loss: 0.0112 ||:  62%|######1   | 40/65 [07:49<04:54, 11.78s/it]
2022-04-09 00:23:11,269 - INFO - tqdm - accuracy: 0.9962, batch_loss: 0.0046, loss: 0.0111 ||:  63%|######3   | 41/65 [08:01<04:42, 11.78s/it]
2022-04-09 00:23:23,047 - INFO - tqdm - accuracy: 0.9963, batch_loss: 0.0025, loss: 0.0109 ||:  65%|######4   | 42/65 [08:13<04:30, 11.78s/it]
2022-04-09 00:23:34,817 - INFO - tqdm - accuracy: 0.9964, batch_loss: 0.0007, loss: 0.0106 ||:  66%|######6   | 43/65 [08:24<04:19, 11.78s/it]
2022-04-09 00:23:46,560 - INFO - tqdm - accuracy: 0.9964, batch_loss: 0.0007, loss: 0.0104 ||:  68%|######7   | 44/65 [08:36<04:07, 11.77s/it]
2022-04-09 00:23:58,300 - INFO - tqdm - accuracy: 0.9965, batch_loss: 0.0010, loss: 0.0102 ||:  69%|######9   | 45/65 [08:48<03:55, 11.76s/it]
2022-04-09 00:24:10,012 - INFO - tqdm - accuracy: 0.9966, batch_loss: 0.0016, loss: 0.0100 ||:  71%|#######   | 46/65 [09:00<03:43, 11.74s/it]
2022-04-09 00:24:21,746 - INFO - tqdm - accuracy: 0.9967, batch_loss: 0.0007, loss: 0.0098 ||:  72%|#######2  | 47/65 [09:11<03:31, 11.74s/it]
2022-04-09 00:24:33,456 - INFO - tqdm - accuracy: 0.9967, batch_loss: 0.0178, loss: 0.0100 ||:  74%|#######3  | 48/65 [09:23<03:19, 11.73s/it]
2022-04-09 00:24:45,162 - INFO - tqdm - accuracy: 0.9968, batch_loss: 0.0005, loss: 0.0098 ||:  75%|#######5  | 49/65 [09:35<03:07, 11.72s/it]
2022-04-09 00:24:56,867 - INFO - tqdm - accuracy: 0.9969, batch_loss: 0.0006, loss: 0.0096 ||:  77%|#######6  | 50/65 [09:46<02:55, 11.72s/it]
2022-04-09 00:25:08,594 - INFO - tqdm - accuracy: 0.9969, batch_loss: 0.0114, loss: 0.0096 ||:  78%|#######8  | 51/65 [09:58<02:44, 11.72s/it]
2022-04-09 00:25:20,337 - INFO - tqdm - accuracy: 0.9970, batch_loss: 0.0019, loss: 0.0095 ||:  80%|########  | 52/65 [10:10<02:32, 11.73s/it]
2022-04-09 00:25:32,072 - INFO - tqdm - accuracy: 0.9971, batch_loss: 0.0025, loss: 0.0093 ||:  82%|########1 | 53/65 [10:22<02:20, 11.73s/it]
2022-04-09 00:25:43,826 - INFO - tqdm - accuracy: 0.9971, batch_loss: 0.0014, loss: 0.0092 ||:  83%|########3 | 54/65 [10:33<02:09, 11.74s/it]
2022-04-09 00:25:55,592 - INFO - tqdm - accuracy: 0.9972, batch_loss: 0.0008, loss: 0.0090 ||:  85%|########4 | 55/65 [10:45<01:57, 11.75s/it]
2022-04-09 00:26:07,375 - INFO - tqdm - accuracy: 0.9972, batch_loss: 0.0065, loss: 0.0090 ||:  86%|########6 | 56/65 [10:57<01:45, 11.76s/it]
2022-04-09 00:26:19,181 - INFO - tqdm - accuracy: 0.9973, batch_loss: 0.0077, loss: 0.0090 ||:  88%|########7 | 57/65 [11:09<01:34, 11.77s/it]
2022-04-09 00:26:30,983 - INFO - tqdm - accuracy: 0.9973, batch_loss: 0.0007, loss: 0.0088 ||:  89%|########9 | 58/65 [11:21<01:22, 11.78s/it]
2022-04-09 00:26:42,786 - INFO - tqdm - accuracy: 0.9968, batch_loss: 0.0347, loss: 0.0093 ||:  91%|######### | 59/65 [11:32<01:10, 11.79s/it]
2022-04-09 00:26:54,566 - INFO - tqdm - accuracy: 0.9964, batch_loss: 0.0839, loss: 0.0105 ||:  92%|#########2| 60/65 [11:44<00:58, 11.79s/it]
2022-04-09 00:27:06,340 - INFO - tqdm - accuracy: 0.9964, batch_loss: 0.0003, loss: 0.0103 ||:  94%|#########3| 61/65 [11:56<00:47, 11.78s/it]
2022-04-09 00:27:18,108 - INFO - tqdm - accuracy: 0.9965, batch_loss: 0.0019, loss: 0.0102 ||:  95%|#########5| 62/65 [12:08<00:35, 11.78s/it]
2022-04-09 00:27:29,861 - INFO - tqdm - accuracy: 0.9960, batch_loss: 0.0442, loss: 0.0108 ||:  97%|#########6| 63/65 [12:19<00:23, 11.77s/it]
2022-04-09 00:27:41,489 - INFO - tqdm - accuracy: 0.9956, batch_loss: 0.0661, loss: 0.0116 ||:  98%|#########8| 64/65 [12:31<00:11, 11.73s/it]
2022-04-09 00:27:46,699 - INFO - tqdm - accuracy: 0.9956, batch_loss: 0.0138, loss: 0.0116 ||: 100%|##########| 65/65 [12:36<00:00,  9.77s/it]
2022-04-09 00:27:46,699 - INFO - tqdm - accuracy: 0.9956, batch_loss: 0.0138, loss: 0.0116 ||: 100%|##########| 65/65 [12:36<00:00, 11.64s/it]
2022-04-09 00:27:49,568 - INFO - allennlp.training.trainer - Validating
2022-04-09 00:27:49,569 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-09 00:27:59,691 - INFO - tqdm - accuracy: 0.6709, batch_loss: 0.0001, loss: 1.9187 ||:  35%|###4      | 40/115 [00:10<00:19,  3.90it/s]
2022-04-09 00:28:09,862 - INFO - tqdm - accuracy: 0.6115, batch_loss: 4.2952, loss: 2.3816 ||:  69%|######8   | 79/115 [00:20<00:09,  3.82it/s]
2022-04-09 00:28:19,302 - INFO - tqdm - accuracy: 0.6114, batch_loss: 2.6366, loss: 2.3478 ||: 100%|##########| 115/115 [00:29<00:00,  3.81it/s]
2022-04-09 00:28:19,302 - INFO - tqdm - accuracy: 0.6114, batch_loss: 2.6366, loss: 2.3478 ||: 100%|##########| 115/115 [00:29<00:00,  3.87it/s]
2022-04-09 00:28:19,302 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-09 00:28:19,303 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.996  |     0.611
2022-04-09 00:28:19,304 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-09 00:28:19,305 - INFO - allennlp.training.tensorboard_writer - loss               |     0.012  |     2.348
2022-04-09 00:28:19,305 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5799.516  |       N/A
2022-04-09 00:28:25,091 - INFO - allennlp.training.trainer - Epoch duration: 0:13:15.119984
2022-04-09 00:28:25,092 - INFO - allennlp.training.trainer - Estimated training time remaining: 0:26:33
2022-04-09 00:28:25,092 - INFO - allennlp.training.trainer - Epoch 13/14
2022-04-09 00:28:25,092 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-09 00:28:25,092 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-09 00:28:25,093 - INFO - allennlp.training.trainer - Training
2022-04-09 00:28:25,094 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-09 00:28:36,712 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0008, loss: 0.0008 ||:   2%|1         | 1/65 [00:11<12:23, 11.62s/it]
2022-04-09 00:28:48,610 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0014, loss: 0.0011 ||:   3%|3         | 2/65 [00:23<12:22, 11.78s/it]
2022-04-09 00:29:00,662 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0011, loss: 0.0011 ||:   5%|4         | 3/65 [00:35<12:18, 11.91s/it]
2022-04-09 00:29:12,430 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0038, loss: 0.0018 ||:   6%|6         | 4/65 [00:47<12:02, 11.85s/it]
2022-04-09 00:29:24,030 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0031, loss: 0.0020 ||:   8%|7         | 5/65 [00:58<11:45, 11.76s/it]
2022-04-09 00:29:35,631 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0008, loss: 0.0018 ||:   9%|9         | 6/65 [01:10<11:30, 11.71s/it]
2022-04-09 00:29:47,307 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0017, loss: 0.0018 ||:  11%|#         | 7/65 [01:22<11:18, 11.70s/it]
2022-04-09 00:29:59,075 - INFO - tqdm - accuracy: 0.9922, batch_loss: 0.1440, loss: 0.0196 ||:  12%|#2        | 8/65 [01:33<11:07, 11.72s/it]
2022-04-09 00:30:10,918 - INFO - tqdm - accuracy: 0.9931, batch_loss: 0.0016, loss: 0.0176 ||:  14%|#3        | 9/65 [01:45<10:58, 11.76s/it]
2022-04-09 00:30:22,699 - INFO - tqdm - accuracy: 0.9938, batch_loss: 0.0069, loss: 0.0165 ||:  15%|#5        | 10/65 [01:57<10:47, 11.77s/it]
2022-04-09 00:30:34,448 - INFO - tqdm - accuracy: 0.9943, batch_loss: 0.0005, loss: 0.0151 ||:  17%|#6        | 11/65 [02:09<10:35, 11.76s/it]
2022-04-09 00:30:46,152 - INFO - tqdm - accuracy: 0.9948, batch_loss: 0.0004, loss: 0.0138 ||:  18%|#8        | 12/65 [02:21<10:22, 11.74s/it]
2022-04-09 00:30:57,837 - INFO - tqdm - accuracy: 0.9952, batch_loss: 0.0017, loss: 0.0129 ||:  20%|##        | 13/65 [02:32<10:09, 11.73s/it]
2022-04-09 00:31:09,553 - INFO - tqdm - accuracy: 0.9955, batch_loss: 0.0027, loss: 0.0122 ||:  22%|##1       | 14/65 [02:44<09:57, 11.72s/it]
2022-04-09 00:31:21,313 - INFO - tqdm - accuracy: 0.9958, batch_loss: 0.0008, loss: 0.0114 ||:  23%|##3       | 15/65 [02:56<09:46, 11.73s/it]
2022-04-09 00:31:33,109 - INFO - tqdm - accuracy: 0.9961, batch_loss: 0.0081, loss: 0.0112 ||:  25%|##4       | 16/65 [03:08<09:35, 11.75s/it]
2022-04-09 00:31:44,921 - INFO - tqdm - accuracy: 0.9963, batch_loss: 0.0009, loss: 0.0106 ||:  26%|##6       | 17/65 [03:19<09:24, 11.77s/it]
2022-04-09 00:31:56,356 - INFO - tqdm - accuracy: 0.9965, batch_loss: 0.0022, loss: 0.0101 ||:  28%|##7       | 18/65 [03:31<09:08, 11.67s/it]
2022-04-09 00:32:08,116 - INFO - tqdm - accuracy: 0.9951, batch_loss: 0.0238, loss: 0.0109 ||:  29%|##9       | 19/65 [03:43<08:58, 11.70s/it]
2022-04-09 00:32:19,853 - INFO - tqdm - accuracy: 0.9953, batch_loss: 0.0011, loss: 0.0104 ||:  31%|###       | 20/65 [03:54<08:46, 11.71s/it]
2022-04-09 00:32:31,585 - INFO - tqdm - accuracy: 0.9955, batch_loss: 0.0014, loss: 0.0099 ||:  32%|###2      | 21/65 [04:06<08:35, 11.72s/it]
2022-04-09 00:32:43,294 - INFO - tqdm - accuracy: 0.9957, batch_loss: 0.0064, loss: 0.0098 ||:  34%|###3      | 22/65 [04:18<08:23, 11.71s/it]
2022-04-09 00:32:55,000 - INFO - tqdm - accuracy: 0.9959, batch_loss: 0.0010, loss: 0.0094 ||:  35%|###5      | 23/65 [04:29<08:11, 11.71s/it]
2022-04-09 00:33:06,682 - INFO - tqdm - accuracy: 0.9961, batch_loss: 0.0021, loss: 0.0091 ||:  37%|###6      | 24/65 [04:41<07:59, 11.70s/it]
2022-04-09 00:33:18,446 - INFO - tqdm - accuracy: 0.9950, batch_loss: 0.0384, loss: 0.0103 ||:  38%|###8      | 25/65 [04:53<07:48, 11.72s/it]
2022-04-09 00:33:30,198 - INFO - tqdm - accuracy: 0.9952, batch_loss: 0.0051, loss: 0.0101 ||:  40%|####      | 26/65 [05:05<07:37, 11.73s/it]
2022-04-09 00:33:41,951 - INFO - tqdm - accuracy: 0.9954, batch_loss: 0.0010, loss: 0.0097 ||:  42%|####1     | 27/65 [05:16<07:26, 11.74s/it]
2022-04-09 00:33:53,740 - INFO - tqdm - accuracy: 0.9955, batch_loss: 0.0214, loss: 0.0101 ||:  43%|####3     | 28/65 [05:28<07:14, 11.75s/it]
2022-04-09 00:34:05,548 - INFO - tqdm - accuracy: 0.9957, batch_loss: 0.0026, loss: 0.0099 ||:  45%|####4     | 29/65 [05:40<07:03, 11.77s/it]
2022-04-09 00:34:17,368 - INFO - tqdm - accuracy: 0.9958, batch_loss: 0.0014, loss: 0.0096 ||:  46%|####6     | 30/65 [05:52<06:52, 11.78s/it]
2022-04-09 00:34:29,151 - INFO - tqdm - accuracy: 0.9950, batch_loss: 0.0559, loss: 0.0111 ||:  48%|####7     | 31/65 [06:04<06:40, 11.78s/it]
2022-04-09 00:34:40,850 - INFO - tqdm - accuracy: 0.9951, batch_loss: 0.0029, loss: 0.0108 ||:  49%|####9     | 32/65 [06:15<06:28, 11.76s/it]
2022-04-09 00:34:52,524 - INFO - tqdm - accuracy: 0.9953, batch_loss: 0.0009, loss: 0.0105 ||:  51%|#####     | 33/65 [06:27<06:15, 11.73s/it]
2022-04-09 00:35:04,102 - INFO - tqdm - accuracy: 0.9954, batch_loss: 0.0045, loss: 0.0104 ||:  52%|#####2    | 34/65 [06:39<06:02, 11.69s/it]
2022-04-09 00:35:15,822 - INFO - tqdm - accuracy: 0.9955, batch_loss: 0.0026, loss: 0.0101 ||:  54%|#####3    | 35/65 [06:50<05:50, 11.70s/it]
2022-04-09 00:35:27,544 - INFO - tqdm - accuracy: 0.9957, batch_loss: 0.0214, loss: 0.0105 ||:  55%|#####5    | 36/65 [07:02<05:39, 11.70s/it]
2022-04-09 00:35:39,287 - INFO - tqdm - accuracy: 0.9958, batch_loss: 0.0005, loss: 0.0102 ||:  57%|#####6    | 37/65 [07:14<05:28, 11.72s/it]
2022-04-09 00:35:51,058 - INFO - tqdm - accuracy: 0.9959, batch_loss: 0.0084, loss: 0.0101 ||:  58%|#####8    | 38/65 [07:25<05:16, 11.73s/it]
2022-04-09 00:36:02,820 - INFO - tqdm - accuracy: 0.9960, batch_loss: 0.0010, loss: 0.0099 ||:  60%|######    | 39/65 [07:37<05:05, 11.74s/it]
2022-04-09 00:36:14,581 - INFO - tqdm - accuracy: 0.9961, batch_loss: 0.0122, loss: 0.0100 ||:  62%|######1   | 40/65 [07:49<04:53, 11.75s/it]
2022-04-09 00:36:26,342 - INFO - tqdm - accuracy: 0.9962, batch_loss: 0.0072, loss: 0.0099 ||:  63%|######3   | 41/65 [08:01<04:42, 11.75s/it]
2022-04-09 00:36:38,107 - INFO - tqdm - accuracy: 0.9963, batch_loss: 0.0003, loss: 0.0097 ||:  65%|######4   | 42/65 [08:13<04:30, 11.76s/it]
2022-04-09 00:36:49,875 - INFO - tqdm - accuracy: 0.9964, batch_loss: 0.0020, loss: 0.0095 ||:  66%|######6   | 43/65 [08:24<04:18, 11.76s/it]
2022-04-09 00:37:01,637 - INFO - tqdm - accuracy: 0.9964, batch_loss: 0.0013, loss: 0.0093 ||:  68%|######7   | 44/65 [08:36<04:06, 11.76s/it]
2022-04-09 00:37:13,420 - INFO - tqdm - accuracy: 0.9958, batch_loss: 0.0299, loss: 0.0098 ||:  69%|######9   | 45/65 [08:48<03:55, 11.77s/it]
2022-04-09 00:37:24,929 - INFO - tqdm - accuracy: 0.9959, batch_loss: 0.0012, loss: 0.0096 ||:  71%|#######   | 46/65 [08:59<03:42, 11.69s/it]
2022-04-09 00:37:36,710 - INFO - tqdm - accuracy: 0.9960, batch_loss: 0.0036, loss: 0.0094 ||:  72%|#######2  | 47/65 [09:11<03:30, 11.72s/it]
2022-04-09 00:37:48,514 - INFO - tqdm - accuracy: 0.9961, batch_loss: 0.0014, loss: 0.0093 ||:  74%|#######3  | 48/65 [09:23<03:19, 11.74s/it]
2022-04-09 00:38:00,291 - INFO - tqdm - accuracy: 0.9962, batch_loss: 0.0119, loss: 0.0093 ||:  75%|#######5  | 49/65 [09:35<03:08, 11.75s/it]
2022-04-09 00:38:11,916 - INFO - tqdm - accuracy: 0.9956, batch_loss: 0.0887, loss: 0.0109 ||:  77%|#######6  | 50/65 [09:46<02:55, 11.71s/it]
2022-04-09 00:38:23,667 - INFO - tqdm - accuracy: 0.9957, batch_loss: 0.0047, loss: 0.0108 ||:  78%|#######8  | 51/65 [09:58<02:44, 11.73s/it]
2022-04-09 00:38:35,404 - INFO - tqdm - accuracy: 0.9958, batch_loss: 0.0010, loss: 0.0106 ||:  80%|########  | 52/65 [10:10<02:32, 11.73s/it]
2022-04-09 00:38:47,134 - INFO - tqdm - accuracy: 0.9953, batch_loss: 0.0559, loss: 0.0115 ||:  82%|########1 | 53/65 [10:22<02:20, 11.73s/it]
2022-04-09 00:38:58,859 - INFO - tqdm - accuracy: 0.9954, batch_loss: 0.0024, loss: 0.0113 ||:  83%|########3 | 54/65 [10:33<02:09, 11.73s/it]
2022-04-09 00:39:11,049 - INFO - tqdm - accuracy: 0.9955, batch_loss: 0.0019, loss: 0.0111 ||:  85%|########4 | 55/65 [10:45<01:58, 11.87s/it]
2022-04-09 00:39:22,730 - INFO - tqdm - accuracy: 0.9955, batch_loss: 0.0015, loss: 0.0110 ||:  86%|########6 | 56/65 [10:57<01:46, 11.81s/it]
2022-04-09 00:39:34,492 - INFO - tqdm - accuracy: 0.9951, batch_loss: 0.1670, loss: 0.0137 ||:  88%|########7 | 57/65 [11:09<01:34, 11.80s/it]
2022-04-09 00:39:46,251 - INFO - tqdm - accuracy: 0.9951, batch_loss: 0.0007, loss: 0.0135 ||:  89%|########9 | 58/65 [11:21<01:22, 11.79s/it]
2022-04-09 00:39:58,000 - INFO - tqdm - accuracy: 0.9952, batch_loss: 0.0018, loss: 0.0133 ||:  91%|######### | 59/65 [11:32<01:10, 11.77s/it]
2022-04-09 00:40:09,723 - INFO - tqdm - accuracy: 0.9953, batch_loss: 0.0031, loss: 0.0131 ||:  92%|#########2| 60/65 [11:44<00:58, 11.76s/it]
2022-04-09 00:40:21,446 - INFO - tqdm - accuracy: 0.9954, batch_loss: 0.0008, loss: 0.0129 ||:  94%|#########3| 61/65 [11:56<00:46, 11.75s/it]
2022-04-09 00:40:33,194 - INFO - tqdm - accuracy: 0.9955, batch_loss: 0.0016, loss: 0.0127 ||:  95%|#########5| 62/65 [12:08<00:35, 11.75s/it]
2022-04-09 00:40:44,889 - INFO - tqdm - accuracy: 0.9950, batch_loss: 0.1182, loss: 0.0144 ||:  97%|#########6| 63/65 [12:19<00:23, 11.73s/it]
2022-04-09 00:40:56,589 - INFO - tqdm - accuracy: 0.9951, batch_loss: 0.0004, loss: 0.0142 ||:  98%|#########8| 64/65 [12:31<00:11, 11.72s/it]
2022-04-09 00:41:01,781 - INFO - tqdm - accuracy: 0.9951, batch_loss: 0.0147, loss: 0.0142 ||: 100%|##########| 65/65 [12:36<00:00,  9.76s/it]
2022-04-09 00:41:01,781 - INFO - tqdm - accuracy: 0.9951, batch_loss: 0.0147, loss: 0.0142 ||: 100%|##########| 65/65 [12:36<00:00, 11.64s/it]
2022-04-09 00:41:04,775 - INFO - allennlp.training.trainer - Validating
2022-04-09 00:41:04,777 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-09 00:41:14,905 - INFO - tqdm - accuracy: 0.6962, batch_loss: 0.4777, loss: 1.8750 ||:  35%|###4      | 40/115 [00:10<00:19,  3.89it/s]
2022-04-09 00:41:25,047 - INFO - tqdm - accuracy: 0.6242, batch_loss: 10.2633, loss: 2.3806 ||:  69%|######8   | 79/115 [00:20<00:09,  3.79it/s]
2022-04-09 00:41:34,439 - INFO - tqdm - accuracy: 0.6157, batch_loss: 1.8343, loss: 2.3570 ||: 100%|##########| 115/115 [00:29<00:00,  3.80it/s]
2022-04-09 00:41:34,439 - INFO - tqdm - accuracy: 0.6157, batch_loss: 1.8343, loss: 2.3570 ||: 100%|##########| 115/115 [00:29<00:00,  3.88it/s]
2022-04-09 00:41:34,439 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-09 00:41:34,440 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.995  |     0.616
2022-04-09 00:41:34,440 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-09 00:41:34,441 - INFO - allennlp.training.tensorboard_writer - loss               |     0.014  |     2.357
2022-04-09 00:41:34,441 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5799.516  |       N/A
2022-04-09 00:41:40,572 - INFO - allennlp.training.trainer - Epoch duration: 0:13:15.479672
2022-04-09 00:41:40,572 - INFO - allennlp.training.trainer - Estimated training time remaining: 0:13:16
2022-04-09 00:41:40,572 - INFO - allennlp.training.trainer - Epoch 14/14
2022-04-09 00:41:40,572 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-09 00:41:40,572 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-09 00:41:40,573 - INFO - allennlp.training.trainer - Training
2022-04-09 00:41:40,574 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-09 00:41:52,204 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0009, loss: 0.0009 ||:   2%|1         | 1/65 [00:11<12:24, 11.63s/it]
2022-04-09 00:42:04,074 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0004, loss: 0.0007 ||:   3%|3         | 2/65 [00:23<12:21, 11.77s/it]
2022-04-09 00:42:16,068 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0005, loss: 0.0006 ||:   5%|4         | 3/65 [00:35<12:16, 11.87s/it]
2022-04-09 00:42:27,828 - INFO - tqdm - accuracy: 0.9922, batch_loss: 0.0412, loss: 0.0108 ||:   6%|6         | 4/65 [00:47<12:01, 11.83s/it]
2022-04-09 00:42:39,497 - INFO - tqdm - accuracy: 0.9938, batch_loss: 0.0158, loss: 0.0118 ||:   8%|7         | 5/65 [00:58<11:46, 11.77s/it]
2022-04-09 00:42:51,112 - INFO - tqdm - accuracy: 0.9948, batch_loss: 0.0018, loss: 0.0101 ||:   9%|9         | 6/65 [01:10<11:31, 11.72s/it]
2022-04-09 00:43:02,830 - INFO - tqdm - accuracy: 0.9955, batch_loss: 0.0015, loss: 0.0089 ||:  11%|#         | 7/65 [01:22<11:19, 11.72s/it]
2022-04-09 00:43:14,639 - INFO - tqdm - accuracy: 0.9961, batch_loss: 0.0008, loss: 0.0079 ||:  12%|#2        | 8/65 [01:34<11:09, 11.75s/it]
2022-04-09 00:43:26,483 - INFO - tqdm - accuracy: 0.9965, batch_loss: 0.0016, loss: 0.0072 ||:  14%|#3        | 9/65 [01:45<10:59, 11.78s/it]
2022-04-09 00:43:38,270 - INFO - tqdm - accuracy: 0.9969, batch_loss: 0.0006, loss: 0.0065 ||:  15%|#5        | 10/65 [01:57<10:47, 11.78s/it]
2022-04-09 00:43:49,982 - INFO - tqdm - accuracy: 0.9972, batch_loss: 0.0007, loss: 0.0060 ||:  17%|#6        | 11/65 [02:09<10:35, 11.76s/it]
2022-04-09 00:44:01,696 - INFO - tqdm - accuracy: 0.9974, batch_loss: 0.0014, loss: 0.0056 ||:  18%|#8        | 12/65 [02:21<10:22, 11.75s/it]
2022-04-09 00:44:13,379 - INFO - tqdm - accuracy: 0.9976, batch_loss: 0.0011, loss: 0.0053 ||:  20%|##        | 13/65 [02:32<10:09, 11.73s/it]
2022-04-09 00:44:25,082 - INFO - tqdm - accuracy: 0.9978, batch_loss: 0.0030, loss: 0.0051 ||:  22%|##1       | 14/65 [02:44<09:57, 11.72s/it]
2022-04-09 00:44:36,789 - INFO - tqdm - accuracy: 0.9979, batch_loss: 0.0042, loss: 0.0050 ||:  23%|##3       | 15/65 [02:56<09:45, 11.72s/it]
2022-04-09 00:44:48,474 - INFO - tqdm - accuracy: 0.9980, batch_loss: 0.0043, loss: 0.0050 ||:  25%|##4       | 16/65 [03:07<09:33, 11.71s/it]
2022-04-09 00:45:00,207 - INFO - tqdm - accuracy: 0.9982, batch_loss: 0.0038, loss: 0.0049 ||:  26%|##6       | 17/65 [03:19<09:22, 11.71s/it]
2022-04-09 00:45:11,969 - INFO - tqdm - accuracy: 0.9965, batch_loss: 0.2229, loss: 0.0170 ||:  28%|##7       | 18/65 [03:31<09:11, 11.73s/it]
2022-04-09 00:45:23,717 - INFO - tqdm - accuracy: 0.9967, batch_loss: 0.0006, loss: 0.0162 ||:  29%|##9       | 19/65 [03:43<08:59, 11.73s/it]
2022-04-09 00:45:35,475 - INFO - tqdm - accuracy: 0.9969, batch_loss: 0.0008, loss: 0.0154 ||:  31%|###       | 20/65 [03:54<08:48, 11.74s/it]
2022-04-09 00:45:47,248 - INFO - tqdm - accuracy: 0.9970, batch_loss: 0.0133, loss: 0.0153 ||:  32%|###2      | 21/65 [04:06<08:37, 11.75s/it]
2022-04-09 00:45:58,867 - INFO - tqdm - accuracy: 0.9957, batch_loss: 0.0444, loss: 0.0166 ||:  34%|###3      | 22/65 [04:18<08:23, 11.71s/it]
2022-04-09 00:46:10,628 - INFO - tqdm - accuracy: 0.9959, batch_loss: 0.0016, loss: 0.0160 ||:  35%|###5      | 23/65 [04:30<08:12, 11.73s/it]
2022-04-09 00:46:22,370 - INFO - tqdm - accuracy: 0.9961, batch_loss: 0.0048, loss: 0.0155 ||:  37%|###6      | 24/65 [04:41<08:00, 11.73s/it]
2022-04-09 00:46:34,138 - INFO - tqdm - accuracy: 0.9962, batch_loss: 0.0030, loss: 0.0150 ||:  38%|###8      | 25/65 [04:53<07:49, 11.74s/it]
2022-04-09 00:46:45,894 - INFO - tqdm - accuracy: 0.9964, batch_loss: 0.0009, loss: 0.0145 ||:  40%|####      | 26/65 [05:05<07:38, 11.75s/it]
2022-04-09 00:46:57,641 - INFO - tqdm - accuracy: 0.9965, batch_loss: 0.0016, loss: 0.0140 ||:  42%|####1     | 27/65 [05:17<07:26, 11.75s/it]
2022-04-09 00:47:09,397 - INFO - tqdm - accuracy: 0.9955, batch_loss: 0.0810, loss: 0.0164 ||:  43%|####3     | 28/65 [05:28<07:14, 11.75s/it]
2022-04-09 00:47:21,143 - INFO - tqdm - accuracy: 0.9957, batch_loss: 0.0044, loss: 0.0160 ||:  45%|####4     | 29/65 [05:40<07:02, 11.75s/it]
2022-04-09 00:47:32,885 - INFO - tqdm - accuracy: 0.9958, batch_loss: 0.0009, loss: 0.0155 ||:  46%|####6     | 30/65 [05:52<06:51, 11.75s/it]
2022-04-09 00:47:44,627 - INFO - tqdm - accuracy: 0.9960, batch_loss: 0.0017, loss: 0.0150 ||:  48%|####7     | 31/65 [06:04<06:39, 11.75s/it]
2022-04-09 00:47:56,320 - INFO - tqdm - accuracy: 0.9961, batch_loss: 0.0007, loss: 0.0146 ||:  49%|####9     | 32/65 [06:15<06:27, 11.73s/it]
2022-04-09 00:48:08,033 - INFO - tqdm - accuracy: 0.9962, batch_loss: 0.0010, loss: 0.0142 ||:  51%|#####     | 33/65 [06:27<06:15, 11.72s/it]
2022-04-09 00:48:19,763 - INFO - tqdm - accuracy: 0.9963, batch_loss: 0.0158, loss: 0.0142 ||:  52%|#####2    | 34/65 [06:39<06:03, 11.73s/it]
2022-04-09 00:48:31,530 - INFO - tqdm - accuracy: 0.9964, batch_loss: 0.0007, loss: 0.0138 ||:  54%|#####3    | 35/65 [06:50<05:52, 11.74s/it]
2022-04-09 00:48:43,321 - INFO - tqdm - accuracy: 0.9965, batch_loss: 0.0012, loss: 0.0135 ||:  55%|#####5    | 36/65 [07:02<05:40, 11.75s/it]
2022-04-09 00:48:55,125 - INFO - tqdm - accuracy: 0.9966, batch_loss: 0.0043, loss: 0.0132 ||:  57%|#####6    | 37/65 [07:14<05:29, 11.77s/it]
2022-04-09 00:49:06,898 - INFO - tqdm - accuracy: 0.9967, batch_loss: 0.0021, loss: 0.0129 ||:  58%|#####8    | 38/65 [07:26<05:17, 11.77s/it]
2022-04-09 00:49:18,671 - INFO - tqdm - accuracy: 0.9968, batch_loss: 0.0075, loss: 0.0128 ||:  60%|######    | 39/65 [07:38<05:06, 11.77s/it]
2022-04-09 00:49:30,407 - INFO - tqdm - accuracy: 0.9969, batch_loss: 0.0009, loss: 0.0125 ||:  62%|######1   | 40/65 [07:49<04:54, 11.76s/it]
2022-04-09 00:49:42,158 - INFO - tqdm - accuracy: 0.9970, batch_loss: 0.0047, loss: 0.0123 ||:  63%|######3   | 41/65 [08:01<04:42, 11.76s/it]
2022-04-09 00:49:53,882 - INFO - tqdm - accuracy: 0.9970, batch_loss: 0.0045, loss: 0.0121 ||:  65%|######4   | 42/65 [08:13<04:30, 11.75s/it]
2022-04-09 00:50:05,607 - INFO - tqdm - accuracy: 0.9971, batch_loss: 0.0022, loss: 0.0119 ||:  66%|######6   | 43/65 [08:25<04:18, 11.74s/it]
2022-04-09 00:50:17,385 - INFO - tqdm - accuracy: 0.9972, batch_loss: 0.0025, loss: 0.0117 ||:  68%|######7   | 44/65 [08:36<04:06, 11.75s/it]
2022-04-09 00:50:29,159 - INFO - tqdm - accuracy: 0.9972, batch_loss: 0.0047, loss: 0.0115 ||:  69%|######9   | 45/65 [08:48<03:55, 11.76s/it]
2022-04-09 00:50:40,934 - INFO - tqdm - accuracy: 0.9973, batch_loss: 0.0010, loss: 0.0113 ||:  71%|#######   | 46/65 [09:00<03:43, 11.76s/it]
2022-04-09 00:50:52,697 - INFO - tqdm - accuracy: 0.9967, batch_loss: 0.0237, loss: 0.0116 ||:  72%|#######2  | 47/65 [09:12<03:31, 11.76s/it]
2022-04-09 00:51:04,455 - INFO - tqdm - accuracy: 0.9967, batch_loss: 0.0021, loss: 0.0114 ||:  74%|#######3  | 48/65 [09:23<03:19, 11.76s/it]
2022-04-09 00:51:16,193 - INFO - tqdm - accuracy: 0.9968, batch_loss: 0.0144, loss: 0.0114 ||:  75%|#######5  | 49/65 [09:35<03:08, 11.75s/it]
2022-04-09 00:51:27,919 - INFO - tqdm - accuracy: 0.9969, batch_loss: 0.0217, loss: 0.0116 ||:  77%|#######6  | 50/65 [09:47<02:56, 11.75s/it]
2022-04-09 00:51:39,675 - INFO - tqdm - accuracy: 0.9969, batch_loss: 0.0003, loss: 0.0114 ||:  78%|#######8  | 51/65 [09:59<02:44, 11.75s/it]
2022-04-09 00:51:50,905 - INFO - tqdm - accuracy: 0.9970, batch_loss: 0.0037, loss: 0.0113 ||:  80%|########  | 52/65 [10:10<02:30, 11.59s/it]
2022-04-09 00:52:02,574 - INFO - tqdm - accuracy: 0.9971, batch_loss: 0.0007, loss: 0.0111 ||:  82%|########1 | 53/65 [10:21<02:19, 11.62s/it]
2022-04-09 00:52:14,265 - INFO - tqdm - accuracy: 0.9971, batch_loss: 0.0099, loss: 0.0110 ||:  83%|########3 | 54/65 [10:33<02:08, 11.64s/it]
2022-04-09 00:52:26,007 - INFO - tqdm - accuracy: 0.9972, batch_loss: 0.0150, loss: 0.0111 ||:  85%|########4 | 55/65 [10:45<01:56, 11.67s/it]
2022-04-09 00:52:37,500 - INFO - tqdm - accuracy: 0.9966, batch_loss: 0.0338, loss: 0.0115 ||:  86%|########6 | 56/65 [10:56<01:44, 11.62s/it]
2022-04-09 00:52:49,219 - INFO - tqdm - accuracy: 0.9967, batch_loss: 0.0008, loss: 0.0113 ||:  88%|########7 | 57/65 [11:08<01:33, 11.65s/it]
2022-04-09 00:53:00,973 - INFO - tqdm - accuracy: 0.9962, batch_loss: 0.1761, loss: 0.0142 ||:  89%|########9 | 58/65 [11:20<01:21, 11.68s/it]
2022-04-09 00:53:12,738 - INFO - tqdm - accuracy: 0.9963, batch_loss: 0.0010, loss: 0.0139 ||:  91%|######### | 59/65 [11:32<01:10, 11.71s/it]
2022-04-09 00:53:24,500 - INFO - tqdm - accuracy: 0.9958, batch_loss: 0.0991, loss: 0.0154 ||:  92%|#########2| 60/65 [11:43<00:58, 11.72s/it]
2022-04-09 00:53:36,274 - INFO - tqdm - accuracy: 0.9959, batch_loss: 0.0014, loss: 0.0151 ||:  94%|#########3| 61/65 [11:55<00:46, 11.74s/it]
2022-04-09 00:53:48,056 - INFO - tqdm - accuracy: 0.9960, batch_loss: 0.0009, loss: 0.0149 ||:  95%|#########5| 62/65 [12:07<00:35, 11.75s/it]
2022-04-09 00:53:59,857 - INFO - tqdm - accuracy: 0.9960, batch_loss: 0.0009, loss: 0.0147 ||:  97%|#########6| 63/65 [12:19<00:23, 11.77s/it]
2022-04-09 00:54:11,667 - INFO - tqdm - accuracy: 0.9961, batch_loss: 0.0005, loss: 0.0145 ||:  98%|#########8| 64/65 [12:31<00:11, 11.78s/it]
2022-04-09 00:54:16,911 - INFO - tqdm - accuracy: 0.9961, batch_loss: 0.0025, loss: 0.0143 ||: 100%|##########| 65/65 [12:36<00:00,  9.82s/it]
2022-04-09 00:54:16,911 - INFO - tqdm - accuracy: 0.9961, batch_loss: 0.0025, loss: 0.0143 ||: 100%|##########| 65/65 [12:36<00:00, 11.64s/it]
2022-04-09 00:54:19,847 - INFO - allennlp.training.trainer - Validating
2022-04-09 00:54:19,849 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-09 00:54:29,922 - INFO - tqdm - accuracy: 0.6667, batch_loss: 0.4251, loss: 2.1901 ||:  34%|###3      | 39/115 [00:10<00:19,  3.84it/s]
2022-04-09 00:54:40,132 - INFO - tqdm - accuracy: 0.6474, batch_loss: 0.1621, loss: 2.0677 ||:  68%|######7   | 78/115 [00:20<00:09,  3.80it/s]
2022-04-09 00:54:49,590 - INFO - tqdm - accuracy: 0.6114, batch_loss: 2.9712, loss: 2.3602 ||: 100%|##########| 115/115 [00:29<00:00,  3.83it/s]
2022-04-09 00:54:49,591 - INFO - tqdm - accuracy: 0.6114, batch_loss: 2.9712, loss: 2.3602 ||: 100%|##########| 115/115 [00:29<00:00,  3.87it/s]
2022-04-09 00:54:49,591 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-09 00:54:49,591 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.996  |     0.611
2022-04-09 00:54:49,592 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-09 00:54:49,593 - INFO - allennlp.training.tensorboard_writer - loss               |     0.014  |     2.360
2022-04-09 00:54:49,594 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5799.516  |       N/A
2022-04-09 00:54:55,273 - INFO - allennlp.training.trainer - Epoch duration: 0:13:14.700635
2022-04-09 00:54:55,273 - INFO - allennlp.training.checkpointer - loading best weights
2022-04-09 00:54:56,210 - INFO - allennlp.common.util - Metrics: {
  "best_epoch": 3,
  "peak_worker_0_memory_MB": 5799.515625,
  "peak_gpu_0_memory_MB": 9580.5712890625,
  "training_duration": "3:19:05.087363",
  "training_start_epoch": 0,
  "training_epochs": 14,
  "epoch": 14,
  "training_accuracy": 0.9961183891314895,
  "training_loss": 0.014271923853696605,
  "training_worker_0_memory_MB": 5799.515625,
  "training_gpu_0_memory_MB": 9580.5712890625,
  "validation_accuracy": 0.611353711790393,
  "validation_loss": 2.360182516231852,
  "best_validation_accuracy": 0.6550218340611353,
  "best_validation_loss": 0.7590329193246915
}
2022-04-09 00:54:56,210 - INFO - allennlp.models.archival - archiving weights and vocabulary to ./output_newboolq_ir-ora-d/model.tar.gz
