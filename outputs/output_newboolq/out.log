2022-04-08 15:30:26,262 - INFO - allennlp.common.params - random_seed = 42
2022-04-08 15:30:26,262 - INFO - allennlp.common.params - numpy_seed = 42
2022-04-08 15:30:26,262 - INFO - allennlp.common.params - pytorch_seed = 42
2022-04-08 15:30:26,262 - INFO - allennlp.common.checks - Pytorch version: 1.7.1
2022-04-08 15:30:26,263 - INFO - allennlp.common.params - type = default
2022-04-08 15:30:26,263 - INFO - allennlp.common.params - dataset_reader.type = boolean_qa_reader
2022-04-08 15:30:26,264 - INFO - allennlp.common.params - dataset_reader.lazy = False
2022-04-08 15:30:26,264 - INFO - allennlp.common.params - dataset_reader.cache_directory = None
2022-04-08 15:30:26,264 - INFO - allennlp.common.params - dataset_reader.max_instances = None
2022-04-08 15:30:26,264 - INFO - allennlp.common.params - dataset_reader.manual_distributed_sharding = False
2022-04-08 15:30:26,264 - INFO - allennlp.common.params - dataset_reader.manual_multi_process_sharding = False
2022-04-08 15:30:26,264 - INFO - allennlp.common.params - dataset_reader.tokenizer_wrapper.type = default
2022-04-08 15:30:26,264 - INFO - allennlp.common.params - dataset_reader.tokenizer_wrapper.pretrained_model = roberta-large
2022-04-08 15:30:26,265 - INFO - allennlp.common.params - dataset_reader.tokenizer_wrapper.call_kwargs.truncation = only_first
2022-04-08 15:30:27,493 - INFO - allennlp.common.params - dataset_reader.save_tokenizer = True
2022-04-08 15:30:27,494 - INFO - allennlp.common.params - dataset_reader.is_training = True
2022-04-08 15:30:27,494 - INFO - allennlp.common.params - dataset_reader.pickle.action = None
2022-04-08 15:30:27,494 - INFO - allennlp.common.params - dataset_reader.with_context = True
2022-04-08 15:30:27,494 - INFO - allennlp.common.params - dataset_reader.context_key = passage
2022-04-08 15:30:27,494 - INFO - allennlp.common.params - dataset_reader.answer_key = answer
2022-04-08 15:30:27,494 - INFO - allennlp.common.params - dataset_reader.is_twenty_questions = False
2022-04-08 15:30:27,933 - INFO - allennlp.common.params - train_data_path = data/boolq/train.jsonl
2022-04-08 15:30:27,934 - INFO - allennlp.common.params - vocabulary = <allennlp.common.lazy.Lazy object at 0x7fcfbaab7190>
2022-04-08 15:30:27,934 - INFO - allennlp.common.params - datasets_for_vocab_creation = None
2022-04-08 15:30:27,934 - INFO - allennlp.common.params - validation_dataset_reader.type = boolean_qa_reader
2022-04-08 15:30:27,935 - INFO - allennlp.common.params - validation_dataset_reader.lazy = False
2022-04-08 15:30:27,935 - INFO - allennlp.common.params - validation_dataset_reader.cache_directory = None
2022-04-08 15:30:27,935 - INFO - allennlp.common.params - validation_dataset_reader.max_instances = None
2022-04-08 15:30:27,935 - INFO - allennlp.common.params - validation_dataset_reader.manual_distributed_sharding = False
2022-04-08 15:30:27,935 - INFO - allennlp.common.params - validation_dataset_reader.manual_multi_process_sharding = False
2022-04-08 15:30:27,935 - INFO - allennlp.common.params - validation_dataset_reader.tokenizer_wrapper.type = default
2022-04-08 15:30:27,936 - INFO - allennlp.common.params - validation_dataset_reader.tokenizer_wrapper.pretrained_model = roberta-large
2022-04-08 15:30:27,936 - INFO - allennlp.common.params - validation_dataset_reader.tokenizer_wrapper.call_kwargs.truncation = only_first
2022-04-08 15:30:29,094 - INFO - allennlp.common.params - validation_dataset_reader.save_tokenizer = False
2022-04-08 15:30:29,095 - INFO - allennlp.common.params - validation_dataset_reader.is_training = False
2022-04-08 15:30:29,095 - INFO - allennlp.common.params - validation_dataset_reader.pickle.action = None
2022-04-08 15:30:29,095 - INFO - allennlp.common.params - validation_dataset_reader.with_context = True
2022-04-08 15:30:29,095 - INFO - allennlp.common.params - validation_dataset_reader.context_key = passage
2022-04-08 15:30:29,095 - INFO - allennlp.common.params - validation_dataset_reader.answer_key = answer
2022-04-08 15:30:29,095 - INFO - allennlp.common.params - validation_dataset_reader.is_twenty_questions = False
2022-04-08 15:30:29,096 - INFO - allennlp.common.params - validation_data_path = data/boolq/dev.jsonl
2022-04-08 15:30:29,096 - INFO - allennlp.common.params - validation_data_loader = None
2022-04-08 15:30:29,096 - INFO - allennlp.common.params - test_data_path = None
2022-04-08 15:30:29,096 - INFO - allennlp.common.params - evaluate_on_test = False
2022-04-08 15:30:29,096 - INFO - allennlp.common.params - batch_weight_key = 
2022-04-08 15:30:29,096 - INFO - allennlp.training.util - Reading training data from data/boolq/train.jsonl
2022-04-08 15:30:29,096 - INFO - tqdm - reading instances: 0it [00:00, ?it/s]
2022-04-08 15:30:29,097 - INFO - src.data.dataset_readers.boolean_qa_reader - Reading the dataset:
2022-04-08 15:30:29,097 - INFO - src.data.dataset_readers.boolean_qa_reader - Reading file at data/boolq/train.jsonl
2022-04-08 15:30:39,164 - INFO - tqdm - reading instances: 10646it [00:10, 1135.51it/s]
2022-04-08 15:30:47,014 - INFO - allennlp.training.util - Reading validation data from data/boolq/dev.jsonl
2022-04-08 15:30:47,015 - INFO - tqdm - reading instances: 0it [00:00, ?it/s]
2022-04-08 15:30:47,015 - INFO - src.data.dataset_readers.boolean_qa_reader - Reading the dataset:
2022-04-08 15:30:47,015 - INFO - src.data.dataset_readers.boolean_qa_reader - Reading file at data/boolq/dev.jsonl
2022-04-08 15:30:49,946 - INFO - allennlp.common.params - type = from_instances
2022-04-08 15:30:49,946 - INFO - allennlp.common.params - min_count = None
2022-04-08 15:30:49,947 - INFO - allennlp.common.params - max_vocab_size = None
2022-04-08 15:30:49,947 - INFO - allennlp.common.params - non_padded_namespaces = ('*tags', '*labels')
2022-04-08 15:30:49,947 - INFO - allennlp.common.params - pretrained_files = None
2022-04-08 15:30:49,947 - INFO - allennlp.common.params - only_include_pretrained_words = False
2022-04-08 15:30:49,947 - INFO - allennlp.common.params - tokens_to_add = None
2022-04-08 15:30:49,947 - INFO - allennlp.common.params - min_pretrained_embeddings = None
2022-04-08 15:30:49,947 - INFO - allennlp.common.params - padding_token = @@PADDING@@
2022-04-08 15:30:49,948 - INFO - allennlp.common.params - oov_token = @@UNKNOWN@@
2022-04-08 15:30:49,948 - INFO - allennlp.data.vocabulary - Fitting token dictionary from dataset.
2022-04-08 15:30:49,948 - INFO - tqdm - building vocab: 0it [00:00, ?it/s]
2022-04-08 15:30:50,003 - INFO - allennlp.common.params - model.type = hf_classifier
2022-04-08 15:30:50,003 - INFO - allennlp.common.params - model.regularizer = None
2022-04-08 15:30:50,003 - INFO - allennlp.common.params - model.pretrained_model = roberta-large
2022-04-08 15:30:50,004 - INFO - allennlp.common.params - model.tokenizer_wrapper.type = default
2022-04-08 15:30:50,004 - INFO - allennlp.common.params - model.tokenizer_wrapper.pretrained_model = roberta-large
2022-04-08 15:30:50,004 - INFO - allennlp.common.params - model.tokenizer_wrapper.call_kwargs.truncation = only_first
2022-04-08 15:30:51,213 - INFO - allennlp.common.params - model.num_labels = 2
2022-04-08 15:30:51,213 - INFO - allennlp.common.params - model.label_namespace = labels
2022-04-08 15:30:51,214 - INFO - allennlp.common.params - model.transformer_weights_path = None
2022-04-08 15:30:51,214 - INFO - allennlp.common.params - model.initializer.regexes.0.1.type = pretrained
2022-04-08 15:30:51,215 - INFO - allennlp.common.params - model.initializer.regexes.0.1.weights_file_path = experiments/for_STAR_1_twentyquestions_0/model.tar.gz
2022-04-08 15:30:51,215 - INFO - allennlp.models.archival - extracting archive file experiments/for_STAR_1_twentyquestions_0/model.tar.gz to temp dir /tmp/tmpoxb8nee0
2022-04-08 15:31:03,145 - INFO - allennlp.models.archival - removing temporary unarchived model dir at /tmp/tmpoxb8nee0
2022-04-08 15:31:03,319 - INFO - allennlp.common.params - model.initializer.prevent_regexes = None
2022-04-08 15:31:18,089 - INFO - allennlp.nn.initializers - Initializing parameters
2022-04-08 15:31:18,089 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.embeddings.word_embeddings.weight using .* initializer
2022-04-08 15:31:18,099 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.embeddings.position_embeddings.weight using .* initializer
2022-04-08 15:31:18,099 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.embeddings.token_type_embeddings.weight using .* initializer
2022-04-08 15:31:18,100 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.embeddings.LayerNorm.weight using .* initializer
2022-04-08 15:31:18,100 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.embeddings.LayerNorm.bias using .* initializer
2022-04-08 15:31:18,100 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.self.query.weight using .* initializer
2022-04-08 15:31:18,100 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.self.query.bias using .* initializer
2022-04-08 15:31:18,101 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.self.key.weight using .* initializer
2022-04-08 15:31:18,101 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.self.key.bias using .* initializer
2022-04-08 15:31:18,101 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.self.value.weight using .* initializer
2022-04-08 15:31:18,101 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.self.value.bias using .* initializer
2022-04-08 15:31:18,102 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.output.dense.weight using .* initializer
2022-04-08 15:31:18,102 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.output.dense.bias using .* initializer
2022-04-08 15:31:18,102 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.output.LayerNorm.weight using .* initializer
2022-04-08 15:31:18,102 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.output.LayerNorm.bias using .* initializer
2022-04-08 15:31:18,102 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.intermediate.dense.weight using .* initializer
2022-04-08 15:31:18,103 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.intermediate.dense.bias using .* initializer
2022-04-08 15:31:18,103 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.output.dense.weight using .* initializer
2022-04-08 15:31:18,104 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.output.dense.bias using .* initializer
2022-04-08 15:31:18,105 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.output.LayerNorm.weight using .* initializer
2022-04-08 15:31:18,105 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.output.LayerNorm.bias using .* initializer
2022-04-08 15:31:18,105 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.self.query.weight using .* initializer
2022-04-08 15:31:18,105 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.self.query.bias using .* initializer
2022-04-08 15:31:18,105 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.self.key.weight using .* initializer
2022-04-08 15:31:18,106 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.self.key.bias using .* initializer
2022-04-08 15:31:18,106 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.self.value.weight using .* initializer
2022-04-08 15:31:18,106 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.self.value.bias using .* initializer
2022-04-08 15:31:18,106 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.output.dense.weight using .* initializer
2022-04-08 15:31:18,107 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.output.dense.bias using .* initializer
2022-04-08 15:31:18,107 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.output.LayerNorm.weight using .* initializer
2022-04-08 15:31:18,107 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.output.LayerNorm.bias using .* initializer
2022-04-08 15:31:18,107 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.intermediate.dense.weight using .* initializer
2022-04-08 15:31:18,108 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.intermediate.dense.bias using .* initializer
2022-04-08 15:31:18,108 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.output.dense.weight using .* initializer
2022-04-08 15:31:18,109 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.output.dense.bias using .* initializer
2022-04-08 15:31:18,109 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.output.LayerNorm.weight using .* initializer
2022-04-08 15:31:18,110 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.output.LayerNorm.bias using .* initializer
2022-04-08 15:31:18,110 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.self.query.weight using .* initializer
2022-04-08 15:31:18,110 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.self.query.bias using .* initializer
2022-04-08 15:31:18,110 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.self.key.weight using .* initializer
2022-04-08 15:31:18,111 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.self.key.bias using .* initializer
2022-04-08 15:31:18,111 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.self.value.weight using .* initializer
2022-04-08 15:31:18,111 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.self.value.bias using .* initializer
2022-04-08 15:31:18,111 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.output.dense.weight using .* initializer
2022-04-08 15:31:18,112 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.output.dense.bias using .* initializer
2022-04-08 15:31:18,112 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.output.LayerNorm.weight using .* initializer
2022-04-08 15:31:18,112 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.output.LayerNorm.bias using .* initializer
2022-04-08 15:31:18,112 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.intermediate.dense.weight using .* initializer
2022-04-08 15:31:18,113 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.intermediate.dense.bias using .* initializer
2022-04-08 15:31:18,113 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.output.dense.weight using .* initializer
2022-04-08 15:31:18,114 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.output.dense.bias using .* initializer
2022-04-08 15:31:18,114 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.output.LayerNorm.weight using .* initializer
2022-04-08 15:31:18,114 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.output.LayerNorm.bias using .* initializer
2022-04-08 15:31:18,114 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.self.query.weight using .* initializer
2022-04-08 15:31:18,115 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.self.query.bias using .* initializer
2022-04-08 15:31:18,115 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.self.key.weight using .* initializer
2022-04-08 15:31:18,115 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.self.key.bias using .* initializer
2022-04-08 15:31:18,116 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.self.value.weight using .* initializer
2022-04-08 15:31:18,116 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.self.value.bias using .* initializer
2022-04-08 15:31:18,116 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.output.dense.weight using .* initializer
2022-04-08 15:31:18,116 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.output.dense.bias using .* initializer
2022-04-08 15:31:18,117 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.output.LayerNorm.weight using .* initializer
2022-04-08 15:31:18,117 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.output.LayerNorm.bias using .* initializer
2022-04-08 15:31:18,117 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.intermediate.dense.weight using .* initializer
2022-04-08 15:31:18,118 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.intermediate.dense.bias using .* initializer
2022-04-08 15:31:18,118 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.output.dense.weight using .* initializer
2022-04-08 15:31:18,119 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.output.dense.bias using .* initializer
2022-04-08 15:31:18,119 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.output.LayerNorm.weight using .* initializer
2022-04-08 15:31:18,120 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.output.LayerNorm.bias using .* initializer
2022-04-08 15:31:18,120 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.self.query.weight using .* initializer
2022-04-08 15:31:18,120 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.self.query.bias using .* initializer
2022-04-08 15:31:18,120 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.self.key.weight using .* initializer
2022-04-08 15:31:18,121 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.self.key.bias using .* initializer
2022-04-08 15:31:18,121 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.self.value.weight using .* initializer
2022-04-08 15:31:18,121 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.self.value.bias using .* initializer
2022-04-08 15:31:18,121 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.output.dense.weight using .* initializer
2022-04-08 15:31:18,122 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.output.dense.bias using .* initializer
2022-04-08 15:31:18,122 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.output.LayerNorm.weight using .* initializer
2022-04-08 15:31:18,122 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.output.LayerNorm.bias using .* initializer
2022-04-08 15:31:18,122 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.intermediate.dense.weight using .* initializer
2022-04-08 15:31:18,123 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.intermediate.dense.bias using .* initializer
2022-04-08 15:31:18,123 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.output.dense.weight using .* initializer
2022-04-08 15:31:18,124 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.output.dense.bias using .* initializer
2022-04-08 15:31:18,124 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.output.LayerNorm.weight using .* initializer
2022-04-08 15:31:18,125 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.output.LayerNorm.bias using .* initializer
2022-04-08 15:31:18,125 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.self.query.weight using .* initializer
2022-04-08 15:31:18,125 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.self.query.bias using .* initializer
2022-04-08 15:31:18,125 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.self.key.weight using .* initializer
2022-04-08 15:31:18,126 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.self.key.bias using .* initializer
2022-04-08 15:31:18,126 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.self.value.weight using .* initializer
2022-04-08 15:31:18,126 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.self.value.bias using .* initializer
2022-04-08 15:31:18,126 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.output.dense.weight using .* initializer
2022-04-08 15:31:18,127 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.output.dense.bias using .* initializer
2022-04-08 15:31:18,127 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.output.LayerNorm.weight using .* initializer
2022-04-08 15:31:18,127 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.output.LayerNorm.bias using .* initializer
2022-04-08 15:31:18,127 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.intermediate.dense.weight using .* initializer
2022-04-08 15:31:18,128 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.intermediate.dense.bias using .* initializer
2022-04-08 15:31:18,128 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.output.dense.weight using .* initializer
2022-04-08 15:31:18,129 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.output.dense.bias using .* initializer
2022-04-08 15:31:18,130 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.output.LayerNorm.weight using .* initializer
2022-04-08 15:31:18,130 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.output.LayerNorm.bias using .* initializer
2022-04-08 15:31:18,130 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.self.query.weight using .* initializer
2022-04-08 15:31:18,130 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.self.query.bias using .* initializer
2022-04-08 15:31:18,131 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.self.key.weight using .* initializer
2022-04-08 15:31:18,131 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.self.key.bias using .* initializer
2022-04-08 15:31:18,131 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.self.value.weight using .* initializer
2022-04-08 15:31:18,131 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.self.value.bias using .* initializer
2022-04-08 15:31:18,132 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.output.dense.weight using .* initializer
2022-04-08 15:31:18,132 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.output.dense.bias using .* initializer
2022-04-08 15:31:18,132 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.output.LayerNorm.weight using .* initializer
2022-04-08 15:31:18,132 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.output.LayerNorm.bias using .* initializer
2022-04-08 15:31:18,133 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.intermediate.dense.weight using .* initializer
2022-04-08 15:31:18,134 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.intermediate.dense.bias using .* initializer
2022-04-08 15:31:18,134 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.output.dense.weight using .* initializer
2022-04-08 15:31:18,135 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.output.dense.bias using .* initializer
2022-04-08 15:31:18,135 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.output.LayerNorm.weight using .* initializer
2022-04-08 15:31:18,135 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.output.LayerNorm.bias using .* initializer
2022-04-08 15:31:18,135 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.self.query.weight using .* initializer
2022-04-08 15:31:18,136 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.self.query.bias using .* initializer
2022-04-08 15:31:18,136 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.self.key.weight using .* initializer
2022-04-08 15:31:18,136 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.self.key.bias using .* initializer
2022-04-08 15:31:18,136 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.self.value.weight using .* initializer
2022-04-08 15:31:18,137 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.self.value.bias using .* initializer
2022-04-08 15:31:18,137 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.output.dense.weight using .* initializer
2022-04-08 15:31:18,137 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.output.dense.bias using .* initializer
2022-04-08 15:31:18,138 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.output.LayerNorm.weight using .* initializer
2022-04-08 15:31:18,138 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.output.LayerNorm.bias using .* initializer
2022-04-08 15:31:18,138 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.intermediate.dense.weight using .* initializer
2022-04-08 15:31:18,139 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.intermediate.dense.bias using .* initializer
2022-04-08 15:31:18,139 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.output.dense.weight using .* initializer
2022-04-08 15:31:18,140 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.output.dense.bias using .* initializer
2022-04-08 15:31:18,140 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.output.LayerNorm.weight using .* initializer
2022-04-08 15:31:18,141 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.output.LayerNorm.bias using .* initializer
2022-04-08 15:31:18,141 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.self.query.weight using .* initializer
2022-04-08 15:31:18,141 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.self.query.bias using .* initializer
2022-04-08 15:31:18,141 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.self.key.weight using .* initializer
2022-04-08 15:31:18,142 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.self.key.bias using .* initializer
2022-04-08 15:31:18,142 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.self.value.weight using .* initializer
2022-04-08 15:31:18,142 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.self.value.bias using .* initializer
2022-04-08 15:31:18,142 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.output.dense.weight using .* initializer
2022-04-08 15:31:18,143 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.output.dense.bias using .* initializer
2022-04-08 15:31:18,143 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.output.LayerNorm.weight using .* initializer
2022-04-08 15:31:18,143 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.output.LayerNorm.bias using .* initializer
2022-04-08 15:31:18,143 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.intermediate.dense.weight using .* initializer
2022-04-08 15:31:18,144 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.intermediate.dense.bias using .* initializer
2022-04-08 15:31:18,145 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.output.dense.weight using .* initializer
2022-04-08 15:31:18,146 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.output.dense.bias using .* initializer
2022-04-08 15:31:18,146 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.output.LayerNorm.weight using .* initializer
2022-04-08 15:31:18,146 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.output.LayerNorm.bias using .* initializer
2022-04-08 15:31:18,146 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.self.query.weight using .* initializer
2022-04-08 15:31:18,146 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.self.query.bias using .* initializer
2022-04-08 15:31:18,147 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.self.key.weight using .* initializer
2022-04-08 15:31:18,147 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.self.key.bias using .* initializer
2022-04-08 15:31:18,147 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.self.value.weight using .* initializer
2022-04-08 15:31:18,148 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.self.value.bias using .* initializer
2022-04-08 15:31:18,148 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.output.dense.weight using .* initializer
2022-04-08 15:31:18,148 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.output.dense.bias using .* initializer
2022-04-08 15:31:18,148 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.output.LayerNorm.weight using .* initializer
2022-04-08 15:31:18,149 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.output.LayerNorm.bias using .* initializer
2022-04-08 15:31:18,149 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.intermediate.dense.weight using .* initializer
2022-04-08 15:31:18,150 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.intermediate.dense.bias using .* initializer
2022-04-08 15:31:18,150 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.output.dense.weight using .* initializer
2022-04-08 15:31:18,151 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.output.dense.bias using .* initializer
2022-04-08 15:31:18,151 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.output.LayerNorm.weight using .* initializer
2022-04-08 15:31:18,151 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.output.LayerNorm.bias using .* initializer
2022-04-08 15:31:18,152 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.self.query.weight using .* initializer
2022-04-08 15:31:18,152 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.self.query.bias using .* initializer
2022-04-08 15:31:18,152 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.self.key.weight using .* initializer
2022-04-08 15:31:18,153 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.self.key.bias using .* initializer
2022-04-08 15:31:18,153 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.self.value.weight using .* initializer
2022-04-08 15:31:18,153 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.self.value.bias using .* initializer
2022-04-08 15:31:18,153 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.output.dense.weight using .* initializer
2022-04-08 15:31:18,154 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.output.dense.bias using .* initializer
2022-04-08 15:31:18,154 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.output.LayerNorm.weight using .* initializer
2022-04-08 15:31:18,154 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.output.LayerNorm.bias using .* initializer
2022-04-08 15:31:18,154 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.intermediate.dense.weight using .* initializer
2022-04-08 15:31:18,155 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.intermediate.dense.bias using .* initializer
2022-04-08 15:31:18,155 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.output.dense.weight using .* initializer
2022-04-08 15:31:18,156 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.output.dense.bias using .* initializer
2022-04-08 15:31:18,156 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.output.LayerNorm.weight using .* initializer
2022-04-08 15:31:18,157 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.output.LayerNorm.bias using .* initializer
2022-04-08 15:31:18,157 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.self.query.weight using .* initializer
2022-04-08 15:31:18,157 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.self.query.bias using .* initializer
2022-04-08 15:31:18,157 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.self.key.weight using .* initializer
2022-04-08 15:31:18,158 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.self.key.bias using .* initializer
2022-04-08 15:31:18,158 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.self.value.weight using .* initializer
2022-04-08 15:31:18,158 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.self.value.bias using .* initializer
2022-04-08 15:31:18,159 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.output.dense.weight using .* initializer
2022-04-08 15:31:18,159 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.output.dense.bias using .* initializer
2022-04-08 15:31:18,159 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.output.LayerNorm.weight using .* initializer
2022-04-08 15:31:18,159 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.output.LayerNorm.bias using .* initializer
2022-04-08 15:31:18,159 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.intermediate.dense.weight using .* initializer
2022-04-08 15:31:18,160 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.intermediate.dense.bias using .* initializer
2022-04-08 15:31:18,161 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.output.dense.weight using .* initializer
2022-04-08 15:31:18,162 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.output.dense.bias using .* initializer
2022-04-08 15:31:18,162 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.output.LayerNorm.weight using .* initializer
2022-04-08 15:31:18,162 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.output.LayerNorm.bias using .* initializer
2022-04-08 15:31:18,162 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.self.query.weight using .* initializer
2022-04-08 15:31:18,163 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.self.query.bias using .* initializer
2022-04-08 15:31:18,163 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.self.key.weight using .* initializer
2022-04-08 15:31:18,163 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.self.key.bias using .* initializer
2022-04-08 15:31:18,163 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.self.value.weight using .* initializer
2022-04-08 15:31:18,164 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.self.value.bias using .* initializer
2022-04-08 15:31:18,164 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.output.dense.weight using .* initializer
2022-04-08 15:31:18,164 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.output.dense.bias using .* initializer
2022-04-08 15:31:18,165 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.output.LayerNorm.weight using .* initializer
2022-04-08 15:31:18,165 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.output.LayerNorm.bias using .* initializer
2022-04-08 15:31:18,165 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.intermediate.dense.weight using .* initializer
2022-04-08 15:31:18,166 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.intermediate.dense.bias using .* initializer
2022-04-08 15:31:18,166 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.output.dense.weight using .* initializer
2022-04-08 15:31:18,167 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.output.dense.bias using .* initializer
2022-04-08 15:31:18,167 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.output.LayerNorm.weight using .* initializer
2022-04-08 15:31:18,167 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.output.LayerNorm.bias using .* initializer
2022-04-08 15:31:18,168 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.self.query.weight using .* initializer
2022-04-08 15:31:18,168 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.self.query.bias using .* initializer
2022-04-08 15:31:18,168 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.self.key.weight using .* initializer
2022-04-08 15:31:18,169 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.self.key.bias using .* initializer
2022-04-08 15:31:18,169 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.self.value.weight using .* initializer
2022-04-08 15:31:18,169 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.self.value.bias using .* initializer
2022-04-08 15:31:18,169 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.output.dense.weight using .* initializer
2022-04-08 15:31:18,170 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.output.dense.bias using .* initializer
2022-04-08 15:31:18,170 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.output.LayerNorm.weight using .* initializer
2022-04-08 15:31:18,170 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.output.LayerNorm.bias using .* initializer
2022-04-08 15:31:18,170 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.intermediate.dense.weight using .* initializer
2022-04-08 15:31:18,171 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.intermediate.dense.bias using .* initializer
2022-04-08 15:31:18,171 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.output.dense.weight using .* initializer
2022-04-08 15:31:18,172 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.output.dense.bias using .* initializer
2022-04-08 15:31:18,173 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.output.LayerNorm.weight using .* initializer
2022-04-08 15:31:18,173 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.output.LayerNorm.bias using .* initializer
2022-04-08 15:31:18,173 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.self.query.weight using .* initializer
2022-04-08 15:31:18,173 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.self.query.bias using .* initializer
2022-04-08 15:31:18,173 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.self.key.weight using .* initializer
2022-04-08 15:31:18,174 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.self.key.bias using .* initializer
2022-04-08 15:31:18,174 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.self.value.weight using .* initializer
2022-04-08 15:31:18,174 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.self.value.bias using .* initializer
2022-04-08 15:31:18,175 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.output.dense.weight using .* initializer
2022-04-08 15:31:18,175 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.output.dense.bias using .* initializer
2022-04-08 15:31:18,175 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.output.LayerNorm.weight using .* initializer
2022-04-08 15:31:18,175 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.output.LayerNorm.bias using .* initializer
2022-04-08 15:31:18,176 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.intermediate.dense.weight using .* initializer
2022-04-08 15:31:18,177 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.intermediate.dense.bias using .* initializer
2022-04-08 15:31:18,177 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.output.dense.weight using .* initializer
2022-04-08 15:31:18,178 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.output.dense.bias using .* initializer
2022-04-08 15:31:18,178 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.output.LayerNorm.weight using .* initializer
2022-04-08 15:31:18,178 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.output.LayerNorm.bias using .* initializer
2022-04-08 15:31:18,178 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.self.query.weight using .* initializer
2022-04-08 15:31:18,179 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.self.query.bias using .* initializer
2022-04-08 15:31:18,179 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.self.key.weight using .* initializer
2022-04-08 15:31:18,179 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.self.key.bias using .* initializer
2022-04-08 15:31:18,180 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.self.value.weight using .* initializer
2022-04-08 15:31:18,180 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.self.value.bias using .* initializer
2022-04-08 15:31:18,180 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.output.dense.weight using .* initializer
2022-04-08 15:31:18,181 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.output.dense.bias using .* initializer
2022-04-08 15:31:18,181 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.output.LayerNorm.weight using .* initializer
2022-04-08 15:31:18,181 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.output.LayerNorm.bias using .* initializer
2022-04-08 15:31:18,181 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.intermediate.dense.weight using .* initializer
2022-04-08 15:31:18,182 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.intermediate.dense.bias using .* initializer
2022-04-08 15:31:18,183 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.output.dense.weight using .* initializer
2022-04-08 15:31:18,184 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.output.dense.bias using .* initializer
2022-04-08 15:31:18,184 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.output.LayerNorm.weight using .* initializer
2022-04-08 15:31:18,184 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.output.LayerNorm.bias using .* initializer
2022-04-08 15:31:18,184 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.self.query.weight using .* initializer
2022-04-08 15:31:18,185 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.self.query.bias using .* initializer
2022-04-08 15:31:18,185 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.self.key.weight using .* initializer
2022-04-08 15:31:18,185 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.self.key.bias using .* initializer
2022-04-08 15:31:18,186 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.self.value.weight using .* initializer
2022-04-08 15:31:18,186 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.self.value.bias using .* initializer
2022-04-08 15:31:18,186 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.output.dense.weight using .* initializer
2022-04-08 15:31:18,187 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.output.dense.bias using .* initializer
2022-04-08 15:31:18,187 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.output.LayerNorm.weight using .* initializer
2022-04-08 15:31:18,187 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.output.LayerNorm.bias using .* initializer
2022-04-08 15:31:18,187 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.intermediate.dense.weight using .* initializer
2022-04-08 15:31:18,189 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.intermediate.dense.bias using .* initializer
2022-04-08 15:31:18,189 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.output.dense.weight using .* initializer
2022-04-08 15:31:18,190 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.output.dense.bias using .* initializer
2022-04-08 15:31:18,190 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.output.LayerNorm.weight using .* initializer
2022-04-08 15:31:18,190 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.output.LayerNorm.bias using .* initializer
2022-04-08 15:31:18,190 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.self.query.weight using .* initializer
2022-04-08 15:31:18,191 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.self.query.bias using .* initializer
2022-04-08 15:31:18,191 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.self.key.weight using .* initializer
2022-04-08 15:31:18,192 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.self.key.bias using .* initializer
2022-04-08 15:31:18,192 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.self.value.weight using .* initializer
2022-04-08 15:31:18,192 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.self.value.bias using .* initializer
2022-04-08 15:31:18,192 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.output.dense.weight using .* initializer
2022-04-08 15:31:18,193 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.output.dense.bias using .* initializer
2022-04-08 15:31:18,193 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.output.LayerNorm.weight using .* initializer
2022-04-08 15:31:18,193 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.output.LayerNorm.bias using .* initializer
2022-04-08 15:31:18,194 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.intermediate.dense.weight using .* initializer
2022-04-08 15:31:18,195 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.intermediate.dense.bias using .* initializer
2022-04-08 15:31:18,195 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.output.dense.weight using .* initializer
2022-04-08 15:31:18,196 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.output.dense.bias using .* initializer
2022-04-08 15:31:18,196 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.output.LayerNorm.weight using .* initializer
2022-04-08 15:31:18,196 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.output.LayerNorm.bias using .* initializer
2022-04-08 15:31:18,196 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.self.query.weight using .* initializer
2022-04-08 15:31:18,197 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.self.query.bias using .* initializer
2022-04-08 15:31:18,197 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.self.key.weight using .* initializer
2022-04-08 15:31:18,197 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.self.key.bias using .* initializer
2022-04-08 15:31:18,197 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.self.value.weight using .* initializer
2022-04-08 15:31:18,198 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.self.value.bias using .* initializer
2022-04-08 15:31:18,198 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.output.dense.weight using .* initializer
2022-04-08 15:31:18,198 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.output.dense.bias using .* initializer
2022-04-08 15:31:18,198 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.output.LayerNorm.weight using .* initializer
2022-04-08 15:31:18,199 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.output.LayerNorm.bias using .* initializer
2022-04-08 15:31:18,199 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.intermediate.dense.weight using .* initializer
2022-04-08 15:31:18,200 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.intermediate.dense.bias using .* initializer
2022-04-08 15:31:18,200 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.output.dense.weight using .* initializer
2022-04-08 15:31:18,201 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.output.dense.bias using .* initializer
2022-04-08 15:31:18,201 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.output.LayerNorm.weight using .* initializer
2022-04-08 15:31:18,201 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.output.LayerNorm.bias using .* initializer
2022-04-08 15:31:18,202 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.self.query.weight using .* initializer
2022-04-08 15:31:18,202 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.self.query.bias using .* initializer
2022-04-08 15:31:18,202 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.self.key.weight using .* initializer
2022-04-08 15:31:18,203 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.self.key.bias using .* initializer
2022-04-08 15:31:18,203 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.self.value.weight using .* initializer
2022-04-08 15:31:18,203 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.self.value.bias using .* initializer
2022-04-08 15:31:18,203 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.output.dense.weight using .* initializer
2022-04-08 15:31:18,204 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.output.dense.bias using .* initializer
2022-04-08 15:31:18,204 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.output.LayerNorm.weight using .* initializer
2022-04-08 15:31:18,204 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.output.LayerNorm.bias using .* initializer
2022-04-08 15:31:18,204 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.intermediate.dense.weight using .* initializer
2022-04-08 15:31:18,205 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.intermediate.dense.bias using .* initializer
2022-04-08 15:31:18,206 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.output.dense.weight using .* initializer
2022-04-08 15:31:18,207 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.output.dense.bias using .* initializer
2022-04-08 15:31:18,207 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.output.LayerNorm.weight using .* initializer
2022-04-08 15:31:18,207 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.output.LayerNorm.bias using .* initializer
2022-04-08 15:31:18,207 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.self.query.weight using .* initializer
2022-04-08 15:31:18,208 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.self.query.bias using .* initializer
2022-04-08 15:31:18,208 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.self.key.weight using .* initializer
2022-04-08 15:31:18,208 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.self.key.bias using .* initializer
2022-04-08 15:31:18,208 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.self.value.weight using .* initializer
2022-04-08 15:31:18,209 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.self.value.bias using .* initializer
2022-04-08 15:31:18,209 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.output.dense.weight using .* initializer
2022-04-08 15:31:18,209 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.output.dense.bias using .* initializer
2022-04-08 15:31:18,210 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.output.LayerNorm.weight using .* initializer
2022-04-08 15:31:18,210 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.output.LayerNorm.bias using .* initializer
2022-04-08 15:31:18,210 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.intermediate.dense.weight using .* initializer
2022-04-08 15:31:18,211 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.intermediate.dense.bias using .* initializer
2022-04-08 15:31:18,211 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.output.dense.weight using .* initializer
2022-04-08 15:31:18,212 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.output.dense.bias using .* initializer
2022-04-08 15:31:18,212 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.output.LayerNorm.weight using .* initializer
2022-04-08 15:31:18,212 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.output.LayerNorm.bias using .* initializer
2022-04-08 15:31:18,213 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.self.query.weight using .* initializer
2022-04-08 15:31:18,213 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.self.query.bias using .* initializer
2022-04-08 15:31:18,213 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.self.key.weight using .* initializer
2022-04-08 15:31:18,214 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.self.key.bias using .* initializer
2022-04-08 15:31:18,214 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.self.value.weight using .* initializer
2022-04-08 15:31:18,214 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.self.value.bias using .* initializer
2022-04-08 15:31:18,214 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.output.dense.weight using .* initializer
2022-04-08 15:31:18,215 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.output.dense.bias using .* initializer
2022-04-08 15:31:18,215 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.output.LayerNorm.weight using .* initializer
2022-04-08 15:31:18,215 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.output.LayerNorm.bias using .* initializer
2022-04-08 15:31:18,215 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.intermediate.dense.weight using .* initializer
2022-04-08 15:31:18,216 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.intermediate.dense.bias using .* initializer
2022-04-08 15:31:18,216 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.output.dense.weight using .* initializer
2022-04-08 15:31:18,217 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.output.dense.bias using .* initializer
2022-04-08 15:31:18,218 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.output.LayerNorm.weight using .* initializer
2022-04-08 15:31:18,218 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.output.LayerNorm.bias using .* initializer
2022-04-08 15:31:18,218 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.self.query.weight using .* initializer
2022-04-08 15:31:18,218 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.self.query.bias using .* initializer
2022-04-08 15:31:18,219 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.self.key.weight using .* initializer
2022-04-08 15:31:18,219 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.self.key.bias using .* initializer
2022-04-08 15:31:18,219 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.self.value.weight using .* initializer
2022-04-08 15:31:18,220 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.self.value.bias using .* initializer
2022-04-08 15:31:18,220 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.output.dense.weight using .* initializer
2022-04-08 15:31:18,220 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.output.dense.bias using .* initializer
2022-04-08 15:31:18,221 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.output.LayerNorm.weight using .* initializer
2022-04-08 15:31:18,221 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.output.LayerNorm.bias using .* initializer
2022-04-08 15:31:18,221 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.intermediate.dense.weight using .* initializer
2022-04-08 15:31:18,222 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.intermediate.dense.bias using .* initializer
2022-04-08 15:31:18,222 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.output.dense.weight using .* initializer
2022-04-08 15:31:18,223 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.output.dense.bias using .* initializer
2022-04-08 15:31:18,223 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.output.LayerNorm.weight using .* initializer
2022-04-08 15:31:18,223 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.output.LayerNorm.bias using .* initializer
2022-04-08 15:31:18,223 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.self.query.weight using .* initializer
2022-04-08 15:31:18,224 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.self.query.bias using .* initializer
2022-04-08 15:31:18,224 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.self.key.weight using .* initializer
2022-04-08 15:31:18,224 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.self.key.bias using .* initializer
2022-04-08 15:31:18,225 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.self.value.weight using .* initializer
2022-04-08 15:31:18,225 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.self.value.bias using .* initializer
2022-04-08 15:31:18,225 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.output.dense.weight using .* initializer
2022-04-08 15:31:18,225 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.output.dense.bias using .* initializer
2022-04-08 15:31:18,226 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.output.LayerNorm.weight using .* initializer
2022-04-08 15:31:18,226 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.output.LayerNorm.bias using .* initializer
2022-04-08 15:31:18,226 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.intermediate.dense.weight using .* initializer
2022-04-08 15:31:18,227 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.intermediate.dense.bias using .* initializer
2022-04-08 15:31:18,227 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.output.dense.weight using .* initializer
2022-04-08 15:31:18,228 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.output.dense.bias using .* initializer
2022-04-08 15:31:18,228 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.output.LayerNorm.weight using .* initializer
2022-04-08 15:31:18,229 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.output.LayerNorm.bias using .* initializer
2022-04-08 15:31:18,229 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.pooler.dense.weight using .* initializer
2022-04-08 15:31:18,229 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.pooler.dense.bias using .* initializer
2022-04-08 15:31:18,229 - INFO - allennlp.nn.initializers - Initializing _classifier.classifier.dense.weight using .* initializer
2022-04-08 15:31:18,230 - INFO - allennlp.nn.initializers - Initializing _classifier.classifier.dense.bias using .* initializer
2022-04-08 15:31:18,230 - INFO - allennlp.nn.initializers - Initializing _classifier.classifier.out_proj.weight using .* initializer
2022-04-08 15:31:18,230 - INFO - allennlp.nn.initializers - Initializing _classifier.classifier.out_proj.bias using .* initializer
2022-04-08 15:31:18,230 - INFO - allennlp.nn.initializers - Done initializing parameters; the following parameters are using their default initialization from their code
2022-04-08 15:31:18,791 - INFO - filelock - Lock 140527574203664 acquired on ./output_newboolq/vocabulary/.lock
2022-04-08 15:31:18,791 - INFO - filelock - Lock 140527574203664 released on ./output_newboolq/vocabulary/.lock
2022-04-08 15:31:18,792 - INFO - allennlp.common.params - data_loader.type = pytorch_dataloader
2022-04-08 15:31:18,792 - INFO - allennlp.common.params - data_loader.batch_size = 1
2022-04-08 15:31:18,792 - INFO - allennlp.common.params - data_loader.shuffle = False
2022-04-08 15:31:18,792 - INFO - allennlp.common.params - data_loader.sampler = None
2022-04-08 15:31:18,793 - INFO - allennlp.common.params - data_loader.num_workers = 0
2022-04-08 15:31:18,793 - INFO - allennlp.common.params - data_loader.pin_memory = False
2022-04-08 15:31:18,793 - INFO - allennlp.common.params - data_loader.drop_last = False
2022-04-08 15:31:18,793 - INFO - allennlp.common.params - data_loader.timeout = 0
2022-04-08 15:31:18,794 - INFO - allennlp.common.params - data_loader.worker_init_fn = None
2022-04-08 15:31:18,794 - INFO - allennlp.common.params - data_loader.multiprocessing_context = None
2022-04-08 15:31:18,794 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None
2022-04-08 15:31:18,794 - INFO - allennlp.common.params - data_loader.batch_sampler.type = bucket
2022-04-08 15:31:18,794 - INFO - allennlp.common.params - data_loader.batch_sampler.batch_size = 2
2022-04-08 15:31:18,794 - INFO - allennlp.common.params - data_loader.batch_sampler.sorting_keys = None
2022-04-08 15:31:18,795 - INFO - allennlp.common.params - data_loader.batch_sampler.padding_noise = 0.1
2022-04-08 15:31:18,795 - INFO - allennlp.common.params - data_loader.batch_sampler.drop_last = False
2022-04-08 15:31:18,795 - INFO - allennlp.common.params - data_loader.type = pytorch_dataloader
2022-04-08 15:31:18,795 - INFO - allennlp.common.params - data_loader.batch_size = 1
2022-04-08 15:31:18,796 - INFO - allennlp.common.params - data_loader.shuffle = False
2022-04-08 15:31:18,796 - INFO - allennlp.common.params - data_loader.sampler = None
2022-04-08 15:31:18,796 - INFO - allennlp.common.params - data_loader.num_workers = 0
2022-04-08 15:31:18,796 - INFO - allennlp.common.params - data_loader.pin_memory = False
2022-04-08 15:31:18,796 - INFO - allennlp.common.params - data_loader.drop_last = False
2022-04-08 15:31:18,796 - INFO - allennlp.common.params - data_loader.timeout = 0
2022-04-08 15:31:18,797 - INFO - allennlp.common.params - data_loader.worker_init_fn = None
2022-04-08 15:31:18,797 - INFO - allennlp.common.params - data_loader.multiprocessing_context = None
2022-04-08 15:31:18,797 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None
2022-04-08 15:31:18,797 - INFO - allennlp.common.params - data_loader.batch_sampler.type = bucket
2022-04-08 15:31:18,797 - INFO - allennlp.common.params - data_loader.batch_sampler.batch_size = 2
2022-04-08 15:31:18,797 - INFO - allennlp.common.params - data_loader.batch_sampler.sorting_keys = None
2022-04-08 15:31:18,798 - INFO - allennlp.common.params - data_loader.batch_sampler.padding_noise = 0.1
2022-04-08 15:31:18,798 - INFO - allennlp.common.params - data_loader.batch_sampler.drop_last = False
2022-04-08 15:31:18,798 - INFO - allennlp.common.params - trainer.type = gradient_descent
2022-04-08 15:31:18,798 - INFO - allennlp.common.params - trainer.patience = None
2022-04-08 15:31:18,799 - INFO - allennlp.common.params - trainer.validation_metric = +accuracy
2022-04-08 15:31:18,799 - INFO - allennlp.common.params - trainer.num_epochs = 10
2022-04-08 15:31:18,799 - INFO - allennlp.common.params - trainer.cuda_device = 0
2022-04-08 15:31:18,799 - INFO - allennlp.common.params - trainer.grad_norm = None
2022-04-08 15:31:18,799 - INFO - allennlp.common.params - trainer.grad_clipping = None
2022-04-08 15:31:18,799 - INFO - allennlp.common.params - trainer.distributed = False
2022-04-08 15:31:18,800 - INFO - allennlp.common.params - trainer.world_size = 1
2022-04-08 15:31:18,800 - INFO - allennlp.common.params - trainer.num_gradient_accumulation_steps = 16
2022-04-08 15:31:18,800 - INFO - allennlp.common.params - trainer.use_amp = False
2022-04-08 15:31:18,800 - INFO - allennlp.common.params - trainer.no_grad = None
2022-04-08 15:31:18,800 - INFO - allennlp.common.params - trainer.momentum_scheduler = None
2022-04-08 15:31:18,800 - INFO - allennlp.common.params - trainer.tensorboard_writer = <allennlp.common.lazy.Lazy object at 0x7fcfbaa90450>
2022-04-08 15:31:18,801 - INFO - allennlp.common.params - trainer.moving_average = None
2022-04-08 15:31:18,801 - INFO - allennlp.common.params - trainer.batch_callbacks = None
2022-04-08 15:31:18,801 - INFO - allennlp.common.params - trainer.epoch_callbacks = None
2022-04-08 15:31:18,801 - INFO - allennlp.common.params - trainer.end_callbacks = None
2022-04-08 15:31:18,801 - INFO - allennlp.common.params - trainer.trainer_callbacks = None
2022-04-08 15:31:19,139 - INFO - allennlp.common.params - trainer.optimizer.type = huggingface_adamw
2022-04-08 15:31:19,140 - INFO - allennlp.common.params - trainer.optimizer.parameter_groups = None
2022-04-08 15:31:19,140 - INFO - allennlp.common.params - trainer.optimizer.lr = 1e-05
2022-04-08 15:31:19,140 - INFO - allennlp.common.params - trainer.optimizer.betas = [0.9, 0.98]
2022-04-08 15:31:19,140 - INFO - allennlp.common.params - trainer.optimizer.eps = 1e-06
2022-04-08 15:31:19,140 - INFO - allennlp.common.params - trainer.optimizer.weight_decay = 0.1
2022-04-08 15:31:19,140 - INFO - allennlp.common.params - trainer.optimizer.correct_bias = True
2022-04-08 15:31:19,141 - INFO - allennlp.training.optimizers - Number of trainable parameters: 356411394
2022-04-08 15:31:19,142 - INFO - allennlp.common.util - The following parameters are Frozen (without gradient):
2022-04-08 15:31:19,145 - INFO - allennlp.common.util - The following parameters are Tunable (with gradient):
2022-04-08 15:31:19,145 - INFO - allennlp.common.util - _classifier.roberta.embeddings.word_embeddings.weight
2022-04-08 15:31:19,145 - INFO - allennlp.common.util - _classifier.roberta.embeddings.position_embeddings.weight
2022-04-08 15:31:19,146 - INFO - allennlp.common.util - _classifier.roberta.embeddings.token_type_embeddings.weight
2022-04-08 15:31:19,146 - INFO - allennlp.common.util - _classifier.roberta.embeddings.LayerNorm.weight
2022-04-08 15:31:19,146 - INFO - allennlp.common.util - _classifier.roberta.embeddings.LayerNorm.bias
2022-04-08 15:31:19,146 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.self.query.weight
2022-04-08 15:31:19,146 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.self.query.bias
2022-04-08 15:31:19,146 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.self.key.weight
2022-04-08 15:31:19,146 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.self.key.bias
2022-04-08 15:31:19,146 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.self.value.weight
2022-04-08 15:31:19,147 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.self.value.bias
2022-04-08 15:31:19,147 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.output.dense.weight
2022-04-08 15:31:19,147 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.output.dense.bias
2022-04-08 15:31:19,147 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.output.LayerNorm.weight
2022-04-08 15:31:19,147 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.output.LayerNorm.bias
2022-04-08 15:31:19,147 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.intermediate.dense.weight
2022-04-08 15:31:19,147 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.intermediate.dense.bias
2022-04-08 15:31:19,147 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.output.dense.weight
2022-04-08 15:31:19,148 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.output.dense.bias
2022-04-08 15:31:19,148 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.output.LayerNorm.weight
2022-04-08 15:31:19,148 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.output.LayerNorm.bias
2022-04-08 15:31:19,148 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.self.query.weight
2022-04-08 15:31:19,148 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.self.query.bias
2022-04-08 15:31:19,148 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.self.key.weight
2022-04-08 15:31:19,148 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.self.key.bias
2022-04-08 15:31:19,148 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.self.value.weight
2022-04-08 15:31:19,149 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.self.value.bias
2022-04-08 15:31:19,149 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.output.dense.weight
2022-04-08 15:31:19,149 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.output.dense.bias
2022-04-08 15:31:19,149 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.output.LayerNorm.weight
2022-04-08 15:31:19,149 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.output.LayerNorm.bias
2022-04-08 15:31:19,149 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.intermediate.dense.weight
2022-04-08 15:31:19,149 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.intermediate.dense.bias
2022-04-08 15:31:19,149 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.output.dense.weight
2022-04-08 15:31:19,150 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.output.dense.bias
2022-04-08 15:31:19,150 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.output.LayerNorm.weight
2022-04-08 15:31:19,150 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.output.LayerNorm.bias
2022-04-08 15:31:19,150 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.self.query.weight
2022-04-08 15:31:19,150 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.self.query.bias
2022-04-08 15:31:19,150 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.self.key.weight
2022-04-08 15:31:19,150 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.self.key.bias
2022-04-08 15:31:19,150 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.self.value.weight
2022-04-08 15:31:19,150 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.self.value.bias
2022-04-08 15:31:19,151 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.output.dense.weight
2022-04-08 15:31:19,151 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.output.dense.bias
2022-04-08 15:31:19,151 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.output.LayerNorm.weight
2022-04-08 15:31:19,151 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.output.LayerNorm.bias
2022-04-08 15:31:19,151 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.intermediate.dense.weight
2022-04-08 15:31:19,151 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.intermediate.dense.bias
2022-04-08 15:31:19,151 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.output.dense.weight
2022-04-08 15:31:19,151 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.output.dense.bias
2022-04-08 15:31:19,152 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.output.LayerNorm.weight
2022-04-08 15:31:19,152 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.output.LayerNorm.bias
2022-04-08 15:31:19,152 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.self.query.weight
2022-04-08 15:31:19,152 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.self.query.bias
2022-04-08 15:31:19,152 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.self.key.weight
2022-04-08 15:31:19,152 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.self.key.bias
2022-04-08 15:31:19,152 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.self.value.weight
2022-04-08 15:31:19,152 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.self.value.bias
2022-04-08 15:31:19,152 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.output.dense.weight
2022-04-08 15:31:19,153 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.output.dense.bias
2022-04-08 15:31:19,153 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.output.LayerNorm.weight
2022-04-08 15:31:19,153 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.output.LayerNorm.bias
2022-04-08 15:31:19,153 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.intermediate.dense.weight
2022-04-08 15:31:19,153 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.intermediate.dense.bias
2022-04-08 15:31:19,153 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.output.dense.weight
2022-04-08 15:31:19,153 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.output.dense.bias
2022-04-08 15:31:19,153 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.output.LayerNorm.weight
2022-04-08 15:31:19,153 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.output.LayerNorm.bias
2022-04-08 15:31:19,154 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.self.query.weight
2022-04-08 15:31:19,154 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.self.query.bias
2022-04-08 15:31:19,154 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.self.key.weight
2022-04-08 15:31:19,154 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.self.key.bias
2022-04-08 15:31:19,154 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.self.value.weight
2022-04-08 15:31:19,154 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.self.value.bias
2022-04-08 15:31:19,154 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.output.dense.weight
2022-04-08 15:31:19,154 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.output.dense.bias
2022-04-08 15:31:19,154 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.output.LayerNorm.weight
2022-04-08 15:31:19,155 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.output.LayerNorm.bias
2022-04-08 15:31:19,155 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.intermediate.dense.weight
2022-04-08 15:31:19,155 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.intermediate.dense.bias
2022-04-08 15:31:19,155 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.output.dense.weight
2022-04-08 15:31:19,155 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.output.dense.bias
2022-04-08 15:31:19,155 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.output.LayerNorm.weight
2022-04-08 15:31:19,155 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.output.LayerNorm.bias
2022-04-08 15:31:19,155 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.self.query.weight
2022-04-08 15:31:19,156 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.self.query.bias
2022-04-08 15:31:19,156 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.self.key.weight
2022-04-08 15:31:19,156 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.self.key.bias
2022-04-08 15:31:19,156 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.self.value.weight
2022-04-08 15:31:19,156 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.self.value.bias
2022-04-08 15:31:19,156 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.output.dense.weight
2022-04-08 15:31:19,156 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.output.dense.bias
2022-04-08 15:31:19,156 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.output.LayerNorm.weight
2022-04-08 15:31:19,156 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.output.LayerNorm.bias
2022-04-08 15:31:19,157 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.intermediate.dense.weight
2022-04-08 15:31:19,157 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.intermediate.dense.bias
2022-04-08 15:31:19,157 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.output.dense.weight
2022-04-08 15:31:19,157 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.output.dense.bias
2022-04-08 15:31:19,157 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.output.LayerNorm.weight
2022-04-08 15:31:19,157 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.output.LayerNorm.bias
2022-04-08 15:31:19,157 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.self.query.weight
2022-04-08 15:31:19,157 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.self.query.bias
2022-04-08 15:31:19,158 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.self.key.weight
2022-04-08 15:31:19,158 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.self.key.bias
2022-04-08 15:31:19,158 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.self.value.weight
2022-04-08 15:31:19,158 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.self.value.bias
2022-04-08 15:31:19,158 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.output.dense.weight
2022-04-08 15:31:19,158 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.output.dense.bias
2022-04-08 15:31:19,158 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.output.LayerNorm.weight
2022-04-08 15:31:19,158 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.output.LayerNorm.bias
2022-04-08 15:31:19,159 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.intermediate.dense.weight
2022-04-08 15:31:19,159 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.intermediate.dense.bias
2022-04-08 15:31:19,159 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.output.dense.weight
2022-04-08 15:31:19,159 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.output.dense.bias
2022-04-08 15:31:19,159 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.output.LayerNorm.weight
2022-04-08 15:31:19,159 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.output.LayerNorm.bias
2022-04-08 15:31:19,159 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.self.query.weight
2022-04-08 15:31:19,159 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.self.query.bias
2022-04-08 15:31:19,160 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.self.key.weight
2022-04-08 15:31:19,160 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.self.key.bias
2022-04-08 15:31:19,160 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.self.value.weight
2022-04-08 15:31:19,160 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.self.value.bias
2022-04-08 15:31:19,160 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.output.dense.weight
2022-04-08 15:31:19,160 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.output.dense.bias
2022-04-08 15:31:19,160 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.output.LayerNorm.weight
2022-04-08 15:31:19,160 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.output.LayerNorm.bias
2022-04-08 15:31:19,161 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.intermediate.dense.weight
2022-04-08 15:31:19,161 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.intermediate.dense.bias
2022-04-08 15:31:19,161 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.output.dense.weight
2022-04-08 15:31:19,161 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.output.dense.bias
2022-04-08 15:31:19,161 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.output.LayerNorm.weight
2022-04-08 15:31:19,161 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.output.LayerNorm.bias
2022-04-08 15:31:19,161 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.self.query.weight
2022-04-08 15:31:19,161 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.self.query.bias
2022-04-08 15:31:19,161 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.self.key.weight
2022-04-08 15:31:19,162 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.self.key.bias
2022-04-08 15:31:19,162 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.self.value.weight
2022-04-08 15:31:19,162 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.self.value.bias
2022-04-08 15:31:19,162 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.output.dense.weight
2022-04-08 15:31:19,162 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.output.dense.bias
2022-04-08 15:31:19,162 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.output.LayerNorm.weight
2022-04-08 15:31:19,162 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.output.LayerNorm.bias
2022-04-08 15:31:19,162 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.intermediate.dense.weight
2022-04-08 15:31:19,163 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.intermediate.dense.bias
2022-04-08 15:31:19,163 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.output.dense.weight
2022-04-08 15:31:19,163 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.output.dense.bias
2022-04-08 15:31:19,163 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.output.LayerNorm.weight
2022-04-08 15:31:19,163 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.output.LayerNorm.bias
2022-04-08 15:31:19,163 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.self.query.weight
2022-04-08 15:31:19,163 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.self.query.bias
2022-04-08 15:31:19,163 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.self.key.weight
2022-04-08 15:31:19,164 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.self.key.bias
2022-04-08 15:31:19,164 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.self.value.weight
2022-04-08 15:31:19,164 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.self.value.bias
2022-04-08 15:31:19,164 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.output.dense.weight
2022-04-08 15:31:19,164 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.output.dense.bias
2022-04-08 15:31:19,164 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.output.LayerNorm.weight
2022-04-08 15:31:19,164 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.output.LayerNorm.bias
2022-04-08 15:31:19,164 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.intermediate.dense.weight
2022-04-08 15:31:19,164 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.intermediate.dense.bias
2022-04-08 15:31:19,165 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.output.dense.weight
2022-04-08 15:31:19,165 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.output.dense.bias
2022-04-08 15:31:19,165 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.output.LayerNorm.weight
2022-04-08 15:31:19,165 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.output.LayerNorm.bias
2022-04-08 15:31:19,165 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.self.query.weight
2022-04-08 15:31:19,165 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.self.query.bias
2022-04-08 15:31:19,165 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.self.key.weight
2022-04-08 15:31:19,165 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.self.key.bias
2022-04-08 15:31:19,166 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.self.value.weight
2022-04-08 15:31:19,166 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.self.value.bias
2022-04-08 15:31:19,166 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.output.dense.weight
2022-04-08 15:31:19,166 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.output.dense.bias
2022-04-08 15:31:19,166 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.output.LayerNorm.weight
2022-04-08 15:31:19,166 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.output.LayerNorm.bias
2022-04-08 15:31:19,166 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.intermediate.dense.weight
2022-04-08 15:31:19,166 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.intermediate.dense.bias
2022-04-08 15:31:19,166 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.output.dense.weight
2022-04-08 15:31:19,167 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.output.dense.bias
2022-04-08 15:31:19,167 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.output.LayerNorm.weight
2022-04-08 15:31:19,167 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.output.LayerNorm.bias
2022-04-08 15:31:19,167 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.self.query.weight
2022-04-08 15:31:19,167 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.self.query.bias
2022-04-08 15:31:19,167 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.self.key.weight
2022-04-08 15:31:19,167 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.self.key.bias
2022-04-08 15:31:19,167 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.self.value.weight
2022-04-08 15:31:19,168 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.self.value.bias
2022-04-08 15:31:19,168 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.output.dense.weight
2022-04-08 15:31:19,168 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.output.dense.bias
2022-04-08 15:31:19,168 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.output.LayerNorm.weight
2022-04-08 15:31:19,168 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.output.LayerNorm.bias
2022-04-08 15:31:19,168 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.intermediate.dense.weight
2022-04-08 15:31:19,168 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.intermediate.dense.bias
2022-04-08 15:31:19,168 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.output.dense.weight
2022-04-08 15:31:19,169 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.output.dense.bias
2022-04-08 15:31:19,169 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.output.LayerNorm.weight
2022-04-08 15:31:19,169 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.output.LayerNorm.bias
2022-04-08 15:31:19,169 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.self.query.weight
2022-04-08 15:31:19,169 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.self.query.bias
2022-04-08 15:31:19,169 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.self.key.weight
2022-04-08 15:31:19,169 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.self.key.bias
2022-04-08 15:31:19,169 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.self.value.weight
2022-04-08 15:31:19,170 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.self.value.bias
2022-04-08 15:31:19,170 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.output.dense.weight
2022-04-08 15:31:19,170 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.output.dense.bias
2022-04-08 15:31:19,170 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.output.LayerNorm.weight
2022-04-08 15:31:19,170 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.output.LayerNorm.bias
2022-04-08 15:31:19,170 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.intermediate.dense.weight
2022-04-08 15:31:19,170 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.intermediate.dense.bias
2022-04-08 15:31:19,170 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.output.dense.weight
2022-04-08 15:31:19,171 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.output.dense.bias
2022-04-08 15:31:19,171 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.output.LayerNorm.weight
2022-04-08 15:31:19,171 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.output.LayerNorm.bias
2022-04-08 15:31:19,171 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.self.query.weight
2022-04-08 15:31:19,171 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.self.query.bias
2022-04-08 15:31:19,171 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.self.key.weight
2022-04-08 15:31:19,171 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.self.key.bias
2022-04-08 15:31:19,171 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.self.value.weight
2022-04-08 15:31:19,171 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.self.value.bias
2022-04-08 15:31:19,172 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.output.dense.weight
2022-04-08 15:31:19,172 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.output.dense.bias
2022-04-08 15:31:19,172 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.output.LayerNorm.weight
2022-04-08 15:31:19,172 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.output.LayerNorm.bias
2022-04-08 15:31:19,172 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.intermediate.dense.weight
2022-04-08 15:31:19,172 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.intermediate.dense.bias
2022-04-08 15:31:19,172 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.output.dense.weight
2022-04-08 15:31:19,172 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.output.dense.bias
2022-04-08 15:31:19,173 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.output.LayerNorm.weight
2022-04-08 15:31:19,173 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.output.LayerNorm.bias
2022-04-08 15:31:19,173 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.self.query.weight
2022-04-08 15:31:19,173 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.self.query.bias
2022-04-08 15:31:19,173 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.self.key.weight
2022-04-08 15:31:19,173 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.self.key.bias
2022-04-08 15:31:19,173 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.self.value.weight
2022-04-08 15:31:19,173 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.self.value.bias
2022-04-08 15:31:19,174 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.output.dense.weight
2022-04-08 15:31:19,174 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.output.dense.bias
2022-04-08 15:31:19,174 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.output.LayerNorm.weight
2022-04-08 15:31:19,174 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.output.LayerNorm.bias
2022-04-08 15:31:19,174 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.intermediate.dense.weight
2022-04-08 15:31:19,174 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.intermediate.dense.bias
2022-04-08 15:31:19,174 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.output.dense.weight
2022-04-08 15:31:19,174 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.output.dense.bias
2022-04-08 15:31:19,174 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.output.LayerNorm.weight
2022-04-08 15:31:19,175 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.output.LayerNorm.bias
2022-04-08 15:31:19,175 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.self.query.weight
2022-04-08 15:31:19,175 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.self.query.bias
2022-04-08 15:31:19,175 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.self.key.weight
2022-04-08 15:31:19,175 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.self.key.bias
2022-04-08 15:31:19,175 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.self.value.weight
2022-04-08 15:31:19,175 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.self.value.bias
2022-04-08 15:31:19,175 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.output.dense.weight
2022-04-08 15:31:19,176 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.output.dense.bias
2022-04-08 15:31:19,176 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.output.LayerNorm.weight
2022-04-08 15:31:19,176 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.output.LayerNorm.bias
2022-04-08 15:31:19,176 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.intermediate.dense.weight
2022-04-08 15:31:19,176 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.intermediate.dense.bias
2022-04-08 15:31:19,176 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.output.dense.weight
2022-04-08 15:31:19,176 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.output.dense.bias
2022-04-08 15:31:19,176 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.output.LayerNorm.weight
2022-04-08 15:31:19,177 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.output.LayerNorm.bias
2022-04-08 15:31:19,177 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.self.query.weight
2022-04-08 15:31:19,177 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.self.query.bias
2022-04-08 15:31:19,177 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.self.key.weight
2022-04-08 15:31:19,177 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.self.key.bias
2022-04-08 15:31:19,177 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.self.value.weight
2022-04-08 15:31:19,177 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.self.value.bias
2022-04-08 15:31:19,177 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.output.dense.weight
2022-04-08 15:31:19,177 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.output.dense.bias
2022-04-08 15:31:19,178 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.output.LayerNorm.weight
2022-04-08 15:31:19,178 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.output.LayerNorm.bias
2022-04-08 15:31:19,178 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.intermediate.dense.weight
2022-04-08 15:31:19,178 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.intermediate.dense.bias
2022-04-08 15:31:19,178 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.output.dense.weight
2022-04-08 15:31:19,178 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.output.dense.bias
2022-04-08 15:31:19,178 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.output.LayerNorm.weight
2022-04-08 15:31:19,178 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.output.LayerNorm.bias
2022-04-08 15:31:19,179 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.self.query.weight
2022-04-08 15:31:19,179 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.self.query.bias
2022-04-08 15:31:19,179 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.self.key.weight
2022-04-08 15:31:19,179 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.self.key.bias
2022-04-08 15:31:19,179 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.self.value.weight
2022-04-08 15:31:19,179 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.self.value.bias
2022-04-08 15:31:19,179 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.output.dense.weight
2022-04-08 15:31:19,180 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.output.dense.bias
2022-04-08 15:31:19,180 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.output.LayerNorm.weight
2022-04-08 15:31:19,180 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.output.LayerNorm.bias
2022-04-08 15:31:19,180 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.intermediate.dense.weight
2022-04-08 15:31:19,180 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.intermediate.dense.bias
2022-04-08 15:31:19,180 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.output.dense.weight
2022-04-08 15:31:19,180 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.output.dense.bias
2022-04-08 15:31:19,180 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.output.LayerNorm.weight
2022-04-08 15:31:19,180 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.output.LayerNorm.bias
2022-04-08 15:31:19,181 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.self.query.weight
2022-04-08 15:31:19,181 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.self.query.bias
2022-04-08 15:31:19,181 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.self.key.weight
2022-04-08 15:31:19,181 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.self.key.bias
2022-04-08 15:31:19,181 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.self.value.weight
2022-04-08 15:31:19,181 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.self.value.bias
2022-04-08 15:31:19,181 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.output.dense.weight
2022-04-08 15:31:19,181 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.output.dense.bias
2022-04-08 15:31:19,182 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.output.LayerNorm.weight
2022-04-08 15:31:19,182 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.output.LayerNorm.bias
2022-04-08 15:31:19,182 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.intermediate.dense.weight
2022-04-08 15:31:19,182 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.intermediate.dense.bias
2022-04-08 15:31:19,182 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.output.dense.weight
2022-04-08 15:31:19,182 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.output.dense.bias
2022-04-08 15:31:19,182 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.output.LayerNorm.weight
2022-04-08 15:31:19,182 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.output.LayerNorm.bias
2022-04-08 15:31:19,182 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.self.query.weight
2022-04-08 15:31:19,183 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.self.query.bias
2022-04-08 15:31:19,183 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.self.key.weight
2022-04-08 15:31:19,183 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.self.key.bias
2022-04-08 15:31:19,183 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.self.value.weight
2022-04-08 15:31:19,183 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.self.value.bias
2022-04-08 15:31:19,183 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.output.dense.weight
2022-04-08 15:31:19,183 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.output.dense.bias
2022-04-08 15:31:19,183 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.output.LayerNorm.weight
2022-04-08 15:31:19,184 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.output.LayerNorm.bias
2022-04-08 15:31:19,184 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.intermediate.dense.weight
2022-04-08 15:31:19,184 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.intermediate.dense.bias
2022-04-08 15:31:19,184 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.output.dense.weight
2022-04-08 15:31:19,184 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.output.dense.bias
2022-04-08 15:31:19,184 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.output.LayerNorm.weight
2022-04-08 15:31:19,184 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.output.LayerNorm.bias
2022-04-08 15:31:19,184 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.self.query.weight
2022-04-08 15:31:19,184 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.self.query.bias
2022-04-08 15:31:19,185 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.self.key.weight
2022-04-08 15:31:19,185 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.self.key.bias
2022-04-08 15:31:19,185 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.self.value.weight
2022-04-08 15:31:19,185 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.self.value.bias
2022-04-08 15:31:19,185 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.output.dense.weight
2022-04-08 15:31:19,185 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.output.dense.bias
2022-04-08 15:31:19,185 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.output.LayerNorm.weight
2022-04-08 15:31:19,185 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.output.LayerNorm.bias
2022-04-08 15:31:19,185 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.intermediate.dense.weight
2022-04-08 15:31:19,186 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.intermediate.dense.bias
2022-04-08 15:31:19,186 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.output.dense.weight
2022-04-08 15:31:19,186 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.output.dense.bias
2022-04-08 15:31:19,186 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.output.LayerNorm.weight
2022-04-08 15:31:19,186 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.output.LayerNorm.bias
2022-04-08 15:31:19,186 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.self.query.weight
2022-04-08 15:31:19,186 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.self.query.bias
2022-04-08 15:31:19,186 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.self.key.weight
2022-04-08 15:31:19,186 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.self.key.bias
2022-04-08 15:31:19,187 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.self.value.weight
2022-04-08 15:31:19,187 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.self.value.bias
2022-04-08 15:31:19,187 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.output.dense.weight
2022-04-08 15:31:19,187 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.output.dense.bias
2022-04-08 15:31:19,187 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.output.LayerNorm.weight
2022-04-08 15:31:19,187 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.output.LayerNorm.bias
2022-04-08 15:31:19,187 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.intermediate.dense.weight
2022-04-08 15:31:19,187 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.intermediate.dense.bias
2022-04-08 15:31:19,188 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.output.dense.weight
2022-04-08 15:31:19,188 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.output.dense.bias
2022-04-08 15:31:19,188 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.output.LayerNorm.weight
2022-04-08 15:31:19,188 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.output.LayerNorm.bias
2022-04-08 15:31:19,188 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.self.query.weight
2022-04-08 15:31:19,188 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.self.query.bias
2022-04-08 15:31:19,188 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.self.key.weight
2022-04-08 15:31:19,188 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.self.key.bias
2022-04-08 15:31:19,188 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.self.value.weight
2022-04-08 15:31:19,189 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.self.value.bias
2022-04-08 15:31:19,189 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.output.dense.weight
2022-04-08 15:31:19,189 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.output.dense.bias
2022-04-08 15:31:19,189 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.output.LayerNorm.weight
2022-04-08 15:31:19,189 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.output.LayerNorm.bias
2022-04-08 15:31:19,189 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.intermediate.dense.weight
2022-04-08 15:31:19,189 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.intermediate.dense.bias
2022-04-08 15:31:19,189 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.output.dense.weight
2022-04-08 15:31:19,190 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.output.dense.bias
2022-04-08 15:31:19,190 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.output.LayerNorm.weight
2022-04-08 15:31:19,190 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.output.LayerNorm.bias
2022-04-08 15:31:19,190 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.self.query.weight
2022-04-08 15:31:19,190 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.self.query.bias
2022-04-08 15:31:19,190 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.self.key.weight
2022-04-08 15:31:19,190 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.self.key.bias
2022-04-08 15:31:19,190 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.self.value.weight
2022-04-08 15:31:19,190 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.self.value.bias
2022-04-08 15:31:19,191 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.output.dense.weight
2022-04-08 15:31:19,191 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.output.dense.bias
2022-04-08 15:31:19,191 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.output.LayerNorm.weight
2022-04-08 15:31:19,191 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.output.LayerNorm.bias
2022-04-08 15:31:19,191 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.intermediate.dense.weight
2022-04-08 15:31:19,191 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.intermediate.dense.bias
2022-04-08 15:31:19,191 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.output.dense.weight
2022-04-08 15:31:19,191 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.output.dense.bias
2022-04-08 15:31:19,192 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.output.LayerNorm.weight
2022-04-08 15:31:19,192 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.output.LayerNorm.bias
2022-04-08 15:31:19,192 - INFO - allennlp.common.util - _classifier.roberta.pooler.dense.weight
2022-04-08 15:31:19,192 - INFO - allennlp.common.util - _classifier.roberta.pooler.dense.bias
2022-04-08 15:31:19,192 - INFO - allennlp.common.util - _classifier.classifier.dense.weight
2022-04-08 15:31:19,192 - INFO - allennlp.common.util - _classifier.classifier.dense.bias
2022-04-08 15:31:19,192 - INFO - allennlp.common.util - _classifier.classifier.out_proj.weight
2022-04-08 15:31:19,192 - INFO - allennlp.common.util - _classifier.classifier.out_proj.bias
2022-04-08 15:31:19,193 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.type = slanted_triangular
2022-04-08 15:31:19,193 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.cut_frac = 0.06
2022-04-08 15:31:19,193 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.ratio = 32
2022-04-08 15:31:19,193 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.last_epoch = -1
2022-04-08 15:31:19,193 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.gradual_unfreezing = False
2022-04-08 15:31:19,193 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.discriminative_fine_tuning = False
2022-04-08 15:31:19,194 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.decay_factor = 0.38
2022-04-08 15:31:19,194 - INFO - allennlp.common.params - trainer.checkpointer.type = default
2022-04-08 15:31:19,194 - INFO - allennlp.common.params - trainer.checkpointer.keep_serialized_model_every_num_seconds = None
2022-04-08 15:31:19,194 - INFO - allennlp.common.params - trainer.checkpointer.num_serialized_models_to_keep = -1
2022-04-08 15:31:19,194 - INFO - allennlp.common.params - trainer.checkpointer.model_save_interval = None
2022-04-08 15:31:19,194 - INFO - allennlp.common.params - summary_interval = 100
2022-04-08 15:31:19,195 - INFO - allennlp.common.params - histogram_interval = None
2022-04-08 15:31:19,195 - INFO - allennlp.common.params - batch_size_interval = None
2022-04-08 15:31:19,195 - INFO - allennlp.common.params - should_log_parameter_statistics = True
2022-04-08 15:31:19,195 - INFO - allennlp.common.params - should_log_learning_rate = False
2022-04-08 15:31:19,195 - INFO - allennlp.common.params - get_batch_num_total = None
2022-04-08 15:31:19,197 - WARNING - allennlp.training.trainer - You provided a validation dataset but patience was set to None, meaning that early stopping is disabled
2022-04-08 15:31:19,216 - INFO - allennlp.training.trainer - Beginning training.
2022-04-08 15:31:19,217 - INFO - allennlp.training.trainer - Epoch 0/9
2022-04-08 15:31:19,217 - INFO - allennlp.training.trainer - Worker 0 memory usage: 7.1G
2022-04-08 15:31:19,217 - INFO - allennlp.training.trainer - GPU 0 memory usage: 5.6G
2022-04-08 15:31:19,219 - INFO - allennlp.training.trainer - Training
2022-04-08 15:31:19,219 - INFO - tqdm - 0%|          | 0/590 [00:00<?, ?it/s]
2022-04-08 15:31:19,219 - INFO - allennlp.data.samplers.bucket_batch_sampler - No sorting keys given; trying to guess a good one
2022-04-08 15:31:19,220 - INFO - allennlp.data.samplers.bucket_batch_sampler - Using ['tokens'] as the sorting keys
2022-04-08 15:31:29,342 - INFO - tqdm - accuracy: 0.5000, batch_loss: 0.5679, loss: 0.7030 ||:   1%|          | 3/590 [00:10<32:13,  3.29s/it]
2022-04-08 15:31:41,020 - INFO - tqdm - accuracy: 0.5268, batch_loss: 0.7824, loss: 0.7195 ||:   1%|1         | 7/590 [00:21<29:20,  3.02s/it]
2022-04-08 15:31:53,320 - INFO - tqdm - accuracy: 0.5710, batch_loss: 0.5888, loss: 0.6884 ||:   2%|1         | 11/590 [00:34<29:12,  3.03s/it]
2022-04-08 15:32:05,102 - INFO - tqdm - accuracy: 0.5792, batch_loss: 0.7172, loss: 0.6881 ||:   3%|2         | 15/590 [00:45<27:58,  2.92s/it]
2022-04-08 15:32:16,811 - INFO - tqdm - accuracy: 0.5872, batch_loss: 0.6387, loss: 0.6843 ||:   3%|3         | 19/590 [00:57<27:53,  2.93s/it]
2022-04-08 15:32:29,281 - INFO - tqdm - accuracy: 0.5924, batch_loss: 0.5629, loss: 0.6746 ||:   4%|3         | 23/590 [01:10<29:53,  3.16s/it]
2022-04-08 15:32:39,657 - INFO - tqdm - accuracy: 0.5925, batch_loss: 0.6711, loss: 0.6710 ||:   4%|4         | 26/590 [01:20<31:52,  3.39s/it]
2022-04-08 15:32:49,923 - INFO - tqdm - accuracy: 0.5991, batch_loss: 0.6090, loss: 0.6701 ||:   5%|4         | 29/590 [01:30<31:52,  3.41s/it]
2022-04-08 15:33:01,729 - INFO - tqdm - accuracy: 0.6004, batch_loss: 0.7375, loss: 0.6688 ||:   6%|5         | 33/590 [01:42<28:53,  3.11s/it]
2022-04-08 15:33:14,135 - INFO - tqdm - accuracy: 0.5980, batch_loss: 0.6589, loss: 0.6694 ||:   6%|6         | 37/590 [01:54<29:00,  3.15s/it]
2022-04-08 15:33:26,027 - INFO - tqdm - accuracy: 0.5960, batch_loss: 0.6794, loss: 0.6703 ||:   7%|6         | 41/590 [02:06<27:23,  2.99s/it]
2022-04-08 15:33:39,048 - INFO - tqdm - accuracy: 0.6014, batch_loss: 0.6718, loss: 0.6706 ||:   8%|7         | 45/590 [02:19<28:26,  3.13s/it]
2022-04-08 15:33:51,995 - INFO - tqdm - accuracy: 0.6027, batch_loss: 0.6101, loss: 0.6678 ||:   8%|8         | 49/590 [02:32<29:20,  3.25s/it]
2022-04-08 15:34:04,992 - INFO - tqdm - accuracy: 0.6067, batch_loss: 0.6710, loss: 0.6656 ||:   9%|8         | 53/590 [02:45<29:18,  3.27s/it]
2022-04-08 15:34:16,121 - INFO - tqdm - accuracy: 0.6060, batch_loss: 0.6571, loss: 0.6651 ||:   9%|9         | 56/590 [02:56<31:23,  3.53s/it]
2022-04-08 15:34:26,481 - INFO - tqdm - accuracy: 0.6065, batch_loss: 0.6418, loss: 0.6628 ||:  10%|#         | 59/590 [03:07<30:40,  3.47s/it]
2022-04-08 15:34:38,029 - INFO - tqdm - accuracy: 0.6096, batch_loss: 0.6178, loss: 0.6602 ||:  11%|#         | 63/590 [03:18<26:12,  2.98s/it]
2022-04-08 15:34:49,878 - INFO - tqdm - accuracy: 0.6077, batch_loss: 0.6411, loss: 0.6606 ||:  11%|#1        | 67/590 [03:30<25:57,  2.98s/it]
2022-04-08 15:35:00,894 - INFO - tqdm - accuracy: 0.6089, batch_loss: 0.7429, loss: 0.6610 ||:  12%|#1        | 70/590 [03:41<29:41,  3.43s/it]
2022-04-08 15:35:11,259 - INFO - tqdm - accuracy: 0.6134, batch_loss: 0.5353, loss: 0.6561 ||:  12%|#2        | 73/590 [03:52<29:27,  3.42s/it]
2022-04-08 15:35:21,833 - INFO - tqdm - accuracy: 0.6143, batch_loss: 0.6760, loss: 0.6553 ||:  13%|#2        | 76/590 [04:02<29:51,  3.49s/it]
2022-04-08 15:35:31,878 - INFO - tqdm - accuracy: 0.6155, batch_loss: 0.7043, loss: 0.6542 ||:  13%|#3        | 79/590 [04:12<29:05,  3.42s/it]
2022-04-08 15:35:42,764 - INFO - tqdm - accuracy: 0.6166, batch_loss: 0.6550, loss: 0.6526 ||:  14%|#3        | 82/590 [04:23<30:19,  3.58s/it]
2022-04-08 15:35:53,008 - INFO - tqdm - accuracy: 0.6173, batch_loss: 0.4884, loss: 0.6504 ||:  14%|#4        | 85/590 [04:33<28:43,  3.41s/it]
2022-04-08 15:36:03,578 - INFO - tqdm - accuracy: 0.6186, batch_loss: 0.6147, loss: 0.6487 ||:  15%|#4        | 88/590 [04:44<29:11,  3.49s/it]
2022-04-08 15:36:13,713 - INFO - tqdm - accuracy: 0.6198, batch_loss: 0.5888, loss: 0.6460 ||:  15%|#5        | 91/590 [04:54<28:45,  3.46s/it]
2022-04-08 15:36:26,940 - INFO - tqdm - accuracy: 0.6230, batch_loss: 0.7517, loss: 0.6451 ||:  16%|#6        | 95/590 [05:07<27:42,  3.36s/it]
2022-04-08 15:36:37,848 - INFO - tqdm - accuracy: 0.6260, batch_loss: 0.5598, loss: 0.6420 ||:  17%|#6        | 98/590 [05:18<29:12,  3.56s/it]
2022-04-08 15:36:48,217 - INFO - tqdm - accuracy: 0.6300, batch_loss: 0.4842, loss: 0.6392 ||:  17%|#7        | 101/590 [05:28<28:36,  3.51s/it]
2022-04-08 15:36:58,670 - INFO - tqdm - accuracy: 0.6304, batch_loss: 0.4741, loss: 0.6377 ||:  18%|#7        | 104/590 [05:39<28:23,  3.51s/it]
2022-04-08 15:37:08,909 - INFO - tqdm - accuracy: 0.6343, batch_loss: 0.5859, loss: 0.6344 ||:  18%|#8        | 107/590 [05:49<27:30,  3.42s/it]
2022-04-08 15:37:19,381 - INFO - tqdm - accuracy: 0.6369, batch_loss: 0.5418, loss: 0.6317 ||:  19%|#8        | 110/590 [06:00<27:46,  3.47s/it]
2022-04-08 15:37:29,568 - INFO - tqdm - accuracy: 0.6369, batch_loss: 0.6622, loss: 0.6310 ||:  19%|#9        | 113/590 [06:10<27:30,  3.46s/it]
2022-04-08 15:37:42,295 - INFO - tqdm - accuracy: 0.6394, batch_loss: 0.5846, loss: 0.6293 ||:  20%|#9        | 117/590 [06:23<25:09,  3.19s/it]
2022-04-08 15:37:53,468 - INFO - tqdm - accuracy: 0.6398, batch_loss: 0.5812, loss: 0.6296 ||:  20%|##        | 120/590 [06:34<27:42,  3.54s/it]
2022-04-08 15:38:03,909 - INFO - tqdm - accuracy: 0.6413, batch_loss: 0.6249, loss: 0.6283 ||:  21%|##        | 123/590 [06:44<27:16,  3.50s/it]
2022-04-08 15:38:14,270 - INFO - tqdm - accuracy: 0.6424, batch_loss: 0.6135, loss: 0.6279 ||:  21%|##1       | 126/590 [06:55<26:49,  3.47s/it]
2022-04-08 15:38:24,468 - INFO - tqdm - accuracy: 0.6458, batch_loss: 0.5197, loss: 0.6241 ||:  22%|##1       | 129/590 [07:05<26:39,  3.47s/it]
2022-04-08 15:38:38,259 - INFO - tqdm - accuracy: 0.6480, batch_loss: 0.5623, loss: 0.6219 ||:  23%|##2       | 133/590 [07:19<26:49,  3.52s/it]
2022-04-08 15:38:49,922 - INFO - tqdm - accuracy: 0.6459, batch_loss: 0.7641, loss: 0.6237 ||:  23%|##3       | 136/590 [07:30<28:22,  3.75s/it]
2022-04-08 15:39:00,274 - INFO - tqdm - accuracy: 0.6470, batch_loss: 0.4873, loss: 0.6221 ||:  24%|##3       | 139/590 [07:41<26:37,  3.54s/it]
2022-04-08 15:39:10,411 - INFO - tqdm - accuracy: 0.6463, batch_loss: 0.6454, loss: 0.6226 ||:  24%|##4       | 142/590 [07:51<26:23,  3.54s/it]
2022-04-08 15:39:21,510 - INFO - tqdm - accuracy: 0.6478, batch_loss: 0.4878, loss: 0.6204 ||:  25%|##4       | 145/590 [08:02<26:43,  3.60s/it]
2022-04-08 15:39:34,741 - INFO - tqdm - accuracy: 0.6512, batch_loss: 0.4811, loss: 0.6175 ||:  25%|##5       | 149/590 [08:15<25:09,  3.42s/it]
2022-04-08 15:39:47,757 - INFO - tqdm - accuracy: 0.6532, batch_loss: 0.4803, loss: 0.6150 ||:  26%|##5       | 153/590 [08:28<24:00,  3.30s/it]
2022-04-08 15:39:58,587 - INFO - tqdm - accuracy: 0.6540, batch_loss: 0.5171, loss: 0.6147 ||:  26%|##6       | 156/590 [08:39<25:13,  3.49s/it]
2022-04-08 15:40:09,015 - INFO - tqdm - accuracy: 0.6568, batch_loss: 0.5232, loss: 0.6122 ||:  27%|##6       | 159/590 [08:49<25:17,  3.52s/it]
2022-04-08 15:40:19,726 - INFO - tqdm - accuracy: 0.6582, batch_loss: 0.6458, loss: 0.6111 ||:  27%|##7       | 162/590 [09:00<25:23,  3.56s/it]
2022-04-08 15:40:30,070 - INFO - tqdm - accuracy: 0.6598, batch_loss: 0.4033, loss: 0.6089 ||:  28%|##7       | 165/590 [09:10<24:27,  3.45s/it]
2022-04-08 15:40:42,730 - INFO - tqdm - accuracy: 0.6625, batch_loss: 0.4263, loss: 0.6063 ||:  29%|##8       | 169/590 [09:23<22:39,  3.23s/it]
2022-04-08 15:40:52,883 - INFO - tqdm - accuracy: 0.6635, batch_loss: 0.5572, loss: 0.6055 ||:  29%|##9       | 172/590 [09:33<23:12,  3.33s/it]
2022-04-08 15:41:06,434 - INFO - tqdm - accuracy: 0.6651, batch_loss: 0.4567, loss: 0.6032 ||:  30%|##9       | 176/590 [09:47<23:27,  3.40s/it]
2022-04-08 15:41:16,727 - INFO - tqdm - accuracy: 0.6653, batch_loss: 0.5915, loss: 0.6025 ||:  30%|###       | 179/590 [09:57<23:29,  3.43s/it]
2022-04-08 15:41:26,760 - INFO - tqdm - accuracy: 0.6674, batch_loss: 0.4517, loss: 0.5999 ||:  31%|###       | 182/590 [10:07<23:12,  3.41s/it]
2022-04-08 15:41:37,764 - INFO - tqdm - accuracy: 0.6698, batch_loss: 0.4238, loss: 0.5965 ||:  31%|###1      | 185/590 [10:18<24:20,  3.61s/it]
2022-04-08 15:41:48,255 - INFO - tqdm - accuracy: 0.6707, batch_loss: 0.6249, loss: 0.5960 ||:  32%|###1      | 188/590 [10:29<23:12,  3.46s/it]
2022-04-08 15:41:58,638 - INFO - tqdm - accuracy: 0.6723, batch_loss: 0.6385, loss: 0.5942 ||:  32%|###2      | 191/590 [10:39<23:16,  3.50s/it]
2022-04-08 15:42:08,999 - INFO - tqdm - accuracy: 0.6733, batch_loss: 0.5113, loss: 0.5935 ||:  33%|###2      | 194/590 [10:49<22:51,  3.46s/it]
2022-04-08 15:42:19,884 - INFO - tqdm - accuracy: 0.6747, batch_loss: 0.5279, loss: 0.5911 ||:  33%|###3      | 197/590 [11:00<23:17,  3.56s/it]
2022-04-08 15:42:32,844 - INFO - tqdm - accuracy: 0.6769, batch_loss: 0.4547, loss: 0.5888 ||:  34%|###4      | 201/590 [11:13<21:37,  3.34s/it]
2022-04-08 15:42:45,010 - INFO - tqdm - accuracy: 0.6793, batch_loss: 0.3781, loss: 0.5858 ||:  35%|###4      | 205/590 [11:25<19:51,  3.09s/it]
2022-04-08 15:42:59,088 - INFO - tqdm - accuracy: 0.6803, batch_loss: 0.3835, loss: 0.5842 ||:  35%|###5      | 209/590 [11:39<22:08,  3.49s/it]
2022-04-08 15:43:09,443 - INFO - tqdm - accuracy: 0.6807, batch_loss: 0.4943, loss: 0.5833 ||:  36%|###5      | 212/590 [11:50<21:56,  3.48s/it]
2022-04-08 15:43:20,296 - INFO - tqdm - accuracy: 0.6826, batch_loss: 0.4035, loss: 0.5815 ||:  36%|###6      | 215/590 [12:01<22:06,  3.54s/it]
2022-04-08 15:43:30,384 - INFO - tqdm - accuracy: 0.6829, batch_loss: 0.5734, loss: 0.5810 ||:  37%|###6      | 218/590 [12:11<21:22,  3.45s/it]
2022-04-08 15:43:40,737 - INFO - tqdm - accuracy: 0.6831, batch_loss: 0.4920, loss: 0.5804 ||:  37%|###7      | 221/590 [12:21<21:29,  3.49s/it]
2022-04-08 15:43:51,503 - INFO - tqdm - accuracy: 0.6843, batch_loss: 0.4755, loss: 0.5793 ||:  38%|###7      | 224/590 [12:32<21:51,  3.58s/it]
2022-04-08 15:44:04,541 - INFO - tqdm - accuracy: 0.6853, batch_loss: 0.5734, loss: 0.5782 ||:  39%|###8      | 228/590 [12:45<20:14,  3.36s/it]
2022-04-08 15:44:15,825 - INFO - tqdm - accuracy: 0.6870, batch_loss: 0.3680, loss: 0.5761 ||:  39%|###9      | 231/590 [12:56<21:52,  3.65s/it]
2022-04-08 15:44:27,786 - INFO - tqdm - accuracy: 0.6872, batch_loss: 0.6080, loss: 0.5757 ||:  40%|###9      | 234/590 [13:08<22:14,  3.75s/it]
2022-04-08 15:44:40,333 - INFO - tqdm - accuracy: 0.6883, batch_loss: 0.5796, loss: 0.5745 ||:  40%|####      | 238/590 [13:21<19:26,  3.31s/it]
2022-04-08 15:44:53,235 - INFO - tqdm - accuracy: 0.6896, batch_loss: 0.3805, loss: 0.5723 ||:  41%|####1     | 242/590 [13:34<18:57,  3.27s/it]
2022-04-08 15:45:03,638 - INFO - tqdm - accuracy: 0.6907, batch_loss: 0.4101, loss: 0.5709 ||:  42%|####1     | 245/590 [13:44<19:34,  3.40s/it]
2022-04-08 15:45:14,659 - INFO - tqdm - accuracy: 0.6920, batch_loss: 0.4404, loss: 0.5693 ||:  42%|####2     | 248/590 [13:55<20:16,  3.56s/it]
2022-04-08 15:45:24,706 - INFO - tqdm - accuracy: 0.6930, batch_loss: 0.5781, loss: 0.5682 ||:  43%|####2     | 251/590 [14:05<19:45,  3.50s/it]
2022-04-08 15:45:38,520 - INFO - tqdm - accuracy: 0.6944, batch_loss: 0.5092, loss: 0.5664 ||:  43%|####3     | 255/590 [14:19<19:34,  3.51s/it]
2022-04-08 15:45:48,894 - INFO - tqdm - accuracy: 0.6957, batch_loss: 0.4511, loss: 0.5646 ||:  44%|####3     | 258/590 [14:29<18:50,  3.41s/it]
2022-04-08 15:45:59,072 - INFO - tqdm - accuracy: 0.6962, batch_loss: 0.3741, loss: 0.5645 ||:  44%|####4     | 261/590 [14:39<18:40,  3.41s/it]
2022-04-08 15:46:11,913 - INFO - tqdm - accuracy: 0.6972, batch_loss: 0.5034, loss: 0.5640 ||:  45%|####4     | 265/590 [14:52<17:26,  3.22s/it]
2022-04-08 15:46:22,011 - INFO - tqdm - accuracy: 0.6987, batch_loss: 0.5211, loss: 0.5624 ||:  45%|####5     | 268/590 [15:02<17:50,  3.32s/it]
2022-04-08 15:46:35,078 - INFO - tqdm - accuracy: 0.7000, batch_loss: 0.4351, loss: 0.5608 ||:  46%|####6     | 272/590 [15:15<17:26,  3.29s/it]
2022-04-08 15:46:45,632 - INFO - tqdm - accuracy: 0.7018, batch_loss: 0.2891, loss: 0.5586 ||:  47%|####6     | 275/590 [15:26<17:33,  3.34s/it]
2022-04-08 15:46:59,129 - INFO - tqdm - accuracy: 0.7027, batch_loss: 0.4280, loss: 0.5574 ||:  47%|####7     | 279/590 [15:39<17:50,  3.44s/it]
2022-04-08 15:47:09,298 - INFO - tqdm - accuracy: 0.7032, batch_loss: 0.4148, loss: 0.5563 ||:  48%|####7     | 282/590 [15:50<17:32,  3.42s/it]
2022-04-08 15:47:19,482 - INFO - tqdm - accuracy: 0.7027, batch_loss: 0.4963, loss: 0.5560 ||:  48%|####8     | 285/590 [16:00<17:48,  3.50s/it]
2022-04-08 15:47:32,526 - INFO - tqdm - accuracy: 0.7039, batch_loss: 0.4784, loss: 0.5548 ||:  49%|####8     | 289/590 [16:13<16:50,  3.36s/it]
2022-04-08 15:47:43,428 - INFO - tqdm - accuracy: 0.7048, batch_loss: 0.4733, loss: 0.5539 ||:  49%|####9     | 292/590 [16:24<17:30,  3.53s/it]
2022-04-08 15:47:56,423 - INFO - tqdm - accuracy: 0.7063, batch_loss: 0.4342, loss: 0.5518 ||:  50%|#####     | 296/590 [16:37<16:27,  3.36s/it]
2022-04-08 15:48:09,946 - INFO - tqdm - accuracy: 0.7073, batch_loss: 0.5055, loss: 0.5505 ||:  51%|#####     | 300/590 [16:50<16:49,  3.48s/it]
2022-04-08 15:48:20,818 - INFO - tqdm - accuracy: 0.7076, batch_loss: 0.4741, loss: 0.5503 ||:  51%|#####1    | 303/590 [17:01<17:10,  3.59s/it]
2022-04-08 15:48:31,943 - INFO - tqdm - accuracy: 0.7074, batch_loss: 0.6127, loss: 0.5505 ||:  52%|#####1    | 306/590 [17:12<17:31,  3.70s/it]
2022-04-08 15:48:44,003 - INFO - tqdm - accuracy: 0.7091, batch_loss: 0.4690, loss: 0.5486 ||:  53%|#####2    | 310/590 [17:24<14:49,  3.18s/it]
2022-04-08 15:48:54,265 - INFO - tqdm - accuracy: 0.7103, batch_loss: 0.4224, loss: 0.5472 ||:  53%|#####3    | 313/590 [17:35<15:40,  3.40s/it]
2022-04-08 15:49:07,151 - INFO - tqdm - accuracy: 0.7108, batch_loss: 0.5571, loss: 0.5468 ||:  54%|#####3    | 317/590 [17:47<14:44,  3.24s/it]
2022-04-08 15:49:18,182 - INFO - tqdm - accuracy: 0.7113, batch_loss: 0.4761, loss: 0.5463 ||:  54%|#####4    | 320/590 [17:58<15:54,  3.54s/it]
2022-04-08 15:49:31,752 - INFO - tqdm - accuracy: 0.7120, batch_loss: 0.4647, loss: 0.5452 ||:  55%|#####4    | 324/590 [18:12<15:20,  3.46s/it]
2022-04-08 15:49:44,872 - INFO - tqdm - accuracy: 0.7127, batch_loss: 0.3429, loss: 0.5448 ||:  56%|#####5    | 328/590 [18:25<14:18,  3.28s/it]
2022-04-08 15:49:57,665 - INFO - tqdm - accuracy: 0.7138, batch_loss: 0.5297, loss: 0.5438 ||:  56%|#####6    | 332/590 [18:38<13:52,  3.23s/it]
2022-04-08 15:50:10,785 - INFO - tqdm - accuracy: 0.7150, batch_loss: 0.3665, loss: 0.5424 ||:  57%|#####6    | 336/590 [18:51<13:58,  3.30s/it]
2022-04-08 15:50:23,909 - INFO - tqdm - accuracy: 0.7165, batch_loss: 0.2815, loss: 0.5412 ||:  58%|#####7    | 340/590 [19:04<13:47,  3.31s/it]
2022-04-08 15:50:37,560 - INFO - tqdm - accuracy: 0.7171, batch_loss: 0.4316, loss: 0.5403 ||:  58%|#####8    | 344/590 [19:18<13:54,  3.39s/it]
2022-04-08 15:50:49,508 - INFO - tqdm - accuracy: 0.7180, batch_loss: 0.4566, loss: 0.5394 ||:  59%|#####8    | 347/590 [19:30<15:14,  3.76s/it]
2022-04-08 15:51:00,051 - INFO - tqdm - accuracy: 0.7189, batch_loss: 0.4510, loss: 0.5383 ||:  59%|#####9    | 350/590 [19:40<14:21,  3.59s/it]
2022-04-08 15:51:12,923 - INFO - tqdm - accuracy: 0.7202, batch_loss: 0.4761, loss: 0.5374 ||:  60%|######    | 354/590 [19:53<12:48,  3.26s/it]
2022-04-08 15:51:26,756 - INFO - tqdm - accuracy: 0.7213, batch_loss: 0.4146, loss: 0.5364 ||:  61%|######    | 358/590 [20:07<13:19,  3.45s/it]
2022-04-08 15:51:36,793 - INFO - tqdm - accuracy: 0.7220, batch_loss: 0.3454, loss: 0.5356 ||:  61%|######1   | 361/590 [20:17<12:59,  3.40s/it]
2022-04-08 15:51:50,101 - INFO - tqdm - accuracy: 0.7223, batch_loss: 0.6123, loss: 0.5353 ||:  62%|######1   | 365/590 [20:30<12:38,  3.37s/it]
2022-04-08 15:52:00,297 - INFO - tqdm - accuracy: 0.7229, batch_loss: 0.4057, loss: 0.5345 ||:  62%|######2   | 368/590 [20:41<12:31,  3.39s/it]
2022-04-08 15:52:10,348 - INFO - tqdm - accuracy: 0.7236, batch_loss: 0.4834, loss: 0.5340 ||:  63%|######2   | 371/590 [20:51<12:09,  3.33s/it]
2022-04-08 15:52:20,553 - INFO - tqdm - accuracy: 0.7244, batch_loss: 0.4140, loss: 0.5327 ||:  63%|######3   | 374/590 [21:01<12:05,  3.36s/it]
2022-04-08 15:52:33,571 - INFO - tqdm - accuracy: 0.7250, batch_loss: 0.4681, loss: 0.5323 ||:  64%|######4   | 378/590 [21:14<11:35,  3.28s/it]
2022-04-08 15:52:46,309 - INFO - tqdm - accuracy: 0.7253, batch_loss: 0.4734, loss: 0.5319 ||:  65%|######4   | 382/590 [21:27<10:53,  3.14s/it]
2022-04-08 15:52:59,709 - INFO - tqdm - accuracy: 0.7263, batch_loss: 0.3125, loss: 0.5313 ||:  65%|######5   | 386/590 [21:40<11:18,  3.32s/it]
2022-04-08 15:53:10,556 - INFO - tqdm - accuracy: 0.7269, batch_loss: 0.5606, loss: 0.5306 ||:  66%|######5   | 389/590 [21:51<12:02,  3.60s/it]
2022-04-08 15:53:20,792 - INFO - tqdm - accuracy: 0.7271, batch_loss: 0.4552, loss: 0.5304 ||:  66%|######6   | 392/590 [22:01<11:31,  3.49s/it]
2022-04-08 15:53:30,875 - INFO - tqdm - accuracy: 0.7284, batch_loss: 0.3260, loss: 0.5288 ||:  67%|######6   | 395/590 [22:11<11:15,  3.46s/it]
2022-04-08 15:53:40,941 - INFO - tqdm - accuracy: 0.7290, batch_loss: 0.3618, loss: 0.5281 ||:  67%|######7   | 398/590 [22:21<10:54,  3.41s/it]
2022-04-08 15:53:53,452 - INFO - tqdm - accuracy: 0.7294, batch_loss: 0.5085, loss: 0.5276 ||:  68%|######7   | 401/590 [22:34<12:08,  3.85s/it]
2022-04-08 15:54:03,981 - INFO - tqdm - accuracy: 0.7300, batch_loss: 0.4231, loss: 0.5271 ||:  68%|######8   | 404/590 [22:44<11:10,  3.60s/it]
2022-04-08 15:54:17,504 - INFO - tqdm - accuracy: 0.7305, batch_loss: 0.3838, loss: 0.5262 ||:  69%|######9   | 408/590 [22:58<10:48,  3.56s/it]
2022-04-08 15:54:30,138 - INFO - tqdm - accuracy: 0.7313, batch_loss: 0.3998, loss: 0.5252 ||:  70%|######9   | 412/590 [23:10<09:45,  3.29s/it]
2022-04-08 15:54:40,492 - INFO - tqdm - accuracy: 0.7319, batch_loss: 0.2456, loss: 0.5245 ||:  70%|#######   | 415/590 [23:21<09:51,  3.38s/it]
2022-04-08 15:54:53,522 - INFO - tqdm - accuracy: 0.7327, batch_loss: 0.4849, loss: 0.5235 ||:  71%|#######1  | 419/590 [23:34<09:35,  3.37s/it]
2022-04-08 15:55:04,814 - INFO - tqdm - accuracy: 0.7333, batch_loss: 0.2605, loss: 0.5225 ||:  72%|#######1  | 422/590 [23:45<10:13,  3.65s/it]
2022-04-08 15:55:17,680 - INFO - tqdm - accuracy: 0.7336, batch_loss: 0.4136, loss: 0.5221 ||:  72%|#######2  | 426/590 [23:58<09:09,  3.35s/it]
2022-04-08 15:55:30,731 - INFO - tqdm - accuracy: 0.7344, batch_loss: 0.3184, loss: 0.5206 ||:  73%|#######2  | 430/590 [24:11<08:42,  3.27s/it]
2022-04-08 15:55:43,760 - INFO - tqdm - accuracy: 0.7352, batch_loss: 0.5345, loss: 0.5195 ||:  74%|#######3  | 434/590 [24:24<08:20,  3.21s/it]
2022-04-08 15:55:54,751 - INFO - tqdm - accuracy: 0.7355, batch_loss: 0.3654, loss: 0.5191 ||:  74%|#######4  | 437/590 [24:35<08:53,  3.49s/it]
2022-04-08 15:56:07,512 - INFO - tqdm - accuracy: 0.7362, batch_loss: 0.3942, loss: 0.5182 ||:  75%|#######4  | 441/590 [24:48<08:11,  3.30s/it]
2022-04-08 15:56:20,406 - INFO - tqdm - accuracy: 0.7365, batch_loss: 0.4646, loss: 0.5178 ||:  75%|#######5  | 445/590 [25:01<07:51,  3.25s/it]
2022-04-08 15:56:31,104 - INFO - tqdm - accuracy: 0.7372, batch_loss: 0.4330, loss: 0.5170 ||:  76%|#######5  | 448/590 [25:11<08:25,  3.56s/it]
2022-04-08 15:56:41,647 - INFO - tqdm - accuracy: 0.7379, batch_loss: 0.4395, loss: 0.5165 ||:  76%|#######6  | 451/590 [25:22<08:11,  3.54s/it]
2022-04-08 15:56:51,833 - INFO - tqdm - accuracy: 0.7382, batch_loss: 0.4480, loss: 0.5160 ||:  77%|#######6  | 454/590 [25:32<07:44,  3.41s/it]
2022-04-08 15:57:04,588 - INFO - tqdm - accuracy: 0.7386, batch_loss: 0.5278, loss: 0.5153 ||:  78%|#######7  | 458/590 [25:45<07:08,  3.25s/it]
2022-04-08 15:57:17,706 - INFO - tqdm - accuracy: 0.7392, batch_loss: 0.4379, loss: 0.5146 ||:  78%|#######8  | 462/590 [25:58<07:04,  3.32s/it]
2022-04-08 15:57:30,349 - INFO - tqdm - accuracy: 0.7395, batch_loss: 0.4945, loss: 0.5141 ||:  79%|#######8  | 466/590 [26:11<06:30,  3.15s/it]
2022-04-08 15:57:40,684 - INFO - tqdm - accuracy: 0.7401, batch_loss: 0.2921, loss: 0.5131 ||:  79%|#######9  | 469/590 [26:21<06:44,  3.34s/it]
2022-04-08 15:57:51,208 - INFO - tqdm - accuracy: 0.7405, batch_loss: 0.5031, loss: 0.5125 ||:  80%|########  | 472/590 [26:31<06:41,  3.41s/it]
2022-04-08 15:58:04,272 - INFO - tqdm - accuracy: 0.7409, batch_loss: 0.3333, loss: 0.5115 ||:  81%|########  | 476/590 [26:45<06:18,  3.32s/it]
2022-04-08 15:58:17,774 - INFO - tqdm - accuracy: 0.7419, batch_loss: 0.1895, loss: 0.5098 ||:  81%|########1 | 480/590 [26:58<06:19,  3.45s/it]
2022-04-08 15:58:30,609 - INFO - tqdm - accuracy: 0.7428, batch_loss: 0.4053, loss: 0.5086 ||:  82%|########2 | 484/590 [27:11<05:46,  3.26s/it]
2022-04-08 15:58:43,883 - INFO - tqdm - accuracy: 0.7433, batch_loss: 0.4540, loss: 0.5076 ||:  83%|########2 | 488/590 [27:24<05:33,  3.27s/it]
2022-04-08 15:58:54,748 - INFO - tqdm - accuracy: 0.7437, batch_loss: 0.3229, loss: 0.5068 ||:  83%|########3 | 491/590 [27:35<05:47,  3.51s/it]
2022-04-08 15:59:05,013 - INFO - tqdm - accuracy: 0.7441, batch_loss: 0.5143, loss: 0.5063 ||:  84%|########3 | 494/590 [27:45<05:20,  3.34s/it]
2022-04-08 15:59:18,615 - INFO - tqdm - accuracy: 0.7448, batch_loss: 0.3994, loss: 0.5048 ||:  84%|########4 | 498/590 [27:59<05:13,  3.41s/it]
2022-04-08 15:59:29,593 - INFO - tqdm - accuracy: 0.7451, batch_loss: 0.3767, loss: 0.5045 ||:  85%|########4 | 501/590 [28:10<05:15,  3.54s/it]
2022-04-08 15:59:42,403 - INFO - tqdm - accuracy: 0.7457, batch_loss: 0.4032, loss: 0.5033 ||:  86%|########5 | 505/590 [28:23<04:37,  3.26s/it]
2022-04-08 15:59:52,565 - INFO - tqdm - accuracy: 0.7460, batch_loss: 0.4183, loss: 0.5028 ||:  86%|########6 | 508/590 [28:33<04:34,  3.35s/it]
2022-04-08 16:00:05,651 - INFO - tqdm - accuracy: 0.7464, batch_loss: 0.3247, loss: 0.5023 ||:  87%|########6 | 512/590 [28:46<04:20,  3.34s/it]
2022-04-08 16:00:16,189 - INFO - tqdm - accuracy: 0.7468, batch_loss: 0.4703, loss: 0.5020 ||:  87%|########7 | 515/590 [28:56<04:20,  3.47s/it]
2022-04-08 16:00:29,753 - INFO - tqdm - accuracy: 0.7473, batch_loss: 0.3534, loss: 0.5012 ||:  88%|########7 | 519/590 [29:10<04:03,  3.43s/it]
2022-04-08 16:00:42,671 - INFO - tqdm - accuracy: 0.7477, batch_loss: 0.3575, loss: 0.5008 ||:  89%|########8 | 523/590 [29:23<03:39,  3.28s/it]
2022-04-08 16:00:52,773 - INFO - tqdm - accuracy: 0.7483, batch_loss: 0.3735, loss: 0.5000 ||:  89%|########9 | 526/590 [29:33<03:33,  3.33s/it]
2022-04-08 16:01:03,553 - INFO - tqdm - accuracy: 0.7486, batch_loss: 0.3735, loss: 0.4994 ||:  90%|########9 | 529/590 [29:44<03:34,  3.52s/it]
2022-04-08 16:01:14,538 - INFO - tqdm - accuracy: 0.7494, batch_loss: 0.1746, loss: 0.4985 ||:  90%|######### | 532/590 [29:55<03:28,  3.59s/it]
2022-04-08 16:01:27,383 - INFO - tqdm - accuracy: 0.7498, batch_loss: 0.4457, loss: 0.4982 ||:  91%|######### | 536/590 [30:08<03:00,  3.33s/it]
2022-04-08 16:01:37,518 - INFO - tqdm - accuracy: 0.7506, batch_loss: 0.3449, loss: 0.4972 ||:  91%|#########1| 539/590 [30:18<02:51,  3.36s/it]
2022-04-08 16:01:50,232 - INFO - tqdm - accuracy: 0.7517, batch_loss: 0.2585, loss: 0.4957 ||:  92%|#########2| 543/590 [30:31<02:29,  3.19s/it]
2022-04-08 16:02:02,001 - INFO - tqdm - accuracy: 0.7518, batch_loss: 0.9328, loss: 0.4966 ||:  93%|#########2| 547/590 [30:42<02:09,  3.00s/it]
2022-04-08 16:02:15,547 - INFO - tqdm - accuracy: 0.7523, batch_loss: 0.3753, loss: 0.4957 ||:  93%|#########3| 551/590 [30:56<02:09,  3.32s/it]
2022-04-08 16:02:27,813 - INFO - tqdm - accuracy: 0.7529, batch_loss: 0.2617, loss: 0.4946 ||:  94%|#########4| 555/590 [31:08<01:51,  3.18s/it]
2022-04-08 16:02:37,917 - INFO - tqdm - accuracy: 0.7532, batch_loss: 0.4647, loss: 0.4942 ||:  95%|#########4| 558/590 [31:18<01:47,  3.36s/it]
2022-04-08 16:02:51,058 - INFO - tqdm - accuracy: 0.7538, batch_loss: 0.4386, loss: 0.4934 ||:  95%|#########5| 562/590 [31:31<01:31,  3.27s/it]
2022-04-08 16:03:01,160 - INFO - tqdm - accuracy: 0.7540, batch_loss: 0.3131, loss: 0.4932 ||:  96%|#########5| 565/590 [31:41<01:24,  3.39s/it]
2022-04-08 16:03:14,722 - INFO - tqdm - accuracy: 0.7543, batch_loss: 0.4396, loss: 0.4927 ||:  96%|#########6| 569/590 [31:55<01:11,  3.41s/it]
2022-04-08 16:03:27,737 - INFO - tqdm - accuracy: 0.7549, batch_loss: 0.2479, loss: 0.4921 ||:  97%|#########7| 573/590 [32:08<00:57,  3.36s/it]
2022-04-08 16:03:38,742 - INFO - tqdm - accuracy: 0.7553, batch_loss: 0.4863, loss: 0.4913 ||:  98%|#########7| 576/590 [32:19<00:49,  3.56s/it]
2022-04-08 16:03:49,019 - INFO - tqdm - accuracy: 0.7558, batch_loss: 0.2274, loss: 0.4903 ||:  98%|#########8| 579/590 [32:29<00:37,  3.45s/it]
2022-04-08 16:04:01,768 - INFO - tqdm - accuracy: 0.7564, batch_loss: 0.2686, loss: 0.4895 ||:  99%|#########8| 583/590 [32:42<00:22,  3.24s/it]
2022-04-08 16:04:12,029 - INFO - tqdm - accuracy: 0.7566, batch_loss: 0.3301, loss: 0.4893 ||:  99%|#########9| 586/590 [32:52<00:13,  3.37s/it]
2022-04-08 16:04:19,609 - INFO - tqdm - accuracy: 0.7570, batch_loss: 0.2960, loss: 0.4888 ||: 100%|#########9| 588/590 [33:00<00:07,  3.53s/it]
2022-04-08 16:04:23,707 - INFO - tqdm - accuracy: 0.7572, batch_loss: 0.2986, loss: 0.4885 ||: 100%|#########9| 589/590 [33:04<00:03,  3.70s/it]
2022-04-08 16:04:24,339 - INFO - tqdm - accuracy: 0.7572, batch_loss: 0.3582, loss: 0.4882 ||: 100%|##########| 590/590 [33:05<00:00,  2.78s/it]
2022-04-08 16:04:24,340 - INFO - tqdm - accuracy: 0.7572, batch_loss: 0.3582, loss: 0.4882 ||: 100%|##########| 590/590 [33:05<00:00,  3.36s/it]
2022-04-08 16:04:26,788 - INFO - allennlp.training.trainer - Validating
2022-04-08 16:04:26,790 - INFO - tqdm - 0%|          | 0/1635 [00:00<?, ?it/s]
2022-04-08 16:04:26,790 - INFO - allennlp.data.samplers.bucket_batch_sampler - No sorting keys given; trying to guess a good one
2022-04-08 16:04:26,791 - INFO - allennlp.data.samplers.bucket_batch_sampler - Using ['tokens'] as the sorting keys
2022-04-08 16:04:36,930 - INFO - tqdm - accuracy: 0.8715, batch_loss: 0.0402, loss: 0.3049 ||:   9%|8         | 144/1635 [00:10<01:47, 13.91it/s]
2022-04-08 16:04:46,972 - INFO - tqdm - accuracy: 0.8594, batch_loss: 1.7440, loss: 0.3471 ||:  18%|#7        | 288/1635 [00:20<01:29, 15.09it/s]
2022-04-08 16:04:56,972 - INFO - tqdm - accuracy: 0.8633, batch_loss: 0.0201, loss: 0.3517 ||:  26%|##6       | 428/1635 [00:30<01:25, 14.18it/s]
2022-04-08 16:05:07,014 - INFO - tqdm - accuracy: 0.8551, batch_loss: 0.1220, loss: 0.3581 ||:  35%|###5      | 573/1635 [00:40<01:12, 14.61it/s]
2022-04-08 16:05:17,215 - INFO - tqdm - accuracy: 0.8484, batch_loss: 0.0304, loss: 0.3571 ||:  43%|####3     | 706/1635 [00:50<01:23, 11.12it/s]
2022-04-08 16:05:27,311 - INFO - tqdm - accuracy: 0.8415, batch_loss: 0.3978, loss: 0.3680 ||:  52%|#####2    | 852/1635 [01:00<00:44, 17.74it/s]
2022-04-08 16:05:37,388 - INFO - tqdm - accuracy: 0.8417, batch_loss: 0.0092, loss: 0.3638 ||:  61%|######1   | 998/1635 [01:10<00:34, 18.33it/s]
2022-04-08 16:05:47,441 - INFO - tqdm - accuracy: 0.8409, batch_loss: 0.4214, loss: 0.3657 ||:  70%|######9   | 1138/1635 [01:20<00:35, 14.15it/s]
2022-04-08 16:05:57,533 - INFO - tqdm - accuracy: 0.8426, batch_loss: 0.1489, loss: 0.3600 ||:  78%|#######8  | 1277/1635 [01:30<00:23, 15.05it/s]
2022-04-08 16:06:07,667 - INFO - tqdm - accuracy: 0.8428, batch_loss: 0.3357, loss: 0.3564 ||:  87%|########6 | 1419/1635 [01:40<00:16, 13.18it/s]
2022-04-08 16:06:17,731 - INFO - tqdm - accuracy: 0.8429, batch_loss: 0.0260, loss: 0.3556 ||:  95%|#########5| 1560/1635 [01:50<00:05, 12.90it/s]
2022-04-08 16:06:22,365 - INFO - tqdm - accuracy: 0.8433, batch_loss: 0.1630, loss: 0.3557 ||: 100%|#########9| 1627/1635 [01:55<00:00, 14.56it/s]
2022-04-08 16:06:22,569 - INFO - tqdm - accuracy: 0.8435, batch_loss: 0.0553, loss: 0.3554 ||: 100%|#########9| 1629/1635 [01:55<00:00, 12.88it/s]
2022-04-08 16:06:22,768 - INFO - tqdm - accuracy: 0.8433, batch_loss: 0.0172, loss: 0.3554 ||: 100%|#########9| 1631/1635 [01:55<00:00, 11.95it/s]
2022-04-08 16:06:22,934 - INFO - tqdm - accuracy: 0.8435, batch_loss: 0.0707, loss: 0.3551 ||: 100%|#########9| 1633/1635 [01:56<00:00, 11.98it/s]
2022-04-08 16:06:23,162 - INFO - tqdm - accuracy: 0.8437, batch_loss: 0.1932, loss: 0.3548 ||: 100%|##########| 1635/1635 [01:56<00:00, 10.83it/s]
2022-04-08 16:06:23,163 - INFO - tqdm - accuracy: 0.8437, batch_loss: 0.1932, loss: 0.3548 ||: 100%|##########| 1635/1635 [01:56<00:00, 14.05it/s]
2022-04-08 16:06:23,163 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-08 16:06:23,164 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.757  |     0.844
2022-04-08 16:06:23,165 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  5756.214  |       N/A
2022-04-08 16:06:23,165 - INFO - allennlp.training.tensorboard_writer - loss               |     0.488  |     0.355
2022-04-08 16:06:23,166 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  7239.301  |       N/A
2022-04-08 16:06:27,929 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to './output_newboolq/best.th'.
2022-04-08 16:06:29,204 - INFO - allennlp.training.trainer - Epoch duration: 0:35:09.987491
2022-04-08 16:06:29,205 - INFO - allennlp.training.trainer - Estimated training time remaining: 5:16:29
2022-04-08 16:06:29,205 - INFO - allennlp.training.trainer - Epoch 1/9
2022-04-08 16:06:29,205 - INFO - allennlp.training.trainer - Worker 0 memory usage: 7.1G
2022-04-08 16:06:29,205 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-08 16:06:29,206 - INFO - allennlp.training.trainer - Training
2022-04-08 16:06:29,207 - INFO - tqdm - 0%|          | 0/590 [00:00<?, ?it/s]
2022-04-08 16:06:39,530 - INFO - tqdm - accuracy: 0.9062, batch_loss: 0.2543, loss: 0.2768 ||:   1%|          | 3/590 [00:10<33:25,  3.42s/it]
2022-04-08 16:06:53,006 - INFO - tqdm - accuracy: 0.9062, batch_loss: 0.4213, loss: 0.2899 ||:   1%|1         | 7/590 [00:23<32:53,  3.38s/it]
2022-04-08 16:07:03,374 - INFO - tqdm - accuracy: 0.8938, batch_loss: 0.2915, loss: 0.2910 ||:   2%|1         | 10/590 [00:34<33:48,  3.50s/it]
2022-04-08 16:07:13,868 - INFO - tqdm - accuracy: 0.8870, batch_loss: 0.2294, loss: 0.3015 ||:   2%|2         | 13/590 [00:44<33:14,  3.46s/it]
2022-04-08 16:07:26,058 - INFO - tqdm - accuracy: 0.8952, batch_loss: 0.4076, loss: 0.2863 ||:   3%|2         | 17/590 [00:56<30:38,  3.21s/it]
2022-04-08 16:07:36,343 - INFO - tqdm - accuracy: 0.8875, batch_loss: 0.4373, loss: 0.2964 ||:   3%|3         | 20/590 [01:07<31:55,  3.36s/it]
2022-04-08 16:07:49,447 - INFO - tqdm - accuracy: 0.8932, batch_loss: 0.2190, loss: 0.2860 ||:   4%|4         | 24/590 [01:20<31:10,  3.31s/it]
2022-04-08 16:08:00,595 - INFO - tqdm - accuracy: 0.8831, batch_loss: 0.5608, loss: 0.3040 ||:   5%|4         | 27/590 [01:31<34:10,  3.64s/it]
2022-04-08 16:08:11,849 - INFO - tqdm - accuracy: 0.8812, batch_loss: 0.3915, loss: 0.3113 ||:   5%|5         | 30/590 [01:42<34:59,  3.75s/it]
2022-04-08 16:08:22,154 - INFO - tqdm - accuracy: 0.8769, batch_loss: 0.3847, loss: 0.3116 ||:   6%|5         | 33/590 [01:52<32:40,  3.52s/it]
2022-04-08 16:08:32,293 - INFO - tqdm - accuracy: 0.8785, batch_loss: 0.3842, loss: 0.3075 ||:   6%|6         | 36/590 [02:03<31:13,  3.38s/it]
2022-04-08 16:08:42,317 - INFO - tqdm - accuracy: 0.8846, batch_loss: 0.2538, loss: 0.3012 ||:   7%|6         | 39/590 [02:13<30:21,  3.31s/it]
2022-04-08 16:08:54,690 - INFO - tqdm - accuracy: 0.8859, batch_loss: 0.1791, loss: 0.2941 ||:   7%|7         | 43/590 [02:25<28:32,  3.13s/it]
2022-04-08 16:09:05,612 - INFO - tqdm - accuracy: 0.8886, batch_loss: 0.2629, loss: 0.2909 ||:   8%|7         | 46/590 [02:36<31:33,  3.48s/it]
2022-04-08 16:09:18,742 - INFO - tqdm - accuracy: 0.8844, batch_loss: 0.2681, loss: 0.2999 ||:   8%|8         | 50/590 [02:49<29:47,  3.31s/it]
2022-04-08 16:09:28,910 - INFO - tqdm - accuracy: 0.8827, batch_loss: 0.3129, loss: 0.3071 ||:   9%|8         | 53/590 [02:59<29:42,  3.32s/it]
2022-04-08 16:09:39,267 - INFO - tqdm - accuracy: 0.8806, batch_loss: 0.2593, loss: 0.3111 ||:   9%|9         | 56/590 [03:10<29:35,  3.33s/it]
2022-04-08 16:09:49,286 - INFO - tqdm - accuracy: 0.8803, batch_loss: 0.3419, loss: 0.3109 ||:  10%|#         | 59/590 [03:20<29:42,  3.36s/it]
2022-04-08 16:10:00,270 - INFO - tqdm - accuracy: 0.8805, batch_loss: 0.2409, loss: 0.3076 ||:  11%|#         | 62/590 [03:31<32:11,  3.66s/it]
2022-04-08 16:10:12,911 - INFO - tqdm - accuracy: 0.8778, batch_loss: 0.3227, loss: 0.3125 ||:  11%|#1        | 66/590 [03:43<29:26,  3.37s/it]
2022-04-08 16:10:25,892 - INFO - tqdm - accuracy: 0.8799, batch_loss: 0.1478, loss: 0.3098 ||:  12%|#1        | 70/590 [03:56<27:56,  3.22s/it]
2022-04-08 16:10:36,744 - INFO - tqdm - accuracy: 0.8789, batch_loss: 0.3913, loss: 0.3123 ||:  12%|#2        | 73/590 [04:07<30:12,  3.50s/it]
2022-04-08 16:10:47,596 - INFO - tqdm - accuracy: 0.8787, batch_loss: 0.3289, loss: 0.3121 ||:  13%|#2        | 76/590 [04:18<30:52,  3.60s/it]
2022-04-08 16:11:00,775 - INFO - tqdm - accuracy: 0.8770, batch_loss: 0.3918, loss: 0.3144 ||:  14%|#3        | 80/590 [04:31<28:25,  3.34s/it]
2022-04-08 16:11:11,593 - INFO - tqdm - accuracy: 0.8776, batch_loss: 0.3329, loss: 0.3140 ||:  14%|#4        | 83/590 [04:42<30:08,  3.57s/it]
2022-04-08 16:11:23,765 - INFO - tqdm - accuracy: 0.8775, batch_loss: 0.2979, loss: 0.3154 ||:  15%|#4        | 87/590 [04:54<26:26,  3.15s/it]
2022-04-08 16:11:33,796 - INFO - tqdm - accuracy: 0.8781, batch_loss: 0.3307, loss: 0.3128 ||:  15%|#5        | 90/590 [05:04<27:30,  3.30s/it]
2022-04-08 16:11:43,823 - INFO - tqdm - accuracy: 0.8767, batch_loss: 0.4831, loss: 0.3126 ||:  16%|#5        | 93/590 [05:14<27:28,  3.32s/it]
2022-04-08 16:11:54,022 - INFO - tqdm - accuracy: 0.8766, batch_loss: 0.1597, loss: 0.3103 ||:  16%|#6        | 96/590 [05:24<28:00,  3.40s/it]
2022-04-08 16:12:08,371 - INFO - tqdm - accuracy: 0.8772, batch_loss: 0.5339, loss: 0.3101 ||:  17%|#6        | 100/590 [05:39<29:57,  3.67s/it]
2022-04-08 16:12:21,307 - INFO - tqdm - accuracy: 0.8762, batch_loss: 0.3455, loss: 0.3113 ||:  18%|#7        | 104/590 [05:52<26:56,  3.33s/it]
2022-04-08 16:12:35,227 - INFO - tqdm - accuracy: 0.8753, batch_loss: 0.1769, loss: 0.3108 ||:  18%|#8        | 108/590 [06:06<28:32,  3.55s/it]
2022-04-08 16:12:49,306 - INFO - tqdm - accuracy: 0.8781, batch_loss: 0.2797, loss: 0.3061 ||:  19%|#8        | 112/590 [06:20<28:59,  3.64s/it]
2022-04-08 16:13:02,920 - INFO - tqdm - accuracy: 0.8780, batch_loss: 0.2855, loss: 0.3060 ||:  20%|#9        | 116/590 [06:33<28:09,  3.56s/it]
2022-04-08 16:13:16,344 - INFO - tqdm - accuracy: 0.8781, batch_loss: 0.4534, loss: 0.3048 ||:  20%|##        | 120/590 [06:47<27:25,  3.50s/it]
2022-04-08 16:13:28,446 - INFO - tqdm - accuracy: 0.8788, batch_loss: 0.2018, loss: 0.3029 ||:  21%|##1       | 124/590 [06:59<24:26,  3.15s/it]
2022-04-08 16:13:38,817 - INFO - tqdm - accuracy: 0.8799, batch_loss: 0.3351, loss: 0.3015 ||:  22%|##1       | 127/590 [07:09<25:35,  3.32s/it]
2022-04-08 16:13:49,085 - INFO - tqdm - accuracy: 0.8803, batch_loss: 0.2911, loss: 0.3005 ||:  22%|##2       | 130/590 [07:19<25:31,  3.33s/it]
2022-04-08 16:14:00,501 - INFO - tqdm - accuracy: 0.8804, batch_loss: 0.4963, loss: 0.3009 ||:  23%|##2       | 133/590 [07:31<27:28,  3.61s/it]
2022-04-08 16:14:10,526 - INFO - tqdm - accuracy: 0.8807, batch_loss: 0.2314, loss: 0.3011 ||:  23%|##3       | 136/590 [07:41<25:38,  3.39s/it]
2022-04-08 16:14:20,888 - INFO - tqdm - accuracy: 0.8795, batch_loss: 0.3589, loss: 0.3021 ||:  24%|##3       | 139/590 [07:51<25:33,  3.40s/it]
2022-04-08 16:14:32,268 - INFO - tqdm - accuracy: 0.8794, batch_loss: 0.3114, loss: 0.3014 ||:  24%|##4       | 142/590 [08:03<27:23,  3.67s/it]
2022-04-08 16:14:43,009 - INFO - tqdm - accuracy: 0.8789, batch_loss: 0.3719, loss: 0.3013 ||:  25%|##4       | 145/590 [08:13<27:14,  3.67s/it]
2022-04-08 16:14:53,279 - INFO - tqdm - accuracy: 0.8782, batch_loss: 0.3293, loss: 0.3018 ||:  25%|##5       | 148/590 [08:24<25:42,  3.49s/it]
2022-04-08 16:15:03,470 - INFO - tqdm - accuracy: 0.8785, batch_loss: 0.2313, loss: 0.2997 ||:  26%|##5       | 151/590 [08:34<25:14,  3.45s/it]
2022-04-08 16:15:16,127 - INFO - tqdm - accuracy: 0.8786, batch_loss: 0.2394, loss: 0.2982 ||:  26%|##6       | 155/590 [08:46<23:33,  3.25s/it]
2022-04-08 16:15:26,834 - INFO - tqdm - accuracy: 0.8784, batch_loss: 0.3522, loss: 0.2995 ||:  27%|##6       | 158/590 [08:57<24:55,  3.46s/it]
2022-04-08 16:15:40,339 - INFO - tqdm - accuracy: 0.8787, batch_loss: 0.3778, loss: 0.2991 ||:  27%|##7       | 162/590 [09:11<24:29,  3.43s/it]
2022-04-08 16:15:51,790 - INFO - tqdm - accuracy: 0.8782, batch_loss: 0.3561, loss: 0.2997 ||:  28%|##7       | 165/590 [09:22<26:01,  3.67s/it]
2022-04-08 16:16:01,796 - INFO - tqdm - accuracy: 0.8793, batch_loss: 0.1041, loss: 0.2977 ||:  28%|##8       | 168/590 [09:32<24:01,  3.42s/it]
2022-04-08 16:16:11,863 - INFO - tqdm - accuracy: 0.8798, batch_loss: 0.2028, loss: 0.2965 ||:  29%|##8       | 171/590 [09:42<23:39,  3.39s/it]
2022-04-08 16:16:22,701 - INFO - tqdm - accuracy: 0.8800, batch_loss: 0.2167, loss: 0.2960 ||:  29%|##9       | 174/590 [09:53<24:22,  3.51s/it]
2022-04-08 16:16:34,081 - INFO - tqdm - accuracy: 0.8796, batch_loss: 0.2810, loss: 0.2964 ||:  30%|###       | 177/590 [10:04<25:39,  3.73s/it]
2022-04-08 16:16:46,632 - INFO - tqdm - accuracy: 0.8809, batch_loss: 0.4269, loss: 0.2949 ||:  31%|###       | 181/590 [10:17<22:01,  3.23s/it]
2022-04-08 16:16:57,616 - INFO - tqdm - accuracy: 0.8809, batch_loss: 0.3428, loss: 0.2959 ||:  31%|###1      | 184/590 [10:28<24:02,  3.55s/it]
2022-04-08 16:17:08,216 - INFO - tqdm - accuracy: 0.8807, batch_loss: 0.2359, loss: 0.2965 ||:  32%|###1      | 187/590 [10:39<23:31,  3.50s/it]
2022-04-08 16:17:21,058 - INFO - tqdm - accuracy: 0.8814, batch_loss: 0.2530, loss: 0.2946 ||:  32%|###2      | 191/590 [10:51<21:34,  3.24s/it]
2022-04-08 16:17:31,283 - INFO - tqdm - accuracy: 0.8813, batch_loss: 0.4130, loss: 0.2947 ||:  33%|###2      | 194/590 [11:02<22:21,  3.39s/it]
2022-04-08 16:17:44,602 - INFO - tqdm - accuracy: 0.8819, batch_loss: 0.3056, loss: 0.2934 ||:  34%|###3      | 198/590 [11:15<22:06,  3.38s/it]
2022-04-08 16:17:54,885 - INFO - tqdm - accuracy: 0.8826, batch_loss: 0.2216, loss: 0.2921 ||:  34%|###4      | 201/590 [11:25<21:51,  3.37s/it]
2022-04-08 16:18:05,336 - INFO - tqdm - accuracy: 0.8830, batch_loss: 0.1607, loss: 0.2911 ||:  35%|###4      | 204/590 [11:36<22:06,  3.44s/it]
2022-04-08 16:18:19,090 - INFO - tqdm - accuracy: 0.8830, batch_loss: 0.5011, loss: 0.2919 ||:  35%|###5      | 208/590 [11:49<22:03,  3.47s/it]
2022-04-08 16:18:31,947 - INFO - tqdm - accuracy: 0.8837, batch_loss: 0.2353, loss: 0.2906 ||:  36%|###5      | 212/590 [12:02<21:32,  3.42s/it]
2022-04-08 16:18:42,020 - INFO - tqdm - accuracy: 0.8827, batch_loss: 0.4679, loss: 0.2922 ||:  36%|###6      | 215/590 [12:12<20:57,  3.35s/it]
2022-04-08 16:18:54,559 - INFO - tqdm - accuracy: 0.8830, batch_loss: 0.1701, loss: 0.2915 ||:  37%|###7      | 219/590 [12:25<19:28,  3.15s/it]
2022-04-08 16:19:04,745 - INFO - tqdm - accuracy: 0.8837, batch_loss: 0.2265, loss: 0.2899 ||:  38%|###7      | 222/590 [12:35<20:57,  3.42s/it]
2022-04-08 16:19:15,550 - INFO - tqdm - accuracy: 0.8842, batch_loss: 0.2417, loss: 0.2900 ||:  38%|###8      | 225/590 [12:46<21:28,  3.53s/it]
2022-04-08 16:19:27,240 - INFO - tqdm - accuracy: 0.8850, batch_loss: 0.1591, loss: 0.2881 ||:  39%|###8      | 229/590 [12:58<17:57,  2.99s/it]
2022-04-08 16:19:41,000 - INFO - tqdm - accuracy: 0.8847, batch_loss: 0.3252, loss: 0.2880 ||:  39%|###9      | 233/590 [13:11<20:22,  3.42s/it]
2022-04-08 16:19:53,186 - INFO - tqdm - accuracy: 0.8844, batch_loss: 0.2476, loss: 0.2870 ||:  40%|####      | 237/590 [13:23<18:06,  3.08s/it]
2022-04-08 16:20:03,941 - INFO - tqdm - accuracy: 0.8842, batch_loss: 0.4813, loss: 0.2876 ||:  41%|####      | 240/590 [13:34<20:03,  3.44s/it]
2022-04-08 16:20:17,413 - INFO - tqdm - accuracy: 0.8840, batch_loss: 0.2893, loss: 0.2870 ||:  41%|####1     | 244/590 [13:48<19:41,  3.42s/it]
2022-04-08 16:20:30,908 - INFO - tqdm - accuracy: 0.8839, batch_loss: 0.3328, loss: 0.2872 ||:  42%|####2     | 248/590 [14:01<19:56,  3.50s/it]
2022-04-08 16:20:44,136 - INFO - tqdm - accuracy: 0.8838, batch_loss: 0.3320, loss: 0.2876 ||:  43%|####2     | 252/590 [14:14<18:59,  3.37s/it]
2022-04-08 16:20:57,194 - INFO - tqdm - accuracy: 0.8835, batch_loss: 0.4393, loss: 0.2876 ||:  43%|####3     | 256/590 [14:27<18:09,  3.26s/it]
2022-04-08 16:21:07,985 - INFO - tqdm - accuracy: 0.8838, batch_loss: 0.2513, loss: 0.2871 ||:  44%|####3     | 259/590 [14:38<18:52,  3.42s/it]
2022-04-08 16:21:19,155 - INFO - tqdm - accuracy: 0.8838, batch_loss: 0.1654, loss: 0.2872 ||:  44%|####4     | 262/590 [14:49<19:56,  3.65s/it]
2022-04-08 16:21:29,512 - INFO - tqdm - accuracy: 0.8841, batch_loss: 0.2518, loss: 0.2866 ||:  45%|####4     | 265/590 [15:00<18:58,  3.50s/it]
2022-04-08 16:21:42,712 - INFO - tqdm - accuracy: 0.8843, batch_loss: 0.3801, loss: 0.2860 ||:  46%|####5     | 269/590 [15:13<17:54,  3.35s/it]
2022-04-08 16:21:53,735 - INFO - tqdm - accuracy: 0.8841, batch_loss: 0.2177, loss: 0.2866 ||:  46%|####6     | 272/590 [15:24<18:57,  3.58s/it]
2022-04-08 16:22:05,008 - INFO - tqdm - accuracy: 0.8840, batch_loss: 0.2476, loss: 0.2873 ||:  47%|####6     | 275/590 [15:35<19:54,  3.79s/it]
2022-04-08 16:22:15,732 - INFO - tqdm - accuracy: 0.8841, batch_loss: 0.3108, loss: 0.2869 ||:  47%|####7     | 278/590 [15:46<18:45,  3.61s/it]
2022-04-08 16:22:28,852 - INFO - tqdm - accuracy: 0.8836, batch_loss: 0.3301, loss: 0.2870 ||:  48%|####7     | 282/590 [15:59<17:05,  3.33s/it]
2022-04-08 16:22:41,659 - INFO - tqdm - accuracy: 0.8842, batch_loss: 0.2089, loss: 0.2859 ||:  48%|####8     | 286/590 [16:12<16:11,  3.20s/it]
2022-04-08 16:22:54,843 - INFO - tqdm - accuracy: 0.8844, batch_loss: 0.2230, loss: 0.2851 ||:  49%|####9     | 290/590 [16:25<16:22,  3.27s/it]
2022-04-08 16:23:08,693 - INFO - tqdm - accuracy: 0.8848, batch_loss: 0.3465, loss: 0.2846 ||:  50%|####9     | 294/590 [16:39<17:01,  3.45s/it]
2022-04-08 16:23:19,579 - INFO - tqdm - accuracy: 0.8852, batch_loss: 0.2940, loss: 0.2840 ||:  50%|#####     | 297/590 [16:50<17:44,  3.63s/it]
2022-04-08 16:23:32,518 - INFO - tqdm - accuracy: 0.8853, batch_loss: 0.3870, loss: 0.2841 ||:  51%|#####1    | 301/590 [17:03<15:54,  3.30s/it]
2022-04-08 16:23:43,676 - INFO - tqdm - accuracy: 0.8853, batch_loss: 0.1959, loss: 0.2834 ||:  52%|#####1    | 304/590 [17:14<16:52,  3.54s/it]
2022-04-08 16:23:57,251 - INFO - tqdm - accuracy: 0.8855, batch_loss: 0.1888, loss: 0.2828 ||:  52%|#####2    | 308/590 [17:28<16:13,  3.45s/it]
2022-04-08 16:24:08,553 - INFO - tqdm - accuracy: 0.8860, batch_loss: 0.1366, loss: 0.2817 ||:  53%|#####2    | 311/590 [17:39<16:54,  3.64s/it]
2022-04-08 16:24:19,981 - INFO - tqdm - accuracy: 0.8859, batch_loss: 0.3047, loss: 0.2817 ||:  53%|#####3    | 314/590 [17:50<17:34,  3.82s/it]
2022-04-08 16:24:32,815 - INFO - tqdm - accuracy: 0.8860, batch_loss: 0.3175, loss: 0.2817 ||:  54%|#####3    | 318/590 [18:03<15:27,  3.41s/it]
2022-04-08 16:24:45,833 - INFO - tqdm - accuracy: 0.8864, batch_loss: 0.2903, loss: 0.2811 ||:  55%|#####4    | 322/590 [18:16<14:47,  3.31s/it]
2022-04-08 16:24:55,875 - INFO - tqdm - accuracy: 0.8864, batch_loss: 0.1632, loss: 0.2804 ||:  55%|#####5    | 325/590 [18:26<14:36,  3.31s/it]
2022-04-08 16:25:08,914 - INFO - tqdm - accuracy: 0.8867, batch_loss: 0.3665, loss: 0.2810 ||:  56%|#####5    | 329/590 [18:39<14:08,  3.25s/it]
2022-04-08 16:25:21,562 - INFO - tqdm - accuracy: 0.8867, batch_loss: 0.3039, loss: 0.2811 ||:  56%|#####6    | 333/590 [18:52<13:31,  3.16s/it]
2022-04-08 16:25:33,951 - INFO - tqdm - accuracy: 0.8871, batch_loss: 0.1424, loss: 0.2804 ||:  57%|#####7    | 337/590 [19:04<13:09,  3.12s/it]
2022-04-08 16:25:46,372 - INFO - tqdm - accuracy: 0.8867, batch_loss: 0.4535, loss: 0.2815 ||:  58%|#####7    | 341/590 [19:17<12:43,  3.07s/it]
2022-04-08 16:25:58,715 - INFO - tqdm - accuracy: 0.8873, batch_loss: 0.1876, loss: 0.2799 ||:  58%|#####8    | 345/590 [19:29<12:58,  3.18s/it]
2022-04-08 16:26:10,896 - INFO - tqdm - accuracy: 0.8867, batch_loss: 0.2027, loss: 0.2802 ||:  59%|#####9    | 349/590 [19:41<12:19,  3.07s/it]
2022-04-08 16:26:23,392 - INFO - tqdm - accuracy: 0.8868, batch_loss: 0.4158, loss: 0.2804 ||:  60%|#####9    | 353/590 [19:54<12:09,  3.08s/it]
2022-04-08 16:26:33,714 - INFO - tqdm - accuracy: 0.8862, batch_loss: 0.3424, loss: 0.2811 ||:  60%|######    | 356/590 [20:04<12:38,  3.24s/it]
2022-04-08 16:26:43,938 - INFO - tqdm - accuracy: 0.8865, batch_loss: 0.1659, loss: 0.2805 ||:  61%|######    | 359/590 [20:14<12:58,  3.37s/it]
2022-04-08 16:26:56,397 - INFO - tqdm - accuracy: 0.8864, batch_loss: 0.2508, loss: 0.2803 ||:  62%|######1   | 363/590 [20:27<12:17,  3.25s/it]
2022-04-08 16:27:09,549 - INFO - tqdm - accuracy: 0.8859, batch_loss: 0.2617, loss: 0.2807 ||:  62%|######2   | 367/590 [20:40<12:06,  3.26s/it]
2022-04-08 16:27:19,952 - INFO - tqdm - accuracy: 0.8860, batch_loss: 0.3898, loss: 0.2804 ||:  63%|######2   | 370/590 [20:50<12:37,  3.44s/it]
2022-04-08 16:27:32,913 - INFO - tqdm - accuracy: 0.8865, batch_loss: 0.2367, loss: 0.2796 ||:  63%|######3   | 374/590 [21:03<11:47,  3.28s/it]
2022-04-08 16:27:44,454 - INFO - tqdm - accuracy: 0.8863, batch_loss: 0.3554, loss: 0.2799 ||:  64%|######3   | 377/590 [21:15<12:52,  3.63s/it]
2022-04-08 16:27:55,016 - INFO - tqdm - accuracy: 0.8867, batch_loss: 0.0885, loss: 0.2791 ||:  64%|######4   | 380/590 [21:25<12:18,  3.51s/it]
2022-04-08 16:28:08,225 - INFO - tqdm - accuracy: 0.8869, batch_loss: 0.2375, loss: 0.2786 ||:  65%|######5   | 384/590 [21:39<11:32,  3.36s/it]
2022-04-08 16:28:21,588 - INFO - tqdm - accuracy: 0.8868, batch_loss: 0.2004, loss: 0.2783 ||:  66%|######5   | 388/590 [21:52<11:23,  3.38s/it]
2022-04-08 16:28:34,201 - INFO - tqdm - accuracy: 0.8872, batch_loss: 0.1515, loss: 0.2772 ||:  66%|######6   | 392/590 [22:04<10:33,  3.20s/it]
2022-04-08 16:28:47,291 - INFO - tqdm - accuracy: 0.8874, batch_loss: 0.2266, loss: 0.2771 ||:  67%|######7   | 396/590 [22:18<10:46,  3.33s/it]
2022-04-08 16:29:00,565 - INFO - tqdm - accuracy: 0.8876, batch_loss: 0.2448, loss: 0.2767 ||:  68%|######7   | 400/590 [22:31<10:44,  3.39s/it]
2022-04-08 16:29:11,200 - INFO - tqdm - accuracy: 0.8874, batch_loss: 0.3093, loss: 0.2769 ||:  68%|######8   | 403/590 [22:41<11:01,  3.54s/it]
2022-04-08 16:29:21,405 - INFO - tqdm - accuracy: 0.8878, batch_loss: 0.1278, loss: 0.2761 ||:  69%|######8   | 406/590 [22:52<10:41,  3.48s/it]
2022-04-08 16:29:31,599 - INFO - tqdm - accuracy: 0.8879, batch_loss: 0.3084, loss: 0.2757 ||:  69%|######9   | 409/590 [23:02<10:16,  3.41s/it]
2022-04-08 16:29:41,891 - INFO - tqdm - accuracy: 0.8880, batch_loss: 0.2895, loss: 0.2753 ||:  70%|######9   | 412/590 [23:12<10:11,  3.44s/it]
2022-04-08 16:29:52,235 - INFO - tqdm - accuracy: 0.8881, batch_loss: 0.1247, loss: 0.2750 ||:  70%|#######   | 415/590 [23:23<09:56,  3.41s/it]
2022-04-08 16:30:05,126 - INFO - tqdm - accuracy: 0.8889, batch_loss: 0.1374, loss: 0.2736 ||:  71%|#######1  | 419/590 [23:35<09:20,  3.28s/it]
2022-04-08 16:30:15,548 - INFO - tqdm - accuracy: 0.8890, batch_loss: 0.1119, loss: 0.2736 ||:  72%|#######1  | 422/590 [23:46<09:29,  3.39s/it]
2022-04-08 16:30:27,047 - INFO - tqdm - accuracy: 0.8888, batch_loss: 0.3827, loss: 0.2739 ||:  72%|#######2  | 425/590 [23:57<10:00,  3.64s/it]
2022-04-08 16:30:37,208 - INFO - tqdm - accuracy: 0.8889, batch_loss: 0.1755, loss: 0.2732 ||:  73%|#######2  | 428/590 [24:08<09:29,  3.52s/it]
2022-04-08 16:30:47,876 - INFO - tqdm - accuracy: 0.8892, batch_loss: 0.1942, loss: 0.2726 ||:  73%|#######3  | 431/590 [24:18<09:16,  3.50s/it]
2022-04-08 16:30:58,933 - INFO - tqdm - accuracy: 0.8893, batch_loss: 0.3954, loss: 0.2726 ||:  74%|#######3  | 434/590 [24:29<09:39,  3.71s/it]
2022-04-08 16:31:08,986 - INFO - tqdm - accuracy: 0.8892, batch_loss: 0.3713, loss: 0.2727 ||:  74%|#######4  | 437/590 [24:39<08:52,  3.48s/it]
2022-04-08 16:31:19,623 - INFO - tqdm - accuracy: 0.8893, batch_loss: 0.1319, loss: 0.2722 ||:  75%|#######4  | 440/590 [24:50<08:51,  3.54s/it]
2022-04-08 16:31:32,798 - INFO - tqdm - accuracy: 0.8891, batch_loss: 0.2940, loss: 0.2728 ||:  75%|#######5  | 444/590 [25:03<08:12,  3.37s/it]
2022-04-08 16:31:45,901 - INFO - tqdm - accuracy: 0.8890, batch_loss: 0.3736, loss: 0.2730 ||:  76%|#######5  | 448/590 [25:16<07:49,  3.31s/it]
2022-04-08 16:31:58,770 - INFO - tqdm - accuracy: 0.8888, batch_loss: 0.3081, loss: 0.2732 ||:  77%|#######6  | 452/590 [25:29<07:30,  3.26s/it]
2022-04-08 16:32:09,989 - INFO - tqdm - accuracy: 0.8887, batch_loss: 0.2159, loss: 0.2734 ||:  77%|#######7  | 455/590 [25:40<07:54,  3.51s/it]
2022-04-08 16:32:20,386 - INFO - tqdm - accuracy: 0.8889, batch_loss: 0.2623, loss: 0.2731 ||:  78%|#######7  | 458/590 [25:51<07:30,  3.42s/it]
2022-04-08 16:32:33,054 - INFO - tqdm - accuracy: 0.8892, batch_loss: 0.2697, loss: 0.2728 ||:  78%|#######8  | 462/590 [26:03<06:43,  3.15s/it]
2022-04-08 16:32:45,384 - INFO - tqdm - accuracy: 0.8896, batch_loss: 0.0810, loss: 0.2719 ||:  79%|#######8  | 466/590 [26:16<06:23,  3.09s/it]
2022-04-08 16:32:55,626 - INFO - tqdm - accuracy: 0.8897, batch_loss: 0.1947, loss: 0.2716 ||:  79%|#######9  | 469/590 [26:26<06:38,  3.29s/it]
2022-04-08 16:33:05,895 - INFO - tqdm - accuracy: 0.8900, batch_loss: 0.2247, loss: 0.2714 ||:  80%|########  | 472/590 [26:36<06:32,  3.33s/it]
2022-04-08 16:33:19,028 - INFO - tqdm - accuracy: 0.8898, batch_loss: 0.1826, loss: 0.2714 ||:  81%|########  | 476/590 [26:49<06:22,  3.36s/it]
2022-04-08 16:33:32,308 - INFO - tqdm - accuracy: 0.8901, batch_loss: 0.2793, loss: 0.2706 ||:  81%|########1 | 480/590 [27:03<06:12,  3.39s/it]
2022-04-08 16:33:42,864 - INFO - tqdm - accuracy: 0.8905, batch_loss: 0.1538, loss: 0.2700 ||:  82%|########1 | 483/590 [27:13<06:05,  3.42s/it]
2022-04-08 16:33:56,040 - INFO - tqdm - accuracy: 0.8906, batch_loss: 0.3559, loss: 0.2702 ||:  83%|########2 | 487/590 [27:26<05:50,  3.40s/it]
2022-04-08 16:34:06,113 - INFO - tqdm - accuracy: 0.8909, batch_loss: 0.1586, loss: 0.2700 ||:  83%|########3 | 490/590 [27:36<05:28,  3.28s/it]
2022-04-08 16:34:16,405 - INFO - tqdm - accuracy: 0.8910, batch_loss: 0.2255, loss: 0.2696 ||:  84%|########3 | 493/590 [27:47<05:22,  3.32s/it]
2022-04-08 16:34:26,993 - INFO - tqdm - accuracy: 0.8906, batch_loss: 0.3715, loss: 0.2705 ||:  84%|########4 | 496/590 [27:57<05:23,  3.44s/it]
2022-04-08 16:34:38,003 - INFO - tqdm - accuracy: 0.8907, batch_loss: 0.2884, loss: 0.2702 ||:  85%|########4 | 499/590 [28:08<05:36,  3.70s/it]
2022-04-08 16:34:48,476 - INFO - tqdm - accuracy: 0.8903, batch_loss: 0.2222, loss: 0.2705 ||:  85%|########5 | 502/590 [28:19<05:12,  3.55s/it]
2022-04-08 16:35:01,196 - INFO - tqdm - accuracy: 0.8903, batch_loss: 0.3977, loss: 0.2705 ||:  86%|########5 | 506/590 [28:31<04:32,  3.25s/it]
2022-04-08 16:35:14,209 - INFO - tqdm - accuracy: 0.8904, batch_loss: 0.1101, loss: 0.2708 ||:  86%|########6 | 510/590 [28:45<04:23,  3.29s/it]
2022-04-08 16:35:25,484 - INFO - tqdm - accuracy: 0.8905, batch_loss: 0.3133, loss: 0.2706 ||:  87%|########6 | 513/590 [28:56<04:36,  3.59s/it]
2022-04-08 16:35:36,137 - INFO - tqdm - accuracy: 0.8903, batch_loss: 0.2486, loss: 0.2703 ||:  87%|########7 | 516/590 [29:06<04:31,  3.67s/it]
2022-04-08 16:35:46,968 - INFO - tqdm - accuracy: 0.8904, batch_loss: 0.2119, loss: 0.2705 ||:  88%|########7 | 519/590 [29:17<04:15,  3.60s/it]
2022-04-08 16:35:58,056 - INFO - tqdm - accuracy: 0.8901, batch_loss: 0.5746, loss: 0.2712 ||:  88%|########8 | 522/590 [29:28<04:12,  3.71s/it]
2022-04-08 16:36:08,238 - INFO - tqdm - accuracy: 0.8902, batch_loss: 0.1955, loss: 0.2709 ||:  89%|########8 | 525/590 [29:39<03:44,  3.46s/it]
2022-04-08 16:36:18,731 - INFO - tqdm - accuracy: 0.8904, batch_loss: 0.1539, loss: 0.2707 ||:  89%|########9 | 528/590 [29:49<03:39,  3.55s/it]
2022-04-08 16:36:32,415 - INFO - tqdm - accuracy: 0.8906, batch_loss: 0.2104, loss: 0.2702 ||:  90%|######### | 532/590 [30:03<03:23,  3.50s/it]
2022-04-08 16:36:44,669 - INFO - tqdm - accuracy: 0.8907, batch_loss: 0.3751, loss: 0.2701 ||:  91%|######### | 536/590 [30:15<02:50,  3.15s/it]
2022-04-08 16:36:55,684 - INFO - tqdm - accuracy: 0.8905, batch_loss: 0.3537, loss: 0.2702 ||:  91%|#########1| 539/590 [30:26<02:56,  3.47s/it]
2022-04-08 16:37:06,089 - INFO - tqdm - accuracy: 0.8905, batch_loss: 0.3234, loss: 0.2701 ||:  92%|#########1| 542/590 [30:36<02:45,  3.44s/it]
2022-04-08 16:37:16,250 - INFO - tqdm - accuracy: 0.8908, batch_loss: 0.3128, loss: 0.2698 ||:  92%|#########2| 545/590 [30:47<02:29,  3.33s/it]
2022-04-08 16:37:26,869 - INFO - tqdm - accuracy: 0.8909, batch_loss: 0.0860, loss: 0.2696 ||:  93%|#########2| 548/590 [30:57<02:25,  3.46s/it]
2022-04-08 16:37:37,202 - INFO - tqdm - accuracy: 0.8912, batch_loss: 0.0922, loss: 0.2690 ||:  93%|#########3| 551/590 [31:07<02:12,  3.39s/it]
2022-04-08 16:37:47,685 - INFO - tqdm - accuracy: 0.8914, batch_loss: 0.2125, loss: 0.2691 ||:  94%|#########3| 554/590 [31:18<02:03,  3.43s/it]
2022-04-08 16:37:59,466 - INFO - tqdm - accuracy: 0.8917, batch_loss: 0.3713, loss: 0.2690 ||:  95%|#########4| 558/590 [31:30<01:35,  3.00s/it]
2022-04-08 16:38:09,472 - INFO - tqdm - accuracy: 0.8918, batch_loss: 0.1653, loss: 0.2686 ||:  95%|#########5| 561/590 [31:40<01:34,  3.25s/it]
2022-04-08 16:38:20,002 - INFO - tqdm - accuracy: 0.8918, batch_loss: 0.5278, loss: 0.2690 ||:  96%|#########5| 564/590 [31:50<01:28,  3.41s/it]
2022-04-08 16:38:31,055 - INFO - tqdm - accuracy: 0.8918, batch_loss: 0.2401, loss: 0.2687 ||:  96%|#########6| 567/590 [32:01<01:23,  3.61s/it]
2022-04-08 16:38:43,814 - INFO - tqdm - accuracy: 0.8916, batch_loss: 0.4338, loss: 0.2692 ||:  97%|#########6| 571/590 [32:14<01:01,  3.22s/it]
2022-04-08 16:38:54,693 - INFO - tqdm - accuracy: 0.8918, batch_loss: 0.2437, loss: 0.2690 ||:  97%|#########7| 574/590 [32:25<00:56,  3.52s/it]
2022-04-08 16:39:07,440 - INFO - tqdm - accuracy: 0.8919, batch_loss: 0.1254, loss: 0.2686 ||:  98%|#########7| 578/590 [32:38<00:40,  3.35s/it]
2022-04-08 16:39:17,485 - INFO - tqdm - accuracy: 0.8921, batch_loss: 0.2028, loss: 0.2684 ||:  98%|#########8| 581/590 [32:48<00:30,  3.33s/it]
2022-04-08 16:39:29,726 - INFO - tqdm - accuracy: 0.8920, batch_loss: 0.1940, loss: 0.2683 ||:  99%|#########9| 585/590 [33:00<00:15,  3.10s/it]
2022-04-08 16:39:40,029 - INFO - tqdm - accuracy: 0.8923, batch_loss: 0.1520, loss: 0.2677 ||: 100%|#########9| 588/590 [33:10<00:06,  3.28s/it]
2022-04-08 16:39:43,372 - INFO - tqdm - accuracy: 0.8923, batch_loss: 0.1259, loss: 0.2675 ||: 100%|#########9| 589/590 [33:14<00:03,  3.30s/it]
2022-04-08 16:39:44,214 - INFO - tqdm - accuracy: 0.8924, batch_loss: 0.0401, loss: 0.2671 ||: 100%|##########| 590/590 [33:15<00:00,  2.56s/it]
2022-04-08 16:39:44,214 - INFO - tqdm - accuracy: 0.8924, batch_loss: 0.0401, loss: 0.2671 ||: 100%|##########| 590/590 [33:15<00:00,  3.38s/it]
2022-04-08 16:39:46,666 - INFO - allennlp.training.trainer - Validating
2022-04-08 16:39:46,667 - INFO - tqdm - 0%|          | 0/1635 [00:00<?, ?it/s]
2022-04-08 16:39:56,716 - INFO - tqdm - accuracy: 0.8404, batch_loss: 0.0214, loss: 0.4714 ||:   9%|8         | 141/1635 [00:10<01:35, 15.68it/s]
2022-04-08 16:40:06,803 - INFO - tqdm - accuracy: 0.8475, batch_loss: 0.0127, loss: 0.4203 ||:  17%|#7        | 282/1635 [00:20<01:57, 11.52it/s]
2022-04-08 16:40:16,825 - INFO - tqdm - accuracy: 0.8365, batch_loss: 0.0070, loss: 0.4721 ||:  26%|##5       | 419/1635 [00:30<01:32, 13.14it/s]
2022-04-08 16:40:26,899 - INFO - tqdm - accuracy: 0.8427, batch_loss: 0.8179, loss: 0.4616 ||:  34%|###3      | 553/1635 [00:40<01:18, 13.78it/s]
2022-04-08 16:40:37,025 - INFO - tqdm - accuracy: 0.8410, batch_loss: 0.0153, loss: 0.4703 ||:  43%|####2     | 698/1635 [00:50<00:51, 18.32it/s]
2022-04-08 16:40:47,029 - INFO - tqdm - accuracy: 0.8440, batch_loss: 1.7958, loss: 0.4727 ||:  51%|#####1    | 840/1635 [01:00<00:54, 14.64it/s]
2022-04-08 16:40:57,148 - INFO - tqdm - accuracy: 0.8473, batch_loss: 1.4599, loss: 0.4595 ||:  61%|######    | 992/1635 [01:10<00:43, 14.66it/s]
2022-04-08 16:41:07,177 - INFO - tqdm - accuracy: 0.8521, batch_loss: 0.0798, loss: 0.4426 ||:  69%|######9   | 1136/1635 [01:20<00:30, 16.61it/s]
2022-04-08 16:41:17,209 - INFO - tqdm - accuracy: 0.8521, batch_loss: 0.0317, loss: 0.4438 ||:  78%|#######8  | 1281/1635 [01:30<00:21, 16.85it/s]
2022-04-08 16:41:27,340 - INFO - tqdm - accuracy: 0.8510, batch_loss: 0.0070, loss: 0.4471 ||:  87%|########7 | 1423/1635 [01:40<00:16, 12.66it/s]
2022-04-08 16:41:37,468 - INFO - tqdm - accuracy: 0.8533, batch_loss: 0.0369, loss: 0.4437 ||:  95%|#########5| 1561/1635 [01:50<00:05, 13.38it/s]
2022-04-08 16:41:42,647 - INFO - tqdm - accuracy: 0.8506, batch_loss: 1.3133, loss: 0.4497 ||: 100%|#########9| 1627/1635 [01:55<00:00, 17.22it/s]
2022-04-08 16:41:42,782 - INFO - tqdm - accuracy: 0.8508, batch_loss: 0.0074, loss: 0.4491 ||: 100%|#########9| 1629/1635 [01:56<00:00, 16.51it/s]
2022-04-08 16:41:42,984 - INFO - tqdm - accuracy: 0.8510, batch_loss: 0.0063, loss: 0.4486 ||: 100%|#########9| 1631/1635 [01:56<00:00, 13.96it/s]
2022-04-08 16:41:43,136 - INFO - tqdm - accuracy: 0.8513, batch_loss: 0.0124, loss: 0.4479 ||: 100%|#########9| 1634/1635 [01:56<00:00, 15.66it/s]
2022-04-08 16:41:43,272 - INFO - tqdm - accuracy: 0.8514, batch_loss: 0.0085, loss: 0.4476 ||: 100%|##########| 1635/1635 [01:56<00:00, 14.02it/s]
2022-04-08 16:41:43,272 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-08 16:41:43,272 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.892  |     0.851
2022-04-08 16:41:43,273 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9592.449  |       N/A
2022-04-08 16:41:43,274 - INFO - allennlp.training.tensorboard_writer - loss               |     0.267  |     0.448
2022-04-08 16:41:43,274 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  7239.301  |       N/A
2022-04-08 16:41:47,846 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to './output_newboolq/best.th'.
2022-04-08 16:41:58,752 - INFO - allennlp.training.trainer - Epoch duration: 0:35:29.547522
2022-04-08 16:41:58,752 - INFO - allennlp.training.trainer - Estimated training time remaining: 4:42:38
2022-04-08 16:41:58,753 - INFO - allennlp.training.trainer - Epoch 2/9
2022-04-08 16:41:58,753 - INFO - allennlp.training.trainer - Worker 0 memory usage: 7.1G
2022-04-08 16:41:58,753 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-08 16:41:58,754 - INFO - allennlp.training.trainer - Training
2022-04-08 16:41:58,755 - INFO - tqdm - 0%|          | 0/590 [00:00<?, ?it/s]
2022-04-08 16:42:11,457 - INFO - tqdm - accuracy: 0.9609, batch_loss: 0.1588, loss: 0.1226 ||:   1%|          | 4/590 [00:12<30:48,  3.15s/it]
2022-04-08 16:42:21,977 - INFO - tqdm - accuracy: 0.9598, batch_loss: 0.2655, loss: 0.1430 ||:   1%|1         | 7/590 [00:23<32:35,  3.35s/it]
2022-04-08 16:42:32,989 - INFO - tqdm - accuracy: 0.9344, batch_loss: 0.2213, loss: 0.1896 ||:   2%|1         | 10/590 [00:34<34:10,  3.54s/it]
2022-04-08 16:42:45,395 - INFO - tqdm - accuracy: 0.9420, batch_loss: 0.0880, loss: 0.1619 ||:   2%|2         | 14/590 [00:46<31:15,  3.26s/it]
2022-04-08 16:42:55,587 - INFO - tqdm - accuracy: 0.9467, batch_loss: 0.0509, loss: 0.1525 ||:   3%|2         | 17/590 [00:56<32:00,  3.35s/it]
2022-04-08 16:43:06,729 - INFO - tqdm - accuracy: 0.9516, batch_loss: 0.0632, loss: 0.1408 ||:   3%|3         | 20/590 [01:07<34:16,  3.61s/it]
2022-04-08 16:43:17,017 - INFO - tqdm - accuracy: 0.9552, batch_loss: 0.1398, loss: 0.1337 ||:   4%|3         | 23/590 [01:18<32:48,  3.47s/it]
2022-04-08 16:43:29,511 - INFO - tqdm - accuracy: 0.9537, batch_loss: 0.1545, loss: 0.1297 ||:   5%|4         | 27/590 [01:30<30:22,  3.24s/it]
2022-04-08 16:43:43,787 - INFO - tqdm - accuracy: 0.9536, batch_loss: 0.0654, loss: 0.1307 ||:   5%|5         | 31/590 [01:45<34:01,  3.65s/it]
2022-04-08 16:43:53,859 - INFO - tqdm - accuracy: 0.9540, batch_loss: 0.3673, loss: 0.1362 ||:   6%|5         | 34/590 [01:55<32:07,  3.47s/it]
2022-04-08 16:44:05,036 - INFO - tqdm - accuracy: 0.9561, batch_loss: 0.0686, loss: 0.1299 ||:   6%|6         | 37/590 [02:06<33:01,  3.58s/it]
2022-04-08 16:44:15,060 - INFO - tqdm - accuracy: 0.9555, batch_loss: 0.2399, loss: 0.1304 ||:   7%|6         | 40/590 [02:16<31:33,  3.44s/it]
2022-04-08 16:44:27,124 - INFO - tqdm - accuracy: 0.9560, batch_loss: 0.3013, loss: 0.1293 ||:   7%|7         | 44/590 [02:28<28:44,  3.16s/it]
2022-04-08 16:44:40,042 - INFO - tqdm - accuracy: 0.9570, batch_loss: 0.0348, loss: 0.1273 ||:   8%|8         | 48/590 [02:41<29:12,  3.23s/it]
2022-04-08 16:44:53,382 - INFO - tqdm - accuracy: 0.9573, batch_loss: 0.0772, loss: 0.1252 ||:   9%|8         | 52/590 [02:54<29:49,  3.33s/it]
2022-04-08 16:45:05,821 - INFO - tqdm - accuracy: 0.9554, batch_loss: 0.5873, loss: 0.1303 ||:   9%|9         | 56/590 [03:07<28:00,  3.15s/it]
2022-04-08 16:45:16,384 - INFO - tqdm - accuracy: 0.9560, batch_loss: 0.1470, loss: 0.1287 ||:  10%|#         | 59/590 [03:17<29:41,  3.35s/it]
2022-04-08 16:45:27,364 - INFO - tqdm - accuracy: 0.9551, batch_loss: 0.0398, loss: 0.1288 ||:  11%|#         | 62/590 [03:28<31:01,  3.53s/it]
2022-04-08 16:45:40,830 - INFO - tqdm - accuracy: 0.9560, batch_loss: 0.0541, loss: 0.1266 ||:  11%|#1        | 66/590 [03:42<30:15,  3.47s/it]
2022-04-08 16:45:53,936 - INFO - tqdm - accuracy: 0.9563, batch_loss: 0.0892, loss: 0.1243 ||:  12%|#1        | 70/590 [03:55<29:24,  3.39s/it]
2022-04-08 16:46:07,290 - INFO - tqdm - accuracy: 0.9557, batch_loss: 0.2342, loss: 0.1256 ||:  13%|#2        | 74/590 [04:08<28:55,  3.36s/it]
2022-04-08 16:46:17,719 - INFO - tqdm - accuracy: 0.9550, batch_loss: 0.1059, loss: 0.1252 ||:  13%|#3        | 77/590 [04:18<28:55,  3.38s/it]
2022-04-08 16:46:28,059 - INFO - tqdm - accuracy: 0.9551, batch_loss: 0.0743, loss: 0.1242 ||:  14%|#3        | 80/590 [04:29<29:26,  3.46s/it]
2022-04-08 16:46:41,197 - INFO - tqdm - accuracy: 0.9542, batch_loss: 0.4146, loss: 0.1295 ||:  14%|#4        | 84/590 [04:42<28:06,  3.33s/it]
2022-04-08 16:46:51,822 - INFO - tqdm - accuracy: 0.9544, batch_loss: 0.0737, loss: 0.1276 ||:  15%|#4        | 87/590 [04:53<29:32,  3.52s/it]
2022-04-08 16:47:01,921 - INFO - tqdm - accuracy: 0.9556, batch_loss: 0.1213, loss: 0.1258 ||:  15%|#5        | 90/590 [05:03<29:05,  3.49s/it]
2022-04-08 16:47:13,400 - INFO - tqdm - accuracy: 0.9556, batch_loss: 0.0314, loss: 0.1242 ||:  16%|#5        | 93/590 [05:14<30:46,  3.72s/it]
2022-04-08 16:47:25,891 - INFO - tqdm - accuracy: 0.9546, batch_loss: 0.2256, loss: 0.1265 ||:  16%|#6        | 97/590 [05:27<26:40,  3.25s/it]
2022-04-08 16:47:39,386 - INFO - tqdm - accuracy: 0.9545, batch_loss: 0.0330, loss: 0.1284 ||:  17%|#7        | 101/590 [05:40<27:44,  3.40s/it]
2022-04-08 16:47:52,256 - INFO - tqdm - accuracy: 0.9545, batch_loss: 0.2589, loss: 0.1289 ||:  18%|#7        | 105/590 [05:53<25:58,  3.21s/it]
2022-04-08 16:48:02,324 - INFO - tqdm - accuracy: 0.9546, batch_loss: 0.1906, loss: 0.1288 ||:  18%|#8        | 108/590 [06:03<26:24,  3.29s/it]
2022-04-08 16:48:15,304 - INFO - tqdm - accuracy: 0.9545, batch_loss: 0.1175, loss: 0.1278 ||:  19%|#8        | 112/590 [06:16<26:17,  3.30s/it]
2022-04-08 16:48:28,378 - INFO - tqdm - accuracy: 0.9534, batch_loss: 0.5184, loss: 0.1324 ||:  20%|#9        | 116/590 [06:29<25:51,  3.27s/it]
2022-04-08 16:48:42,478 - INFO - tqdm - accuracy: 0.9529, batch_loss: 0.2291, loss: 0.1336 ||:  20%|##        | 120/590 [06:43<28:11,  3.60s/it]
2022-04-08 16:48:52,576 - INFO - tqdm - accuracy: 0.9530, batch_loss: 0.1930, loss: 0.1335 ||:  21%|##        | 123/590 [06:53<26:26,  3.40s/it]
2022-04-08 16:49:04,836 - INFO - tqdm - accuracy: 0.9523, batch_loss: 0.1571, loss: 0.1344 ||:  22%|##1       | 127/590 [07:06<24:55,  3.23s/it]
2022-04-08 16:49:15,099 - INFO - tqdm - accuracy: 0.9524, batch_loss: 0.0866, loss: 0.1345 ||:  22%|##2       | 130/590 [07:16<26:02,  3.40s/it]
2022-04-08 16:49:27,711 - INFO - tqdm - accuracy: 0.9524, batch_loss: 0.1157, loss: 0.1338 ||:  23%|##2       | 134/590 [07:28<24:39,  3.24s/it]
2022-04-08 16:49:40,632 - INFO - tqdm - accuracy: 0.9527, batch_loss: 0.1420, loss: 0.1329 ||:  23%|##3       | 138/590 [07:41<23:53,  3.17s/it]
2022-04-08 16:49:51,972 - INFO - tqdm - accuracy: 0.9521, batch_loss: 0.1882, loss: 0.1342 ||:  24%|##3       | 141/590 [07:53<26:36,  3.56s/it]
2022-04-08 16:50:05,017 - INFO - tqdm - accuracy: 0.9513, batch_loss: 0.1680, loss: 0.1351 ||:  25%|##4       | 145/590 [08:06<24:48,  3.34s/it]
2022-04-08 16:50:16,611 - INFO - tqdm - accuracy: 0.9514, batch_loss: 0.3067, loss: 0.1357 ||:  25%|##5       | 148/590 [08:17<27:17,  3.70s/it]
2022-04-08 16:50:29,958 - INFO - tqdm - accuracy: 0.9519, batch_loss: 0.1239, loss: 0.1342 ||:  26%|##5       | 152/590 [08:31<25:06,  3.44s/it]
2022-04-08 16:50:42,924 - INFO - tqdm - accuracy: 0.9523, batch_loss: 0.0280, loss: 0.1339 ||:  26%|##6       | 156/590 [08:44<23:54,  3.31s/it]
2022-04-08 16:50:55,605 - INFO - tqdm - accuracy: 0.9521, batch_loss: 0.1474, loss: 0.1334 ||:  27%|##7       | 160/590 [08:56<22:54,  3.20s/it]
2022-04-08 16:51:05,932 - INFO - tqdm - accuracy: 0.9525, batch_loss: 0.0119, loss: 0.1323 ||:  28%|##7       | 163/590 [09:07<24:04,  3.38s/it]
2022-04-08 16:51:16,155 - INFO - tqdm - accuracy: 0.9531, batch_loss: 0.0519, loss: 0.1305 ||:  28%|##8       | 166/590 [09:17<24:16,  3.44s/it]
2022-04-08 16:51:26,287 - INFO - tqdm - accuracy: 0.9525, batch_loss: 0.0823, loss: 0.1309 ||:  29%|##8       | 169/590 [09:27<23:58,  3.42s/it]
2022-04-08 16:51:36,648 - INFO - tqdm - accuracy: 0.9524, batch_loss: 0.3976, loss: 0.1316 ||:  29%|##9       | 172/590 [09:37<24:16,  3.48s/it]
2022-04-08 16:51:48,691 - INFO - tqdm - accuracy: 0.9515, batch_loss: 0.0950, loss: 0.1328 ||:  30%|##9       | 176/590 [09:49<21:34,  3.13s/it]
2022-04-08 16:52:01,884 - INFO - tqdm - accuracy: 0.9512, batch_loss: 0.2753, loss: 0.1335 ||:  31%|###       | 180/590 [10:03<22:19,  3.27s/it]
2022-04-08 16:52:14,466 - INFO - tqdm - accuracy: 0.9513, batch_loss: 0.4241, loss: 0.1334 ||:  31%|###1      | 184/590 [10:15<20:52,  3.08s/it]
2022-04-08 16:52:24,720 - INFO - tqdm - accuracy: 0.9509, batch_loss: 0.1838, loss: 0.1354 ||:  32%|###1      | 187/590 [10:25<22:24,  3.34s/it]
2022-04-08 16:52:35,013 - INFO - tqdm - accuracy: 0.9508, batch_loss: 0.1414, loss: 0.1355 ||:  32%|###2      | 190/590 [10:36<22:28,  3.37s/it]
2022-04-08 16:52:45,919 - INFO - tqdm - accuracy: 0.9513, batch_loss: 0.0850, loss: 0.1347 ||:  33%|###2      | 193/590 [10:47<23:48,  3.60s/it]
2022-04-08 16:52:58,233 - INFO - tqdm - accuracy: 0.9507, batch_loss: 0.2102, loss: 0.1354 ||:  33%|###3      | 197/590 [10:59<21:04,  3.22s/it]
2022-04-08 16:53:11,187 - INFO - tqdm - accuracy: 0.9504, batch_loss: 0.2040, loss: 0.1365 ||:  34%|###4      | 201/590 [11:12<20:55,  3.23s/it]
2022-04-08 16:53:21,928 - INFO - tqdm - accuracy: 0.9499, batch_loss: 0.1381, loss: 0.1374 ||:  35%|###4      | 204/590 [11:23<22:11,  3.45s/it]
2022-04-08 16:53:34,608 - INFO - tqdm - accuracy: 0.9501, batch_loss: 0.1334, loss: 0.1369 ||:  35%|###5      | 208/590 [11:35<20:57,  3.29s/it]
2022-04-08 16:53:47,261 - INFO - tqdm - accuracy: 0.9500, batch_loss: 0.1683, loss: 0.1371 ||:  36%|###5      | 212/590 [11:48<20:24,  3.24s/it]
2022-04-08 16:54:00,260 - INFO - tqdm - accuracy: 0.9505, batch_loss: 0.0993, loss: 0.1360 ||:  37%|###6      | 216/590 [12:01<20:31,  3.29s/it]
2022-04-08 16:54:10,943 - INFO - tqdm - accuracy: 0.9509, batch_loss: 0.1691, loss: 0.1354 ||:  37%|###7      | 219/590 [12:12<21:33,  3.49s/it]
2022-04-08 16:54:24,817 - INFO - tqdm - accuracy: 0.9508, batch_loss: 0.2424, loss: 0.1358 ||:  38%|###7      | 223/590 [12:26<21:32,  3.52s/it]
2022-04-08 16:54:38,386 - INFO - tqdm - accuracy: 0.9507, batch_loss: 0.1556, loss: 0.1357 ||:  38%|###8      | 227/590 [12:39<20:39,  3.41s/it]
2022-04-08 16:54:50,551 - INFO - tqdm - accuracy: 0.9504, batch_loss: 0.1278, loss: 0.1372 ||:  39%|###9      | 231/590 [12:51<18:53,  3.16s/it]
2022-04-08 16:55:03,726 - INFO - tqdm - accuracy: 0.9503, batch_loss: 0.1029, loss: 0.1376 ||:  40%|###9      | 235/590 [13:04<19:56,  3.37s/it]
2022-04-08 16:55:14,599 - INFO - tqdm - accuracy: 0.9502, batch_loss: 0.0772, loss: 0.1378 ||:  40%|####      | 238/590 [13:15<20:28,  3.49s/it]
2022-04-08 16:55:24,794 - INFO - tqdm - accuracy: 0.9502, batch_loss: 0.0636, loss: 0.1378 ||:  41%|####      | 241/590 [13:26<20:10,  3.47s/it]
2022-04-08 16:55:34,857 - INFO - tqdm - accuracy: 0.9499, batch_loss: 0.1483, loss: 0.1386 ||:  41%|####1     | 244/590 [13:36<19:40,  3.41s/it]
2022-04-08 16:55:45,540 - INFO - tqdm - accuracy: 0.9500, batch_loss: 0.0744, loss: 0.1383 ||:  42%|####1     | 247/590 [13:46<19:55,  3.49s/it]
2022-04-08 16:55:58,696 - INFO - tqdm - accuracy: 0.9500, batch_loss: 0.1644, loss: 0.1389 ||:  43%|####2     | 251/590 [13:59<18:46,  3.32s/it]
2022-04-08 16:56:12,068 - INFO - tqdm - accuracy: 0.9495, batch_loss: 0.0685, loss: 0.1395 ||:  43%|####3     | 255/590 [14:13<19:04,  3.42s/it]
2022-04-08 16:56:22,116 - INFO - tqdm - accuracy: 0.9499, batch_loss: 0.0973, loss: 0.1390 ||:  44%|####3     | 258/590 [14:23<18:45,  3.39s/it]
2022-04-08 16:56:35,729 - INFO - tqdm - accuracy: 0.9497, batch_loss: 0.1590, loss: 0.1399 ||:  44%|####4     | 262/590 [14:36<18:38,  3.41s/it]
2022-04-08 16:56:48,654 - INFO - tqdm - accuracy: 0.9496, batch_loss: 0.1537, loss: 0.1394 ||:  45%|####5     | 266/590 [14:49<17:50,  3.31s/it]
2022-04-08 16:56:59,229 - INFO - tqdm - accuracy: 0.9499, batch_loss: 0.0301, loss: 0.1386 ||:  46%|####5     | 269/590 [15:00<18:27,  3.45s/it]
2022-04-08 16:57:09,381 - INFO - tqdm - accuracy: 0.9496, batch_loss: 0.0938, loss: 0.1388 ||:  46%|####6     | 272/590 [15:10<18:07,  3.42s/it]
2022-04-08 16:57:22,741 - INFO - tqdm - accuracy: 0.9497, batch_loss: 0.0431, loss: 0.1381 ||:  47%|####6     | 276/590 [15:23<17:26,  3.33s/it]
2022-04-08 16:57:33,172 - INFO - tqdm - accuracy: 0.9500, batch_loss: 0.0151, loss: 0.1372 ||:  47%|####7     | 279/590 [15:34<17:55,  3.46s/it]
2022-04-08 16:57:43,592 - INFO - tqdm - accuracy: 0.9500, batch_loss: 0.3162, loss: 0.1381 ||:  48%|####7     | 282/590 [15:44<17:45,  3.46s/it]
2022-04-08 16:57:53,726 - INFO - tqdm - accuracy: 0.9499, batch_loss: 0.0789, loss: 0.1379 ||:  48%|####8     | 285/590 [15:54<17:09,  3.37s/it]
2022-04-08 16:58:06,741 - INFO - tqdm - accuracy: 0.9498, batch_loss: 0.1093, loss: 0.1376 ||:  49%|####8     | 289/590 [16:07<16:54,  3.37s/it]
2022-04-08 16:58:18,229 - INFO - tqdm - accuracy: 0.9498, batch_loss: 0.1184, loss: 0.1373 ||:  49%|####9     | 292/590 [16:19<18:09,  3.65s/it]
2022-04-08 16:58:30,361 - INFO - tqdm - accuracy: 0.9499, batch_loss: 0.0684, loss: 0.1365 ||:  50%|#####     | 296/590 [16:31<15:35,  3.18s/it]
2022-04-08 16:58:41,555 - INFO - tqdm - accuracy: 0.9499, batch_loss: 0.0746, loss: 0.1360 ||:  51%|#####     | 299/590 [16:42<17:17,  3.56s/it]
2022-04-08 16:58:51,744 - INFO - tqdm - accuracy: 0.9501, batch_loss: 0.0169, loss: 0.1356 ||:  51%|#####1    | 302/590 [16:52<16:23,  3.42s/it]
2022-04-08 16:59:05,244 - INFO - tqdm - accuracy: 0.9502, batch_loss: 0.0137, loss: 0.1356 ||:  52%|#####1    | 306/590 [17:06<16:01,  3.39s/it]
2022-04-08 16:59:18,952 - INFO - tqdm - accuracy: 0.9504, batch_loss: 0.2960, loss: 0.1354 ||:  53%|#####2    | 310/590 [17:20<16:25,  3.52s/it]
2022-04-08 16:59:32,189 - INFO - tqdm - accuracy: 0.9499, batch_loss: 0.1888, loss: 0.1361 ||:  53%|#####3    | 314/590 [17:33<15:31,  3.37s/it]
2022-04-08 16:59:42,693 - INFO - tqdm - accuracy: 0.9501, batch_loss: 0.0834, loss: 0.1359 ||:  54%|#####3    | 317/590 [17:43<15:47,  3.47s/it]
2022-04-08 16:59:53,556 - INFO - tqdm - accuracy: 0.9502, batch_loss: 0.1011, loss: 0.1356 ||:  54%|#####4    | 320/590 [17:54<15:55,  3.54s/it]
2022-04-08 17:00:06,866 - INFO - tqdm - accuracy: 0.9505, batch_loss: 0.0763, loss: 0.1354 ||:  55%|#####4    | 324/590 [18:08<15:11,  3.43s/it]
2022-04-08 17:00:19,064 - INFO - tqdm - accuracy: 0.9506, batch_loss: 0.0978, loss: 0.1348 ||:  56%|#####5    | 328/590 [18:20<13:29,  3.09s/it]
2022-04-08 17:00:29,131 - INFO - tqdm - accuracy: 0.9507, batch_loss: 0.2348, loss: 0.1348 ||:  56%|#####6    | 331/590 [18:30<14:15,  3.30s/it]
2022-04-08 17:00:41,824 - INFO - tqdm - accuracy: 0.9505, batch_loss: 0.0557, loss: 0.1355 ||:  57%|#####6    | 335/590 [18:43<13:25,  3.16s/it]
2022-04-08 17:00:52,094 - INFO - tqdm - accuracy: 0.9503, batch_loss: 0.1452, loss: 0.1353 ||:  57%|#####7    | 338/590 [18:53<13:57,  3.32s/it]
2022-04-08 17:01:05,238 - INFO - tqdm - accuracy: 0.9506, batch_loss: 0.1324, loss: 0.1345 ||:  58%|#####7    | 342/590 [19:06<13:51,  3.35s/it]
2022-04-08 17:01:15,426 - INFO - tqdm - accuracy: 0.9507, batch_loss: 0.0793, loss: 0.1340 ||:  58%|#####8    | 345/590 [19:16<14:08,  3.47s/it]
2022-04-08 17:01:29,117 - INFO - tqdm - accuracy: 0.9508, batch_loss: 0.0315, loss: 0.1339 ||:  59%|#####9    | 349/590 [19:30<13:56,  3.47s/it]
2022-04-08 17:01:42,075 - INFO - tqdm - accuracy: 0.9508, batch_loss: 0.0570, loss: 0.1338 ||:  60%|#####9    | 353/590 [19:43<12:53,  3.27s/it]
2022-04-08 17:01:55,704 - INFO - tqdm - accuracy: 0.9505, batch_loss: 0.0774, loss: 0.1338 ||:  61%|######    | 357/590 [19:56<13:35,  3.50s/it]
2022-04-08 17:02:06,754 - INFO - tqdm - accuracy: 0.9507, batch_loss: 0.0217, loss: 0.1334 ||:  61%|######1   | 360/590 [20:07<14:01,  3.66s/it]
2022-04-08 17:02:17,393 - INFO - tqdm - accuracy: 0.9508, batch_loss: 0.1041, loss: 0.1331 ||:  62%|######1   | 363/590 [20:18<13:52,  3.67s/it]
2022-04-08 17:02:29,906 - INFO - tqdm - accuracy: 0.9508, batch_loss: 0.1169, loss: 0.1337 ||:  62%|######2   | 367/590 [20:31<12:01,  3.23s/it]
2022-04-08 17:02:44,119 - INFO - tqdm - accuracy: 0.9506, batch_loss: 0.0511, loss: 0.1338 ||:  63%|######2   | 371/590 [20:45<13:04,  3.58s/it]
2022-04-08 17:02:54,502 - INFO - tqdm - accuracy: 0.9505, batch_loss: 0.1354, loss: 0.1338 ||:  63%|######3   | 374/590 [20:55<12:23,  3.44s/it]
2022-04-08 17:03:05,065 - INFO - tqdm - accuracy: 0.9506, batch_loss: 0.0965, loss: 0.1338 ||:  64%|######3   | 377/590 [21:06<12:26,  3.51s/it]
2022-04-08 17:03:15,677 - INFO - tqdm - accuracy: 0.9507, batch_loss: 0.1061, loss: 0.1334 ||:  64%|######4   | 380/590 [21:16<12:10,  3.48s/it]
2022-04-08 17:03:27,196 - INFO - tqdm - accuracy: 0.9507, batch_loss: 0.1325, loss: 0.1331 ||:  65%|######4   | 383/590 [21:28<13:08,  3.81s/it]
2022-04-08 17:03:37,251 - INFO - tqdm - accuracy: 0.9508, batch_loss: 0.2702, loss: 0.1334 ||:  65%|######5   | 386/590 [21:38<12:01,  3.54s/it]
2022-04-08 17:03:48,495 - INFO - tqdm - accuracy: 0.9510, batch_loss: 0.0600, loss: 0.1329 ||:  66%|######5   | 389/590 [21:49<12:35,  3.76s/it]
2022-04-08 17:04:01,835 - INFO - tqdm - accuracy: 0.9505, batch_loss: 0.1045, loss: 0.1338 ||:  67%|######6   | 393/590 [22:03<11:23,  3.47s/it]
2022-04-08 17:04:14,866 - INFO - tqdm - accuracy: 0.9502, batch_loss: 0.1866, loss: 0.1348 ||:  67%|######7   | 397/590 [22:16<10:54,  3.39s/it]
2022-04-08 17:04:25,034 - INFO - tqdm - accuracy: 0.9499, batch_loss: 0.0631, loss: 0.1354 ||:  68%|######7   | 400/590 [22:26<10:47,  3.41s/it]
2022-04-08 17:04:36,812 - INFO - tqdm - accuracy: 0.9498, batch_loss: 0.2159, loss: 0.1356 ||:  68%|######8   | 403/590 [22:38<11:34,  3.71s/it]
2022-04-08 17:04:47,946 - INFO - tqdm - accuracy: 0.9499, batch_loss: 0.0527, loss: 0.1353 ||:  69%|######8   | 406/590 [22:49<11:29,  3.75s/it]
2022-04-08 17:04:58,660 - INFO - tqdm - accuracy: 0.9500, batch_loss: 0.0813, loss: 0.1350 ||:  69%|######9   | 409/590 [22:59<10:55,  3.62s/it]
2022-04-08 17:05:12,018 - INFO - tqdm - accuracy: 0.9498, batch_loss: 0.0884, loss: 0.1354 ||:  70%|#######   | 413/590 [23:13<10:06,  3.43s/it]
2022-04-08 17:05:22,032 - INFO - tqdm - accuracy: 0.9496, batch_loss: 0.1975, loss: 0.1359 ||:  71%|#######   | 416/590 [23:23<09:53,  3.41s/it]
2022-04-08 17:05:32,567 - INFO - tqdm - accuracy: 0.9494, batch_loss: 0.2025, loss: 0.1361 ||:  71%|#######1  | 419/590 [23:33<09:58,  3.50s/it]
2022-04-08 17:05:42,838 - INFO - tqdm - accuracy: 0.9496, batch_loss: 0.1959, loss: 0.1361 ||:  72%|#######1  | 422/590 [23:44<09:32,  3.41s/it]
2022-04-08 17:05:55,402 - INFO - tqdm - accuracy: 0.9495, batch_loss: 0.1970, loss: 0.1365 ||:  72%|#######2  | 426/590 [23:56<08:50,  3.24s/it]
2022-04-08 17:06:08,366 - INFO - tqdm - accuracy: 0.9493, batch_loss: 0.1070, loss: 0.1366 ||:  73%|#######2  | 430/590 [24:09<08:35,  3.22s/it]
2022-04-08 17:06:22,140 - INFO - tqdm - accuracy: 0.9495, batch_loss: 0.0814, loss: 0.1360 ||:  74%|#######3  | 434/590 [24:23<08:59,  3.46s/it]
2022-04-08 17:06:32,193 - INFO - tqdm - accuracy: 0.9496, batch_loss: 0.1161, loss: 0.1357 ||:  74%|#######4  | 437/590 [24:33<08:30,  3.34s/it]
2022-04-08 17:06:44,590 - INFO - tqdm - accuracy: 0.9495, batch_loss: 0.0458, loss: 0.1362 ||:  75%|#######4  | 441/590 [24:45<07:59,  3.22s/it]
2022-04-08 17:06:57,484 - INFO - tqdm - accuracy: 0.9494, batch_loss: 0.0679, loss: 0.1366 ||:  75%|#######5  | 445/590 [24:58<07:44,  3.20s/it]
2022-04-08 17:07:10,803 - INFO - tqdm - accuracy: 0.9497, batch_loss: 0.0370, loss: 0.1361 ||:  76%|#######6  | 449/590 [25:12<07:47,  3.31s/it]
2022-04-08 17:07:23,731 - INFO - tqdm - accuracy: 0.9498, batch_loss: 0.1250, loss: 0.1361 ||:  77%|#######6  | 453/590 [25:24<07:35,  3.32s/it]
2022-04-08 17:07:34,223 - INFO - tqdm - accuracy: 0.9498, batch_loss: 0.0559, loss: 0.1361 ||:  77%|#######7  | 456/590 [25:35<07:37,  3.42s/it]
2022-04-08 17:07:44,573 - INFO - tqdm - accuracy: 0.9498, batch_loss: 0.1410, loss: 0.1362 ||:  78%|#######7  | 459/590 [25:45<07:27,  3.41s/it]
2022-04-08 17:07:57,399 - INFO - tqdm - accuracy: 0.9501, batch_loss: 0.0859, loss: 0.1358 ||:  78%|#######8  | 463/590 [25:58<06:52,  3.25s/it]
2022-04-08 17:08:09,840 - INFO - tqdm - accuracy: 0.9499, batch_loss: 0.2557, loss: 0.1361 ||:  79%|#######9  | 467/590 [26:11<06:30,  3.18s/it]
2022-04-08 17:08:20,313 - INFO - tqdm - accuracy: 0.9499, batch_loss: 0.2113, loss: 0.1365 ||:  80%|#######9  | 470/590 [26:21<06:42,  3.35s/it]
2022-04-08 17:08:33,792 - INFO - tqdm - accuracy: 0.9500, batch_loss: 0.0674, loss: 0.1367 ||:  80%|########  | 474/590 [26:35<06:38,  3.44s/it]
2022-04-08 17:08:46,411 - INFO - tqdm - accuracy: 0.9497, batch_loss: 0.1214, loss: 0.1371 ||:  81%|########1 | 478/590 [26:47<06:12,  3.33s/it]
2022-04-08 17:08:57,644 - INFO - tqdm - accuracy: 0.9498, batch_loss: 0.0670, loss: 0.1370 ||:  82%|########1 | 481/590 [26:58<06:35,  3.63s/it]
2022-04-08 17:09:10,033 - INFO - tqdm - accuracy: 0.9498, batch_loss: 0.1682, loss: 0.1368 ||:  82%|########2 | 485/590 [27:11<05:38,  3.22s/it]
2022-04-08 17:09:23,934 - INFO - tqdm - accuracy: 0.9500, batch_loss: 0.0844, loss: 0.1369 ||:  83%|########2 | 489/590 [27:25<05:53,  3.50s/it]
2022-04-08 17:09:36,981 - INFO - tqdm - accuracy: 0.9499, batch_loss: 0.2306, loss: 0.1371 ||:  84%|########3 | 493/590 [27:38<05:21,  3.32s/it]
2022-04-08 17:09:48,040 - INFO - tqdm - accuracy: 0.9500, batch_loss: 0.0880, loss: 0.1369 ||:  84%|########4 | 496/590 [27:49<05:39,  3.61s/it]
2022-04-08 17:09:58,999 - INFO - tqdm - accuracy: 0.9502, batch_loss: 0.0322, loss: 0.1364 ||:  85%|########4 | 499/590 [28:00<05:30,  3.63s/it]
2022-04-08 17:10:12,637 - INFO - tqdm - accuracy: 0.9504, batch_loss: 0.0614, loss: 0.1360 ||:  85%|########5 | 503/590 [28:13<05:01,  3.47s/it]
2022-04-08 17:10:23,289 - INFO - tqdm - accuracy: 0.9503, batch_loss: 0.0152, loss: 0.1357 ||:  86%|########5 | 506/590 [28:24<04:55,  3.52s/it]
2022-04-08 17:10:36,432 - INFO - tqdm - accuracy: 0.9504, batch_loss: 0.0903, loss: 0.1356 ||:  86%|########6 | 510/590 [28:37<04:35,  3.45s/it]
2022-04-08 17:10:46,755 - INFO - tqdm - accuracy: 0.9504, batch_loss: 0.1359, loss: 0.1354 ||:  87%|########6 | 513/590 [28:47<04:23,  3.43s/it]
2022-04-08 17:10:57,749 - INFO - tqdm - accuracy: 0.9504, batch_loss: 0.0169, loss: 0.1359 ||:  87%|########7 | 516/590 [28:58<04:19,  3.51s/it]
2022-04-08 17:11:08,059 - INFO - tqdm - accuracy: 0.9506, batch_loss: 0.0261, loss: 0.1354 ||:  88%|########7 | 519/590 [29:09<04:06,  3.47s/it]
2022-04-08 17:11:19,056 - INFO - tqdm - accuracy: 0.9507, batch_loss: 0.2112, loss: 0.1351 ||:  88%|########8 | 522/590 [29:20<04:05,  3.60s/it]
2022-04-08 17:11:31,876 - INFO - tqdm - accuracy: 0.9508, batch_loss: 0.2002, loss: 0.1350 ||:  89%|########9 | 526/590 [29:33<03:34,  3.35s/it]
2022-04-08 17:11:42,238 - INFO - tqdm - accuracy: 0.9507, batch_loss: 0.0639, loss: 0.1347 ||:  90%|########9 | 529/590 [29:43<03:29,  3.44s/it]
2022-04-08 17:11:55,293 - INFO - tqdm - accuracy: 0.9509, batch_loss: 0.1166, loss: 0.1343 ||:  90%|######### | 533/590 [29:56<03:12,  3.38s/it]
2022-04-08 17:12:05,802 - INFO - tqdm - accuracy: 0.9509, batch_loss: 0.2176, loss: 0.1345 ||:  91%|######### | 536/590 [30:07<03:05,  3.44s/it]
2022-04-08 17:12:15,998 - INFO - tqdm - accuracy: 0.9508, batch_loss: 0.2043, loss: 0.1350 ||:  91%|#########1| 539/590 [30:17<02:53,  3.41s/it]
2022-04-08 17:12:27,094 - INFO - tqdm - accuracy: 0.9509, batch_loss: 0.1774, loss: 0.1349 ||:  92%|#########1| 542/590 [30:28<02:53,  3.61s/it]
2022-04-08 17:12:37,968 - INFO - tqdm - accuracy: 0.9510, batch_loss: 0.0792, loss: 0.1349 ||:  92%|#########2| 545/590 [30:39<02:43,  3.62s/it]
2022-04-08 17:12:48,966 - INFO - tqdm - accuracy: 0.9511, batch_loss: 0.1327, loss: 0.1349 ||:  93%|#########2| 548/590 [30:50<02:33,  3.67s/it]
2022-04-08 17:12:59,020 - INFO - tqdm - accuracy: 0.9509, batch_loss: 0.1128, loss: 0.1350 ||:  93%|#########3| 551/590 [31:00<02:14,  3.45s/it]
2022-04-08 17:13:09,690 - INFO - tqdm - accuracy: 0.9510, batch_loss: 0.0793, loss: 0.1349 ||:  94%|#########3| 554/590 [31:10<02:07,  3.54s/it]
2022-04-08 17:13:22,902 - INFO - tqdm - accuracy: 0.9509, batch_loss: 0.1723, loss: 0.1347 ||:  95%|#########4| 558/590 [31:24<01:48,  3.38s/it]
2022-04-08 17:13:35,767 - INFO - tqdm - accuracy: 0.9511, batch_loss: 0.1957, loss: 0.1344 ||:  95%|#########5| 562/590 [31:37<01:32,  3.31s/it]
2022-04-08 17:13:49,051 - INFO - tqdm - accuracy: 0.9511, batch_loss: 0.1842, loss: 0.1340 ||:  96%|#########5| 566/590 [31:50<01:19,  3.32s/it]
2022-04-08 17:13:59,377 - INFO - tqdm - accuracy: 0.9512, batch_loss: 0.1148, loss: 0.1338 ||:  96%|#########6| 569/590 [32:00<01:11,  3.38s/it]
2022-04-08 17:14:12,357 - INFO - tqdm - accuracy: 0.9512, batch_loss: 0.0349, loss: 0.1335 ||:  97%|#########7| 573/590 [32:13<00:56,  3.30s/it]
2022-04-08 17:14:25,304 - INFO - tqdm - accuracy: 0.9513, batch_loss: 0.1155, loss: 0.1332 ||:  98%|#########7| 577/590 [32:26<00:42,  3.25s/it]
2022-04-08 17:14:35,457 - INFO - tqdm - accuracy: 0.9514, batch_loss: 0.1053, loss: 0.1330 ||:  98%|#########8| 580/590 [32:36<00:33,  3.33s/it]
2022-04-08 17:14:47,943 - INFO - tqdm - accuracy: 0.9514, batch_loss: 0.0638, loss: 0.1327 ||:  99%|#########8| 584/590 [32:49<00:19,  3.18s/it]
2022-04-08 17:14:58,416 - INFO - tqdm - accuracy: 0.9516, batch_loss: 0.1635, loss: 0.1325 ||:  99%|#########9| 587/590 [32:59<00:10,  3.41s/it]
2022-04-08 17:15:02,082 - INFO - tqdm - accuracy: 0.9515, batch_loss: 0.1575, loss: 0.1325 ||: 100%|#########9| 588/590 [33:03<00:06,  3.48s/it]
2022-04-08 17:15:05,057 - INFO - tqdm - accuracy: 0.9514, batch_loss: 0.1364, loss: 0.1325 ||: 100%|#########9| 589/590 [33:06<00:03,  3.33s/it]
2022-04-08 17:15:05,809 - INFO - tqdm - accuracy: 0.9514, batch_loss: 0.0776, loss: 0.1324 ||: 100%|##########| 590/590 [33:07<00:00,  2.56s/it]
2022-04-08 17:15:05,810 - INFO - tqdm - accuracy: 0.9514, batch_loss: 0.0776, loss: 0.1324 ||: 100%|##########| 590/590 [33:07<00:00,  3.37s/it]
2022-04-08 17:15:08,076 - INFO - allennlp.training.trainer - Validating
2022-04-08 17:15:08,078 - INFO - tqdm - 0%|          | 0/1635 [00:00<?, ?it/s]
2022-04-08 17:15:18,236 - INFO - tqdm - accuracy: 0.8536, batch_loss: 0.2681, loss: 0.6161 ||:   9%|8         | 140/1635 [00:10<02:03, 12.06it/s]
2022-04-08 17:15:28,362 - INFO - tqdm - accuracy: 0.8566, batch_loss: 0.0050, loss: 0.6136 ||:  17%|#6        | 272/1635 [00:20<01:35, 14.29it/s]
2022-04-08 17:15:38,414 - INFO - tqdm - accuracy: 0.8507, batch_loss: 0.0648, loss: 0.6168 ||:  25%|##5       | 412/1635 [00:30<01:22, 14.80it/s]
2022-04-08 17:15:48,498 - INFO - tqdm - accuracy: 0.8471, batch_loss: 0.0008, loss: 0.6287 ||:  34%|###4      | 556/1635 [00:40<01:25, 12.63it/s]
2022-04-08 17:15:58,578 - INFO - tqdm - accuracy: 0.8414, batch_loss: 0.0022, loss: 0.6629 ||:  43%|####2     | 700/1635 [00:50<01:04, 14.46it/s]
2022-04-08 17:16:08,602 - INFO - tqdm - accuracy: 0.8458, batch_loss: 2.9497, loss: 0.6428 ||:  51%|#####     | 830/1635 [01:00<00:51, 15.75it/s]
2022-04-08 17:16:18,675 - INFO - tqdm - accuracy: 0.8470, batch_loss: 0.0020, loss: 0.6385 ||:  60%|#####9    | 977/1635 [01:10<00:50, 13.03it/s]
2022-04-08 17:16:28,676 - INFO - tqdm - accuracy: 0.8446, batch_loss: 0.0045, loss: 0.6462 ||:  69%|######8   | 1120/1635 [01:20<00:42, 12.21it/s]
2022-04-08 17:16:38,749 - INFO - tqdm - accuracy: 0.8451, batch_loss: 0.0007, loss: 0.6411 ||:  78%|#######7  | 1275/1635 [01:30<00:23, 15.62it/s]
2022-04-08 17:16:48,817 - INFO - tqdm - accuracy: 0.8453, batch_loss: 0.3717, loss: 0.6331 ||:  87%|########6 | 1422/1635 [01:40<00:14, 14.81it/s]
2022-04-08 17:16:58,923 - INFO - tqdm - accuracy: 0.8450, batch_loss: 0.0011, loss: 0.6361 ||:  95%|#########5| 1561/1635 [01:50<00:05, 13.47it/s]
2022-04-08 17:17:03,772 - INFO - tqdm - accuracy: 0.8477, batch_loss: 0.0028, loss: 0.6245 ||: 100%|#########9| 1628/1635 [01:55<00:00, 17.68it/s]
2022-04-08 17:17:03,925 - INFO - tqdm - accuracy: 0.8479, batch_loss: 0.0030, loss: 0.6237 ||: 100%|#########9| 1630/1635 [01:55<00:00, 16.19it/s]
2022-04-08 17:17:04,060 - INFO - tqdm - accuracy: 0.8480, batch_loss: 0.0007, loss: 0.6230 ||: 100%|#########9| 1632/1635 [01:55<00:00, 15.77it/s]
2022-04-08 17:17:04,196 - INFO - tqdm - accuracy: 0.8479, batch_loss: 0.0139, loss: 0.6243 ||: 100%|#########9| 1634/1635 [01:56<00:00, 15.48it/s]
2022-04-08 17:17:04,271 - INFO - tqdm - accuracy: 0.8480, batch_loss: 0.0007, loss: 0.6239 ||: 100%|##########| 1635/1635 [01:56<00:00, 14.07it/s]
2022-04-08 17:17:04,271 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-08 17:17:04,272 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.951  |     0.848
2022-04-08 17:17:04,273 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9596.021  |       N/A
2022-04-08 17:17:04,273 - INFO - allennlp.training.tensorboard_writer - loss               |     0.132  |     0.624
2022-04-08 17:17:04,274 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  7239.301  |       N/A
2022-04-08 17:17:08,760 - INFO - allennlp.training.trainer - Epoch duration: 0:35:10.007786
2022-04-08 17:17:08,761 - INFO - allennlp.training.trainer - Estimated training time remaining: 4:06:55
2022-04-08 17:17:08,761 - INFO - allennlp.training.trainer - Epoch 3/9
2022-04-08 17:17:08,761 - INFO - allennlp.training.trainer - Worker 0 memory usage: 7.1G
2022-04-08 17:17:08,761 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-08 17:17:08,762 - INFO - allennlp.training.trainer - Training
2022-04-08 17:17:08,763 - INFO - tqdm - 0%|          | 0/590 [00:00<?, ?it/s]
2022-04-08 17:17:21,468 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0337, loss: 0.0433 ||:   1%|          | 4/590 [00:12<31:22,  3.21s/it]
2022-04-08 17:17:31,682 - INFO - tqdm - accuracy: 0.9821, batch_loss: 0.0245, loss: 0.0619 ||:   1%|1         | 7/590 [00:22<32:36,  3.36s/it]
2022-04-08 17:17:42,041 - INFO - tqdm - accuracy: 0.9812, batch_loss: 0.2248, loss: 0.0719 ||:   2%|1         | 10/590 [00:33<33:02,  3.42s/it]
2022-04-08 17:17:52,761 - INFO - tqdm - accuracy: 0.9808, batch_loss: 0.0148, loss: 0.0653 ||:   2%|2         | 13/590 [00:43<33:53,  3.52s/it]
2022-04-08 17:18:05,359 - INFO - tqdm - accuracy: 0.9816, batch_loss: 0.0373, loss: 0.0558 ||:   3%|2         | 17/590 [00:56<30:56,  3.24s/it]
2022-04-08 17:18:17,597 - INFO - tqdm - accuracy: 0.9732, batch_loss: 0.0952, loss: 0.0750 ||:   4%|3         | 21/590 [01:08<29:21,  3.10s/it]
2022-04-08 17:18:30,729 - INFO - tqdm - accuracy: 0.9700, batch_loss: 0.3007, loss: 0.0882 ||:   4%|4         | 25/590 [01:21<31:05,  3.30s/it]
2022-04-08 17:18:43,756 - INFO - tqdm - accuracy: 0.9720, batch_loss: 0.0183, loss: 0.0821 ||:   5%|4         | 29/590 [01:34<30:38,  3.28s/it]
2022-04-08 17:18:55,352 - INFO - tqdm - accuracy: 0.9736, batch_loss: 0.0241, loss: 0.0788 ||:   5%|5         | 32/590 [01:46<34:06,  3.67s/it]
2022-04-08 17:19:07,714 - INFO - tqdm - accuracy: 0.9714, batch_loss: 0.0540, loss: 0.0837 ||:   6%|6         | 36/590 [01:58<29:00,  3.14s/it]
2022-04-08 17:19:18,680 - INFO - tqdm - accuracy: 0.9712, batch_loss: 0.1069, loss: 0.0820 ||:   7%|6         | 39/590 [02:09<31:48,  3.46s/it]
2022-04-08 17:19:32,173 - INFO - tqdm - accuracy: 0.9724, batch_loss: 0.0238, loss: 0.0777 ||:   7%|7         | 43/590 [02:23<31:18,  3.43s/it]
2022-04-08 17:19:45,110 - INFO - tqdm - accuracy: 0.9741, batch_loss: 0.0556, loss: 0.0738 ||:   8%|7         | 47/590 [02:36<29:52,  3.30s/it]
2022-04-08 17:19:58,323 - INFO - tqdm - accuracy: 0.9749, batch_loss: 0.1276, loss: 0.0722 ||:   9%|8         | 51/590 [02:49<30:22,  3.38s/it]
2022-04-08 17:20:09,544 - INFO - tqdm - accuracy: 0.9763, batch_loss: 0.0054, loss: 0.0693 ||:   9%|9         | 54/590 [03:00<32:16,  3.61s/it]
2022-04-08 17:20:19,628 - INFO - tqdm - accuracy: 0.9764, batch_loss: 0.0166, loss: 0.0683 ||:  10%|9         | 57/590 [03:10<30:20,  3.42s/it]
2022-04-08 17:20:30,021 - INFO - tqdm - accuracy: 0.9776, batch_loss: 0.0185, loss: 0.0660 ||:  10%|#         | 60/590 [03:21<30:27,  3.45s/it]
2022-04-08 17:20:40,821 - INFO - tqdm - accuracy: 0.9777, batch_loss: 0.0684, loss: 0.0652 ||:  11%|#         | 63/590 [03:32<31:22,  3.57s/it]
2022-04-08 17:20:50,846 - INFO - tqdm - accuracy: 0.9773, batch_loss: 0.1602, loss: 0.0694 ||:  11%|#1        | 66/590 [03:42<29:35,  3.39s/it]
2022-04-08 17:21:01,183 - INFO - tqdm - accuracy: 0.9774, batch_loss: 0.1071, loss: 0.0692 ||:  12%|#1        | 69/590 [03:52<30:07,  3.47s/it]
2022-04-08 17:21:13,435 - INFO - tqdm - accuracy: 0.9777, batch_loss: 0.0387, loss: 0.0693 ||:  12%|#2        | 73/590 [04:04<27:33,  3.20s/it]
2022-04-08 17:21:26,195 - INFO - tqdm - accuracy: 0.9777, batch_loss: 0.0065, loss: 0.0677 ||:  13%|#3        | 77/590 [04:17<27:18,  3.19s/it]
2022-04-08 17:21:36,425 - INFO - tqdm - accuracy: 0.9781, batch_loss: 0.2256, loss: 0.0681 ||:  14%|#3        | 80/590 [04:27<28:22,  3.34s/it]
2022-04-08 17:21:46,796 - INFO - tqdm - accuracy: 0.9778, batch_loss: 0.0316, loss: 0.0671 ||:  14%|#4        | 83/590 [04:38<29:00,  3.43s/it]
2022-04-08 17:21:57,407 - INFO - tqdm - accuracy: 0.9775, batch_loss: 0.0708, loss: 0.0685 ||:  15%|#4        | 86/590 [04:48<28:42,  3.42s/it]
2022-04-08 17:22:07,540 - INFO - tqdm - accuracy: 0.9775, batch_loss: 0.0434, loss: 0.0673 ||:  15%|#5        | 89/590 [04:58<28:22,  3.40s/it]
2022-04-08 17:22:17,681 - INFO - tqdm - accuracy: 0.9779, batch_loss: 0.0672, loss: 0.0660 ||:  16%|#5        | 92/590 [05:08<27:55,  3.36s/it]
2022-04-08 17:22:28,195 - INFO - tqdm - accuracy: 0.9780, batch_loss: 0.0986, loss: 0.0654 ||:  16%|#6        | 95/590 [05:19<28:57,  3.51s/it]
2022-04-08 17:22:41,794 - INFO - tqdm - accuracy: 0.9776, batch_loss: 0.0052, loss: 0.0663 ||:  17%|#6        | 99/590 [05:33<28:27,  3.48s/it]
2022-04-08 17:22:53,078 - INFO - tqdm - accuracy: 0.9773, batch_loss: 0.0056, loss: 0.0673 ||:  17%|#7        | 102/590 [05:44<30:04,  3.70s/it]
2022-04-08 17:23:06,357 - INFO - tqdm - accuracy: 0.9770, batch_loss: 0.0081, loss: 0.0693 ||:  18%|#7        | 106/590 [05:57<28:22,  3.52s/it]
2022-04-08 17:23:19,492 - INFO - tqdm - accuracy: 0.9773, batch_loss: 0.0284, loss: 0.0684 ||:  19%|#8        | 110/590 [06:10<26:45,  3.34s/it]
2022-04-08 17:23:32,196 - INFO - tqdm - accuracy: 0.9767, batch_loss: 0.2123, loss: 0.0687 ||:  19%|#9        | 114/590 [06:23<25:26,  3.21s/it]
2022-04-08 17:23:42,428 - INFO - tqdm - accuracy: 0.9768, batch_loss: 0.0470, loss: 0.0680 ||:  20%|#9        | 117/590 [06:33<26:35,  3.37s/it]
2022-04-08 17:23:52,469 - INFO - tqdm - accuracy: 0.9768, batch_loss: 0.0176, loss: 0.0679 ||:  20%|##        | 120/590 [06:43<25:45,  3.29s/it]
2022-04-08 17:24:02,690 - INFO - tqdm - accuracy: 0.9769, batch_loss: 0.1816, loss: 0.0691 ||:  21%|##        | 123/590 [06:53<25:49,  3.32s/it]
2022-04-08 17:24:15,037 - INFO - tqdm - accuracy: 0.9769, batch_loss: 0.0396, loss: 0.0690 ||:  22%|##1       | 127/590 [07:06<24:03,  3.12s/it]
2022-04-08 17:24:25,962 - INFO - tqdm - accuracy: 0.9774, batch_loss: 0.0113, loss: 0.0679 ||:  22%|##2       | 130/590 [07:17<26:52,  3.50s/it]
2022-04-08 17:24:38,876 - INFO - tqdm - accuracy: 0.9774, batch_loss: 0.0548, loss: 0.0689 ||:  23%|##2       | 134/590 [07:30<24:55,  3.28s/it]
2022-04-08 17:24:51,924 - INFO - tqdm - accuracy: 0.9778, batch_loss: 0.0079, loss: 0.0678 ||:  23%|##3       | 138/590 [07:43<24:40,  3.27s/it]
2022-04-08 17:25:05,444 - INFO - tqdm - accuracy: 0.9782, batch_loss: 0.0060, loss: 0.0667 ||:  24%|##4       | 142/590 [07:56<25:27,  3.41s/it]
2022-04-08 17:25:16,036 - INFO - tqdm - accuracy: 0.9784, batch_loss: 0.0283, loss: 0.0660 ||:  25%|##4       | 145/590 [08:07<25:49,  3.48s/it]
2022-04-08 17:25:26,184 - INFO - tqdm - accuracy: 0.9780, batch_loss: 0.1937, loss: 0.0674 ||:  25%|##5       | 148/590 [08:17<24:51,  3.38s/it]
2022-04-08 17:25:36,197 - INFO - tqdm - accuracy: 0.9783, batch_loss: 0.0160, loss: 0.0671 ||:  26%|##5       | 151/590 [08:27<24:27,  3.34s/it]
2022-04-08 17:25:48,974 - INFO - tqdm - accuracy: 0.9782, batch_loss: 0.0618, loss: 0.0673 ||:  26%|##6       | 155/590 [08:40<23:00,  3.17s/it]
2022-04-08 17:25:59,413 - INFO - tqdm - accuracy: 0.9778, batch_loss: 0.0876, loss: 0.0674 ||:  27%|##6       | 158/590 [08:50<24:36,  3.42s/it]
2022-04-08 17:26:09,426 - INFO - tqdm - accuracy: 0.9781, batch_loss: 0.0295, loss: 0.0668 ||:  27%|##7       | 161/590 [09:00<24:13,  3.39s/it]
2022-04-08 17:26:20,040 - INFO - tqdm - accuracy: 0.9781, batch_loss: 0.0596, loss: 0.0662 ||:  28%|##7       | 164/590 [09:11<24:37,  3.47s/it]
2022-04-08 17:26:33,902 - INFO - tqdm - accuracy: 0.9781, batch_loss: 0.0787, loss: 0.0660 ||:  28%|##8       | 168/590 [09:25<24:54,  3.54s/it]
2022-04-08 17:26:47,549 - INFO - tqdm - accuracy: 0.9780, batch_loss: 0.0070, loss: 0.0657 ||:  29%|##9       | 172/590 [09:38<24:07,  3.46s/it]
2022-04-08 17:26:58,042 - INFO - tqdm - accuracy: 0.9777, batch_loss: 0.0165, loss: 0.0667 ||:  30%|##9       | 175/590 [09:49<23:52,  3.45s/it]
2022-04-08 17:27:08,257 - INFO - tqdm - accuracy: 0.9777, batch_loss: 0.0528, loss: 0.0666 ||:  30%|###       | 178/590 [09:59<23:25,  3.41s/it]
2022-04-08 17:27:19,476 - INFO - tqdm - accuracy: 0.9777, batch_loss: 0.0834, loss: 0.0664 ||:  31%|###       | 181/590 [10:10<24:36,  3.61s/it]
2022-04-08 17:27:30,433 - INFO - tqdm - accuracy: 0.9776, batch_loss: 0.1445, loss: 0.0670 ||:  31%|###1      | 184/590 [10:21<25:19,  3.74s/it]
2022-04-08 17:27:43,230 - INFO - tqdm - accuracy: 0.9777, batch_loss: 0.0564, loss: 0.0669 ||:  32%|###1      | 188/590 [10:34<22:38,  3.38s/it]
2022-04-08 17:27:53,277 - INFO - tqdm - accuracy: 0.9776, batch_loss: 0.2437, loss: 0.0679 ||:  32%|###2      | 191/590 [10:44<22:19,  3.36s/it]
2022-04-08 17:28:05,748 - INFO - tqdm - accuracy: 0.9776, batch_loss: 0.0176, loss: 0.0683 ||:  33%|###3      | 195/590 [10:56<20:46,  3.16s/it]
2022-04-08 17:28:19,055 - INFO - tqdm - accuracy: 0.9769, batch_loss: 0.0410, loss: 0.0693 ||:  34%|###3      | 199/590 [11:10<21:49,  3.35s/it]
2022-04-08 17:28:29,363 - INFO - tqdm - accuracy: 0.9769, batch_loss: 0.0451, loss: 0.0690 ||:  34%|###4      | 202/590 [11:20<21:43,  3.36s/it]
2022-04-08 17:28:41,560 - INFO - tqdm - accuracy: 0.9769, batch_loss: 0.0709, loss: 0.0692 ||:  35%|###4      | 206/590 [11:32<20:12,  3.16s/it]
2022-04-08 17:28:55,311 - INFO - tqdm - accuracy: 0.9763, batch_loss: 0.0971, loss: 0.0698 ||:  36%|###5      | 210/590 [11:46<21:44,  3.43s/it]
2022-04-08 17:29:06,193 - INFO - tqdm - accuracy: 0.9765, batch_loss: 0.0740, loss: 0.0695 ||:  36%|###6      | 213/590 [11:57<22:02,  3.51s/it]
2022-04-08 17:29:16,531 - INFO - tqdm - accuracy: 0.9764, batch_loss: 0.0959, loss: 0.0696 ||:  37%|###6      | 216/590 [12:07<21:33,  3.46s/it]
2022-04-08 17:29:26,843 - INFO - tqdm - accuracy: 0.9765, batch_loss: 0.0100, loss: 0.0691 ||:  37%|###7      | 219/590 [12:18<20:48,  3.36s/it]
2022-04-08 17:29:39,531 - INFO - tqdm - accuracy: 0.9769, batch_loss: 0.0140, loss: 0.0683 ||:  38%|###7      | 223/590 [12:30<19:48,  3.24s/it]
2022-04-08 17:29:50,507 - INFO - tqdm - accuracy: 0.9769, batch_loss: 0.0202, loss: 0.0684 ||:  38%|###8      | 226/590 [12:41<21:17,  3.51s/it]
2022-04-08 17:30:04,242 - INFO - tqdm - accuracy: 0.9768, batch_loss: 0.0516, loss: 0.0682 ||:  39%|###8      | 230/590 [12:55<20:59,  3.50s/it]
2022-04-08 17:30:17,872 - INFO - tqdm - accuracy: 0.9770, batch_loss: 0.0586, loss: 0.0676 ||:  40%|###9      | 234/590 [13:09<20:43,  3.49s/it]
2022-04-08 17:30:31,076 - INFO - tqdm - accuracy: 0.9769, batch_loss: 0.0425, loss: 0.0674 ||:  40%|####      | 238/590 [13:22<19:39,  3.35s/it]
2022-04-08 17:30:41,687 - INFO - tqdm - accuracy: 0.9768, batch_loss: 0.0216, loss: 0.0674 ||:  41%|####      | 241/590 [13:32<19:55,  3.43s/it]
2022-04-08 17:30:54,654 - INFO - tqdm - accuracy: 0.9767, batch_loss: 0.0424, loss: 0.0676 ||:  42%|####1     | 245/590 [13:45<18:44,  3.26s/it]
2022-04-08 17:31:08,155 - INFO - tqdm - accuracy: 0.9767, batch_loss: 0.0299, loss: 0.0677 ||:  42%|####2     | 249/590 [13:59<19:27,  3.42s/it]
2022-04-08 17:31:18,342 - INFO - tqdm - accuracy: 0.9767, batch_loss: 0.0522, loss: 0.0675 ||:  43%|####2     | 252/590 [14:09<19:19,  3.43s/it]
2022-04-08 17:31:29,190 - INFO - tqdm - accuracy: 0.9768, batch_loss: 0.0139, loss: 0.0669 ||:  43%|####3     | 255/590 [14:20<19:33,  3.50s/it]
2022-04-08 17:31:39,840 - INFO - tqdm - accuracy: 0.9769, batch_loss: 0.0090, loss: 0.0667 ||:  44%|####3     | 258/590 [14:31<19:28,  3.52s/it]
2022-04-08 17:31:51,373 - INFO - tqdm - accuracy: 0.9769, batch_loss: 0.2022, loss: 0.0672 ||:  44%|####4     | 262/590 [14:42<16:31,  3.02s/it]
2022-04-08 17:32:04,393 - INFO - tqdm - accuracy: 0.9771, batch_loss: 0.0071, loss: 0.0671 ||:  45%|####5     | 266/590 [14:55<17:08,  3.18s/it]
2022-04-08 17:32:17,040 - INFO - tqdm - accuracy: 0.9767, batch_loss: 0.1551, loss: 0.0675 ||:  46%|####5     | 270/590 [15:08<17:05,  3.20s/it]
2022-04-08 17:32:30,629 - INFO - tqdm - accuracy: 0.9770, batch_loss: 0.0194, loss: 0.0673 ||:  46%|####6     | 274/590 [15:21<17:56,  3.41s/it]
2022-04-08 17:32:40,811 - INFO - tqdm - accuracy: 0.9769, batch_loss: 0.1370, loss: 0.0673 ||:  47%|####6     | 277/590 [15:32<18:02,  3.46s/it]
2022-04-08 17:32:51,145 - INFO - tqdm - accuracy: 0.9767, batch_loss: 0.0344, loss: 0.0678 ||:  47%|####7     | 280/590 [15:42<17:39,  3.42s/it]
2022-04-08 17:33:01,186 - INFO - tqdm - accuracy: 0.9768, batch_loss: 0.0060, loss: 0.0673 ||:  48%|####7     | 283/590 [15:52<17:04,  3.34s/it]
2022-04-08 17:33:11,587 - INFO - tqdm - accuracy: 0.9768, batch_loss: 0.0529, loss: 0.0671 ||:  48%|####8     | 286/590 [16:02<17:20,  3.42s/it]
2022-04-08 17:33:24,915 - INFO - tqdm - accuracy: 0.9769, batch_loss: 0.0056, loss: 0.0669 ||:  49%|####9     | 290/590 [16:16<16:40,  3.34s/it]
2022-04-08 17:33:35,501 - INFO - tqdm - accuracy: 0.9770, batch_loss: 0.2256, loss: 0.0671 ||:  50%|####9     | 293/590 [16:26<17:14,  3.48s/it]
2022-04-08 17:33:45,766 - INFO - tqdm - accuracy: 0.9771, batch_loss: 0.0103, loss: 0.0667 ||:  50%|#####     | 296/590 [16:37<17:05,  3.49s/it]
2022-04-08 17:33:56,409 - INFO - tqdm - accuracy: 0.9773, batch_loss: 0.0269, loss: 0.0662 ||:  51%|#####     | 299/590 [16:47<16:57,  3.49s/it]
2022-04-08 17:34:06,462 - INFO - tqdm - accuracy: 0.9773, batch_loss: 0.0054, loss: 0.0658 ||:  51%|#####1    | 302/590 [16:57<16:06,  3.36s/it]
2022-04-08 17:34:19,040 - INFO - tqdm - accuracy: 0.9773, batch_loss: 0.0040, loss: 0.0655 ||:  52%|#####1    | 306/590 [17:10<15:15,  3.22s/it]
2022-04-08 17:34:29,365 - INFO - tqdm - accuracy: 0.9774, batch_loss: 0.0437, loss: 0.0651 ||:  52%|#####2    | 309/590 [17:20<15:32,  3.32s/it]
2022-04-08 17:34:41,383 - INFO - tqdm - accuracy: 0.9771, batch_loss: 0.1487, loss: 0.0660 ||:  53%|#####3    | 313/590 [17:32<14:12,  3.08s/it]
2022-04-08 17:34:55,013 - INFO - tqdm - accuracy: 0.9770, batch_loss: 0.0239, loss: 0.0663 ||:  54%|#####3    | 317/590 [17:46<15:22,  3.38s/it]
2022-04-08 17:35:05,755 - INFO - tqdm - accuracy: 0.9766, batch_loss: 0.2312, loss: 0.0676 ||:  54%|#####4    | 320/590 [17:56<15:36,  3.47s/it]
2022-04-08 17:35:18,686 - INFO - tqdm - accuracy: 0.9767, batch_loss: 0.0156, loss: 0.0680 ||:  55%|#####4    | 324/590 [18:09<14:34,  3.29s/it]
2022-04-08 17:35:32,540 - INFO - tqdm - accuracy: 0.9766, batch_loss: 0.0454, loss: 0.0679 ||:  56%|#####5    | 328/590 [18:23<15:39,  3.58s/it]
2022-04-08 17:35:43,051 - INFO - tqdm - accuracy: 0.9767, batch_loss: 0.0381, loss: 0.0676 ||:  56%|#####6    | 331/590 [18:34<15:19,  3.55s/it]
2022-04-08 17:35:56,473 - INFO - tqdm - accuracy: 0.9763, batch_loss: 0.2158, loss: 0.0682 ||:  57%|#####6    | 335/590 [18:47<14:31,  3.42s/it]
2022-04-08 17:36:09,894 - INFO - tqdm - accuracy: 0.9761, batch_loss: 0.1030, loss: 0.0683 ||:  57%|#####7    | 339/590 [19:01<14:27,  3.46s/it]
2022-04-08 17:36:20,501 - INFO - tqdm - accuracy: 0.9761, batch_loss: 0.0997, loss: 0.0684 ||:  58%|#####7    | 342/590 [19:11<14:47,  3.58s/it]
2022-04-08 17:36:30,594 - INFO - tqdm - accuracy: 0.9761, batch_loss: 0.0483, loss: 0.0685 ||:  58%|#####8    | 345/590 [19:21<14:10,  3.47s/it]
2022-04-08 17:36:43,491 - INFO - tqdm - accuracy: 0.9758, batch_loss: 0.1966, loss: 0.0690 ||:  59%|#####9    | 349/590 [19:34<13:20,  3.32s/it]
2022-04-08 17:36:54,489 - INFO - tqdm - accuracy: 0.9758, batch_loss: 0.0326, loss: 0.0692 ||:  60%|#####9    | 352/590 [19:45<14:00,  3.53s/it]
2022-04-08 17:37:05,037 - INFO - tqdm - accuracy: 0.9758, batch_loss: 0.0815, loss: 0.0692 ||:  60%|######    | 355/590 [19:56<13:56,  3.56s/it]
2022-04-08 17:37:15,492 - INFO - tqdm - accuracy: 0.9757, batch_loss: 0.0444, loss: 0.0692 ||:  61%|######    | 358/590 [20:06<13:40,  3.54s/it]
2022-04-08 17:37:25,856 - INFO - tqdm - accuracy: 0.9757, batch_loss: 0.0534, loss: 0.0692 ||:  61%|######1   | 361/590 [20:17<13:23,  3.51s/it]
2022-04-08 17:37:36,353 - INFO - tqdm - accuracy: 0.9755, batch_loss: 0.1776, loss: 0.0699 ||:  62%|######1   | 364/590 [20:27<13:17,  3.53s/it]
2022-04-08 17:37:46,750 - INFO - tqdm - accuracy: 0.9756, batch_loss: 0.0505, loss: 0.0698 ||:  62%|######2   | 367/590 [20:37<13:07,  3.53s/it]
2022-04-08 17:37:57,144 - INFO - tqdm - accuracy: 0.9756, batch_loss: 0.2350, loss: 0.0701 ||:  63%|######2   | 370/590 [20:48<12:38,  3.45s/it]
2022-04-08 17:38:07,981 - INFO - tqdm - accuracy: 0.9754, batch_loss: 0.0372, loss: 0.0706 ||:  63%|######3   | 373/590 [20:59<12:47,  3.54s/it]
2022-04-08 17:38:21,170 - INFO - tqdm - accuracy: 0.9754, batch_loss: 0.0356, loss: 0.0707 ||:  64%|######3   | 377/590 [21:12<12:08,  3.42s/it]
2022-04-08 17:38:31,938 - INFO - tqdm - accuracy: 0.9751, batch_loss: 0.1560, loss: 0.0713 ||:  64%|######4   | 380/590 [21:23<12:21,  3.53s/it]
2022-04-08 17:38:42,812 - INFO - tqdm - accuracy: 0.9751, batch_loss: 0.0849, loss: 0.0712 ||:  65%|######4   | 383/590 [21:34<12:40,  3.67s/it]
2022-04-08 17:38:53,100 - INFO - tqdm - accuracy: 0.9751, batch_loss: 0.1491, loss: 0.0714 ||:  65%|######5   | 386/590 [21:44<11:58,  3.52s/it]
2022-04-08 17:39:06,189 - INFO - tqdm - accuracy: 0.9753, batch_loss: 0.0208, loss: 0.0710 ||:  66%|######6   | 390/590 [21:57<11:14,  3.37s/it]
2022-04-08 17:39:19,438 - INFO - tqdm - accuracy: 0.9755, batch_loss: 0.0341, loss: 0.0707 ||:  67%|######6   | 394/590 [22:10<10:56,  3.35s/it]
2022-04-08 17:39:30,884 - INFO - tqdm - accuracy: 0.9756, batch_loss: 0.0055, loss: 0.0705 ||:  67%|######7   | 397/590 [22:22<11:33,  3.59s/it]
2022-04-08 17:39:40,958 - INFO - tqdm - accuracy: 0.9756, batch_loss: 0.0798, loss: 0.0706 ||:  68%|######7   | 400/590 [22:32<10:46,  3.40s/it]
2022-04-08 17:39:53,446 - INFO - tqdm - accuracy: 0.9756, batch_loss: 0.0093, loss: 0.0704 ||:  68%|######8   | 404/590 [22:44<09:54,  3.19s/it]
2022-04-08 17:40:03,702 - INFO - tqdm - accuracy: 0.9757, batch_loss: 0.0103, loss: 0.0701 ||:  69%|######8   | 407/590 [22:54<10:16,  3.37s/it]
2022-04-08 17:40:13,806 - INFO - tqdm - accuracy: 0.9757, batch_loss: 0.0402, loss: 0.0702 ||:  69%|######9   | 410/590 [23:05<10:08,  3.38s/it]
2022-04-08 17:40:27,227 - INFO - tqdm - accuracy: 0.9757, batch_loss: 0.0321, loss: 0.0705 ||:  70%|#######   | 414/590 [23:18<10:14,  3.49s/it]
2022-04-08 17:40:37,238 - INFO - tqdm - accuracy: 0.9756, batch_loss: 0.0206, loss: 0.0705 ||:  71%|#######   | 417/590 [23:28<09:50,  3.41s/it]
2022-04-08 17:40:47,634 - INFO - tqdm - accuracy: 0.9753, batch_loss: 0.0994, loss: 0.0711 ||:  71%|#######1  | 420/590 [23:38<09:48,  3.46s/it]
2022-04-08 17:40:59,356 - INFO - tqdm - accuracy: 0.9753, batch_loss: 0.0418, loss: 0.0709 ||:  72%|#######1  | 423/590 [23:50<10:48,  3.88s/it]
2022-04-08 17:41:09,931 - INFO - tqdm - accuracy: 0.9754, batch_loss: 0.0254, loss: 0.0709 ||:  72%|#######2  | 426/590 [24:01<10:02,  3.68s/it]
2022-04-08 17:41:20,920 - INFO - tqdm - accuracy: 0.9752, batch_loss: 0.1508, loss: 0.0717 ||:  73%|#######2  | 429/590 [24:12<09:48,  3.66s/it]
2022-04-08 17:41:31,697 - INFO - tqdm - accuracy: 0.9752, batch_loss: 0.0173, loss: 0.0716 ||:  73%|#######3  | 432/590 [24:22<09:38,  3.66s/it]
2022-04-08 17:41:45,034 - INFO - tqdm - accuracy: 0.9753, batch_loss: 0.1581, loss: 0.0719 ||:  74%|#######3  | 436/590 [24:36<08:48,  3.43s/it]
2022-04-08 17:41:55,034 - INFO - tqdm - accuracy: 0.9754, batch_loss: 0.0380, loss: 0.0720 ||:  74%|#######4  | 439/590 [24:46<08:36,  3.42s/it]
2022-04-08 17:42:05,116 - INFO - tqdm - accuracy: 0.9755, batch_loss: 0.0510, loss: 0.0719 ||:  75%|#######4  | 442/590 [24:56<08:24,  3.41s/it]
2022-04-08 17:42:15,295 - INFO - tqdm - accuracy: 0.9756, batch_loss: 0.0294, loss: 0.0718 ||:  75%|#######5  | 445/590 [25:06<08:24,  3.48s/it]
2022-04-08 17:42:25,579 - INFO - tqdm - accuracy: 0.9755, batch_loss: 0.0725, loss: 0.0717 ||:  76%|#######5  | 448/590 [25:16<08:08,  3.44s/it]
2022-04-08 17:42:38,673 - INFO - tqdm - accuracy: 0.9755, batch_loss: 0.0237, loss: 0.0717 ||:  77%|#######6  | 452/590 [25:29<07:39,  3.33s/it]
2022-04-08 17:42:51,701 - INFO - tqdm - accuracy: 0.9755, batch_loss: 0.2002, loss: 0.0716 ||:  77%|#######7  | 456/590 [25:42<07:37,  3.42s/it]
2022-04-08 17:43:02,080 - INFO - tqdm - accuracy: 0.9756, batch_loss: 0.0246, loss: 0.0714 ||:  78%|#######7  | 459/590 [25:53<07:33,  3.46s/it]
2022-04-08 17:43:12,645 - INFO - tqdm - accuracy: 0.9758, batch_loss: 0.0164, loss: 0.0710 ||:  78%|#######8  | 462/590 [26:03<07:37,  3.58s/it]
2022-04-08 17:43:22,861 - INFO - tqdm - accuracy: 0.9759, batch_loss: 0.0742, loss: 0.0708 ||:  79%|#######8  | 465/590 [26:14<07:15,  3.48s/it]
2022-04-08 17:43:35,159 - INFO - tqdm - accuracy: 0.9760, batch_loss: 0.1100, loss: 0.0705 ||:  79%|#######9  | 469/590 [26:26<06:22,  3.16s/it]
2022-04-08 17:43:47,187 - INFO - tqdm - accuracy: 0.9761, batch_loss: 0.0254, loss: 0.0701 ||:  80%|########  | 473/590 [26:38<05:52,  3.01s/it]
2022-04-08 17:43:58,459 - INFO - tqdm - accuracy: 0.9761, batch_loss: 0.2528, loss: 0.0705 ||:  81%|########  | 476/590 [26:49<06:45,  3.56s/it]
2022-04-08 17:44:11,391 - INFO - tqdm - accuracy: 0.9762, batch_loss: 0.0551, loss: 0.0702 ||:  81%|########1 | 480/590 [27:02<06:06,  3.33s/it]
2022-04-08 17:44:23,061 - INFO - tqdm - accuracy: 0.9762, batch_loss: 0.0081, loss: 0.0701 ||:  82%|########1 | 483/590 [27:14<06:35,  3.70s/it]
2022-04-08 17:44:36,963 - INFO - tqdm - accuracy: 0.9762, batch_loss: 0.0174, loss: 0.0703 ||:  83%|########2 | 487/590 [27:28<06:12,  3.61s/it]
2022-04-08 17:44:49,796 - INFO - tqdm - accuracy: 0.9762, batch_loss: 0.0381, loss: 0.0700 ||:  83%|########3 | 491/590 [27:41<05:28,  3.32s/it]
2022-04-08 17:45:02,700 - INFO - tqdm - accuracy: 0.9762, batch_loss: 0.0094, loss: 0.0700 ||:  84%|########3 | 495/590 [27:53<05:10,  3.27s/it]
2022-04-08 17:45:12,977 - INFO - tqdm - accuracy: 0.9763, batch_loss: 0.0206, loss: 0.0697 ||:  84%|########4 | 498/590 [28:04<05:10,  3.38s/it]
2022-04-08 17:45:23,202 - INFO - tqdm - accuracy: 0.9764, batch_loss: 0.2929, loss: 0.0699 ||:  85%|########4 | 501/590 [28:14<05:00,  3.37s/it]
2022-04-08 17:45:33,506 - INFO - tqdm - accuracy: 0.9765, batch_loss: 0.0171, loss: 0.0696 ||:  85%|########5 | 504/590 [28:24<04:54,  3.42s/it]
2022-04-08 17:45:43,986 - INFO - tqdm - accuracy: 0.9765, batch_loss: 0.0383, loss: 0.0695 ||:  86%|########5 | 507/590 [28:35<04:53,  3.54s/it]
2022-04-08 17:45:54,327 - INFO - tqdm - accuracy: 0.9767, batch_loss: 0.0287, loss: 0.0692 ||:  86%|########6 | 510/590 [28:45<04:39,  3.49s/it]
2022-04-08 17:46:04,407 - INFO - tqdm - accuracy: 0.9765, batch_loss: 0.0325, loss: 0.0693 ||:  87%|########6 | 513/590 [28:55<04:27,  3.47s/it]
2022-04-08 17:46:16,882 - INFO - tqdm - accuracy: 0.9765, batch_loss: 0.3016, loss: 0.0694 ||:  88%|########7 | 517/590 [29:08<03:49,  3.15s/it]
2022-04-08 17:46:29,212 - INFO - tqdm - accuracy: 0.9764, batch_loss: 0.0148, loss: 0.0702 ||:  88%|########8 | 521/590 [29:20<03:31,  3.07s/it]
2022-04-08 17:46:42,150 - INFO - tqdm - accuracy: 0.9763, batch_loss: 0.3634, loss: 0.0706 ||:  89%|########8 | 525/590 [29:33<03:27,  3.19s/it]
2022-04-08 17:46:54,508 - INFO - tqdm - accuracy: 0.9764, batch_loss: 0.0250, loss: 0.0702 ||:  90%|########9 | 529/590 [29:45<03:11,  3.13s/it]
2022-04-08 17:47:05,681 - INFO - tqdm - accuracy: 0.9763, batch_loss: 0.1743, loss: 0.0706 ||:  90%|######### | 532/590 [29:56<03:24,  3.53s/it]
2022-04-08 17:47:16,102 - INFO - tqdm - accuracy: 0.9763, batch_loss: 0.1048, loss: 0.0706 ||:  91%|######### | 535/590 [30:07<03:13,  3.51s/it]
2022-04-08 17:47:27,941 - INFO - tqdm - accuracy: 0.9764, batch_loss: 0.0255, loss: 0.0704 ||:  91%|#########1| 538/590 [30:19<03:20,  3.85s/it]
2022-04-08 17:47:38,609 - INFO - tqdm - accuracy: 0.9763, batch_loss: 0.1451, loss: 0.0706 ||:  92%|#########1| 541/590 [30:29<02:58,  3.63s/it]
2022-04-08 17:47:48,632 - INFO - tqdm - accuracy: 0.9762, batch_loss: 0.1714, loss: 0.0709 ||:  92%|#########2| 544/590 [30:39<02:38,  3.45s/it]
2022-04-08 17:47:58,642 - INFO - tqdm - accuracy: 0.9763, batch_loss: 0.0233, loss: 0.0707 ||:  93%|#########2| 547/590 [30:49<02:26,  3.42s/it]
2022-04-08 17:48:12,924 - INFO - tqdm - accuracy: 0.9765, batch_loss: 0.0295, loss: 0.0704 ||:  93%|#########3| 551/590 [31:04<02:23,  3.67s/it]
2022-04-08 17:48:25,907 - INFO - tqdm - accuracy: 0.9764, batch_loss: 0.1301, loss: 0.0706 ||:  94%|#########4| 555/590 [31:17<01:58,  3.39s/it]
2022-04-08 17:48:38,747 - INFO - tqdm - accuracy: 0.9764, batch_loss: 0.0126, loss: 0.0706 ||:  95%|#########4| 559/590 [31:29<01:41,  3.28s/it]
2022-04-08 17:48:49,657 - INFO - tqdm - accuracy: 0.9765, batch_loss: 0.0224, loss: 0.0705 ||:  95%|#########5| 562/590 [31:40<01:37,  3.48s/it]
2022-04-08 17:49:02,850 - INFO - tqdm - accuracy: 0.9764, batch_loss: 0.1706, loss: 0.0708 ||:  96%|#########5| 566/590 [31:54<01:20,  3.35s/it]
2022-04-08 17:49:13,190 - INFO - tqdm - accuracy: 0.9764, batch_loss: 0.0141, loss: 0.0706 ||:  96%|#########6| 569/590 [32:04<01:11,  3.38s/it]
2022-04-08 17:49:24,275 - INFO - tqdm - accuracy: 0.9764, batch_loss: 0.0185, loss: 0.0704 ||:  97%|#########6| 572/590 [32:15<01:03,  3.55s/it]
2022-04-08 17:49:37,702 - INFO - tqdm - accuracy: 0.9765, batch_loss: 0.1126, loss: 0.0703 ||:  98%|#########7| 576/590 [32:28<00:48,  3.44s/it]
2022-04-08 17:49:49,778 - INFO - tqdm - accuracy: 0.9764, batch_loss: 0.0157, loss: 0.0704 ||:  98%|#########8| 580/590 [32:41<00:31,  3.12s/it]
2022-04-08 17:50:03,115 - INFO - tqdm - accuracy: 0.9765, batch_loss: 0.0382, loss: 0.0701 ||:  99%|#########8| 584/590 [32:54<00:19,  3.32s/it]
2022-04-08 17:50:15,511 - INFO - tqdm - accuracy: 0.9764, batch_loss: 0.0134, loss: 0.0702 ||: 100%|#########9| 588/590 [33:06<00:06,  3.08s/it]
2022-04-08 17:50:18,764 - INFO - tqdm - accuracy: 0.9764, batch_loss: 0.0422, loss: 0.0701 ||: 100%|#########9| 589/590 [33:10<00:03,  3.13s/it]
2022-04-08 17:50:19,670 - INFO - tqdm - accuracy: 0.9765, batch_loss: 0.0220, loss: 0.0700 ||: 100%|##########| 590/590 [33:10<00:00,  2.46s/it]
2022-04-08 17:50:19,670 - INFO - tqdm - accuracy: 0.9765, batch_loss: 0.0220, loss: 0.0700 ||: 100%|##########| 590/590 [33:10<00:00,  3.37s/it]
2022-04-08 17:50:21,948 - INFO - allennlp.training.trainer - Validating
2022-04-08 17:50:21,949 - INFO - tqdm - 0%|          | 0/1635 [00:00<?, ?it/s]
2022-04-08 17:50:32,071 - INFO - tqdm - accuracy: 0.8414, batch_loss: 0.0331, loss: 0.6999 ||:   9%|8         | 145/1635 [00:10<01:36, 15.46it/s]
2022-04-08 17:50:42,181 - INFO - tqdm - accuracy: 0.8281, batch_loss: 0.0089, loss: 0.7657 ||:  17%|#7        | 285/1635 [00:20<01:23, 16.16it/s]
2022-04-08 17:50:52,325 - INFO - tqdm - accuracy: 0.8384, batch_loss: 0.0008, loss: 0.6958 ||:  26%|##6       | 427/1635 [00:30<01:26, 13.99it/s]
2022-04-08 17:51:02,334 - INFO - tqdm - accuracy: 0.8442, batch_loss: 0.0020, loss: 0.6785 ||:  35%|###4      | 568/1635 [00:40<01:07, 15.75it/s]
2022-04-08 17:51:12,348 - INFO - tqdm - accuracy: 0.8473, batch_loss: 0.0122, loss: 0.6632 ||:  44%|####3     | 714/1635 [00:50<00:57, 16.00it/s]
2022-04-08 17:51:22,379 - INFO - tqdm - accuracy: 0.8539, batch_loss: 0.0007, loss: 0.6279 ||:  52%|#####2    | 852/1635 [01:00<01:09, 11.27it/s]
2022-04-08 17:51:32,454 - INFO - tqdm - accuracy: 0.8538, batch_loss: 3.7438, loss: 0.6191 ||:  61%|######    | 995/1635 [01:10<00:44, 14.48it/s]
2022-04-08 17:51:42,472 - INFO - tqdm - accuracy: 0.8495, batch_loss: 3.5940, loss: 0.6411 ||:  69%|######9   | 1133/1635 [01:20<00:34, 14.53it/s]
2022-04-08 17:51:52,496 - INFO - tqdm - accuracy: 0.8493, batch_loss: 0.0072, loss: 0.6388 ||:  78%|#######8  | 1281/1635 [01:30<00:18, 19.32it/s]
2022-04-08 17:52:02,526 - INFO - tqdm - accuracy: 0.8489, batch_loss: 0.4711, loss: 0.6418 ||:  87%|########7 | 1426/1635 [01:40<00:13, 15.18it/s]
2022-04-08 17:52:12,600 - INFO - tqdm - accuracy: 0.8493, batch_loss: 0.0018, loss: 0.6376 ||:  96%|#########5| 1566/1635 [01:50<00:05, 13.20it/s]
2022-04-08 17:52:17,274 - INFO - tqdm - accuracy: 0.8499, batch_loss: 0.0224, loss: 0.6321 ||: 100%|#########9| 1629/1635 [01:55<00:00, 14.48it/s]
2022-04-08 17:52:17,443 - INFO - tqdm - accuracy: 0.8495, batch_loss: 2.2569, loss: 0.6348 ||: 100%|#########9| 1631/1635 [01:55<00:00, 13.66it/s]
2022-04-08 17:52:17,582 - INFO - tqdm - accuracy: 0.8497, batch_loss: 0.0005, loss: 0.6340 ||: 100%|#########9| 1633/1635 [01:55<00:00, 13.85it/s]
2022-04-08 17:52:17,733 - INFO - tqdm - accuracy: 0.8498, batch_loss: 0.0004, loss: 0.6333 ||: 100%|##########| 1635/1635 [01:55<00:00, 13.68it/s]
2022-04-08 17:52:17,734 - INFO - tqdm - accuracy: 0.8498, batch_loss: 0.0004, loss: 0.6333 ||: 100%|##########| 1635/1635 [01:55<00:00, 14.12it/s]
2022-04-08 17:52:17,734 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-08 17:52:17,734 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.976  |     0.850
2022-04-08 17:52:17,735 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9604.118  |       N/A
2022-04-08 17:52:17,735 - INFO - allennlp.training.tensorboard_writer - loss               |     0.070  |     0.633
2022-04-08 17:52:17,736 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  7239.301  |       N/A
2022-04-08 17:52:22,207 - INFO - allennlp.training.trainer - Epoch duration: 0:35:13.445902
2022-04-08 17:52:22,207 - INFO - allennlp.training.trainer - Estimated training time remaining: 3:31:34
2022-04-08 17:52:22,207 - INFO - allennlp.training.trainer - Epoch 4/9
2022-04-08 17:52:22,207 - INFO - allennlp.training.trainer - Worker 0 memory usage: 7.1G
2022-04-08 17:52:22,208 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-08 17:52:22,209 - INFO - allennlp.training.trainer - Training
2022-04-08 17:52:22,209 - INFO - tqdm - 0%|          | 0/590 [00:00<?, ?it/s]
2022-04-08 17:52:32,435 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.1911, loss: 0.0839 ||:   1%|          | 3/590 [00:10<33:49,  3.46s/it]
2022-04-08 17:52:43,539 - INFO - tqdm - accuracy: 0.9792, batch_loss: 0.0143, loss: 0.0725 ||:   1%|1         | 6/590 [00:21<35:23,  3.64s/it]
2022-04-08 17:52:56,690 - INFO - tqdm - accuracy: 0.9812, batch_loss: 0.0269, loss: 0.0669 ||:   2%|1         | 10/590 [00:34<32:34,  3.37s/it]
2022-04-08 17:53:09,515 - INFO - tqdm - accuracy: 0.9821, batch_loss: 0.0465, loss: 0.0605 ||:   2%|2         | 14/590 [00:47<30:56,  3.22s/it]
2022-04-08 17:53:21,077 - INFO - tqdm - accuracy: 0.9835, batch_loss: 0.0041, loss: 0.0554 ||:   3%|2         | 17/590 [00:58<34:42,  3.63s/it]
2022-04-08 17:53:34,238 - INFO - tqdm - accuracy: 0.9821, batch_loss: 0.0126, loss: 0.0630 ||:   4%|3         | 21/590 [01:12<32:39,  3.44s/it]
2022-04-08 17:53:45,251 - INFO - tqdm - accuracy: 0.9831, batch_loss: 0.0459, loss: 0.0678 ||:   4%|4         | 24/590 [01:23<34:03,  3.61s/it]
2022-04-08 17:53:55,908 - INFO - tqdm - accuracy: 0.9838, batch_loss: 0.0064, loss: 0.0689 ||:   5%|4         | 27/590 [01:33<33:24,  3.56s/it]
2022-04-08 17:54:07,492 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0052, loss: 0.0653 ||:   5%|5         | 30/590 [01:45<36:15,  3.88s/it]
2022-04-08 17:54:19,638 - INFO - tqdm - accuracy: 0.9853, batch_loss: 0.1388, loss: 0.0624 ||:   6%|5         | 34/590 [01:57<30:33,  3.30s/it]
2022-04-08 17:54:30,036 - INFO - tqdm - accuracy: 0.9865, batch_loss: 0.0024, loss: 0.0584 ||:   6%|6         | 37/590 [02:07<31:38,  3.43s/it]
2022-04-08 17:54:41,048 - INFO - tqdm - accuracy: 0.9867, batch_loss: 0.0090, loss: 0.0579 ||:   7%|6         | 40/590 [02:18<33:33,  3.66s/it]
2022-04-08 17:54:55,371 - INFO - tqdm - accuracy: 0.9858, batch_loss: 0.1356, loss: 0.0577 ||:   7%|7         | 44/590 [02:33<33:57,  3.73s/it]
2022-04-08 17:55:05,666 - INFO - tqdm - accuracy: 0.9860, batch_loss: 0.1527, loss: 0.0582 ||:   8%|7         | 47/590 [02:43<31:54,  3.53s/it]
2022-04-08 17:55:16,294 - INFO - tqdm - accuracy: 0.9856, batch_loss: 0.0193, loss: 0.0567 ||:   8%|8         | 50/590 [02:54<31:44,  3.53s/it]
2022-04-08 17:55:26,550 - INFO - tqdm - accuracy: 0.9858, batch_loss: 0.0156, loss: 0.0549 ||:   9%|8         | 53/590 [03:04<31:16,  3.49s/it]
2022-04-08 17:55:37,438 - INFO - tqdm - accuracy: 0.9860, batch_loss: 0.0925, loss: 0.0540 ||:   9%|9         | 56/590 [03:15<31:52,  3.58s/it]
2022-04-08 17:55:50,465 - INFO - tqdm - accuracy: 0.9870, batch_loss: 0.0056, loss: 0.0509 ||:  10%|#         | 60/590 [03:28<29:45,  3.37s/it]
2022-04-08 17:56:01,703 - INFO - tqdm - accuracy: 0.9861, batch_loss: 0.0535, loss: 0.0514 ||:  11%|#         | 63/590 [03:39<32:25,  3.69s/it]
2022-04-08 17:56:11,996 - INFO - tqdm - accuracy: 0.9863, batch_loss: 0.0050, loss: 0.0502 ||:  11%|#1        | 66/590 [03:49<31:10,  3.57s/it]
2022-04-08 17:56:22,799 - INFO - tqdm - accuracy: 0.9860, batch_loss: 0.0229, loss: 0.0494 ||:  12%|#1        | 69/590 [04:00<31:54,  3.67s/it]
2022-04-08 17:56:33,520 - INFO - tqdm - accuracy: 0.9865, batch_loss: 0.0085, loss: 0.0477 ||:  12%|#2        | 72/590 [04:11<31:43,  3.68s/it]
2022-04-08 17:56:46,293 - INFO - tqdm - accuracy: 0.9868, batch_loss: 0.0054, loss: 0.0469 ||:  13%|#2        | 76/590 [04:24<28:33,  3.33s/it]
2022-04-08 17:56:56,447 - INFO - tqdm - accuracy: 0.9858, batch_loss: 0.1291, loss: 0.0502 ||:  13%|#3        | 79/590 [04:34<28:53,  3.39s/it]
2022-04-08 17:57:06,688 - INFO - tqdm - accuracy: 0.9859, batch_loss: 0.0033, loss: 0.0495 ||:  14%|#3        | 82/590 [04:44<29:02,  3.43s/it]
2022-04-08 17:57:19,192 - INFO - tqdm - accuracy: 0.9862, batch_loss: 0.0270, loss: 0.0483 ||:  15%|#4        | 86/590 [04:56<26:53,  3.20s/it]
2022-04-08 17:57:31,853 - INFO - tqdm - accuracy: 0.9868, batch_loss: 0.0048, loss: 0.0465 ||:  15%|#5        | 90/590 [05:09<26:37,  3.20s/it]
2022-04-08 17:57:41,957 - INFO - tqdm - accuracy: 0.9862, batch_loss: 0.1778, loss: 0.0473 ||:  16%|#5        | 93/590 [05:19<27:32,  3.32s/it]
2022-04-08 17:57:52,728 - INFO - tqdm - accuracy: 0.9867, batch_loss: 0.0091, loss: 0.0463 ||:  16%|#6        | 96/590 [05:30<28:41,  3.49s/it]
2022-04-08 17:58:05,683 - INFO - tqdm - accuracy: 0.9872, batch_loss: 0.0139, loss: 0.0449 ||:  17%|#6        | 100/590 [05:43<26:50,  3.29s/it]
2022-04-08 17:58:19,188 - INFO - tqdm - accuracy: 0.9865, batch_loss: 0.0026, loss: 0.0463 ||:  18%|#7        | 104/590 [05:56<27:40,  3.42s/it]
2022-04-08 17:58:31,511 - INFO - tqdm - accuracy: 0.9858, batch_loss: 0.1984, loss: 0.0482 ||:  18%|#8        | 108/590 [06:09<25:22,  3.16s/it]
2022-04-08 17:58:41,647 - INFO - tqdm - accuracy: 0.9856, batch_loss: 0.2533, loss: 0.0493 ||:  19%|#8        | 111/590 [06:19<26:03,  3.26s/it]
2022-04-08 17:58:54,667 - INFO - tqdm - accuracy: 0.9851, batch_loss: 0.0738, loss: 0.0511 ||:  19%|#9        | 115/590 [06:32<25:38,  3.24s/it]
2022-04-08 17:59:07,082 - INFO - tqdm - accuracy: 0.9850, batch_loss: 0.0950, loss: 0.0508 ||:  20%|##        | 119/590 [06:44<25:00,  3.19s/it]
2022-04-08 17:59:19,935 - INFO - tqdm - accuracy: 0.9855, batch_loss: 0.0053, loss: 0.0497 ||:  21%|##        | 123/590 [06:57<24:47,  3.18s/it]
2022-04-08 17:59:30,222 - INFO - tqdm - accuracy: 0.9854, batch_loss: 0.0636, loss: 0.0498 ||:  21%|##1       | 126/590 [07:08<25:58,  3.36s/it]
2022-04-08 17:59:42,801 - INFO - tqdm - accuracy: 0.9856, batch_loss: 0.1994, loss: 0.0500 ||:  22%|##2       | 130/590 [07:20<24:10,  3.15s/it]
2022-04-08 17:59:56,080 - INFO - tqdm - accuracy: 0.9853, batch_loss: 0.0648, loss: 0.0498 ||:  23%|##2       | 134/590 [07:33<25:02,  3.30s/it]
2022-04-08 18:00:08,215 - INFO - tqdm - accuracy: 0.9853, batch_loss: 0.0360, loss: 0.0496 ||:  23%|##3       | 138/590 [07:46<22:53,  3.04s/it]
2022-04-08 18:00:18,760 - INFO - tqdm - accuracy: 0.9854, batch_loss: 0.0087, loss: 0.0501 ||:  24%|##3       | 141/590 [07:56<24:43,  3.30s/it]
2022-04-08 18:00:29,764 - INFO - tqdm - accuracy: 0.9852, batch_loss: 0.0504, loss: 0.0496 ||:  24%|##4       | 144/590 [08:07<25:33,  3.44s/it]
2022-04-08 18:00:39,995 - INFO - tqdm - accuracy: 0.9851, batch_loss: 0.1041, loss: 0.0501 ||:  25%|##4       | 147/590 [08:17<25:20,  3.43s/it]
2022-04-08 18:00:50,748 - INFO - tqdm - accuracy: 0.9852, batch_loss: 0.0034, loss: 0.0502 ||:  25%|##5       | 150/590 [08:28<25:44,  3.51s/it]
2022-04-08 18:01:01,044 - INFO - tqdm - accuracy: 0.9851, batch_loss: 0.0116, loss: 0.0509 ||:  26%|##5       | 153/590 [08:38<25:12,  3.46s/it]
2022-04-08 18:01:11,951 - INFO - tqdm - accuracy: 0.9850, batch_loss: 0.0045, loss: 0.0505 ||:  26%|##6       | 156/590 [08:49<26:05,  3.61s/it]
2022-04-08 18:01:25,298 - INFO - tqdm - accuracy: 0.9848, batch_loss: 0.0502, loss: 0.0503 ||:  27%|##7       | 160/590 [09:03<24:57,  3.48s/it]
2022-04-08 18:01:38,687 - INFO - tqdm - accuracy: 0.9849, batch_loss: 0.0053, loss: 0.0495 ||:  28%|##7       | 164/590 [09:16<24:40,  3.48s/it]
2022-04-08 18:01:50,861 - INFO - tqdm - accuracy: 0.9853, batch_loss: 0.0077, loss: 0.0486 ||:  28%|##8       | 168/590 [09:28<21:59,  3.13s/it]
2022-04-08 18:02:01,594 - INFO - tqdm - accuracy: 0.9852, batch_loss: 0.0038, loss: 0.0491 ||:  29%|##8       | 171/590 [09:39<24:07,  3.46s/it]
2022-04-08 18:02:14,512 - INFO - tqdm - accuracy: 0.9852, batch_loss: 0.0273, loss: 0.0488 ||:  30%|##9       | 175/590 [09:52<22:35,  3.27s/it]
2022-04-08 18:02:27,630 - INFO - tqdm - accuracy: 0.9852, batch_loss: 0.0082, loss: 0.0493 ||:  30%|###       | 179/590 [10:05<22:43,  3.32s/it]
2022-04-08 18:02:40,675 - INFO - tqdm - accuracy: 0.9855, batch_loss: 0.0023, loss: 0.0484 ||:  31%|###1      | 183/590 [10:18<22:01,  3.25s/it]
2022-04-08 18:02:50,746 - INFO - tqdm - accuracy: 0.9852, batch_loss: 0.1664, loss: 0.0490 ||:  32%|###1      | 186/590 [10:28<22:22,  3.32s/it]
2022-04-08 18:03:03,556 - INFO - tqdm - accuracy: 0.9849, batch_loss: 0.1026, loss: 0.0496 ||:  32%|###2      | 190/590 [10:41<21:23,  3.21s/it]
2022-04-08 18:03:14,374 - INFO - tqdm - accuracy: 0.9849, batch_loss: 0.0040, loss: 0.0498 ||:  33%|###2      | 193/590 [10:52<22:56,  3.47s/it]
2022-04-08 18:03:25,059 - INFO - tqdm - accuracy: 0.9849, batch_loss: 0.1242, loss: 0.0497 ||:  33%|###3      | 196/590 [11:02<23:04,  3.51s/it]
2022-04-08 18:03:37,983 - INFO - tqdm - accuracy: 0.9850, batch_loss: 0.0364, loss: 0.0493 ||:  34%|###3      | 200/590 [11:15<21:13,  3.26s/it]
2022-04-08 18:03:50,577 - INFO - tqdm - accuracy: 0.9851, batch_loss: 0.0188, loss: 0.0489 ||:  35%|###4      | 204/590 [11:28<20:18,  3.16s/it]
2022-04-08 18:04:01,246 - INFO - tqdm - accuracy: 0.9852, batch_loss: 0.0212, loss: 0.0487 ||:  35%|###5      | 207/590 [11:39<21:57,  3.44s/it]
2022-04-08 18:04:11,271 - INFO - tqdm - accuracy: 0.9850, batch_loss: 0.0506, loss: 0.0488 ||:  36%|###5      | 210/590 [11:49<21:14,  3.35s/it]
2022-04-08 18:04:21,878 - INFO - tqdm - accuracy: 0.9852, batch_loss: 0.0038, loss: 0.0482 ||:  36%|###6      | 213/590 [11:59<21:54,  3.49s/it]
2022-04-08 18:04:32,516 - INFO - tqdm - accuracy: 0.9854, batch_loss: 0.0323, loss: 0.0478 ||:  37%|###6      | 216/590 [12:10<21:47,  3.49s/it]
2022-04-08 18:04:42,834 - INFO - tqdm - accuracy: 0.9852, batch_loss: 0.0085, loss: 0.0478 ||:  37%|###7      | 219/590 [12:20<21:44,  3.52s/it]
2022-04-08 18:04:52,893 - INFO - tqdm - accuracy: 0.9852, batch_loss: 0.0125, loss: 0.0475 ||:  38%|###7      | 222/590 [12:30<21:03,  3.43s/it]
2022-04-08 18:05:05,910 - INFO - tqdm - accuracy: 0.9849, batch_loss: 0.1198, loss: 0.0478 ||:  38%|###8      | 226/590 [12:43<20:17,  3.35s/it]
2022-04-08 18:05:16,977 - INFO - tqdm - accuracy: 0.9850, batch_loss: 0.0122, loss: 0.0477 ||:  39%|###8      | 229/590 [12:54<21:35,  3.59s/it]
2022-04-08 18:05:27,007 - INFO - tqdm - accuracy: 0.9852, batch_loss: 0.0223, loss: 0.0472 ||:  39%|###9      | 232/590 [13:04<20:34,  3.45s/it]
2022-04-08 18:05:37,589 - INFO - tqdm - accuracy: 0.9851, batch_loss: 0.0103, loss: 0.0473 ||:  40%|###9      | 235/590 [13:15<20:35,  3.48s/it]
2022-04-08 18:05:50,670 - INFO - tqdm - accuracy: 0.9850, batch_loss: 0.0177, loss: 0.0478 ||:  41%|####      | 239/590 [13:28<19:13,  3.29s/it]
2022-04-08 18:06:00,899 - INFO - tqdm - accuracy: 0.9850, batch_loss: 0.0070, loss: 0.0478 ||:  41%|####1     | 242/590 [13:38<19:32,  3.37s/it]
2022-04-08 18:06:13,414 - INFO - tqdm - accuracy: 0.9851, batch_loss: 0.0667, loss: 0.0474 ||:  42%|####1     | 246/590 [13:51<18:12,  3.18s/it]
2022-04-08 18:06:23,933 - INFO - tqdm - accuracy: 0.9849, batch_loss: 0.0537, loss: 0.0478 ||:  42%|####2     | 249/590 [14:01<19:01,  3.35s/it]
2022-04-08 18:06:36,367 - INFO - tqdm - accuracy: 0.9851, batch_loss: 0.0105, loss: 0.0474 ||:  43%|####2     | 253/590 [14:14<17:34,  3.13s/it]
2022-04-08 18:06:49,304 - INFO - tqdm - accuracy: 0.9850, batch_loss: 0.0657, loss: 0.0471 ||:  44%|####3     | 257/590 [14:27<17:33,  3.16s/it]
2022-04-08 18:07:03,636 - INFO - tqdm - accuracy: 0.9850, batch_loss: 0.0087, loss: 0.0468 ||:  44%|####4     | 261/590 [14:41<19:43,  3.60s/it]
2022-04-08 18:07:14,446 - INFO - tqdm - accuracy: 0.9851, batch_loss: 0.0387, loss: 0.0470 ||:  45%|####4     | 264/590 [14:52<19:31,  3.59s/it]
2022-04-08 18:07:24,513 - INFO - tqdm - accuracy: 0.9849, batch_loss: 0.0087, loss: 0.0478 ||:  45%|####5     | 267/590 [15:02<18:44,  3.48s/it]
2022-04-08 18:07:34,656 - INFO - tqdm - accuracy: 0.9851, batch_loss: 0.0065, loss: 0.0473 ||:  46%|####5     | 270/590 [15:12<18:11,  3.41s/it]
2022-04-08 18:07:45,308 - INFO - tqdm - accuracy: 0.9850, batch_loss: 0.0086, loss: 0.0477 ||:  46%|####6     | 273/590 [15:23<18:43,  3.54s/it]
2022-04-08 18:07:58,179 - INFO - tqdm - accuracy: 0.9850, batch_loss: 0.0376, loss: 0.0474 ||:  47%|####6     | 277/590 [15:35<17:16,  3.31s/it]
2022-04-08 18:08:09,942 - INFO - tqdm - accuracy: 0.9852, batch_loss: 0.0244, loss: 0.0471 ||:  48%|####7     | 281/590 [15:47<15:23,  2.99s/it]
2022-04-08 18:08:20,463 - INFO - tqdm - accuracy: 0.9853, batch_loss: 0.0267, loss: 0.0470 ||:  48%|####8     | 284/590 [15:58<16:43,  3.28s/it]
2022-04-08 18:08:30,856 - INFO - tqdm - accuracy: 0.9851, batch_loss: 0.1806, loss: 0.0474 ||:  49%|####8     | 287/590 [16:08<17:25,  3.45s/it]
2022-04-08 18:08:44,114 - INFO - tqdm - accuracy: 0.9846, batch_loss: 0.4209, loss: 0.0488 ||:  49%|####9     | 291/590 [16:21<16:40,  3.35s/it]
2022-04-08 18:08:54,493 - INFO - tqdm - accuracy: 0.9847, batch_loss: 0.0813, loss: 0.0488 ||:  50%|####9     | 294/590 [16:32<16:32,  3.35s/it]
2022-04-08 18:09:04,811 - INFO - tqdm - accuracy: 0.9847, batch_loss: 0.0715, loss: 0.0486 ||:  50%|#####     | 297/590 [16:42<16:34,  3.39s/it]
2022-04-08 18:09:18,576 - INFO - tqdm - accuracy: 0.9847, batch_loss: 0.0605, loss: 0.0486 ||:  51%|#####1    | 301/590 [16:56<17:00,  3.53s/it]
2022-04-08 18:09:31,160 - INFO - tqdm - accuracy: 0.9845, batch_loss: 0.0084, loss: 0.0491 ||:  52%|#####1    | 305/590 [17:08<15:17,  3.22s/it]
2022-04-08 18:09:41,184 - INFO - tqdm - accuracy: 0.9846, batch_loss: 0.0119, loss: 0.0488 ||:  52%|#####2    | 308/590 [17:18<15:28,  3.29s/it]
2022-04-08 18:09:51,434 - INFO - tqdm - accuracy: 0.9846, batch_loss: 0.0101, loss: 0.0487 ||:  53%|#####2    | 311/590 [17:29<15:35,  3.35s/it]
2022-04-08 18:10:01,504 - INFO - tqdm - accuracy: 0.9847, batch_loss: 0.0641, loss: 0.0486 ||:  53%|#####3    | 314/590 [17:39<15:24,  3.35s/it]
2022-04-08 18:10:14,352 - INFO - tqdm - accuracy: 0.9847, batch_loss: 0.0195, loss: 0.0485 ||:  54%|#####3    | 318/590 [17:52<14:58,  3.30s/it]
2022-04-08 18:10:24,666 - INFO - tqdm - accuracy: 0.9847, batch_loss: 0.0055, loss: 0.0483 ||:  54%|#####4    | 321/590 [18:02<15:28,  3.45s/it]
2022-04-08 18:10:35,076 - INFO - tqdm - accuracy: 0.9849, batch_loss: 0.0046, loss: 0.0479 ||:  55%|#####4    | 324/590 [18:12<15:29,  3.49s/it]
2022-04-08 18:10:48,190 - INFO - tqdm - accuracy: 0.9849, batch_loss: 0.0306, loss: 0.0476 ||:  56%|#####5    | 328/590 [18:25<14:42,  3.37s/it]
2022-04-08 18:10:59,059 - INFO - tqdm - accuracy: 0.9848, batch_loss: 0.0427, loss: 0.0475 ||:  56%|#####6    | 331/590 [18:36<15:15,  3.53s/it]
2022-04-08 18:11:09,640 - INFO - tqdm - accuracy: 0.9847, batch_loss: 0.0848, loss: 0.0480 ||:  57%|#####6    | 334/590 [18:47<15:13,  3.57s/it]
2022-04-08 18:11:20,013 - INFO - tqdm - accuracy: 0.9847, batch_loss: 0.0106, loss: 0.0484 ||:  57%|#####7    | 337/590 [18:57<14:57,  3.55s/it]
2022-04-08 18:11:31,149 - INFO - tqdm - accuracy: 0.9847, batch_loss: 0.0021, loss: 0.0481 ||:  58%|#####7    | 340/590 [19:08<15:53,  3.81s/it]
2022-04-08 18:11:41,926 - INFO - tqdm - accuracy: 0.9849, batch_loss: 0.0106, loss: 0.0478 ||:  58%|#####8    | 343/590 [19:19<15:24,  3.74s/it]
2022-04-08 18:11:52,418 - INFO - tqdm - accuracy: 0.9849, batch_loss: 0.0029, loss: 0.0475 ||:  59%|#####8    | 346/590 [19:30<14:31,  3.57s/it]
2022-04-08 18:12:02,963 - INFO - tqdm - accuracy: 0.9850, batch_loss: 0.0111, loss: 0.0472 ||:  59%|#####9    | 349/590 [19:40<14:32,  3.62s/it]
2022-04-08 18:12:13,850 - INFO - tqdm - accuracy: 0.9851, batch_loss: 0.0668, loss: 0.0470 ||:  60%|#####9    | 352/590 [19:51<14:38,  3.69s/it]
2022-04-08 18:12:24,344 - INFO - tqdm - accuracy: 0.9852, batch_loss: 0.0025, loss: 0.0467 ||:  60%|######    | 355/590 [20:02<13:58,  3.57s/it]
2022-04-08 18:12:38,263 - INFO - tqdm - accuracy: 0.9853, batch_loss: 0.0038, loss: 0.0466 ||:  61%|######    | 359/590 [20:16<13:43,  3.56s/it]
2022-04-08 18:12:48,748 - INFO - tqdm - accuracy: 0.9852, batch_loss: 0.0242, loss: 0.0467 ||:  61%|######1   | 362/590 [20:26<13:01,  3.43s/it]
2022-04-08 18:12:58,946 - INFO - tqdm - accuracy: 0.9853, batch_loss: 0.0035, loss: 0.0464 ||:  62%|######1   | 365/590 [20:36<12:31,  3.34s/it]
2022-04-08 18:13:11,779 - INFO - tqdm - accuracy: 0.9853, batch_loss: 0.0024, loss: 0.0467 ||:  63%|######2   | 369/590 [20:49<12:06,  3.29s/it]
2022-04-08 18:13:21,830 - INFO - tqdm - accuracy: 0.9855, batch_loss: 0.0060, loss: 0.0464 ||:  63%|######3   | 372/590 [20:59<11:57,  3.29s/it]
2022-04-08 18:13:32,987 - INFO - tqdm - accuracy: 0.9854, batch_loss: 0.0447, loss: 0.0463 ||:  64%|######3   | 375/590 [21:10<12:45,  3.56s/it]
2022-04-08 18:13:46,428 - INFO - tqdm - accuracy: 0.9854, batch_loss: 0.0031, loss: 0.0460 ||:  64%|######4   | 379/590 [21:24<12:07,  3.45s/it]
2022-04-08 18:14:00,337 - INFO - tqdm - accuracy: 0.9855, batch_loss: 0.0732, loss: 0.0459 ||:  65%|######4   | 383/590 [21:38<12:10,  3.53s/it]
2022-04-08 18:14:12,433 - INFO - tqdm - accuracy: 0.9854, batch_loss: 0.0148, loss: 0.0460 ||:  66%|######5   | 387/590 [21:50<10:35,  3.13s/it]
2022-04-08 18:14:24,303 - INFO - tqdm - accuracy: 0.9855, batch_loss: 0.0068, loss: 0.0460 ||:  66%|######6   | 391/590 [22:02<10:05,  3.04s/it]
2022-04-08 18:14:34,528 - INFO - tqdm - accuracy: 0.9854, batch_loss: 0.0792, loss: 0.0464 ||:  67%|######6   | 394/590 [22:12<10:50,  3.32s/it]
2022-04-08 18:14:44,589 - INFO - tqdm - accuracy: 0.9854, batch_loss: 0.0863, loss: 0.0464 ||:  67%|######7   | 397/590 [22:22<10:45,  3.34s/it]
2022-04-08 18:14:57,334 - INFO - tqdm - accuracy: 0.9854, batch_loss: 0.0020, loss: 0.0461 ||:  68%|######7   | 401/590 [22:35<10:16,  3.26s/it]
2022-04-08 18:15:08,309 - INFO - tqdm - accuracy: 0.9855, batch_loss: 0.0014, loss: 0.0458 ||:  68%|######8   | 404/590 [22:46<10:47,  3.48s/it]
2022-04-08 18:15:18,655 - INFO - tqdm - accuracy: 0.9855, batch_loss: 0.1106, loss: 0.0458 ||:  69%|######8   | 407/590 [22:56<10:36,  3.48s/it]
2022-04-08 18:15:31,429 - INFO - tqdm - accuracy: 0.9852, batch_loss: 0.0051, loss: 0.0464 ||:  70%|######9   | 411/590 [23:09<09:36,  3.22s/it]
2022-04-08 18:15:41,744 - INFO - tqdm - accuracy: 0.9853, batch_loss: 0.0016, loss: 0.0464 ||:  70%|#######   | 414/590 [23:19<09:47,  3.34s/it]
2022-04-08 18:15:53,669 - INFO - tqdm - accuracy: 0.9852, batch_loss: 0.0106, loss: 0.0468 ||:  71%|#######   | 417/590 [23:31<10:45,  3.73s/it]
2022-04-08 18:16:06,392 - INFO - tqdm - accuracy: 0.9853, batch_loss: 0.0101, loss: 0.0465 ||:  71%|#######1  | 421/590 [23:44<09:22,  3.33s/it]
2022-04-08 18:16:17,232 - INFO - tqdm - accuracy: 0.9853, batch_loss: 0.0891, loss: 0.0465 ||:  72%|#######1  | 424/590 [23:55<09:50,  3.56s/it]
2022-04-08 18:16:28,696 - INFO - tqdm - accuracy: 0.9853, batch_loss: 0.1077, loss: 0.0465 ||:  72%|#######2  | 427/590 [24:06<10:03,  3.70s/it]
2022-04-08 18:16:42,128 - INFO - tqdm - accuracy: 0.9854, batch_loss: 0.0212, loss: 0.0462 ||:  73%|#######3  | 431/590 [24:19<09:05,  3.43s/it]
2022-04-08 18:16:52,240 - INFO - tqdm - accuracy: 0.9855, batch_loss: 0.0033, loss: 0.0460 ||:  74%|#######3  | 434/590 [24:30<08:58,  3.45s/it]
2022-04-08 18:17:02,416 - INFO - tqdm - accuracy: 0.9855, batch_loss: 0.0240, loss: 0.0459 ||:  74%|#######4  | 437/590 [24:40<08:42,  3.41s/it]
2022-04-08 18:17:14,142 - INFO - tqdm - accuracy: 0.9854, batch_loss: 0.0206, loss: 0.0459 ||:  75%|#######4  | 440/590 [24:51<09:32,  3.82s/it]
2022-04-08 18:17:24,213 - INFO - tqdm - accuracy: 0.9855, batch_loss: 0.0295, loss: 0.0457 ||:  75%|#######5  | 443/590 [25:02<08:43,  3.56s/it]
2022-04-08 18:17:38,447 - INFO - tqdm - accuracy: 0.9854, batch_loss: 0.1172, loss: 0.0460 ||:  76%|#######5  | 447/590 [25:16<08:44,  3.67s/it]
2022-04-08 18:17:51,346 - INFO - tqdm - accuracy: 0.9855, batch_loss: 0.0268, loss: 0.0457 ||:  76%|#######6  | 451/590 [25:29<07:48,  3.37s/it]
2022-04-08 18:18:04,660 - INFO - tqdm - accuracy: 0.9856, batch_loss: 0.1210, loss: 0.0458 ||:  77%|#######7  | 455/590 [25:42<07:24,  3.29s/it]
2022-04-08 18:18:17,723 - INFO - tqdm - accuracy: 0.9856, batch_loss: 0.0470, loss: 0.0456 ||:  78%|#######7  | 459/590 [25:55<07:11,  3.30s/it]
2022-04-08 18:18:31,361 - INFO - tqdm - accuracy: 0.9854, batch_loss: 0.0024, loss: 0.0457 ||:  78%|#######8  | 463/590 [26:09<07:21,  3.47s/it]
2022-04-08 18:18:41,372 - INFO - tqdm - accuracy: 0.9854, batch_loss: 0.0324, loss: 0.0456 ||:  79%|#######8  | 466/590 [26:19<07:01,  3.40s/it]
2022-04-08 18:18:52,640 - INFO - tqdm - accuracy: 0.9855, batch_loss: 0.0122, loss: 0.0454 ||:  79%|#######9  | 469/590 [26:30<07:17,  3.62s/it]
2022-04-08 18:19:03,261 - INFO - tqdm - accuracy: 0.9854, batch_loss: 0.0571, loss: 0.0457 ||:  80%|########  | 472/590 [26:41<07:03,  3.59s/it]
2022-04-08 18:19:14,194 - INFO - tqdm - accuracy: 0.9853, batch_loss: 0.0226, loss: 0.0461 ||:  81%|########  | 475/590 [26:51<06:58,  3.64s/it]
2022-04-08 18:19:27,144 - INFO - tqdm - accuracy: 0.9853, batch_loss: 0.1388, loss: 0.0461 ||:  81%|########1 | 479/590 [27:04<06:07,  3.31s/it]
2022-04-08 18:19:37,715 - INFO - tqdm - accuracy: 0.9853, batch_loss: 0.0591, loss: 0.0460 ||:  82%|########1 | 482/590 [27:15<06:13,  3.46s/it]
2022-04-08 18:19:50,336 - INFO - tqdm - accuracy: 0.9853, batch_loss: 0.1341, loss: 0.0464 ||:  82%|########2 | 486/590 [27:28<05:26,  3.14s/it]
2022-04-08 18:20:02,534 - INFO - tqdm - accuracy: 0.9853, batch_loss: 0.0081, loss: 0.0464 ||:  83%|########3 | 490/590 [27:40<05:05,  3.05s/it]
2022-04-08 18:20:12,542 - INFO - tqdm - accuracy: 0.9854, batch_loss: 0.0484, loss: 0.0463 ||:  84%|########3 | 493/590 [27:50<05:10,  3.21s/it]
2022-04-08 18:20:23,226 - INFO - tqdm - accuracy: 0.9853, batch_loss: 0.0901, loss: 0.0464 ||:  84%|########4 | 496/590 [28:01<05:19,  3.40s/it]
2022-04-08 18:20:34,191 - INFO - tqdm - accuracy: 0.9853, batch_loss: 0.0370, loss: 0.0462 ||:  85%|########4 | 499/590 [28:11<05:23,  3.56s/it]
2022-04-08 18:20:44,474 - INFO - tqdm - accuracy: 0.9853, batch_loss: 0.0162, loss: 0.0465 ||:  85%|########5 | 502/590 [28:22<05:02,  3.44s/it]
2022-04-08 18:20:57,272 - INFO - tqdm - accuracy: 0.9853, batch_loss: 0.0648, loss: 0.0464 ||:  86%|########5 | 506/590 [28:35<04:38,  3.31s/it]
2022-04-08 18:21:09,874 - INFO - tqdm - accuracy: 0.9852, batch_loss: 0.0059, loss: 0.0465 ||:  86%|########6 | 510/590 [28:47<04:18,  3.23s/it]
2022-04-08 18:21:20,348 - INFO - tqdm - accuracy: 0.9852, batch_loss: 0.0123, loss: 0.0465 ||:  87%|########6 | 513/590 [28:58<04:21,  3.39s/it]
2022-04-08 18:21:33,194 - INFO - tqdm - accuracy: 0.9851, batch_loss: 0.1359, loss: 0.0466 ||:  88%|########7 | 517/590 [29:10<03:58,  3.27s/it]
2022-04-08 18:21:43,964 - INFO - tqdm - accuracy: 0.9852, batch_loss: 0.0052, loss: 0.0464 ||:  88%|########8 | 520/590 [29:21<04:05,  3.50s/it]
2022-04-08 18:21:57,127 - INFO - tqdm - accuracy: 0.9853, batch_loss: 0.0075, loss: 0.0466 ||:  89%|########8 | 524/590 [29:34<03:40,  3.34s/it]
2022-04-08 18:22:09,684 - INFO - tqdm - accuracy: 0.9852, batch_loss: 0.0033, loss: 0.0465 ||:  89%|########9 | 528/590 [29:47<03:20,  3.24s/it]
2022-04-08 18:22:20,023 - INFO - tqdm - accuracy: 0.9853, batch_loss: 0.0040, loss: 0.0463 ||:  90%|######### | 531/590 [29:57<03:18,  3.37s/it]
2022-04-08 18:22:33,483 - INFO - tqdm - accuracy: 0.9853, batch_loss: 0.0769, loss: 0.0463 ||:  91%|######### | 535/590 [30:11<03:09,  3.45s/it]
2022-04-08 18:22:43,579 - INFO - tqdm - accuracy: 0.9852, batch_loss: 0.0639, loss: 0.0465 ||:  91%|#########1| 538/590 [30:21<03:00,  3.47s/it]
2022-04-08 18:22:54,742 - INFO - tqdm - accuracy: 0.9853, batch_loss: 0.0265, loss: 0.0463 ||:  92%|#########1| 541/590 [30:32<02:54,  3.56s/it]
2022-04-08 18:23:07,826 - INFO - tqdm - accuracy: 0.9854, batch_loss: 0.0078, loss: 0.0460 ||:  92%|#########2| 545/590 [30:45<02:37,  3.49s/it]
2022-04-08 18:23:20,467 - INFO - tqdm - accuracy: 0.9855, batch_loss: 0.0411, loss: 0.0458 ||:  93%|#########3| 549/590 [30:58<02:12,  3.23s/it]
2022-04-08 18:23:32,869 - INFO - tqdm - accuracy: 0.9855, batch_loss: 0.0102, loss: 0.0458 ||:  94%|#########3| 553/590 [31:10<01:56,  3.14s/it]
2022-04-08 18:23:42,982 - INFO - tqdm - accuracy: 0.9855, batch_loss: 0.0023, loss: 0.0456 ||:  94%|#########4| 556/590 [31:20<01:53,  3.33s/it]
2022-04-08 18:23:56,460 - INFO - tqdm - accuracy: 0.9856, batch_loss: 0.0141, loss: 0.0455 ||:  95%|#########4| 560/590 [31:34<01:41,  3.39s/it]
2022-04-08 18:24:06,496 - INFO - tqdm - accuracy: 0.9856, batch_loss: 0.0113, loss: 0.0457 ||:  95%|#########5| 563/590 [31:44<01:29,  3.30s/it]
2022-04-08 18:24:16,779 - INFO - tqdm - accuracy: 0.9856, batch_loss: 0.0018, loss: 0.0456 ||:  96%|#########5| 566/590 [31:54<01:21,  3.38s/it]
2022-04-08 18:24:27,134 - INFO - tqdm - accuracy: 0.9857, batch_loss: 0.0150, loss: 0.0454 ||:  96%|#########6| 569/590 [32:04<01:11,  3.40s/it]
2022-04-08 18:24:40,240 - INFO - tqdm - accuracy: 0.9858, batch_loss: 0.0114, loss: 0.0451 ||:  97%|#########7| 573/590 [32:18<00:56,  3.35s/it]
2022-04-08 18:24:50,358 - INFO - tqdm - accuracy: 0.9856, batch_loss: 0.2286, loss: 0.0458 ||:  98%|#########7| 576/590 [32:28<00:47,  3.36s/it]
2022-04-08 18:25:02,289 - INFO - tqdm - accuracy: 0.9856, batch_loss: 0.0043, loss: 0.0458 ||:  98%|#########8| 580/590 [32:40<00:31,  3.13s/it]
2022-04-08 18:25:12,413 - INFO - tqdm - accuracy: 0.9856, batch_loss: 0.1794, loss: 0.0459 ||:  99%|#########8| 583/590 [32:50<00:22,  3.28s/it]
2022-04-08 18:25:25,568 - INFO - tqdm - accuracy: 0.9856, batch_loss: 0.0495, loss: 0.0460 ||:  99%|#########9| 587/590 [33:03<00:09,  3.28s/it]
2022-04-08 18:25:28,570 - INFO - tqdm - accuracy: 0.9856, batch_loss: 0.0638, loss: 0.0461 ||: 100%|#########9| 588/590 [33:06<00:06,  3.20s/it]
2022-04-08 18:25:32,121 - INFO - tqdm - accuracy: 0.9856, batch_loss: 0.0056, loss: 0.0460 ||: 100%|#########9| 589/590 [33:09<00:03,  3.30s/it]
2022-04-08 18:25:32,758 - INFO - tqdm - accuracy: 0.9856, batch_loss: 0.0221, loss: 0.0459 ||: 100%|##########| 590/590 [33:10<00:00,  2.50s/it]
2022-04-08 18:25:32,758 - INFO - tqdm - accuracy: 0.9856, batch_loss: 0.0221, loss: 0.0459 ||: 100%|##########| 590/590 [33:10<00:00,  3.37s/it]
2022-04-08 18:25:35,097 - INFO - allennlp.training.trainer - Validating
2022-04-08 18:25:35,099 - INFO - tqdm - 0%|          | 0/1635 [00:00<?, ?it/s]
2022-04-08 18:25:45,141 - INFO - tqdm - accuracy: 0.8322, batch_loss: 0.0005, loss: 0.8119 ||:   9%|8         | 146/1635 [00:10<01:38, 15.10it/s]
2022-04-08 18:25:55,161 - INFO - tqdm - accuracy: 0.8439, batch_loss: 0.3160, loss: 0.7721 ||:  17%|#7        | 285/1635 [00:20<01:49, 12.36it/s]
2022-04-08 18:26:05,193 - INFO - tqdm - accuracy: 0.8527, batch_loss: 0.0437, loss: 0.7047 ||:  25%|##5       | 414/1635 [00:30<01:48, 11.29it/s]
2022-04-08 18:26:15,354 - INFO - tqdm - accuracy: 0.8560, batch_loss: 0.0006, loss: 0.7049 ||:  34%|###4      | 559/1635 [00:40<01:24, 12.72it/s]
2022-04-08 18:26:25,500 - INFO - tqdm - accuracy: 0.8582, batch_loss: 0.0004, loss: 0.7087 ||:  43%|####2     | 698/1635 [00:50<01:14, 12.56it/s]
2022-04-08 18:26:35,627 - INFO - tqdm - accuracy: 0.8538, batch_loss: 0.0006, loss: 0.7164 ||:  52%|#####1    | 848/1635 [01:00<01:08, 11.51it/s]
2022-04-08 18:26:45,638 - INFO - tqdm - accuracy: 0.8584, batch_loss: 3.6242, loss: 0.7001 ||:  61%|######    | 996/1635 [01:10<00:42, 15.02it/s]
2022-04-08 18:26:55,841 - INFO - tqdm - accuracy: 0.8599, batch_loss: 0.0079, loss: 0.7079 ||:  70%|#######   | 1146/1635 [01:20<00:33, 14.70it/s]
2022-04-08 18:27:05,863 - INFO - tqdm - accuracy: 0.8599, batch_loss: 0.0003, loss: 0.7057 ||:  78%|#######8  | 1281/1635 [01:30<00:26, 13.56it/s]
2022-04-08 18:27:16,043 - INFO - tqdm - accuracy: 0.8565, batch_loss: 3.4209, loss: 0.7231 ||:  87%|########6 | 1418/1635 [01:40<00:16, 12.87it/s]
2022-04-08 18:27:26,203 - INFO - tqdm - accuracy: 0.8541, batch_loss: 3.8161, loss: 0.7345 ||:  95%|#########5| 1559/1635 [01:51<00:06, 12.04it/s]
2022-04-08 18:27:30,838 - INFO - tqdm - accuracy: 0.8538, batch_loss: 1.8508, loss: 0.7357 ||: 100%|#########9| 1628/1635 [01:55<00:00, 15.16it/s]
2022-04-08 18:27:30,986 - INFO - tqdm - accuracy: 0.8534, batch_loss: 0.7589, loss: 0.7372 ||: 100%|#########9| 1630/1635 [01:55<00:00, 14.67it/s]
2022-04-08 18:27:31,092 - INFO - tqdm - accuracy: 0.8529, batch_loss: 3.7351, loss: 0.7391 ||: 100%|#########9| 1632/1635 [01:55<00:00, 15.63it/s]
2022-04-08 18:27:31,209 - INFO - tqdm - accuracy: 0.8531, batch_loss: 0.0977, loss: 0.7383 ||: 100%|#########9| 1634/1635 [01:56<00:00, 16.02it/s]
2022-04-08 18:27:31,261 - INFO - tqdm - accuracy: 0.8532, batch_loss: 0.0124, loss: 0.7378 ||: 100%|##########| 1635/1635 [01:56<00:00, 14.08it/s]
2022-04-08 18:27:31,261 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-08 18:27:31,262 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.986  |     0.853
2022-04-08 18:27:31,263 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9604.118  |       N/A
2022-04-08 18:27:31,264 - INFO - allennlp.training.tensorboard_writer - loss               |     0.046  |     0.738
2022-04-08 18:27:31,264 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  7239.301  |       N/A
2022-04-08 18:27:35,806 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to './output_newboolq/best.th'.
2022-04-08 18:27:46,741 - INFO - allennlp.training.trainer - Epoch duration: 0:35:24.533847
2022-04-08 18:27:46,741 - INFO - allennlp.training.trainer - Estimated training time remaining: 2:56:27
2022-04-08 18:27:46,741 - INFO - allennlp.training.trainer - Epoch 5/9
2022-04-08 18:27:46,742 - INFO - allennlp.training.trainer - Worker 0 memory usage: 7.1G
2022-04-08 18:27:46,742 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-08 18:27:46,743 - INFO - allennlp.training.trainer - Training
2022-04-08 18:27:46,744 - INFO - tqdm - 0%|          | 0/590 [00:00<?, ?it/s]
2022-04-08 18:27:56,798 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.0018, loss: 0.0243 ||:   1%|          | 3/590 [00:10<32:03,  3.28s/it]
2022-04-08 18:28:10,377 - INFO - tqdm - accuracy: 0.9866, batch_loss: 0.0672, loss: 0.0289 ||:   1%|1         | 7/590 [00:23<33:08,  3.41s/it]
2022-04-08 18:28:21,062 - INFO - tqdm - accuracy: 0.9875, batch_loss: 0.0020, loss: 0.0287 ||:   2%|1         | 10/590 [00:34<34:10,  3.53s/it]
2022-04-08 18:28:34,171 - INFO - tqdm - accuracy: 0.9911, batch_loss: 0.0168, loss: 0.0244 ||:   2%|2         | 14/590 [00:47<32:53,  3.43s/it]
2022-04-08 18:28:44,286 - INFO - tqdm - accuracy: 0.9926, batch_loss: 0.0023, loss: 0.0210 ||:   3%|2         | 17/590 [00:57<33:11,  3.48s/it]
2022-04-08 18:28:54,863 - INFO - tqdm - accuracy: 0.9938, batch_loss: 0.0186, loss: 0.0196 ||:   3%|3         | 20/590 [01:08<33:35,  3.54s/it]
2022-04-08 18:29:05,304 - INFO - tqdm - accuracy: 0.9946, batch_loss: 0.0055, loss: 0.0184 ||:   4%|3         | 23/590 [01:18<33:30,  3.55s/it]
2022-04-08 18:29:16,051 - INFO - tqdm - accuracy: 0.9916, batch_loss: 0.0565, loss: 0.0269 ||:   4%|4         | 26/590 [01:29<34:04,  3.63s/it]
2022-04-08 18:29:26,307 - INFO - tqdm - accuracy: 0.9914, batch_loss: 0.0027, loss: 0.0275 ||:   5%|4         | 29/590 [01:39<32:37,  3.49s/it]
2022-04-08 18:29:39,635 - INFO - tqdm - accuracy: 0.9924, batch_loss: 0.0024, loss: 0.0251 ||:   6%|5         | 33/590 [01:52<31:47,  3.42s/it]
2022-04-08 18:29:51,392 - INFO - tqdm - accuracy: 0.9924, batch_loss: 0.0057, loss: 0.0243 ||:   6%|6         | 37/590 [02:04<27:51,  3.02s/it]
2022-04-08 18:30:01,611 - INFO - tqdm - accuracy: 0.9930, batch_loss: 0.0111, loss: 0.0231 ||:   7%|6         | 40/590 [02:14<29:35,  3.23s/it]
2022-04-08 18:30:12,322 - INFO - tqdm - accuracy: 0.9927, batch_loss: 0.0427, loss: 0.0230 ||:   7%|7         | 43/590 [02:25<31:26,  3.45s/it]
2022-04-08 18:30:23,442 - INFO - tqdm - accuracy: 0.9925, batch_loss: 0.0178, loss: 0.0228 ||:   8%|7         | 46/590 [02:36<33:15,  3.67s/it]
2022-04-08 18:30:34,399 - INFO - tqdm - accuracy: 0.9930, batch_loss: 0.0027, loss: 0.0217 ||:   8%|8         | 49/590 [02:47<33:13,  3.68s/it]
2022-04-08 18:30:46,802 - INFO - tqdm - accuracy: 0.9929, batch_loss: 0.0019, loss: 0.0219 ||:   9%|8         | 53/590 [03:00<28:46,  3.22s/it]
2022-04-08 18:30:56,880 - INFO - tqdm - accuracy: 0.9911, batch_loss: 0.1724, loss: 0.0253 ||:   9%|9         | 56/590 [03:10<29:23,  3.30s/it]
2022-04-08 18:31:07,126 - INFO - tqdm - accuracy: 0.9915, batch_loss: 0.0016, loss: 0.0244 ||:  10%|#         | 59/590 [03:20<28:53,  3.26s/it]
2022-04-08 18:31:20,813 - INFO - tqdm - accuracy: 0.9916, batch_loss: 0.0130, loss: 0.0241 ||:  11%|#         | 63/590 [03:34<29:57,  3.41s/it]
2022-04-08 18:31:31,367 - INFO - tqdm - accuracy: 0.9915, batch_loss: 0.0321, loss: 0.0237 ||:  11%|#1        | 66/590 [03:44<30:09,  3.45s/it]
2022-04-08 18:31:44,550 - INFO - tqdm - accuracy: 0.9906, batch_loss: 0.0018, loss: 0.0264 ||:  12%|#1        | 70/590 [03:57<28:57,  3.34s/it]
2022-04-08 18:31:54,663 - INFO - tqdm - accuracy: 0.9906, batch_loss: 0.0047, loss: 0.0261 ||:  12%|#2        | 73/590 [04:07<29:18,  3.40s/it]
2022-04-08 18:32:04,927 - INFO - tqdm - accuracy: 0.9897, batch_loss: 0.0119, loss: 0.0291 ||:  13%|#2        | 76/590 [04:18<29:59,  3.50s/it]
2022-04-08 18:32:15,465 - INFO - tqdm - accuracy: 0.9893, batch_loss: 0.0026, loss: 0.0302 ||:  13%|#3        | 79/590 [04:28<30:18,  3.56s/it]
2022-04-08 18:32:28,301 - INFO - tqdm - accuracy: 0.9895, batch_loss: 0.0027, loss: 0.0296 ||:  14%|#4        | 83/590 [04:41<27:55,  3.30s/it]
2022-04-08 18:32:38,410 - INFO - tqdm - accuracy: 0.9895, batch_loss: 0.0089, loss: 0.0294 ||:  15%|#4        | 86/590 [04:51<28:04,  3.34s/it]
2022-04-08 18:32:48,634 - INFO - tqdm - accuracy: 0.9898, batch_loss: 0.0042, loss: 0.0288 ||:  15%|#5        | 89/590 [05:01<28:11,  3.38s/it]
2022-04-08 18:32:59,230 - INFO - tqdm - accuracy: 0.9898, batch_loss: 0.0238, loss: 0.0287 ||:  16%|#5        | 92/590 [05:12<28:30,  3.43s/it]
2022-04-08 18:33:12,036 - INFO - tqdm - accuracy: 0.9902, batch_loss: 0.0067, loss: 0.0278 ||:  16%|#6        | 96/590 [05:25<26:47,  3.25s/it]
2022-04-08 18:33:24,514 - INFO - tqdm - accuracy: 0.9902, batch_loss: 0.0527, loss: 0.0278 ||:  17%|#6        | 99/590 [05:37<31:27,  3.84s/it]
2022-04-08 18:33:37,464 - INFO - tqdm - accuracy: 0.9906, batch_loss: 0.0026, loss: 0.0271 ||:  17%|#7        | 103/590 [05:50<27:25,  3.38s/it]
2022-04-08 18:33:49,048 - INFO - tqdm - accuracy: 0.9906, batch_loss: 0.0157, loss: 0.0271 ||:  18%|#7        | 106/590 [06:02<29:45,  3.69s/it]
2022-04-08 18:34:01,705 - INFO - tqdm - accuracy: 0.9906, batch_loss: 0.0014, loss: 0.0266 ||:  19%|#8        | 110/590 [06:14<26:16,  3.29s/it]
2022-04-08 18:34:14,144 - INFO - tqdm - accuracy: 0.9910, batch_loss: 0.0280, loss: 0.0261 ||:  19%|#9        | 114/590 [06:27<25:12,  3.18s/it]
2022-04-08 18:34:24,413 - INFO - tqdm - accuracy: 0.9907, batch_loss: 0.0362, loss: 0.0260 ||:  20%|#9        | 117/590 [06:37<26:36,  3.37s/it]
2022-04-08 18:34:37,756 - INFO - tqdm - accuracy: 0.9910, batch_loss: 0.0028, loss: 0.0255 ||:  21%|##        | 121/590 [06:51<26:30,  3.39s/it]
2022-04-08 18:34:49,938 - INFO - tqdm - accuracy: 0.9910, batch_loss: 0.0023, loss: 0.0253 ||:  21%|##1       | 125/590 [07:03<25:29,  3.29s/it]
2022-04-08 18:35:00,347 - INFO - tqdm - accuracy: 0.9910, batch_loss: 0.0029, loss: 0.0255 ||:  22%|##1       | 128/590 [07:13<26:55,  3.50s/it]
2022-04-08 18:35:12,569 - INFO - tqdm - accuracy: 0.9912, batch_loss: 0.0031, loss: 0.0250 ||:  22%|##2       | 131/590 [07:25<30:18,  3.96s/it]
2022-04-08 18:35:22,836 - INFO - tqdm - accuracy: 0.9907, batch_loss: 0.0052, loss: 0.0271 ||:  23%|##2       | 134/590 [07:36<27:19,  3.60s/it]
2022-04-08 18:35:33,109 - INFO - tqdm - accuracy: 0.9909, batch_loss: 0.0023, loss: 0.0266 ||:  23%|##3       | 137/590 [07:46<26:10,  3.47s/it]
2022-04-08 18:35:45,904 - INFO - tqdm - accuracy: 0.9907, batch_loss: 0.0052, loss: 0.0269 ||:  24%|##3       | 141/590 [07:59<24:20,  3.25s/it]
2022-04-08 18:35:56,134 - INFO - tqdm - accuracy: 0.9909, batch_loss: 0.0099, loss: 0.0265 ||:  24%|##4       | 144/590 [08:09<24:50,  3.34s/it]
2022-04-08 18:36:08,677 - INFO - tqdm - accuracy: 0.9903, batch_loss: 0.3297, loss: 0.0285 ||:  25%|##5       | 148/590 [08:21<24:00,  3.26s/it]
2022-04-08 18:36:20,244 - INFO - tqdm - accuracy: 0.9903, batch_loss: 0.0029, loss: 0.0282 ||:  26%|##5       | 151/590 [08:33<26:07,  3.57s/it]
2022-04-08 18:36:30,295 - INFO - tqdm - accuracy: 0.9901, batch_loss: 0.0394, loss: 0.0280 ||:  26%|##6       | 154/590 [08:43<24:59,  3.44s/it]
2022-04-08 18:36:40,383 - INFO - tqdm - accuracy: 0.9900, batch_loss: 0.0097, loss: 0.0278 ||:  27%|##6       | 157/590 [08:53<24:18,  3.37s/it]
2022-04-08 18:36:50,695 - INFO - tqdm - accuracy: 0.9902, batch_loss: 0.0022, loss: 0.0275 ||:  27%|##7       | 160/590 [09:03<24:54,  3.48s/it]
2022-04-08 18:37:00,952 - INFO - tqdm - accuracy: 0.9902, batch_loss: 0.0016, loss: 0.0281 ||:  28%|##7       | 163/590 [09:14<24:40,  3.47s/it]
2022-04-08 18:37:11,303 - INFO - tqdm - accuracy: 0.9902, batch_loss: 0.0067, loss: 0.0280 ||:  28%|##8       | 166/590 [09:24<23:54,  3.38s/it]
2022-04-08 18:37:21,453 - INFO - tqdm - accuracy: 0.9902, batch_loss: 0.0802, loss: 0.0282 ||:  29%|##8       | 169/590 [09:34<23:06,  3.29s/it]
2022-04-08 18:37:31,766 - INFO - tqdm - accuracy: 0.9900, batch_loss: 0.2329, loss: 0.0295 ||:  29%|##9       | 172/590 [09:45<23:08,  3.32s/it]
2022-04-08 18:37:42,745 - INFO - tqdm - accuracy: 0.9900, batch_loss: 0.0683, loss: 0.0294 ||:  30%|##9       | 175/590 [09:56<24:30,  3.54s/it]
2022-04-08 18:37:56,567 - INFO - tqdm - accuracy: 0.9900, batch_loss: 0.0078, loss: 0.0291 ||:  30%|###       | 179/590 [10:09<24:41,  3.60s/it]
2022-04-08 18:38:09,532 - INFO - tqdm - accuracy: 0.9901, batch_loss: 0.2038, loss: 0.0298 ||:  31%|###1      | 183/590 [10:22<22:58,  3.39s/it]
2022-04-08 18:38:20,525 - INFO - tqdm - accuracy: 0.9903, batch_loss: 0.0042, loss: 0.0294 ||:  32%|###1      | 186/590 [10:33<24:03,  3.57s/it]
2022-04-08 18:38:30,561 - INFO - tqdm - accuracy: 0.9904, batch_loss: 0.0027, loss: 0.0292 ||:  32%|###2      | 189/590 [10:43<22:46,  3.41s/it]
2022-04-08 18:38:41,072 - INFO - tqdm - accuracy: 0.9904, batch_loss: 0.2221, loss: 0.0301 ||:  33%|###2      | 192/590 [10:54<23:09,  3.49s/it]
2022-04-08 18:38:51,285 - INFO - tqdm - accuracy: 0.9905, batch_loss: 0.0042, loss: 0.0298 ||:  33%|###3      | 195/590 [11:04<22:40,  3.44s/it]
2022-04-08 18:39:01,553 - INFO - tqdm - accuracy: 0.9907, batch_loss: 0.0135, loss: 0.0294 ||:  34%|###3      | 198/590 [11:14<22:31,  3.45s/it]
2022-04-08 18:39:12,464 - INFO - tqdm - accuracy: 0.9907, batch_loss: 0.0396, loss: 0.0294 ||:  34%|###4      | 201/590 [11:25<22:59,  3.55s/it]
2022-04-08 18:39:22,753 - INFO - tqdm - accuracy: 0.9907, batch_loss: 0.0111, loss: 0.0298 ||:  35%|###4      | 204/590 [11:36<22:08,  3.44s/it]
2022-04-08 18:39:34,643 - INFO - tqdm - accuracy: 0.9905, batch_loss: 0.0028, loss: 0.0296 ||:  35%|###5      | 208/590 [11:47<19:45,  3.10s/it]
2022-04-08 18:39:44,727 - INFO - tqdm - accuracy: 0.9905, batch_loss: 0.0137, loss: 0.0295 ||:  36%|###5      | 211/590 [11:57<20:51,  3.30s/it]
2022-04-08 18:39:56,984 - INFO - tqdm - accuracy: 0.9900, batch_loss: 0.0512, loss: 0.0308 ||:  36%|###6      | 215/590 [12:10<19:33,  3.13s/it]
2022-04-08 18:40:07,655 - INFO - tqdm - accuracy: 0.9898, batch_loss: 0.0082, loss: 0.0316 ||:  37%|###6      | 218/590 [12:20<21:10,  3.42s/it]
2022-04-08 18:40:20,856 - INFO - tqdm - accuracy: 0.9899, batch_loss: 0.0022, loss: 0.0323 ||:  38%|###7      | 222/590 [12:34<20:50,  3.40s/it]
2022-04-08 18:40:33,523 - INFO - tqdm - accuracy: 0.9898, batch_loss: 0.0029, loss: 0.0325 ||:  38%|###8      | 226/590 [12:46<19:30,  3.22s/it]
2022-04-08 18:40:46,734 - INFO - tqdm - accuracy: 0.9895, batch_loss: 0.1262, loss: 0.0332 ||:  39%|###8      | 230/590 [12:59<19:46,  3.29s/it]
2022-04-08 18:40:57,333 - INFO - tqdm - accuracy: 0.9893, batch_loss: 0.0365, loss: 0.0336 ||:  39%|###9      | 233/590 [13:10<20:48,  3.50s/it]
2022-04-08 18:41:08,075 - INFO - tqdm - accuracy: 0.9894, batch_loss: 0.0124, loss: 0.0332 ||:  40%|####      | 236/590 [13:21<21:01,  3.56s/it]
2022-04-08 18:41:18,785 - INFO - tqdm - accuracy: 0.9894, batch_loss: 0.0742, loss: 0.0332 ||:  41%|####      | 239/590 [13:32<20:54,  3.57s/it]
2022-04-08 18:41:29,793 - INFO - tqdm - accuracy: 0.9894, batch_loss: 0.0201, loss: 0.0332 ||:  41%|####1     | 242/590 [13:43<20:48,  3.59s/it]
2022-04-08 18:41:42,662 - INFO - tqdm - accuracy: 0.9895, batch_loss: 0.0227, loss: 0.0331 ||:  42%|####1     | 246/590 [13:55<18:48,  3.28s/it]
2022-04-08 18:41:53,334 - INFO - tqdm - accuracy: 0.9895, batch_loss: 0.0166, loss: 0.0330 ||:  42%|####2     | 249/590 [14:06<19:49,  3.49s/it]
2022-04-08 18:42:03,951 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.0032, loss: 0.0326 ||:  43%|####2     | 252/590 [14:17<19:58,  3.55s/it]
2022-04-08 18:42:14,495 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.0043, loss: 0.0324 ||:  43%|####3     | 255/590 [14:27<19:44,  3.54s/it]
2022-04-08 18:42:27,730 - INFO - tqdm - accuracy: 0.9897, batch_loss: 0.0316, loss: 0.0321 ||:  44%|####3     | 259/590 [14:40<19:01,  3.45s/it]
2022-04-08 18:42:38,526 - INFO - tqdm - accuracy: 0.9897, batch_loss: 0.0065, loss: 0.0321 ||:  44%|####4     | 262/590 [14:51<19:23,  3.55s/it]
2022-04-08 18:42:48,887 - INFO - tqdm - accuracy: 0.9899, batch_loss: 0.0104, loss: 0.0318 ||:  45%|####4     | 265/590 [15:02<18:49,  3.48s/it]
2022-04-08 18:43:02,108 - INFO - tqdm - accuracy: 0.9899, batch_loss: 0.0114, loss: 0.0315 ||:  46%|####5     | 269/590 [15:15<17:46,  3.32s/it]
2022-04-08 18:43:12,439 - INFO - tqdm - accuracy: 0.9900, batch_loss: 0.0012, loss: 0.0312 ||:  46%|####6     | 272/590 [15:25<17:50,  3.37s/it]
2022-04-08 18:43:22,564 - INFO - tqdm - accuracy: 0.9899, batch_loss: 0.1393, loss: 0.0320 ||:  47%|####6     | 275/590 [15:35<17:53,  3.41s/it]
2022-04-08 18:43:32,733 - INFO - tqdm - accuracy: 0.9899, batch_loss: 0.1178, loss: 0.0322 ||:  47%|####7     | 278/590 [15:45<17:25,  3.35s/it]
2022-04-08 18:43:45,313 - INFO - tqdm - accuracy: 0.9898, batch_loss: 0.1255, loss: 0.0327 ||:  48%|####7     | 282/590 [15:58<16:42,  3.25s/it]
2022-04-08 18:43:58,248 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.0027, loss: 0.0327 ||:  48%|####8     | 286/590 [16:11<16:47,  3.32s/it]
2022-04-08 18:44:08,729 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.0683, loss: 0.0327 ||:  49%|####8     | 289/590 [16:21<17:23,  3.47s/it]
2022-04-08 18:44:21,785 - INFO - tqdm - accuracy: 0.9894, batch_loss: 0.2317, loss: 0.0333 ||:  50%|####9     | 293/590 [16:35<16:34,  3.35s/it]
2022-04-08 18:44:31,894 - INFO - tqdm - accuracy: 0.9894, batch_loss: 0.0152, loss: 0.0334 ||:  50%|#####     | 296/590 [16:45<16:14,  3.32s/it]
2022-04-08 18:44:42,472 - INFO - tqdm - accuracy: 0.9895, batch_loss: 0.0026, loss: 0.0331 ||:  51%|#####     | 299/590 [16:55<16:59,  3.50s/it]
2022-04-08 18:44:52,671 - INFO - tqdm - accuracy: 0.9894, batch_loss: 0.0504, loss: 0.0332 ||:  51%|#####1    | 302/590 [17:05<16:45,  3.49s/it]
2022-04-08 18:45:03,971 - INFO - tqdm - accuracy: 0.9894, batch_loss: 0.0563, loss: 0.0331 ||:  52%|#####1    | 305/590 [17:17<17:08,  3.61s/it]
2022-04-08 18:45:14,405 - INFO - tqdm - accuracy: 0.9894, batch_loss: 0.0485, loss: 0.0330 ||:  52%|#####2    | 308/590 [17:27<16:38,  3.54s/it]
2022-04-08 18:45:25,319 - INFO - tqdm - accuracy: 0.9892, batch_loss: 0.0498, loss: 0.0334 ||:  53%|#####2    | 311/590 [17:38<16:30,  3.55s/it]
2022-04-08 18:45:37,886 - INFO - tqdm - accuracy: 0.9894, batch_loss: 0.0082, loss: 0.0332 ||:  53%|#####3    | 315/590 [17:51<14:56,  3.26s/it]
2022-04-08 18:45:49,870 - INFO - tqdm - accuracy: 0.9894, batch_loss: 0.0619, loss: 0.0330 ||:  54%|#####4    | 319/590 [18:03<13:35,  3.01s/it]
2022-04-08 18:46:01,431 - INFO - tqdm - accuracy: 0.9895, batch_loss: 0.1703, loss: 0.0332 ||:  55%|#####4    | 323/590 [18:14<12:53,  2.90s/it]
2022-04-08 18:46:11,618 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.0176, loss: 0.0331 ||:  55%|#####5    | 326/590 [18:24<14:16,  3.24s/it]
2022-04-08 18:46:22,624 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.0788, loss: 0.0331 ||:  56%|#####5    | 329/590 [18:35<15:15,  3.51s/it]
2022-04-08 18:46:33,251 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.0036, loss: 0.0328 ||:  56%|#####6    | 332/590 [18:46<15:27,  3.60s/it]
2022-04-08 18:46:46,375 - INFO - tqdm - accuracy: 0.9897, batch_loss: 0.0043, loss: 0.0327 ||:  57%|#####6    | 336/590 [18:59<14:30,  3.43s/it]
2022-04-08 18:46:59,146 - INFO - tqdm - accuracy: 0.9897, batch_loss: 0.0602, loss: 0.0325 ||:  58%|#####7    | 340/590 [19:12<13:36,  3.27s/it]
2022-04-08 18:47:09,660 - INFO - tqdm - accuracy: 0.9897, batch_loss: 0.1306, loss: 0.0327 ||:  58%|#####8    | 343/590 [19:22<14:02,  3.41s/it]
2022-04-08 18:47:20,407 - INFO - tqdm - accuracy: 0.9897, batch_loss: 0.0039, loss: 0.0326 ||:  59%|#####8    | 346/590 [19:33<14:29,  3.56s/it]
2022-04-08 18:47:30,569 - INFO - tqdm - accuracy: 0.9897, batch_loss: 0.0014, loss: 0.0325 ||:  59%|#####9    | 349/590 [19:43<13:54,  3.46s/it]
2022-04-08 18:47:41,313 - INFO - tqdm - accuracy: 0.9897, batch_loss: 0.0526, loss: 0.0324 ||:  60%|#####9    | 352/590 [19:54<13:43,  3.46s/it]
2022-04-08 18:47:51,703 - INFO - tqdm - accuracy: 0.9897, batch_loss: 0.0118, loss: 0.0324 ||:  60%|######    | 355/590 [20:04<13:41,  3.50s/it]
2022-04-08 18:48:04,485 - INFO - tqdm - accuracy: 0.9893, batch_loss: 0.0114, loss: 0.0334 ||:  61%|######    | 359/590 [20:17<13:09,  3.42s/it]
2022-04-08 18:48:15,617 - INFO - tqdm - accuracy: 0.9894, batch_loss: 0.0131, loss: 0.0333 ||:  61%|######1   | 362/590 [20:28<13:36,  3.58s/it]
2022-04-08 18:48:28,451 - INFO - tqdm - accuracy: 0.9893, batch_loss: 0.0129, loss: 0.0336 ||:  62%|######2   | 366/590 [20:41<12:15,  3.28s/it]
2022-04-08 18:48:39,006 - INFO - tqdm - accuracy: 0.9893, batch_loss: 0.0086, loss: 0.0335 ||:  63%|######2   | 369/590 [20:52<12:44,  3.46s/it]
2022-04-08 18:48:51,791 - INFO - tqdm - accuracy: 0.9894, batch_loss: 0.0051, loss: 0.0336 ||:  63%|######3   | 373/590 [21:05<11:46,  3.25s/it]
2022-04-08 18:49:01,975 - INFO - tqdm - accuracy: 0.9894, batch_loss: 0.0027, loss: 0.0339 ||:  64%|######3   | 376/590 [21:15<11:58,  3.36s/it]
2022-04-08 18:49:12,790 - INFO - tqdm - accuracy: 0.9894, batch_loss: 0.0017, loss: 0.0337 ||:  64%|######4   | 379/590 [21:26<12:16,  3.49s/it]
2022-04-08 18:49:26,079 - INFO - tqdm - accuracy: 0.9892, batch_loss: 0.1344, loss: 0.0341 ||:  65%|######4   | 383/590 [21:39<11:50,  3.43s/it]
2022-04-08 18:49:37,038 - INFO - tqdm - accuracy: 0.9892, batch_loss: 0.1344, loss: 0.0343 ||:  65%|######5   | 386/590 [21:50<11:53,  3.50s/it]
2022-04-08 18:49:48,017 - INFO - tqdm - accuracy: 0.9891, batch_loss: 0.0627, loss: 0.0343 ||:  66%|######5   | 389/590 [22:01<12:01,  3.59s/it]
2022-04-08 18:49:59,899 - INFO - tqdm - accuracy: 0.9890, batch_loss: 0.0323, loss: 0.0342 ||:  66%|######6   | 392/590 [22:13<12:26,  3.77s/it]
2022-04-08 18:50:10,173 - INFO - tqdm - accuracy: 0.9890, batch_loss: 0.0867, loss: 0.0342 ||:  67%|######6   | 395/590 [22:23<11:31,  3.55s/it]
2022-04-08 18:50:22,577 - INFO - tqdm - accuracy: 0.9891, batch_loss: 0.0025, loss: 0.0340 ||:  68%|######7   | 399/590 [22:35<10:13,  3.21s/it]
2022-04-08 18:50:34,925 - INFO - tqdm - accuracy: 0.9892, batch_loss: 0.0037, loss: 0.0337 ||:  68%|######8   | 403/590 [22:48<09:40,  3.11s/it]
2022-04-08 18:50:47,010 - INFO - tqdm - accuracy: 0.9892, batch_loss: 0.1237, loss: 0.0338 ||:  69%|######8   | 407/590 [23:00<09:03,  2.97s/it]
2022-04-08 18:50:58,560 - INFO - tqdm - accuracy: 0.9890, batch_loss: 0.0021, loss: 0.0341 ||:  69%|######9   | 410/590 [23:11<10:39,  3.55s/it]
2022-04-08 18:51:11,030 - INFO - tqdm - accuracy: 0.9891, batch_loss: 0.0045, loss: 0.0338 ||:  70%|#######   | 414/590 [23:24<09:26,  3.22s/it]
2022-04-08 18:51:21,978 - INFO - tqdm - accuracy: 0.9892, batch_loss: 0.0032, loss: 0.0337 ||:  71%|#######   | 417/590 [23:35<10:17,  3.57s/it]
2022-04-08 18:51:35,571 - INFO - tqdm - accuracy: 0.9892, batch_loss: 0.0417, loss: 0.0339 ||:  71%|#######1  | 421/590 [23:48<09:50,  3.49s/it]
2022-04-08 18:51:46,279 - INFO - tqdm - accuracy: 0.9892, batch_loss: 0.0150, loss: 0.0338 ||:  72%|#######1  | 424/590 [23:59<09:53,  3.58s/it]
2022-04-08 18:51:59,402 - INFO - tqdm - accuracy: 0.9893, batch_loss: 0.0512, loss: 0.0337 ||:  73%|#######2  | 428/590 [24:12<09:04,  3.36s/it]
2022-04-08 18:52:10,093 - INFO - tqdm - accuracy: 0.9893, batch_loss: 0.0107, loss: 0.0338 ||:  73%|#######3  | 431/590 [24:23<09:09,  3.46s/it]
2022-04-08 18:52:23,206 - INFO - tqdm - accuracy: 0.9892, batch_loss: 0.0084, loss: 0.0340 ||:  74%|#######3  | 435/590 [24:36<08:27,  3.28s/it]
2022-04-08 18:52:33,663 - INFO - tqdm - accuracy: 0.9892, batch_loss: 0.0035, loss: 0.0337 ||:  74%|#######4  | 438/590 [24:46<08:35,  3.39s/it]
2022-04-08 18:52:46,241 - INFO - tqdm - accuracy: 0.9893, batch_loss: 0.0011, loss: 0.0336 ||:  75%|#######4  | 442/590 [24:59<08:01,  3.25s/it]
2022-04-08 18:52:58,645 - INFO - tqdm - accuracy: 0.9893, batch_loss: 0.0013, loss: 0.0333 ||:  76%|#######5  | 446/590 [25:11<07:26,  3.10s/it]
2022-04-08 18:53:09,659 - INFO - tqdm - accuracy: 0.9894, batch_loss: 0.0012, loss: 0.0331 ||:  76%|#######6  | 449/590 [25:22<07:59,  3.40s/it]
2022-04-08 18:53:19,807 - INFO - tqdm - accuracy: 0.9894, batch_loss: 0.2165, loss: 0.0334 ||:  77%|#######6  | 452/590 [25:33<07:48,  3.39s/it]
2022-04-08 18:53:31,090 - INFO - tqdm - accuracy: 0.9894, batch_loss: 0.0278, loss: 0.0333 ||:  77%|#######7  | 455/590 [25:44<08:05,  3.60s/it]
2022-04-08 18:53:44,412 - INFO - tqdm - accuracy: 0.9893, batch_loss: 0.0427, loss: 0.0332 ||:  78%|#######7  | 459/590 [25:57<07:25,  3.40s/it]
2022-04-08 18:53:54,857 - INFO - tqdm - accuracy: 0.9892, batch_loss: 0.0015, loss: 0.0334 ||:  78%|#######8  | 462/590 [26:08<07:19,  3.43s/it]
2022-04-08 18:54:05,798 - INFO - tqdm - accuracy: 0.9892, batch_loss: 0.0755, loss: 0.0334 ||:  79%|#######8  | 465/590 [26:19<07:24,  3.56s/it]
2022-04-08 18:54:18,637 - INFO - tqdm - accuracy: 0.9893, batch_loss: 0.0043, loss: 0.0331 ||:  79%|#######9  | 469/590 [26:31<06:45,  3.35s/it]
2022-04-08 18:54:29,180 - INFO - tqdm - accuracy: 0.9893, batch_loss: 0.0111, loss: 0.0330 ||:  80%|########  | 472/590 [26:42<06:51,  3.49s/it]
2022-04-08 18:54:41,296 - INFO - tqdm - accuracy: 0.9894, batch_loss: 0.0016, loss: 0.0329 ||:  81%|########  | 475/590 [26:54<07:19,  3.82s/it]
2022-04-08 18:54:54,566 - INFO - tqdm - accuracy: 0.9894, batch_loss: 0.0300, loss: 0.0327 ||:  81%|########1 | 479/590 [27:07<06:22,  3.44s/it]
2022-04-08 18:55:04,894 - INFO - tqdm - accuracy: 0.9894, batch_loss: 0.0770, loss: 0.0327 ||:  82%|########1 | 482/590 [27:18<06:12,  3.45s/it]
2022-04-08 18:55:18,088 - INFO - tqdm - accuracy: 0.9895, batch_loss: 0.0374, loss: 0.0326 ||:  82%|########2 | 486/590 [27:31<05:44,  3.31s/it]
2022-04-08 18:55:28,917 - INFO - tqdm - accuracy: 0.9895, batch_loss: 0.0011, loss: 0.0325 ||:  83%|########2 | 489/590 [27:42<05:57,  3.54s/it]
2022-04-08 18:55:42,650 - INFO - tqdm - accuracy: 0.9895, batch_loss: 0.0031, loss: 0.0325 ||:  84%|########3 | 493/590 [27:55<05:34,  3.45s/it]
2022-04-08 18:55:56,158 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.0117, loss: 0.0322 ||:  84%|########4 | 497/590 [28:09<05:16,  3.40s/it]
2022-04-08 18:56:09,352 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.0017, loss: 0.0322 ||:  85%|########4 | 501/590 [28:22<04:58,  3.35s/it]
2022-04-08 18:56:21,346 - INFO - tqdm - accuracy: 0.9897, batch_loss: 0.0042, loss: 0.0320 ||:  86%|########5 | 505/590 [28:34<04:17,  3.03s/it]
2022-04-08 18:56:33,993 - INFO - tqdm - accuracy: 0.9897, batch_loss: 0.0041, loss: 0.0320 ||:  86%|########6 | 509/590 [28:47<04:13,  3.13s/it]
2022-04-08 18:56:46,176 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.0019, loss: 0.0322 ||:  87%|########6 | 513/590 [28:59<03:55,  3.06s/it]
2022-04-08 18:56:59,159 - INFO - tqdm - accuracy: 0.9897, batch_loss: 0.0017, loss: 0.0320 ||:  88%|########7 | 517/590 [29:12<03:51,  3.17s/it]
2022-04-08 18:57:11,146 - INFO - tqdm - accuracy: 0.9897, batch_loss: 0.0986, loss: 0.0320 ||:  88%|########8 | 521/590 [29:24<03:31,  3.06s/it]
2022-04-08 18:57:24,014 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.0926, loss: 0.0320 ||:  89%|########8 | 525/590 [29:37<03:29,  3.22s/it]
2022-04-08 18:57:34,537 - INFO - tqdm - accuracy: 0.9895, batch_loss: 0.1347, loss: 0.0324 ||:  89%|########9 | 528/590 [29:47<03:35,  3.48s/it]
2022-04-08 18:57:47,319 - INFO - tqdm - accuracy: 0.9895, batch_loss: 0.0305, loss: 0.0323 ||:  90%|######### | 532/590 [30:00<03:07,  3.24s/it]
2022-04-08 18:57:57,930 - INFO - tqdm - accuracy: 0.9895, batch_loss: 0.0018, loss: 0.0324 ||:  91%|######### | 535/590 [30:11<03:15,  3.56s/it]
2022-04-08 18:58:09,946 - INFO - tqdm - accuracy: 0.9895, batch_loss: 0.0798, loss: 0.0324 ||:  91%|#########1| 539/590 [30:23<02:40,  3.15s/it]
2022-04-08 18:58:20,698 - INFO - tqdm - accuracy: 0.9895, batch_loss: 0.0999, loss: 0.0325 ||:  92%|#########1| 542/590 [30:33<02:42,  3.39s/it]
2022-04-08 18:58:30,754 - INFO - tqdm - accuracy: 0.9895, batch_loss: 0.0328, loss: 0.0324 ||:  92%|#########2| 545/590 [30:44<02:30,  3.34s/it]
2022-04-08 18:58:43,734 - INFO - tqdm - accuracy: 0.9895, batch_loss: 0.0230, loss: 0.0327 ||:  93%|#########3| 549/590 [30:56<02:16,  3.32s/it]
2022-04-08 18:58:54,736 - INFO - tqdm - accuracy: 0.9894, batch_loss: 0.0451, loss: 0.0329 ||:  94%|#########3| 552/590 [31:07<02:12,  3.49s/it]
2022-04-08 18:59:08,028 - INFO - tqdm - accuracy: 0.9894, batch_loss: 0.0311, loss: 0.0329 ||:  94%|#########4| 556/590 [31:21<01:54,  3.35s/it]
2022-04-08 18:59:18,610 - INFO - tqdm - accuracy: 0.9894, batch_loss: 0.0229, loss: 0.0329 ||:  95%|#########4| 559/590 [31:31<01:48,  3.49s/it]
2022-04-08 18:59:30,559 - INFO - tqdm - accuracy: 0.9895, batch_loss: 0.0078, loss: 0.0327 ||:  95%|#########5| 563/590 [31:43<01:23,  3.10s/it]
2022-04-08 18:59:43,868 - INFO - tqdm - accuracy: 0.9895, batch_loss: 0.0042, loss: 0.0329 ||:  96%|#########6| 567/590 [31:57<01:16,  3.32s/it]
2022-04-08 18:59:54,529 - INFO - tqdm - accuracy: 0.9895, batch_loss: 0.0144, loss: 0.0329 ||:  97%|#########6| 570/590 [32:07<01:09,  3.49s/it]
2022-04-08 19:00:04,848 - INFO - tqdm - accuracy: 0.9894, batch_loss: 0.0056, loss: 0.0329 ||:  97%|#########7| 573/590 [32:18<00:59,  3.47s/it]
2022-04-08 19:00:16,853 - INFO - tqdm - accuracy: 0.9894, batch_loss: 0.0214, loss: 0.0328 ||:  98%|#########7| 577/590 [32:30<00:39,  3.06s/it]
2022-04-08 19:00:28,064 - INFO - tqdm - accuracy: 0.9894, batch_loss: 0.0145, loss: 0.0327 ||:  98%|#########8| 580/590 [32:41<00:35,  3.59s/it]
2022-04-08 19:00:40,767 - INFO - tqdm - accuracy: 0.9895, batch_loss: 0.0052, loss: 0.0325 ||:  99%|#########8| 584/590 [32:54<00:19,  3.31s/it]
2022-04-08 19:00:52,891 - INFO - tqdm - accuracy: 0.9893, batch_loss: 0.1024, loss: 0.0328 ||: 100%|#########9| 588/590 [33:06<00:06,  3.10s/it]
2022-04-08 19:00:56,374 - INFO - tqdm - accuracy: 0.9893, batch_loss: 0.0007, loss: 0.0327 ||: 100%|#########9| 589/590 [33:09<00:03,  3.22s/it]
2022-04-08 19:00:57,170 - INFO - tqdm - accuracy: 0.9893, batch_loss: 0.0006, loss: 0.0326 ||: 100%|##########| 590/590 [33:10<00:00,  2.49s/it]
2022-04-08 19:00:57,171 - INFO - tqdm - accuracy: 0.9893, batch_loss: 0.0006, loss: 0.0326 ||: 100%|##########| 590/590 [33:10<00:00,  3.37s/it]
2022-04-08 19:00:59,478 - INFO - allennlp.training.trainer - Validating
2022-04-08 19:00:59,479 - INFO - tqdm - 0%|          | 0/1635 [00:00<?, ?it/s]
2022-04-08 19:01:09,570 - INFO - tqdm - accuracy: 0.8571, batch_loss: 0.0005, loss: 0.7671 ||:   9%|8         | 140/1635 [00:10<01:42, 14.65it/s]
2022-04-08 19:01:19,606 - INFO - tqdm - accuracy: 0.8558, batch_loss: 0.0007, loss: 0.7699 ||:  17%|#6        | 274/1635 [00:20<01:40, 13.60it/s]
2022-04-08 19:01:29,733 - INFO - tqdm - accuracy: 0.8554, batch_loss: 0.0004, loss: 0.7829 ||:  25%|##5       | 415/1635 [00:30<01:33, 13.10it/s]
2022-04-08 19:01:39,781 - INFO - tqdm - accuracy: 0.8605, batch_loss: 0.0014, loss: 0.7304 ||:  34%|###3      | 552/1635 [00:40<01:20, 13.41it/s]
2022-04-08 19:01:49,843 - INFO - tqdm - accuracy: 0.8599, batch_loss: 0.7625, loss: 0.7289 ||:  43%|####2     | 696/1635 [00:50<00:59, 15.82it/s]
2022-04-08 19:01:59,947 - INFO - tqdm - accuracy: 0.8588, batch_loss: 0.0019, loss: 0.7389 ||:  52%|#####1    | 843/1635 [01:00<00:47, 16.59it/s]
2022-04-08 19:02:10,015 - INFO - tqdm - accuracy: 0.8535, batch_loss: 2.7421, loss: 0.7701 ||:  61%|######    | 990/1635 [01:10<00:43, 14.92it/s]
2022-04-08 19:02:20,098 - INFO - tqdm - accuracy: 0.8547, batch_loss: 0.0017, loss: 0.7678 ||:  69%|######8   | 1125/1635 [01:20<00:39, 12.95it/s]
2022-04-08 19:02:30,146 - INFO - tqdm - accuracy: 0.8535, batch_loss: 0.1354, loss: 0.7726 ||:  78%|#######7  | 1270/1635 [01:30<00:22, 16.06it/s]
2022-04-08 19:02:40,228 - INFO - tqdm - accuracy: 0.8561, batch_loss: 0.0006, loss: 0.7648 ||:  86%|########6 | 1411/1635 [01:40<00:15, 14.41it/s]
2022-04-08 19:02:50,374 - INFO - tqdm - accuracy: 0.8548, batch_loss: 0.4515, loss: 0.7868 ||:  95%|#########5| 1557/1635 [01:50<00:06, 11.54it/s]
2022-04-08 19:02:55,448 - INFO - tqdm - accuracy: 0.8532, batch_loss: 0.0001, loss: 0.7960 ||: 100%|#########9| 1628/1635 [01:55<00:00, 15.32it/s]
2022-04-08 19:02:55,574 - INFO - tqdm - accuracy: 0.8535, batch_loss: 0.0001, loss: 0.7945 ||: 100%|#########9| 1631/1635 [01:56<00:00, 17.74it/s]
2022-04-08 19:02:55,708 - INFO - tqdm - accuracy: 0.8531, batch_loss: 1.5828, loss: 0.7958 ||: 100%|#########9| 1634/1635 [01:56<00:00, 19.19it/s]
2022-04-08 19:02:55,790 - INFO - tqdm - accuracy: 0.8532, batch_loss: 0.0004, loss: 0.7954 ||: 100%|##########| 1635/1635 [01:56<00:00, 14.06it/s]
2022-04-08 19:02:55,790 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-08 19:02:55,791 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.989  |     0.853
2022-04-08 19:02:55,792 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9604.118  |       N/A
2022-04-08 19:02:55,792 - INFO - allennlp.training.tensorboard_writer - loss               |     0.033  |     0.795
2022-04-08 19:02:55,793 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  7239.301  |       N/A
2022-04-08 19:03:00,298 - INFO - allennlp.training.trainer - Epoch duration: 0:35:13.556571
2022-04-08 19:03:00,298 - INFO - allennlp.training.trainer - Estimated training time remaining: 2:21:07
2022-04-08 19:03:00,298 - INFO - allennlp.training.trainer - Epoch 6/9
2022-04-08 19:03:00,299 - INFO - allennlp.training.trainer - Worker 0 memory usage: 7.1G
2022-04-08 19:03:00,299 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-08 19:03:00,300 - INFO - allennlp.training.trainer - Training
2022-04-08 19:03:00,300 - INFO - tqdm - 0%|          | 0/590 [00:00<?, ?it/s]
2022-04-08 19:03:10,569 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.0144, loss: 0.0282 ||:   1%|          | 3/590 [00:10<32:19,  3.30s/it]
2022-04-08 19:03:21,134 - INFO - tqdm - accuracy: 0.9948, batch_loss: 0.0034, loss: 0.0164 ||:   1%|1         | 6/590 [00:20<33:37,  3.45s/it]
2022-04-08 19:03:34,586 - INFO - tqdm - accuracy: 0.9969, batch_loss: 0.0040, loss: 0.0119 ||:   2%|1         | 10/590 [00:34<33:29,  3.46s/it]
2022-04-08 19:03:44,752 - INFO - tqdm - accuracy: 0.9976, batch_loss: 0.0024, loss: 0.0097 ||:   2%|2         | 13/590 [00:44<33:07,  3.45s/it]
2022-04-08 19:03:57,768 - INFO - tqdm - accuracy: 0.9945, batch_loss: 0.0163, loss: 0.0188 ||:   3%|2         | 17/590 [00:57<31:28,  3.30s/it]
2022-04-08 19:04:08,054 - INFO - tqdm - accuracy: 0.9938, batch_loss: 0.0040, loss: 0.0212 ||:   3%|3         | 20/590 [01:07<32:27,  3.42s/it]
2022-04-08 19:04:19,239 - INFO - tqdm - accuracy: 0.9946, batch_loss: 0.0211, loss: 0.0202 ||:   4%|3         | 23/590 [01:18<33:50,  3.58s/it]
2022-04-08 19:04:29,559 - INFO - tqdm - accuracy: 0.9952, batch_loss: 0.0009, loss: 0.0186 ||:   4%|4         | 26/590 [01:29<33:05,  3.52s/it]
2022-04-08 19:04:39,954 - INFO - tqdm - accuracy: 0.9957, batch_loss: 0.0010, loss: 0.0170 ||:   5%|4         | 29/590 [01:39<32:07,  3.44s/it]
2022-04-08 19:04:53,248 - INFO - tqdm - accuracy: 0.9962, batch_loss: 0.0007, loss: 0.0155 ||:   6%|5         | 33/590 [01:52<31:39,  3.41s/it]
2022-04-08 19:05:05,659 - INFO - tqdm - accuracy: 0.9958, batch_loss: 0.0503, loss: 0.0158 ||:   6%|6         | 37/590 [02:05<28:50,  3.13s/it]
2022-04-08 19:05:18,678 - INFO - tqdm - accuracy: 0.9962, batch_loss: 0.0030, loss: 0.0150 ||:   7%|6         | 41/590 [02:18<29:31,  3.23s/it]
2022-04-08 19:05:29,409 - INFO - tqdm - accuracy: 0.9964, batch_loss: 0.0107, loss: 0.0146 ||:   7%|7         | 44/590 [02:29<32:04,  3.52s/it]
2022-04-08 19:05:42,089 - INFO - tqdm - accuracy: 0.9961, batch_loss: 0.0008, loss: 0.0192 ||:   8%|8         | 48/590 [02:41<29:13,  3.23s/it]
2022-04-08 19:05:52,091 - INFO - tqdm - accuracy: 0.9957, batch_loss: 0.0009, loss: 0.0187 ||:   9%|8         | 51/590 [02:51<29:51,  3.32s/it]
2022-04-08 19:06:05,520 - INFO - tqdm - accuracy: 0.9955, batch_loss: 0.0063, loss: 0.0198 ||:   9%|9         | 55/590 [03:05<30:47,  3.45s/it]
2022-04-08 19:06:15,665 - INFO - tqdm - accuracy: 0.9957, batch_loss: 0.0041, loss: 0.0189 ||:  10%|9         | 58/590 [03:15<30:06,  3.40s/it]
2022-04-08 19:06:26,868 - INFO - tqdm - accuracy: 0.9959, batch_loss: 0.0011, loss: 0.0180 ||:  10%|#         | 61/590 [03:26<31:49,  3.61s/it]
2022-04-08 19:06:37,996 - INFO - tqdm - accuracy: 0.9956, batch_loss: 0.0022, loss: 0.0178 ||:  11%|#         | 64/590 [03:37<32:25,  3.70s/it]
2022-04-08 19:06:48,282 - INFO - tqdm - accuracy: 0.9958, batch_loss: 0.0228, loss: 0.0175 ||:  11%|#1        | 67/590 [03:47<30:24,  3.49s/it]
2022-04-08 19:06:58,323 - INFO - tqdm - accuracy: 0.9960, batch_loss: 0.0005, loss: 0.0168 ||:  12%|#1        | 70/590 [03:58<29:18,  3.38s/it]
2022-04-08 19:07:09,125 - INFO - tqdm - accuracy: 0.9957, batch_loss: 0.0020, loss: 0.0181 ||:  12%|#2        | 73/590 [04:08<30:06,  3.49s/it]
2022-04-08 19:07:21,902 - INFO - tqdm - accuracy: 0.9959, batch_loss: 0.0012, loss: 0.0176 ||:  13%|#3        | 77/590 [04:21<27:47,  3.25s/it]
2022-04-08 19:07:31,949 - INFO - tqdm - accuracy: 0.9961, batch_loss: 0.0020, loss: 0.0171 ||:  14%|#3        | 80/590 [04:31<28:00,  3.30s/it]
2022-04-08 19:07:42,266 - INFO - tqdm - accuracy: 0.9962, batch_loss: 0.0155, loss: 0.0169 ||:  14%|#4        | 83/590 [04:41<28:29,  3.37s/it]
2022-04-08 19:07:53,597 - INFO - tqdm - accuracy: 0.9956, batch_loss: 0.1316, loss: 0.0182 ||:  15%|#4        | 86/590 [04:53<30:44,  3.66s/it]
2022-04-08 19:08:06,385 - INFO - tqdm - accuracy: 0.9955, batch_loss: 0.0008, loss: 0.0178 ||:  15%|#5        | 90/590 [05:06<27:19,  3.28s/it]
2022-04-08 19:08:16,739 - INFO - tqdm - accuracy: 0.9953, batch_loss: 0.0010, loss: 0.0177 ||:  16%|#5        | 93/590 [05:16<28:02,  3.39s/it]
2022-04-08 19:08:30,319 - INFO - tqdm - accuracy: 0.9948, batch_loss: 0.1310, loss: 0.0210 ||:  16%|#6        | 97/590 [05:30<27:49,  3.39s/it]
2022-04-08 19:08:40,334 - INFO - tqdm - accuracy: 0.9950, batch_loss: 0.0117, loss: 0.0206 ||:  17%|#6        | 100/590 [05:40<27:28,  3.36s/it]
2022-04-08 19:08:50,837 - INFO - tqdm - accuracy: 0.9951, batch_loss: 0.0015, loss: 0.0201 ||:  17%|#7        | 103/590 [05:50<28:40,  3.53s/it]
2022-04-08 19:09:03,731 - INFO - tqdm - accuracy: 0.9950, batch_loss: 0.0007, loss: 0.0198 ||:  18%|#8        | 107/590 [06:03<26:36,  3.30s/it]
2022-04-08 19:09:16,434 - INFO - tqdm - accuracy: 0.9947, batch_loss: 0.0012, loss: 0.0201 ||:  19%|#8        | 111/590 [06:16<25:39,  3.21s/it]
2022-04-08 19:09:26,976 - INFO - tqdm - accuracy: 0.9942, batch_loss: 0.1003, loss: 0.0205 ||:  19%|#9        | 114/590 [06:26<26:58,  3.40s/it]
2022-04-08 19:09:39,825 - INFO - tqdm - accuracy: 0.9936, batch_loss: 0.0079, loss: 0.0212 ||:  20%|##        | 118/590 [06:39<26:08,  3.32s/it]
2022-04-08 19:09:50,150 - INFO - tqdm - accuracy: 0.9938, batch_loss: 0.0044, loss: 0.0209 ||:  21%|##        | 121/590 [06:49<26:29,  3.39s/it]
2022-04-08 19:10:03,733 - INFO - tqdm - accuracy: 0.9940, batch_loss: 0.0019, loss: 0.0203 ||:  21%|##1       | 125/590 [07:03<26:46,  3.45s/it]
2022-04-08 19:10:16,545 - INFO - tqdm - accuracy: 0.9942, batch_loss: 0.0084, loss: 0.0198 ||:  22%|##1       | 129/590 [07:16<25:09,  3.27s/it]
2022-04-08 19:10:29,835 - INFO - tqdm - accuracy: 0.9944, batch_loss: 0.0023, loss: 0.0193 ||:  23%|##2       | 133/590 [07:29<25:40,  3.37s/it]
2022-04-08 19:10:40,040 - INFO - tqdm - accuracy: 0.9943, batch_loss: 0.0068, loss: 0.0194 ||:  23%|##3       | 136/590 [07:39<25:46,  3.41s/it]
2022-04-08 19:10:50,092 - INFO - tqdm - accuracy: 0.9942, batch_loss: 0.0037, loss: 0.0194 ||:  24%|##3       | 139/590 [07:49<25:37,  3.41s/it]
2022-04-08 19:11:00,883 - INFO - tqdm - accuracy: 0.9941, batch_loss: 0.0063, loss: 0.0197 ||:  24%|##4       | 142/590 [08:00<26:19,  3.52s/it]
2022-04-08 19:11:11,600 - INFO - tqdm - accuracy: 0.9940, batch_loss: 0.0352, loss: 0.0195 ||:  25%|##4       | 145/590 [08:11<26:23,  3.56s/it]
2022-04-08 19:11:24,112 - INFO - tqdm - accuracy: 0.9939, batch_loss: 0.0018, loss: 0.0193 ||:  25%|##5       | 149/590 [08:23<23:55,  3.25s/it]
2022-04-08 19:11:36,705 - INFO - tqdm - accuracy: 0.9941, batch_loss: 0.0017, loss: 0.0190 ||:  26%|##5       | 153/590 [08:36<23:15,  3.19s/it]
2022-04-08 19:11:47,603 - INFO - tqdm - accuracy: 0.9942, batch_loss: 0.0027, loss: 0.0187 ||:  26%|##6       | 156/590 [08:47<25:12,  3.49s/it]
2022-04-08 19:11:57,622 - INFO - tqdm - accuracy: 0.9937, batch_loss: 0.0269, loss: 0.0205 ||:  27%|##6       | 159/590 [08:57<24:28,  3.41s/it]
2022-04-08 19:12:08,216 - INFO - tqdm - accuracy: 0.9936, batch_loss: 0.0012, loss: 0.0206 ||:  27%|##7       | 162/590 [09:07<24:33,  3.44s/it]
2022-04-08 19:12:21,323 - INFO - tqdm - accuracy: 0.9930, batch_loss: 0.0117, loss: 0.0240 ||:  28%|##8       | 166/590 [09:21<23:47,  3.37s/it]
2022-04-08 19:12:31,614 - INFO - tqdm - accuracy: 0.9932, batch_loss: 0.0140, loss: 0.0237 ||:  29%|##8       | 169/590 [09:31<23:58,  3.42s/it]
2022-04-08 19:12:44,233 - INFO - tqdm - accuracy: 0.9933, batch_loss: 0.0042, loss: 0.0233 ||:  29%|##9       | 173/590 [09:43<23:02,  3.32s/it]
2022-04-08 19:12:57,456 - INFO - tqdm - accuracy: 0.9935, batch_loss: 0.0021, loss: 0.0230 ||:  30%|###       | 177/590 [09:57<22:47,  3.31s/it]
2022-04-08 19:13:07,642 - INFO - tqdm - accuracy: 0.9934, batch_loss: 0.0029, loss: 0.0228 ||:  31%|###       | 180/590 [10:07<23:07,  3.39s/it]
2022-04-08 19:13:19,081 - INFO - tqdm - accuracy: 0.9935, batch_loss: 0.0029, loss: 0.0225 ||:  31%|###1      | 183/590 [10:18<25:02,  3.69s/it]
2022-04-08 19:13:30,234 - INFO - tqdm - accuracy: 0.9933, batch_loss: 0.0016, loss: 0.0231 ||:  32%|###1      | 186/590 [10:29<24:17,  3.61s/it]
2022-04-08 19:13:40,911 - INFO - tqdm - accuracy: 0.9934, batch_loss: 0.0009, loss: 0.0228 ||:  32%|###2      | 189/590 [10:40<23:59,  3.59s/it]
2022-04-08 19:13:52,914 - INFO - tqdm - accuracy: 0.9935, batch_loss: 0.0030, loss: 0.0223 ||:  33%|###2      | 193/590 [10:52<20:32,  3.10s/it]
2022-04-08 19:14:03,150 - INFO - tqdm - accuracy: 0.9936, batch_loss: 0.0007, loss: 0.0221 ||:  33%|###3      | 196/590 [11:02<21:30,  3.27s/it]
2022-04-08 19:14:14,081 - INFO - tqdm - accuracy: 0.9937, batch_loss: 0.0010, loss: 0.0219 ||:  34%|###3      | 199/590 [11:13<23:02,  3.54s/it]
2022-04-08 19:14:25,077 - INFO - tqdm - accuracy: 0.9937, batch_loss: 0.0079, loss: 0.0218 ||:  34%|###4      | 202/590 [11:24<23:02,  3.56s/it]
2022-04-08 19:14:35,081 - INFO - tqdm - accuracy: 0.9938, batch_loss: 0.0056, loss: 0.0216 ||:  35%|###4      | 205/590 [11:34<21:58,  3.43s/it]
2022-04-08 19:14:48,410 - INFO - tqdm - accuracy: 0.9939, batch_loss: 0.0025, loss: 0.0212 ||:  35%|###5      | 209/590 [11:48<21:20,  3.36s/it]
2022-04-08 19:15:01,235 - INFO - tqdm - accuracy: 0.9938, batch_loss: 0.0018, loss: 0.0211 ||:  36%|###6      | 213/590 [12:00<20:52,  3.32s/it]
2022-04-08 19:15:14,009 - INFO - tqdm - accuracy: 0.9938, batch_loss: 0.0172, loss: 0.0216 ||:  37%|###6      | 217/590 [12:13<19:59,  3.22s/it]
2022-04-08 19:15:26,764 - INFO - tqdm - accuracy: 0.9938, batch_loss: 0.0114, loss: 0.0216 ||:  37%|###7      | 221/590 [12:26<19:40,  3.20s/it]
2022-04-08 19:15:40,053 - INFO - tqdm - accuracy: 0.9938, batch_loss: 0.0279, loss: 0.0214 ||:  38%|###8      | 225/590 [12:39<20:15,  3.33s/it]
2022-04-08 19:15:53,402 - INFO - tqdm - accuracy: 0.9939, batch_loss: 0.0116, loss: 0.0211 ||:  39%|###8      | 229/590 [12:53<20:13,  3.36s/it]
2022-04-08 19:16:06,803 - INFO - tqdm - accuracy: 0.9940, batch_loss: 0.0013, loss: 0.0208 ||:  39%|###9      | 233/590 [13:06<20:14,  3.40s/it]
2022-04-08 19:16:19,985 - INFO - tqdm - accuracy: 0.9941, batch_loss: 0.0044, loss: 0.0205 ||:  40%|####      | 237/590 [13:19<19:55,  3.39s/it]
2022-04-08 19:16:31,329 - INFO - tqdm - accuracy: 0.9940, batch_loss: 0.0253, loss: 0.0204 ||:  41%|####      | 240/590 [13:31<21:59,  3.77s/it]
2022-04-08 19:16:41,490 - INFO - tqdm - accuracy: 0.9940, batch_loss: 0.0015, loss: 0.0206 ||:  41%|####1     | 243/590 [13:41<20:07,  3.48s/it]
2022-04-08 19:16:55,070 - INFO - tqdm - accuracy: 0.9939, batch_loss: 0.0041, loss: 0.0206 ||:  42%|####1     | 247/590 [13:54<19:43,  3.45s/it]
2022-04-08 19:17:07,993 - INFO - tqdm - accuracy: 0.9939, batch_loss: 0.0146, loss: 0.0206 ||:  43%|####2     | 251/590 [14:07<18:42,  3.31s/it]
2022-04-08 19:17:18,830 - INFO - tqdm - accuracy: 0.9938, batch_loss: 0.0037, loss: 0.0207 ||:  43%|####3     | 254/590 [14:18<19:57,  3.56s/it]
2022-04-08 19:17:32,060 - INFO - tqdm - accuracy: 0.9939, batch_loss: 0.0038, loss: 0.0205 ||:  44%|####3     | 258/590 [14:31<18:44,  3.39s/it]
2022-04-08 19:17:42,295 - INFO - tqdm - accuracy: 0.9939, batch_loss: 0.2227, loss: 0.0211 ||:  44%|####4     | 261/590 [14:41<18:35,  3.39s/it]
2022-04-08 19:17:55,618 - INFO - tqdm - accuracy: 0.9940, batch_loss: 0.0086, loss: 0.0209 ||:  45%|####4     | 265/590 [14:55<18:30,  3.42s/it]
2022-04-08 19:18:05,919 - INFO - tqdm - accuracy: 0.9941, batch_loss: 0.0176, loss: 0.0208 ||:  45%|####5     | 268/590 [15:05<18:29,  3.45s/it]
2022-04-08 19:18:16,136 - INFO - tqdm - accuracy: 0.9940, batch_loss: 0.0045, loss: 0.0212 ||:  46%|####5     | 271/590 [15:15<17:56,  3.38s/it]
2022-04-08 19:18:28,755 - INFO - tqdm - accuracy: 0.9941, batch_loss: 0.0051, loss: 0.0210 ||:  47%|####6     | 275/590 [15:28<17:06,  3.26s/it]
2022-04-08 19:18:39,786 - INFO - tqdm - accuracy: 0.9942, batch_loss: 0.0051, loss: 0.0208 ||:  47%|####7     | 278/590 [15:39<18:20,  3.53s/it]
2022-04-08 19:18:50,511 - INFO - tqdm - accuracy: 0.9942, batch_loss: 0.0020, loss: 0.0206 ||:  48%|####7     | 281/590 [15:50<18:23,  3.57s/it]
2022-04-08 19:19:00,936 - INFO - tqdm - accuracy: 0.9943, batch_loss: 0.0148, loss: 0.0206 ||:  48%|####8     | 284/590 [16:00<17:48,  3.49s/it]
2022-04-08 19:19:11,365 - INFO - tqdm - accuracy: 0.9941, batch_loss: 0.0004, loss: 0.0211 ||:  49%|####8     | 287/590 [16:11<17:26,  3.45s/it]
2022-04-08 19:19:21,763 - INFO - tqdm - accuracy: 0.9941, batch_loss: 0.0017, loss: 0.0214 ||:  49%|####9     | 290/590 [16:21<17:28,  3.50s/it]
2022-04-08 19:19:33,569 - INFO - tqdm - accuracy: 0.9940, batch_loss: 0.0579, loss: 0.0214 ||:  50%|####9     | 294/590 [16:33<15:09,  3.07s/it]
2022-04-08 19:19:43,591 - INFO - tqdm - accuracy: 0.9941, batch_loss: 0.0010, loss: 0.0212 ||:  50%|#####     | 297/590 [16:43<15:42,  3.22s/it]
2022-04-08 19:19:53,904 - INFO - tqdm - accuracy: 0.9942, batch_loss: 0.0009, loss: 0.0210 ||:  51%|#####     | 300/590 [16:53<15:54,  3.29s/it]
2022-04-08 19:20:04,113 - INFO - tqdm - accuracy: 0.9941, batch_loss: 0.0055, loss: 0.0211 ||:  51%|#####1    | 303/590 [17:03<16:04,  3.36s/it]
2022-04-08 19:20:17,124 - INFO - tqdm - accuracy: 0.9940, batch_loss: 0.0198, loss: 0.0219 ||:  52%|#####2    | 307/590 [17:16<15:29,  3.29s/it]
2022-04-08 19:20:27,225 - INFO - tqdm - accuracy: 0.9939, batch_loss: 0.0228, loss: 0.0224 ||:  53%|#####2    | 310/590 [17:26<15:33,  3.34s/it]
2022-04-08 19:20:38,379 - INFO - tqdm - accuracy: 0.9938, batch_loss: 0.0064, loss: 0.0224 ||:  53%|#####3    | 313/590 [17:38<16:16,  3.52s/it]
2022-04-08 19:20:51,749 - INFO - tqdm - accuracy: 0.9938, batch_loss: 0.0016, loss: 0.0223 ||:  54%|#####3    | 317/590 [17:51<15:37,  3.44s/it]
2022-04-08 19:21:02,295 - INFO - tqdm - accuracy: 0.9938, batch_loss: 0.0027, loss: 0.0224 ||:  54%|#####4    | 320/590 [18:01<15:42,  3.49s/it]
2022-04-08 19:21:14,923 - INFO - tqdm - accuracy: 0.9938, batch_loss: 0.0149, loss: 0.0222 ||:  55%|#####4    | 324/590 [18:14<14:23,  3.25s/it]
2022-04-08 19:21:27,225 - INFO - tqdm - accuracy: 0.9939, batch_loss: 0.0100, loss: 0.0220 ||:  56%|#####5    | 328/590 [18:26<13:39,  3.13s/it]
2022-04-08 19:21:40,511 - INFO - tqdm - accuracy: 0.9938, batch_loss: 0.0126, loss: 0.0224 ||:  56%|#####6    | 332/590 [18:40<14:33,  3.38s/it]
2022-04-08 19:21:52,108 - INFO - tqdm - accuracy: 0.9938, batch_loss: 0.0053, loss: 0.0225 ||:  57%|#####6    | 336/590 [18:51<12:51,  3.04s/it]
2022-04-08 19:22:02,706 - INFO - tqdm - accuracy: 0.9938, batch_loss: 0.0026, loss: 0.0223 ||:  57%|#####7    | 339/590 [19:02<14:02,  3.36s/it]
2022-04-08 19:22:15,810 - INFO - tqdm - accuracy: 0.9939, batch_loss: 0.0026, loss: 0.0221 ||:  58%|#####8    | 343/590 [19:15<13:31,  3.28s/it]
2022-04-08 19:22:29,150 - INFO - tqdm - accuracy: 0.9940, batch_loss: 0.0112, loss: 0.0219 ||:  59%|#####8    | 347/590 [19:28<13:40,  3.38s/it]
2022-04-08 19:22:41,785 - INFO - tqdm - accuracy: 0.9939, batch_loss: 0.0010, loss: 0.0224 ||:  59%|#####9    | 351/590 [19:41<12:31,  3.14s/it]
2022-04-08 19:22:55,421 - INFO - tqdm - accuracy: 0.9939, batch_loss: 0.0489, loss: 0.0224 ||:  60%|######    | 355/590 [19:55<13:12,  3.37s/it]
2022-04-08 19:23:05,674 - INFO - tqdm - accuracy: 0.9938, batch_loss: 0.0228, loss: 0.0230 ||:  61%|######    | 358/590 [20:05<13:10,  3.41s/it]
2022-04-08 19:23:15,720 - INFO - tqdm - accuracy: 0.9939, batch_loss: 0.0105, loss: 0.0229 ||:  61%|######1   | 361/590 [20:15<13:01,  3.41s/it]
2022-04-08 19:23:26,309 - INFO - tqdm - accuracy: 0.9939, batch_loss: 0.0053, loss: 0.0227 ||:  62%|######1   | 364/590 [20:26<13:13,  3.51s/it]
2022-04-08 19:23:36,758 - INFO - tqdm - accuracy: 0.9940, batch_loss: 0.0043, loss: 0.0226 ||:  62%|######2   | 367/590 [20:36<13:08,  3.54s/it]
2022-04-08 19:23:49,871 - INFO - tqdm - accuracy: 0.9940, batch_loss: 0.0032, loss: 0.0225 ||:  63%|######2   | 371/590 [20:49<12:25,  3.40s/it]
2022-04-08 19:24:00,560 - INFO - tqdm - accuracy: 0.9941, batch_loss: 0.0016, loss: 0.0224 ||:  63%|######3   | 374/590 [21:00<12:42,  3.53s/it]
2022-04-08 19:24:13,959 - INFO - tqdm - accuracy: 0.9941, batch_loss: 0.0046, loss: 0.0223 ||:  64%|######4   | 378/590 [21:13<12:04,  3.42s/it]
2022-04-08 19:24:27,361 - INFO - tqdm - accuracy: 0.9940, batch_loss: 0.0592, loss: 0.0224 ||:  65%|######4   | 382/590 [21:27<11:48,  3.40s/it]
2022-04-08 19:24:40,974 - INFO - tqdm - accuracy: 0.9938, batch_loss: 0.0061, loss: 0.0229 ||:  65%|######5   | 386/590 [21:40<12:06,  3.56s/it]
2022-04-08 19:24:51,808 - INFO - tqdm - accuracy: 0.9938, batch_loss: 0.0010, loss: 0.0230 ||:  66%|######5   | 389/590 [21:51<11:55,  3.56s/it]
2022-04-08 19:25:03,003 - INFO - tqdm - accuracy: 0.9939, batch_loss: 0.0063, loss: 0.0229 ||:  66%|######6   | 392/590 [22:02<12:11,  3.70s/it]
2022-04-08 19:25:15,656 - INFO - tqdm - accuracy: 0.9938, batch_loss: 0.0605, loss: 0.0228 ||:  67%|######7   | 396/590 [22:15<10:36,  3.28s/it]
2022-04-08 19:25:27,740 - INFO - tqdm - accuracy: 0.9938, batch_loss: 0.0083, loss: 0.0228 ||:  68%|######7   | 399/590 [22:27<12:11,  3.83s/it]
2022-04-08 19:25:39,305 - INFO - tqdm - accuracy: 0.9938, batch_loss: 0.0352, loss: 0.0228 ||:  68%|######8   | 402/590 [22:39<11:52,  3.79s/it]
2022-04-08 19:25:51,584 - INFO - tqdm - accuracy: 0.9938, batch_loss: 0.0185, loss: 0.0227 ||:  69%|######8   | 406/590 [22:51<09:54,  3.23s/it]
2022-04-08 19:26:02,211 - INFO - tqdm - accuracy: 0.9938, batch_loss: 0.0031, loss: 0.0226 ||:  69%|######9   | 409/590 [23:01<10:18,  3.42s/it]
2022-04-08 19:26:15,314 - INFO - tqdm - accuracy: 0.9937, batch_loss: 0.0071, loss: 0.0227 ||:  70%|#######   | 413/590 [23:15<09:53,  3.35s/it]
2022-04-08 19:26:28,068 - INFO - tqdm - accuracy: 0.9938, batch_loss: 0.0011, loss: 0.0225 ||:  71%|#######   | 417/590 [23:27<09:14,  3.20s/it]
2022-04-08 19:26:40,115 - INFO - tqdm - accuracy: 0.9938, batch_loss: 0.0056, loss: 0.0223 ||:  71%|#######1  | 421/590 [23:39<08:34,  3.04s/it]
2022-04-08 19:26:50,899 - INFO - tqdm - accuracy: 0.9938, batch_loss: 0.0016, loss: 0.0224 ||:  72%|#######1  | 424/590 [23:50<09:20,  3.38s/it]
2022-04-08 19:27:03,514 - INFO - tqdm - accuracy: 0.9938, batch_loss: 0.0007, loss: 0.0224 ||:  73%|#######2  | 428/590 [24:03<08:41,  3.22s/it]
2022-04-08 19:27:16,688 - INFO - tqdm - accuracy: 0.9939, batch_loss: 0.0008, loss: 0.0222 ||:  73%|#######3  | 432/590 [24:16<08:36,  3.27s/it]
2022-04-08 19:27:27,516 - INFO - tqdm - accuracy: 0.9939, batch_loss: 0.0028, loss: 0.0221 ||:  74%|#######3  | 435/590 [24:27<08:55,  3.46s/it]
2022-04-08 19:27:40,191 - INFO - tqdm - accuracy: 0.9939, batch_loss: 0.0079, loss: 0.0219 ||:  74%|#######4  | 439/590 [24:39<08:19,  3.31s/it]
2022-04-08 19:27:53,071 - INFO - tqdm - accuracy: 0.9939, batch_loss: 0.0006, loss: 0.0218 ||:  75%|#######5  | 443/590 [24:52<08:03,  3.29s/it]
2022-04-08 19:28:05,515 - INFO - tqdm - accuracy: 0.9939, batch_loss: 0.0080, loss: 0.0219 ||:  76%|#######5  | 447/590 [25:05<07:28,  3.14s/it]
2022-04-08 19:28:18,936 - INFO - tqdm - accuracy: 0.9939, batch_loss: 0.0031, loss: 0.0219 ||:  76%|#######6  | 451/590 [25:18<07:39,  3.31s/it]
2022-04-08 19:28:29,316 - INFO - tqdm - accuracy: 0.9939, batch_loss: 0.0008, loss: 0.0218 ||:  77%|#######6  | 454/590 [25:29<07:45,  3.42s/it]
2022-04-08 19:28:42,435 - INFO - tqdm - accuracy: 0.9939, batch_loss: 0.0088, loss: 0.0217 ||:  78%|#######7  | 458/590 [25:42<07:18,  3.32s/it]
2022-04-08 19:28:53,948 - INFO - tqdm - accuracy: 0.9940, batch_loss: 0.0008, loss: 0.0216 ||:  78%|#######8  | 461/590 [25:53<07:41,  3.58s/it]
2022-04-08 19:29:03,997 - INFO - tqdm - accuracy: 0.9939, batch_loss: 0.0266, loss: 0.0216 ||:  79%|#######8  | 464/590 [26:03<07:09,  3.41s/it]
2022-04-08 19:29:14,878 - INFO - tqdm - accuracy: 0.9939, batch_loss: 0.0007, loss: 0.0216 ||:  79%|#######9  | 467/590 [26:14<07:14,  3.54s/it]
2022-04-08 19:29:25,908 - INFO - tqdm - accuracy: 0.9939, batch_loss: 0.0023, loss: 0.0214 ||:  80%|#######9  | 470/590 [26:25<07:15,  3.63s/it]
2022-04-08 19:29:35,952 - INFO - tqdm - accuracy: 0.9939, batch_loss: 0.2186, loss: 0.0218 ||:  80%|########  | 473/590 [26:35<06:41,  3.43s/it]
2022-04-08 19:29:46,756 - INFO - tqdm - accuracy: 0.9940, batch_loss: 0.0057, loss: 0.0217 ||:  81%|########  | 476/590 [26:46<06:41,  3.53s/it]
2022-04-08 19:29:56,764 - INFO - tqdm - accuracy: 0.9940, batch_loss: 0.0020, loss: 0.0215 ||:  81%|########1 | 479/590 [26:56<06:12,  3.36s/it]
2022-04-08 19:30:09,578 - INFO - tqdm - accuracy: 0.9940, batch_loss: 0.0031, loss: 0.0214 ||:  82%|########1 | 483/590 [27:09<05:51,  3.28s/it]
2022-04-08 19:30:22,673 - INFO - tqdm - accuracy: 0.9941, batch_loss: 0.0096, loss: 0.0212 ||:  83%|########2 | 487/590 [27:22<05:38,  3.29s/it]
2022-04-08 19:30:32,804 - INFO - tqdm - accuracy: 0.9939, batch_loss: 0.1784, loss: 0.0219 ||:  83%|########3 | 490/590 [27:32<05:35,  3.35s/it]
2022-04-08 19:30:43,647 - INFO - tqdm - accuracy: 0.9939, batch_loss: 0.0976, loss: 0.0220 ||:  84%|########3 | 493/590 [27:43<05:42,  3.53s/it]
2022-04-08 19:30:56,116 - INFO - tqdm - accuracy: 0.9940, batch_loss: 0.0012, loss: 0.0218 ||:  84%|########4 | 497/590 [27:55<05:02,  3.25s/it]
2022-04-08 19:31:07,601 - INFO - tqdm - accuracy: 0.9940, batch_loss: 0.0017, loss: 0.0218 ||:  85%|########4 | 500/590 [28:07<05:27,  3.63s/it]
2022-04-08 19:31:19,262 - INFO - tqdm - accuracy: 0.9940, batch_loss: 0.0030, loss: 0.0217 ||:  85%|########5 | 503/590 [28:18<05:30,  3.80s/it]
2022-04-08 19:31:32,024 - INFO - tqdm - accuracy: 0.9941, batch_loss: 0.0014, loss: 0.0215 ||:  86%|########5 | 507/590 [28:31<04:35,  3.32s/it]
2022-04-08 19:31:44,991 - INFO - tqdm - accuracy: 0.9941, batch_loss: 0.0053, loss: 0.0214 ||:  87%|########6 | 511/590 [28:44<04:17,  3.26s/it]
2022-04-08 19:31:58,205 - INFO - tqdm - accuracy: 0.9941, batch_loss: 0.0007, loss: 0.0214 ||:  87%|########7 | 515/590 [28:57<04:11,  3.36s/it]
2022-04-08 19:32:08,518 - INFO - tqdm - accuracy: 0.9941, batch_loss: 0.0056, loss: 0.0214 ||:  88%|########7 | 518/590 [29:08<04:08,  3.46s/it]
2022-04-08 19:32:18,696 - INFO - tqdm - accuracy: 0.9941, batch_loss: 0.0011, loss: 0.0213 ||:  88%|########8 | 521/590 [29:18<03:54,  3.39s/it]
2022-04-08 19:32:28,971 - INFO - tqdm - accuracy: 0.9942, batch_loss: 0.0131, loss: 0.0212 ||:  89%|########8 | 524/590 [29:28<03:44,  3.40s/it]
2022-04-08 19:32:39,486 - INFO - tqdm - accuracy: 0.9941, batch_loss: 0.0006, loss: 0.0211 ||:  89%|########9 | 527/590 [29:39<03:40,  3.50s/it]
2022-04-08 19:32:52,628 - INFO - tqdm - accuracy: 0.9941, batch_loss: 0.0592, loss: 0.0211 ||:  90%|######### | 531/590 [29:52<03:20,  3.41s/it]
2022-04-08 19:33:03,188 - INFO - tqdm - accuracy: 0.9941, batch_loss: 0.0007, loss: 0.0210 ||:  91%|######### | 534/590 [30:02<03:14,  3.47s/it]
2022-04-08 19:33:15,277 - INFO - tqdm - accuracy: 0.9941, batch_loss: 0.0005, loss: 0.0210 ||:  91%|#########1| 538/590 [30:14<02:40,  3.09s/it]
2022-04-08 19:33:26,494 - INFO - tqdm - accuracy: 0.9941, batch_loss: 0.0035, loss: 0.0210 ||:  92%|#########1| 541/590 [30:26<02:56,  3.61s/it]
2022-04-08 19:33:37,913 - INFO - tqdm - accuracy: 0.9941, batch_loss: 0.0077, loss: 0.0208 ||:  92%|#########2| 545/590 [30:37<02:15,  3.01s/it]
2022-04-08 19:33:48,124 - INFO - tqdm - accuracy: 0.9941, batch_loss: 0.0048, loss: 0.0207 ||:  93%|#########2| 548/590 [30:47<02:17,  3.27s/it]
2022-04-08 19:33:58,613 - INFO - tqdm - accuracy: 0.9941, batch_loss: 0.0023, loss: 0.0209 ||:  93%|#########3| 551/590 [30:58<02:12,  3.40s/it]
2022-04-08 19:34:09,409 - INFO - tqdm - accuracy: 0.9941, batch_loss: 0.0025, loss: 0.0208 ||:  94%|#########3| 554/590 [31:09<02:05,  3.49s/it]
2022-04-08 19:34:19,581 - INFO - tqdm - accuracy: 0.9941, batch_loss: 0.0138, loss: 0.0208 ||:  94%|#########4| 557/590 [31:19<01:55,  3.49s/it]
2022-04-08 19:34:30,479 - INFO - tqdm - accuracy: 0.9941, batch_loss: 0.0069, loss: 0.0207 ||:  95%|#########4| 560/590 [31:30<01:46,  3.56s/it]
2022-04-08 19:34:43,790 - INFO - tqdm - accuracy: 0.9941, batch_loss: 0.0036, loss: 0.0206 ||:  96%|#########5| 564/590 [31:43<01:28,  3.40s/it]
2022-04-08 19:34:53,928 - INFO - tqdm - accuracy: 0.9942, batch_loss: 0.0013, loss: 0.0205 ||:  96%|#########6| 567/590 [31:53<01:17,  3.38s/it]
2022-04-08 19:35:04,282 - INFO - tqdm - accuracy: 0.9941, batch_loss: 0.0005, loss: 0.0205 ||:  97%|#########6| 570/590 [32:03<01:07,  3.39s/it]
2022-04-08 19:35:14,897 - INFO - tqdm - accuracy: 0.9941, batch_loss: 0.0043, loss: 0.0206 ||:  97%|#########7| 573/590 [32:14<00:58,  3.46s/it]
2022-04-08 19:35:28,450 - INFO - tqdm - accuracy: 0.9941, batch_loss: 0.0008, loss: 0.0205 ||:  98%|#########7| 577/590 [32:28<00:44,  3.46s/it]
2022-04-08 19:35:39,482 - INFO - tqdm - accuracy: 0.9941, batch_loss: 0.0007, loss: 0.0204 ||:  98%|#########8| 580/590 [32:39<00:35,  3.56s/it]
2022-04-08 19:35:49,931 - INFO - tqdm - accuracy: 0.9941, batch_loss: 0.0710, loss: 0.0205 ||:  99%|#########8| 583/590 [32:49<00:24,  3.53s/it]
2022-04-08 19:36:00,233 - INFO - tqdm - accuracy: 0.9941, batch_loss: 0.0059, loss: 0.0204 ||:  99%|#########9| 586/590 [32:59<00:13,  3.37s/it]
2022-04-08 19:36:07,466 - INFO - tqdm - accuracy: 0.9941, batch_loss: 0.0193, loss: 0.0204 ||: 100%|#########9| 588/590 [33:07<00:06,  3.47s/it]
2022-04-08 19:36:10,960 - INFO - tqdm - accuracy: 0.9941, batch_loss: 0.0058, loss: 0.0204 ||: 100%|#########9| 589/590 [33:10<00:03,  3.48s/it]
2022-04-08 19:36:11,772 - INFO - tqdm - accuracy: 0.9941, batch_loss: 0.0004, loss: 0.0203 ||: 100%|##########| 590/590 [33:11<00:00,  2.68s/it]
2022-04-08 19:36:11,772 - INFO - tqdm - accuracy: 0.9941, batch_loss: 0.0004, loss: 0.0203 ||: 100%|##########| 590/590 [33:11<00:00,  3.38s/it]
2022-04-08 19:36:14,059 - INFO - allennlp.training.trainer - Validating
2022-04-08 19:36:14,060 - INFO - tqdm - 0%|          | 0/1635 [00:00<?, ?it/s]
2022-04-08 19:36:24,180 - INFO - tqdm - accuracy: 0.7987, batch_loss: 2.0477, loss: 1.3135 ||:   9%|9         | 154/1635 [00:10<01:35, 15.56it/s]
2022-04-08 19:36:34,207 - INFO - tqdm - accuracy: 0.8356, batch_loss: 3.0029, loss: 1.0563 ||:  18%|#7        | 292/1635 [00:20<01:13, 18.19it/s]
2022-04-08 19:36:44,314 - INFO - tqdm - accuracy: 0.8349, batch_loss: 3.0037, loss: 1.0705 ||:  26%|##6       | 433/1635 [00:30<01:35, 12.60it/s]
2022-04-08 19:36:54,457 - INFO - tqdm - accuracy: 0.8382, batch_loss: 4.5343, loss: 1.0484 ||:  36%|###5      | 581/1635 [00:40<01:22, 12.76it/s]
2022-04-08 19:37:04,601 - INFO - tqdm - accuracy: 0.8466, batch_loss: 0.0002, loss: 0.9746 ||:  44%|####3     | 714/1635 [00:50<01:03, 14.50it/s]
2022-04-08 19:37:14,727 - INFO - tqdm - accuracy: 0.8550, batch_loss: 0.0001, loss: 0.9184 ||:  52%|#####2    | 852/1635 [01:00<00:51, 15.25it/s]
2022-04-08 19:37:24,776 - INFO - tqdm - accuracy: 0.8555, batch_loss: 0.0006, loss: 0.8993 ||:  61%|######1   | 1000/1635 [01:10<00:45, 14.03it/s]
2022-04-08 19:37:34,777 - INFO - tqdm - accuracy: 0.8531, batch_loss: 4.5798, loss: 0.9063 ||:  70%|######9   | 1140/1635 [01:20<00:33, 14.68it/s]
2022-04-08 19:37:44,937 - INFO - tqdm - accuracy: 0.8508, batch_loss: 0.0002, loss: 0.9114 ||:  78%|#######8  | 1277/1635 [01:30<00:28, 12.64it/s]
2022-04-08 19:37:54,960 - INFO - tqdm - accuracy: 0.8521, batch_loss: 1.3371, loss: 0.9041 ||:  87%|########6 | 1420/1635 [01:40<00:17, 12.15it/s]
2022-04-08 19:38:05,054 - INFO - tqdm - accuracy: 0.8491, batch_loss: 0.0032, loss: 0.9219 ||:  95%|#########5| 1561/1635 [01:50<00:04, 16.38it/s]
2022-04-08 19:38:09,852 - INFO - tqdm - accuracy: 0.8489, batch_loss: 0.0001, loss: 0.9309 ||: 100%|#########9| 1628/1635 [01:55<00:00, 13.39it/s]
2022-04-08 19:38:09,957 - INFO - tqdm - accuracy: 0.8491, batch_loss: 0.0001, loss: 0.9298 ||: 100%|#########9| 1630/1635 [01:55<00:00, 14.60it/s]
2022-04-08 19:38:10,170 - INFO - tqdm - accuracy: 0.8493, batch_loss: 0.0001, loss: 0.9286 ||: 100%|#########9| 1632/1635 [01:56<00:00, 12.60it/s]
2022-04-08 19:38:10,361 - INFO - tqdm - accuracy: 0.8491, batch_loss: 0.5838, loss: 0.9278 ||: 100%|#########9| 1634/1635 [01:56<00:00, 11.89it/s]
2022-04-08 19:38:10,431 - INFO - tqdm - accuracy: 0.8489, batch_loss: 4.2626, loss: 0.9299 ||: 100%|##########| 1635/1635 [01:56<00:00, 14.05it/s]
2022-04-08 19:38:10,431 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-08 19:38:10,432 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.994  |     0.849
2022-04-08 19:38:10,433 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9604.118  |       N/A
2022-04-08 19:38:10,433 - INFO - allennlp.training.tensorboard_writer - loss               |     0.020  |     0.930
2022-04-08 19:38:10,434 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  7239.301  |       N/A
2022-04-08 19:38:14,935 - INFO - allennlp.training.trainer - Epoch duration: 0:35:14.636107
2022-04-08 19:38:14,935 - INFO - allennlp.training.trainer - Estimated training time remaining: 1:45:49
2022-04-08 19:38:14,935 - INFO - allennlp.training.trainer - Epoch 7/9
2022-04-08 19:38:14,935 - INFO - allennlp.training.trainer - Worker 0 memory usage: 7.1G
2022-04-08 19:38:14,935 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-08 19:38:14,937 - INFO - allennlp.training.trainer - Training
2022-04-08 19:38:14,937 - INFO - tqdm - 0%|          | 0/590 [00:00<?, ?it/s]
2022-04-08 19:38:25,411 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0007, loss: 0.0015 ||:   1%|          | 3/590 [00:10<34:05,  3.49s/it]
2022-04-08 19:38:38,828 - INFO - tqdm - accuracy: 0.9955, batch_loss: 0.0005, loss: 0.0087 ||:   1%|1         | 7/590 [00:23<33:09,  3.41s/it]
2022-04-08 19:38:50,326 - INFO - tqdm - accuracy: 0.9969, batch_loss: 0.0076, loss: 0.0079 ||:   2%|1         | 10/590 [00:35<36:07,  3.74s/it]
2022-04-08 19:39:00,885 - INFO - tqdm - accuracy: 0.9952, batch_loss: 0.1703, loss: 0.0202 ||:   2%|2         | 13/590 [00:45<34:16,  3.56s/it]
2022-04-08 19:39:14,224 - INFO - tqdm - accuracy: 0.9963, batch_loss: 0.0014, loss: 0.0165 ||:   3%|2         | 17/590 [00:59<32:37,  3.42s/it]
2022-04-08 19:39:27,214 - INFO - tqdm - accuracy: 0.9970, batch_loss: 0.0102, loss: 0.0140 ||:   4%|3         | 21/590 [01:12<30:42,  3.24s/it]
2022-04-08 19:39:38,781 - INFO - tqdm - accuracy: 0.9974, batch_loss: 0.0012, loss: 0.0134 ||:   4%|4         | 24/590 [01:23<34:21,  3.64s/it]
2022-04-08 19:39:50,780 - INFO - tqdm - accuracy: 0.9978, batch_loss: 0.0006, loss: 0.0118 ||:   5%|4         | 28/590 [01:35<30:27,  3.25s/it]
2022-04-08 19:40:00,913 - INFO - tqdm - accuracy: 0.9980, batch_loss: 0.0004, loss: 0.0108 ||:   5%|5         | 31/590 [01:45<31:41,  3.40s/it]
2022-04-08 19:40:11,774 - INFO - tqdm - accuracy: 0.9982, batch_loss: 0.0015, loss: 0.0100 ||:   6%|5         | 34/590 [01:56<33:10,  3.58s/it]
2022-04-08 19:40:22,201 - INFO - tqdm - accuracy: 0.9975, batch_loss: 0.0017, loss: 0.0114 ||:   6%|6         | 37/590 [02:07<32:26,  3.52s/it]
2022-04-08 19:40:32,360 - INFO - tqdm - accuracy: 0.9969, batch_loss: 0.0008, loss: 0.0115 ||:   7%|6         | 40/590 [02:17<31:49,  3.47s/it]
2022-04-08 19:40:42,455 - INFO - tqdm - accuracy: 0.9964, batch_loss: 0.0007, loss: 0.0119 ||:   7%|7         | 43/590 [02:27<31:13,  3.43s/it]
2022-04-08 19:40:55,491 - INFO - tqdm - accuracy: 0.9960, batch_loss: 0.0659, loss: 0.0124 ||:   8%|7         | 47/590 [02:40<30:03,  3.32s/it]
2022-04-08 19:41:08,699 - INFO - tqdm - accuracy: 0.9963, batch_loss: 0.0006, loss: 0.0124 ||:   9%|8         | 51/590 [02:53<29:45,  3.31s/it]
2022-04-08 19:41:19,920 - INFO - tqdm - accuracy: 0.9965, batch_loss: 0.0022, loss: 0.0118 ||:   9%|9         | 54/590 [03:04<31:13,  3.49s/it]
2022-04-08 19:41:30,672 - INFO - tqdm - accuracy: 0.9967, batch_loss: 0.0019, loss: 0.0115 ||:  10%|9         | 57/590 [03:15<31:34,  3.55s/it]
2022-04-08 19:41:40,843 - INFO - tqdm - accuracy: 0.9958, batch_loss: 0.1486, loss: 0.0135 ||:  10%|#         | 60/590 [03:25<30:37,  3.47s/it]
2022-04-08 19:41:54,459 - INFO - tqdm - accuracy: 0.9961, batch_loss: 0.0005, loss: 0.0127 ||:  11%|#         | 64/590 [03:39<30:59,  3.53s/it]
2022-04-08 19:42:04,909 - INFO - tqdm - accuracy: 0.9963, batch_loss: 0.0060, loss: 0.0123 ||:  11%|#1        | 67/590 [03:49<30:08,  3.46s/it]
2022-04-08 19:42:15,660 - INFO - tqdm - accuracy: 0.9960, batch_loss: 0.2560, loss: 0.0155 ||:  12%|#1        | 70/590 [04:00<30:50,  3.56s/it]
2022-04-08 19:42:28,862 - INFO - tqdm - accuracy: 0.9962, batch_loss: 0.0003, loss: 0.0148 ||:  13%|#2        | 74/590 [04:13<29:23,  3.42s/it]
2022-04-08 19:42:39,411 - INFO - tqdm - accuracy: 0.9963, batch_loss: 0.0006, loss: 0.0142 ||:  13%|#3        | 77/590 [04:24<30:40,  3.59s/it]
2022-04-08 19:42:49,499 - INFO - tqdm - accuracy: 0.9965, batch_loss: 0.0029, loss: 0.0138 ||:  14%|#3        | 80/590 [04:34<28:56,  3.40s/it]
2022-04-08 19:43:00,395 - INFO - tqdm - accuracy: 0.9966, batch_loss: 0.0005, loss: 0.0135 ||:  14%|#4        | 83/590 [04:45<30:11,  3.57s/it]
2022-04-08 19:43:10,826 - INFO - tqdm - accuracy: 0.9967, batch_loss: 0.0007, loss: 0.0130 ||:  15%|#4        | 86/590 [04:55<29:22,  3.50s/it]
2022-04-08 19:43:21,096 - INFO - tqdm - accuracy: 0.9968, batch_loss: 0.0015, loss: 0.0127 ||:  15%|#5        | 89/590 [05:06<28:45,  3.45s/it]
2022-04-08 19:43:31,647 - INFO - tqdm - accuracy: 0.9969, batch_loss: 0.0004, loss: 0.0124 ||:  16%|#5        | 92/590 [05:16<28:54,  3.48s/it]
2022-04-08 19:43:42,308 - INFO - tqdm - accuracy: 0.9970, batch_loss: 0.0019, loss: 0.0120 ||:  16%|#6        | 95/590 [05:27<28:43,  3.48s/it]
2022-04-08 19:43:52,465 - INFO - tqdm - accuracy: 0.9968, batch_loss: 0.0009, loss: 0.0135 ||:  17%|#6        | 98/590 [05:37<28:26,  3.47s/it]
2022-04-08 19:44:03,063 - INFO - tqdm - accuracy: 0.9969, batch_loss: 0.0007, loss: 0.0131 ||:  17%|#7        | 101/590 [05:48<28:31,  3.50s/it]
2022-04-08 19:44:14,613 - INFO - tqdm - accuracy: 0.9970, batch_loss: 0.0010, loss: 0.0128 ||:  18%|#7        | 104/590 [05:59<29:48,  3.68s/it]
2022-04-08 19:44:24,735 - INFO - tqdm - accuracy: 0.9971, batch_loss: 0.0005, loss: 0.0124 ||:  18%|#8        | 107/590 [06:09<28:41,  3.56s/it]
2022-04-08 19:44:35,056 - INFO - tqdm - accuracy: 0.9969, batch_loss: 0.0008, loss: 0.0123 ||:  19%|#8        | 110/590 [06:20<27:44,  3.47s/it]
2022-04-08 19:44:47,696 - INFO - tqdm - accuracy: 0.9970, batch_loss: 0.0126, loss: 0.0120 ||:  19%|#9        | 114/590 [06:32<25:23,  3.20s/it]
2022-04-08 19:44:57,955 - INFO - tqdm - accuracy: 0.9971, batch_loss: 0.0168, loss: 0.0119 ||:  20%|#9        | 117/590 [06:43<26:45,  3.39s/it]
2022-04-08 19:45:08,269 - INFO - tqdm - accuracy: 0.9971, batch_loss: 0.0061, loss: 0.0117 ||:  20%|##        | 120/590 [06:53<27:18,  3.49s/it]
2022-04-08 19:45:19,864 - INFO - tqdm - accuracy: 0.9970, batch_loss: 0.0008, loss: 0.0121 ||:  21%|##        | 123/590 [07:04<28:40,  3.69s/it]
2022-04-08 19:45:29,951 - INFO - tqdm - accuracy: 0.9968, batch_loss: 0.0235, loss: 0.0120 ||:  21%|##1       | 126/590 [07:15<26:54,  3.48s/it]
2022-04-08 19:45:40,518 - INFO - tqdm - accuracy: 0.9969, batch_loss: 0.0003, loss: 0.0117 ||:  22%|##1       | 129/590 [07:25<26:45,  3.48s/it]
2022-04-08 19:45:51,237 - INFO - tqdm - accuracy: 0.9969, batch_loss: 0.0007, loss: 0.0115 ||:  22%|##2       | 132/590 [07:36<26:49,  3.51s/it]
2022-04-08 19:46:04,383 - INFO - tqdm - accuracy: 0.9970, batch_loss: 0.0005, loss: 0.0112 ||:  23%|##3       | 136/590 [07:49<25:45,  3.40s/it]
2022-04-08 19:46:17,535 - INFO - tqdm - accuracy: 0.9971, batch_loss: 0.0004, loss: 0.0109 ||:  24%|##3       | 140/590 [08:02<25:10,  3.36s/it]
2022-04-08 19:46:29,910 - INFO - tqdm - accuracy: 0.9972, batch_loss: 0.0068, loss: 0.0107 ||:  24%|##4       | 144/590 [08:14<23:07,  3.11s/it]
2022-04-08 19:46:40,531 - INFO - tqdm - accuracy: 0.9970, batch_loss: 0.1649, loss: 0.0116 ||:  25%|##4       | 147/590 [08:25<25:14,  3.42s/it]
2022-04-08 19:46:50,898 - INFO - tqdm - accuracy: 0.9971, batch_loss: 0.0024, loss: 0.0115 ||:  25%|##5       | 150/590 [08:35<25:37,  3.49s/it]
2022-04-08 19:47:03,239 - INFO - tqdm - accuracy: 0.9970, batch_loss: 0.0004, loss: 0.0117 ||:  26%|##6       | 154/590 [08:48<23:19,  3.21s/it]
2022-04-08 19:47:14,177 - INFO - tqdm - accuracy: 0.9968, batch_loss: 0.0012, loss: 0.0117 ||:  27%|##6       | 157/590 [08:59<25:11,  3.49s/it]
2022-04-08 19:47:24,453 - INFO - tqdm - accuracy: 0.9969, batch_loss: 0.0003, loss: 0.0116 ||:  27%|##7       | 160/590 [09:09<25:09,  3.51s/it]
2022-04-08 19:47:35,131 - INFO - tqdm - accuracy: 0.9969, batch_loss: 0.0004, loss: 0.0114 ||:  28%|##7       | 163/590 [09:20<25:23,  3.57s/it]
2022-04-08 19:47:47,780 - INFO - tqdm - accuracy: 0.9968, batch_loss: 0.0007, loss: 0.0113 ||:  28%|##8       | 167/590 [09:32<22:56,  3.25s/it]
2022-04-08 19:47:57,993 - INFO - tqdm - accuracy: 0.9967, batch_loss: 0.0234, loss: 0.0112 ||:  29%|##8       | 170/590 [09:43<23:46,  3.40s/it]
2022-04-08 19:48:10,763 - INFO - tqdm - accuracy: 0.9966, batch_loss: 0.0383, loss: 0.0112 ||:  29%|##9       | 174/590 [09:55<22:28,  3.24s/it]
2022-04-08 19:48:24,320 - INFO - tqdm - accuracy: 0.9965, batch_loss: 0.0008, loss: 0.0112 ||:  30%|###       | 178/590 [10:09<23:29,  3.42s/it]
2022-04-08 19:48:37,095 - INFO - tqdm - accuracy: 0.9964, batch_loss: 0.0005, loss: 0.0120 ||:  31%|###       | 182/590 [10:22<21:51,  3.21s/it]
2022-04-08 19:48:48,091 - INFO - tqdm - accuracy: 0.9965, batch_loss: 0.0184, loss: 0.0120 ||:  31%|###1      | 185/590 [10:33<24:05,  3.57s/it]
2022-04-08 19:48:58,162 - INFO - tqdm - accuracy: 0.9965, batch_loss: 0.0026, loss: 0.0118 ||:  32%|###1      | 188/590 [10:43<22:53,  3.42s/it]
2022-04-08 19:49:08,744 - INFO - tqdm - accuracy: 0.9966, batch_loss: 0.0008, loss: 0.0116 ||:  32%|###2      | 191/590 [10:53<23:16,  3.50s/it]
2022-04-08 19:49:20,564 - INFO - tqdm - accuracy: 0.9966, batch_loss: 0.0005, loss: 0.0114 ||:  33%|###3      | 195/590 [11:05<20:37,  3.13s/it]
2022-04-08 19:49:32,936 - INFO - tqdm - accuracy: 0.9965, batch_loss: 0.0053, loss: 0.0114 ||:  34%|###3      | 199/590 [11:17<19:49,  3.04s/it]
2022-04-08 19:49:45,901 - INFO - tqdm - accuracy: 0.9965, batch_loss: 0.0003, loss: 0.0122 ||:  34%|###4      | 203/590 [11:30<20:52,  3.24s/it]
2022-04-08 19:49:58,901 - INFO - tqdm - accuracy: 0.9965, batch_loss: 0.0097, loss: 0.0121 ||:  35%|###5      | 207/590 [11:43<20:54,  3.27s/it]
2022-04-08 19:50:12,439 - INFO - tqdm - accuracy: 0.9964, batch_loss: 0.0013, loss: 0.0124 ||:  36%|###5      | 211/590 [11:57<21:37,  3.42s/it]
2022-04-08 19:50:24,954 - INFO - tqdm - accuracy: 0.9964, batch_loss: 0.0037, loss: 0.0125 ||:  36%|###6      | 215/590 [12:10<19:40,  3.15s/it]
2022-04-08 19:50:35,041 - INFO - tqdm - accuracy: 0.9964, batch_loss: 0.0004, loss: 0.0123 ||:  37%|###6      | 218/590 [12:20<20:34,  3.32s/it]
2022-04-08 19:50:46,003 - INFO - tqdm - accuracy: 0.9965, batch_loss: 0.0083, loss: 0.0122 ||:  37%|###7      | 221/590 [12:31<21:33,  3.50s/it]
2022-04-08 19:50:56,191 - INFO - tqdm - accuracy: 0.9964, batch_loss: 0.0046, loss: 0.0122 ||:  38%|###7      | 224/590 [12:41<20:56,  3.43s/it]
2022-04-08 19:51:09,149 - INFO - tqdm - accuracy: 0.9963, batch_loss: 0.0008, loss: 0.0123 ||:  39%|###8      | 228/590 [12:54<20:05,  3.33s/it]
2022-04-08 19:51:21,217 - INFO - tqdm - accuracy: 0.9964, batch_loss: 0.0013, loss: 0.0121 ||:  39%|###9      | 232/590 [13:06<18:17,  3.07s/it]
2022-04-08 19:51:34,124 - INFO - tqdm - accuracy: 0.9964, batch_loss: 0.0005, loss: 0.0119 ||:  40%|####      | 236/590 [13:19<18:50,  3.19s/it]
2022-04-08 19:51:44,879 - INFO - tqdm - accuracy: 0.9963, batch_loss: 0.0005, loss: 0.0119 ||:  41%|####      | 239/590 [13:29<20:12,  3.45s/it]
2022-04-08 19:51:55,536 - INFO - tqdm - accuracy: 0.9964, batch_loss: 0.0099, loss: 0.0118 ||:  41%|####1     | 242/590 [13:40<20:29,  3.53s/it]
2022-04-08 19:52:05,798 - INFO - tqdm - accuracy: 0.9964, batch_loss: 0.0055, loss: 0.0117 ||:  42%|####1     | 245/590 [13:50<20:34,  3.58s/it]
2022-04-08 19:52:18,664 - INFO - tqdm - accuracy: 0.9965, batch_loss: 0.0008, loss: 0.0115 ||:  42%|####2     | 249/590 [14:03<18:23,  3.23s/it]
2022-04-08 19:52:28,711 - INFO - tqdm - accuracy: 0.9965, batch_loss: 0.0004, loss: 0.0114 ||:  43%|####2     | 252/590 [14:13<18:46,  3.33s/it]
2022-04-08 19:52:41,131 - INFO - tqdm - accuracy: 0.9965, batch_loss: 0.0003, loss: 0.0115 ||:  43%|####3     | 256/590 [14:26<17:53,  3.21s/it]
2022-04-08 19:52:51,716 - INFO - tqdm - accuracy: 0.9965, batch_loss: 0.0020, loss: 0.0114 ||:  44%|####3     | 259/590 [14:36<18:44,  3.40s/it]
2022-04-08 19:53:01,961 - INFO - tqdm - accuracy: 0.9965, batch_loss: 0.0041, loss: 0.0113 ||:  44%|####4     | 262/590 [14:47<18:08,  3.32s/it]
2022-04-08 19:53:12,842 - INFO - tqdm - accuracy: 0.9966, batch_loss: 0.0014, loss: 0.0111 ||:  45%|####4     | 265/590 [14:57<19:14,  3.55s/it]
2022-04-08 19:53:22,854 - INFO - tqdm - accuracy: 0.9966, batch_loss: 0.0004, loss: 0.0111 ||:  45%|####5     | 268/590 [15:07<17:53,  3.33s/it]
2022-04-08 19:53:33,769 - INFO - tqdm - accuracy: 0.9967, batch_loss: 0.0003, loss: 0.0110 ||:  46%|####5     | 271/590 [15:18<18:25,  3.46s/it]
2022-04-08 19:53:46,816 - INFO - tqdm - accuracy: 0.9966, batch_loss: 0.0004, loss: 0.0114 ||:  47%|####6     | 275/590 [15:31<17:22,  3.31s/it]
2022-04-08 19:53:57,176 - INFO - tqdm - accuracy: 0.9966, batch_loss: 0.0026, loss: 0.0114 ||:  47%|####7     | 278/590 [15:42<17:36,  3.38s/it]
2022-04-08 19:54:07,704 - INFO - tqdm - accuracy: 0.9966, batch_loss: 0.0360, loss: 0.0114 ||:  48%|####7     | 281/590 [15:52<17:43,  3.44s/it]
2022-04-08 19:54:17,764 - INFO - tqdm - accuracy: 0.9966, batch_loss: 0.0003, loss: 0.0112 ||:  48%|####8     | 284/590 [16:02<17:00,  3.33s/it]
2022-04-08 19:54:29,854 - INFO - tqdm - accuracy: 0.9966, batch_loss: 0.0041, loss: 0.0111 ||:  49%|####8     | 288/590 [16:14<15:29,  3.08s/it]
2022-04-08 19:54:42,917 - INFO - tqdm - accuracy: 0.9964, batch_loss: 0.1003, loss: 0.0117 ||:  49%|####9     | 292/590 [16:27<15:55,  3.21s/it]
2022-04-08 19:54:53,900 - INFO - tqdm - accuracy: 0.9964, batch_loss: 0.0005, loss: 0.0116 ||:  50%|#####     | 295/590 [16:38<17:20,  3.53s/it]
2022-04-08 19:55:04,187 - INFO - tqdm - accuracy: 0.9964, batch_loss: 0.0018, loss: 0.0116 ||:  51%|#####     | 298/590 [16:49<17:08,  3.52s/it]
2022-04-08 19:55:14,664 - INFO - tqdm - accuracy: 0.9965, batch_loss: 0.0004, loss: 0.0115 ||:  51%|#####1    | 301/590 [16:59<16:43,  3.47s/it]
2022-04-08 19:55:26,360 - INFO - tqdm - accuracy: 0.9965, batch_loss: 0.0130, loss: 0.0115 ||:  52%|#####1    | 304/590 [17:11<17:59,  3.78s/it]
2022-04-08 19:55:39,490 - INFO - tqdm - accuracy: 0.9963, batch_loss: 0.1374, loss: 0.0120 ||:  52%|#####2    | 308/590 [17:24<16:03,  3.42s/it]
2022-04-08 19:55:52,667 - INFO - tqdm - accuracy: 0.9964, batch_loss: 0.0009, loss: 0.0119 ||:  53%|#####2    | 312/590 [17:37<15:28,  3.34s/it]
2022-04-08 19:56:02,799 - INFO - tqdm - accuracy: 0.9964, batch_loss: 0.0007, loss: 0.0118 ||:  53%|#####3    | 315/590 [17:47<15:33,  3.39s/it]
2022-04-08 19:56:13,108 - INFO - tqdm - accuracy: 0.9965, batch_loss: 0.0012, loss: 0.0117 ||:  54%|#####3    | 318/590 [17:58<15:32,  3.43s/it]
2022-04-08 19:56:26,486 - INFO - tqdm - accuracy: 0.9965, batch_loss: 0.0011, loss: 0.0116 ||:  55%|#####4    | 322/590 [18:11<15:12,  3.40s/it]
2022-04-08 19:56:37,542 - INFO - tqdm - accuracy: 0.9964, batch_loss: 0.0027, loss: 0.0115 ||:  55%|#####5    | 325/590 [18:22<15:55,  3.60s/it]
2022-04-08 19:56:49,973 - INFO - tqdm - accuracy: 0.9964, batch_loss: 0.0603, loss: 0.0116 ||:  56%|#####5    | 329/590 [18:35<13:48,  3.18s/it]
2022-04-08 19:57:03,026 - INFO - tqdm - accuracy: 0.9964, batch_loss: 0.0203, loss: 0.0116 ||:  56%|#####6    | 333/590 [18:48<13:57,  3.26s/it]
2022-04-08 19:57:13,031 - INFO - tqdm - accuracy: 0.9965, batch_loss: 0.0011, loss: 0.0115 ||:  57%|#####6    | 336/590 [18:58<14:13,  3.36s/it]
2022-04-08 19:57:26,174 - INFO - tqdm - accuracy: 0.9965, batch_loss: 0.0023, loss: 0.0114 ||:  58%|#####7    | 340/590 [19:11<13:57,  3.35s/it]
2022-04-08 19:57:39,714 - INFO - tqdm - accuracy: 0.9965, batch_loss: 0.0003, loss: 0.0116 ||:  58%|#####8    | 344/590 [19:24<14:04,  3.43s/it]
2022-04-08 19:57:52,188 - INFO - tqdm - accuracy: 0.9964, batch_loss: 0.0304, loss: 0.0116 ||:  59%|#####8    | 348/590 [19:37<12:51,  3.19s/it]
2022-04-08 19:58:05,046 - INFO - tqdm - accuracy: 0.9964, batch_loss: 0.0004, loss: 0.0115 ||:  60%|#####9    | 352/590 [19:50<12:45,  3.22s/it]
2022-04-08 19:58:17,993 - INFO - tqdm - accuracy: 0.9965, batch_loss: 0.0005, loss: 0.0114 ||:  60%|######    | 356/590 [20:03<12:28,  3.20s/it]
2022-04-08 19:58:28,749 - INFO - tqdm - accuracy: 0.9965, batch_loss: 0.0002, loss: 0.0113 ||:  61%|######    | 359/590 [20:13<13:19,  3.46s/it]
2022-04-08 19:58:40,362 - INFO - tqdm - accuracy: 0.9965, batch_loss: 0.0002, loss: 0.0112 ||:  61%|######1   | 362/590 [20:25<14:17,  3.76s/it]
2022-04-08 19:58:50,385 - INFO - tqdm - accuracy: 0.9965, batch_loss: 0.0005, loss: 0.0112 ||:  62%|######1   | 365/590 [20:35<12:49,  3.42s/it]
2022-04-08 19:59:00,971 - INFO - tqdm - accuracy: 0.9965, batch_loss: 0.0004, loss: 0.0111 ||:  62%|######2   | 368/590 [20:46<13:03,  3.53s/it]
2022-04-08 19:59:14,404 - INFO - tqdm - accuracy: 0.9965, batch_loss: 0.0014, loss: 0.0111 ||:  63%|######3   | 372/590 [20:59<12:33,  3.46s/it]
2022-04-08 19:59:26,476 - INFO - tqdm - accuracy: 0.9964, batch_loss: 0.0006, loss: 0.0116 ||:  64%|######3   | 375/590 [21:11<13:23,  3.74s/it]
2022-04-08 19:59:37,355 - INFO - tqdm - accuracy: 0.9964, batch_loss: 0.0006, loss: 0.0116 ||:  64%|######4   | 378/590 [21:22<13:03,  3.69s/it]
2022-04-08 19:59:50,432 - INFO - tqdm - accuracy: 0.9963, batch_loss: 0.2870, loss: 0.0122 ||:  65%|######4   | 382/590 [21:35<11:37,  3.35s/it]
2022-04-08 20:00:01,727 - INFO - tqdm - accuracy: 0.9963, batch_loss: 0.0280, loss: 0.0122 ||:  65%|######5   | 385/590 [21:46<12:09,  3.56s/it]
2022-04-08 20:00:14,739 - INFO - tqdm - accuracy: 0.9962, batch_loss: 0.0268, loss: 0.0122 ||:  66%|######5   | 389/590 [21:59<11:00,  3.29s/it]
2022-04-08 20:00:27,880 - INFO - tqdm - accuracy: 0.9962, batch_loss: 0.0003, loss: 0.0123 ||:  67%|######6   | 393/590 [22:12<10:51,  3.31s/it]
2022-04-08 20:00:37,965 - INFO - tqdm - accuracy: 0.9961, batch_loss: 0.0393, loss: 0.0123 ||:  67%|######7   | 396/590 [22:23<10:45,  3.33s/it]
2022-04-08 20:00:51,663 - INFO - tqdm - accuracy: 0.9962, batch_loss: 0.0035, loss: 0.0123 ||:  68%|######7   | 400/590 [22:36<10:56,  3.46s/it]
2022-04-08 20:01:01,836 - INFO - tqdm - accuracy: 0.9962, batch_loss: 0.0022, loss: 0.0122 ||:  68%|######8   | 403/590 [22:46<10:31,  3.38s/it]
2022-04-08 20:01:15,672 - INFO - tqdm - accuracy: 0.9962, batch_loss: 0.0005, loss: 0.0121 ||:  69%|######8   | 407/590 [23:00<11:04,  3.63s/it]
2022-04-08 20:01:26,438 - INFO - tqdm - accuracy: 0.9962, batch_loss: 0.0003, loss: 0.0121 ||:  69%|######9   | 410/590 [23:11<10:50,  3.62s/it]
2022-04-08 20:01:39,620 - INFO - tqdm - accuracy: 0.9961, batch_loss: 0.0028, loss: 0.0124 ||:  70%|#######   | 414/590 [23:24<09:51,  3.36s/it]
2022-04-08 20:01:52,087 - INFO - tqdm - accuracy: 0.9960, batch_loss: 0.0698, loss: 0.0125 ||:  71%|#######   | 418/590 [23:37<09:02,  3.15s/it]
2022-04-08 20:02:02,870 - INFO - tqdm - accuracy: 0.9960, batch_loss: 0.0009, loss: 0.0127 ||:  71%|#######1  | 421/590 [23:47<09:37,  3.42s/it]
2022-04-08 20:02:15,339 - INFO - tqdm - accuracy: 0.9960, batch_loss: 0.1475, loss: 0.0130 ||:  72%|#######2  | 425/590 [24:00<08:56,  3.25s/it]
2022-04-08 20:02:25,831 - INFO - tqdm - accuracy: 0.9960, batch_loss: 0.0023, loss: 0.0129 ||:  73%|#######2  | 428/590 [24:10<09:15,  3.43s/it]
2022-04-08 20:02:36,591 - INFO - tqdm - accuracy: 0.9959, batch_loss: 0.0027, loss: 0.0130 ||:  73%|#######3  | 431/590 [24:21<09:27,  3.57s/it]
2022-04-08 20:02:50,873 - INFO - tqdm - accuracy: 0.9958, batch_loss: 0.0366, loss: 0.0131 ||:  74%|#######3  | 435/590 [24:35<09:30,  3.68s/it]
2022-04-08 20:03:04,087 - INFO - tqdm - accuracy: 0.9958, batch_loss: 0.0010, loss: 0.0131 ||:  74%|#######4  | 439/590 [24:49<08:45,  3.48s/it]
2022-04-08 20:03:15,260 - INFO - tqdm - accuracy: 0.9958, batch_loss: 0.0006, loss: 0.0130 ||:  75%|#######4  | 442/590 [25:00<09:07,  3.70s/it]
2022-04-08 20:03:25,284 - INFO - tqdm - accuracy: 0.9958, batch_loss: 0.0020, loss: 0.0131 ||:  75%|#######5  | 445/590 [25:10<08:20,  3.45s/it]
2022-04-08 20:03:35,695 - INFO - tqdm - accuracy: 0.9958, batch_loss: 0.0009, loss: 0.0130 ||:  76%|#######5  | 448/590 [25:20<08:13,  3.48s/it]
2022-04-08 20:03:48,758 - INFO - tqdm - accuracy: 0.9958, batch_loss: 0.0856, loss: 0.0131 ||:  77%|#######6  | 452/590 [25:33<07:33,  3.29s/it]
2022-04-08 20:04:01,207 - INFO - tqdm - accuracy: 0.9958, batch_loss: 0.0021, loss: 0.0133 ||:  77%|#######7  | 456/590 [25:46<07:11,  3.22s/it]
2022-04-08 20:04:14,384 - INFO - tqdm - accuracy: 0.9957, batch_loss: 0.0008, loss: 0.0136 ||:  78%|#######7  | 460/590 [25:59<07:06,  3.28s/it]
2022-04-08 20:04:24,869 - INFO - tqdm - accuracy: 0.9957, batch_loss: 0.1016, loss: 0.0138 ||:  78%|#######8  | 463/590 [26:09<07:25,  3.51s/it]
2022-04-08 20:04:38,464 - INFO - tqdm - accuracy: 0.9957, batch_loss: 0.0020, loss: 0.0137 ||:  79%|#######9  | 467/590 [26:23<07:05,  3.46s/it]
2022-04-08 20:04:48,682 - INFO - tqdm - accuracy: 0.9957, batch_loss: 0.0005, loss: 0.0136 ||:  80%|#######9  | 470/590 [26:33<06:48,  3.41s/it]
2022-04-08 20:05:01,017 - INFO - tqdm - accuracy: 0.9958, batch_loss: 0.0050, loss: 0.0135 ||:  80%|########  | 474/590 [26:46<05:55,  3.07s/it]
2022-04-08 20:05:11,341 - INFO - tqdm - accuracy: 0.9958, batch_loss: 0.0033, loss: 0.0135 ||:  81%|########  | 477/590 [26:56<06:15,  3.32s/it]
2022-04-08 20:05:21,746 - INFO - tqdm - accuracy: 0.9957, batch_loss: 0.0278, loss: 0.0136 ||:  81%|########1 | 480/590 [27:06<06:17,  3.43s/it]
2022-04-08 20:05:34,883 - INFO - tqdm - accuracy: 0.9957, batch_loss: 0.1098, loss: 0.0137 ||:  82%|########2 | 484/590 [27:19<05:56,  3.37s/it]
2022-04-08 20:05:45,096 - INFO - tqdm - accuracy: 0.9956, batch_loss: 0.0955, loss: 0.0139 ||:  83%|########2 | 487/590 [27:30<05:46,  3.37s/it]
2022-04-08 20:05:55,471 - INFO - tqdm - accuracy: 0.9955, batch_loss: 0.0014, loss: 0.0142 ||:  83%|########3 | 490/590 [27:40<05:44,  3.45s/it]
2022-04-08 20:06:05,777 - INFO - tqdm - accuracy: 0.9956, batch_loss: 0.0017, loss: 0.0141 ||:  84%|########3 | 493/590 [27:50<05:28,  3.39s/it]
2022-04-08 20:06:17,152 - INFO - tqdm - accuracy: 0.9956, batch_loss: 0.0057, loss: 0.0140 ||:  84%|########4 | 496/590 [28:02<05:41,  3.63s/it]
2022-04-08 20:06:30,504 - INFO - tqdm - accuracy: 0.9956, batch_loss: 0.0006, loss: 0.0140 ||:  85%|########4 | 500/590 [28:15<05:11,  3.46s/it]
2022-04-08 20:06:43,886 - INFO - tqdm - accuracy: 0.9956, batch_loss: 0.0007, loss: 0.0139 ||:  85%|########5 | 504/590 [28:28<04:48,  3.36s/it]
2022-04-08 20:06:53,899 - INFO - tqdm - accuracy: 0.9956, batch_loss: 0.0015, loss: 0.0139 ||:  86%|########5 | 507/590 [28:38<04:40,  3.38s/it]
2022-04-08 20:07:07,152 - INFO - tqdm - accuracy: 0.9956, batch_loss: 0.0304, loss: 0.0138 ||:  87%|########6 | 511/590 [28:52<04:24,  3.35s/it]
2022-04-08 20:07:19,940 - INFO - tqdm - accuracy: 0.9956, batch_loss: 0.0006, loss: 0.0139 ||:  87%|########7 | 515/590 [29:05<04:07,  3.30s/it]
2022-04-08 20:07:33,555 - INFO - tqdm - accuracy: 0.9955, batch_loss: 0.0021, loss: 0.0140 ||:  88%|########7 | 519/590 [29:18<04:06,  3.47s/it]
2022-04-08 20:07:43,911 - INFO - tqdm - accuracy: 0.9955, batch_loss: 0.0035, loss: 0.0141 ||:  88%|########8 | 522/590 [29:28<03:51,  3.41s/it]
2022-04-08 20:07:57,306 - INFO - tqdm - accuracy: 0.9955, batch_loss: 0.0196, loss: 0.0140 ||:  89%|########9 | 526/590 [29:42<03:37,  3.40s/it]
2022-04-08 20:08:10,799 - INFO - tqdm - accuracy: 0.9954, batch_loss: 0.1277, loss: 0.0147 ||:  90%|########9 | 530/590 [29:55<03:24,  3.40s/it]
2022-04-08 20:08:21,349 - INFO - tqdm - accuracy: 0.9954, batch_loss: 0.0009, loss: 0.0147 ||:  90%|######### | 533/590 [30:06<03:19,  3.51s/it]
2022-04-08 20:08:31,674 - INFO - tqdm - accuracy: 0.9955, batch_loss: 0.0024, loss: 0.0146 ||:  91%|######### | 536/590 [30:16<03:08,  3.49s/it]
2022-04-08 20:08:42,047 - INFO - tqdm - accuracy: 0.9955, batch_loss: 0.0010, loss: 0.0145 ||:  91%|#########1| 539/590 [30:27<02:57,  3.48s/it]
2022-04-08 20:08:54,595 - INFO - tqdm - accuracy: 0.9955, batch_loss: 0.0123, loss: 0.0147 ||:  92%|#########2| 543/590 [30:39<02:30,  3.20s/it]
2022-04-08 20:09:07,817 - INFO - tqdm - accuracy: 0.9954, batch_loss: 0.0390, loss: 0.0148 ||:  93%|#########2| 547/590 [30:52<02:18,  3.23s/it]
2022-04-08 20:09:20,770 - INFO - tqdm - accuracy: 0.9953, batch_loss: 0.0377, loss: 0.0148 ||:  93%|#########3| 551/590 [31:05<02:04,  3.20s/it]
2022-04-08 20:09:30,998 - INFO - tqdm - accuracy: 0.9953, batch_loss: 0.0057, loss: 0.0148 ||:  94%|#########3| 554/590 [31:16<01:57,  3.27s/it]
2022-04-08 20:09:43,906 - INFO - tqdm - accuracy: 0.9954, batch_loss: 0.0035, loss: 0.0148 ||:  95%|#########4| 558/590 [31:28<01:42,  3.19s/it]
2022-04-08 20:09:56,232 - INFO - tqdm - accuracy: 0.9954, batch_loss: 0.0034, loss: 0.0147 ||:  95%|#########5| 562/590 [31:41<01:26,  3.10s/it]
2022-04-08 20:10:09,788 - INFO - tqdm - accuracy: 0.9954, batch_loss: 0.0012, loss: 0.0146 ||:  96%|#########5| 566/590 [31:54<01:21,  3.38s/it]
2022-04-08 20:10:22,218 - INFO - tqdm - accuracy: 0.9954, batch_loss: 0.0031, loss: 0.0145 ||:  97%|#########6| 570/590 [32:07<01:04,  3.20s/it]
2022-04-08 20:10:32,274 - INFO - tqdm - accuracy: 0.9955, batch_loss: 0.0085, loss: 0.0145 ||:  97%|#########7| 573/590 [32:17<00:56,  3.33s/it]
2022-04-08 20:10:42,777 - INFO - tqdm - accuracy: 0.9954, batch_loss: 0.0170, loss: 0.0146 ||:  98%|#########7| 576/590 [32:27<00:48,  3.46s/it]
2022-04-08 20:10:55,072 - INFO - tqdm - accuracy: 0.9955, batch_loss: 0.0007, loss: 0.0145 ||:  98%|#########8| 580/590 [32:40<00:31,  3.19s/it]
2022-04-08 20:11:08,757 - INFO - tqdm - accuracy: 0.9955, batch_loss: 0.0050, loss: 0.0144 ||:  99%|#########8| 584/590 [32:53<00:20,  3.46s/it]
2022-04-08 20:11:19,020 - INFO - tqdm - accuracy: 0.9955, batch_loss: 0.0021, loss: 0.0145 ||:  99%|#########9| 587/590 [33:04<00:10,  3.44s/it]
2022-04-08 20:11:22,853 - INFO - tqdm - accuracy: 0.9955, batch_loss: 0.0014, loss: 0.0145 ||: 100%|#########9| 588/590 [33:07<00:07,  3.55s/it]
2022-04-08 20:11:26,155 - INFO - tqdm - accuracy: 0.9955, batch_loss: 0.0013, loss: 0.0145 ||: 100%|#########9| 589/590 [33:11<00:03,  3.48s/it]
2022-04-08 20:11:26,990 - INFO - tqdm - accuracy: 0.9955, batch_loss: 0.0003, loss: 0.0145 ||: 100%|##########| 590/590 [33:12<00:00,  2.69s/it]
2022-04-08 20:11:26,991 - INFO - tqdm - accuracy: 0.9955, batch_loss: 0.0003, loss: 0.0145 ||: 100%|##########| 590/590 [33:12<00:00,  3.38s/it]
2022-04-08 20:11:29,279 - INFO - allennlp.training.trainer - Validating
2022-04-08 20:11:29,280 - INFO - tqdm - 0%|          | 0/1635 [00:00<?, ?it/s]
2022-04-08 20:11:39,391 - INFO - tqdm - accuracy: 0.8297, batch_loss: 3.4842, loss: 0.9485 ||:   8%|8         | 138/1635 [00:10<01:37, 15.34it/s]
2022-04-08 20:11:49,620 - INFO - tqdm - accuracy: 0.8536, batch_loss: 0.0001, loss: 0.8442 ||:  17%|#7        | 280/1635 [00:20<02:07, 10.66it/s]
2022-04-08 20:11:59,713 - INFO - tqdm - accuracy: 0.8568, batch_loss: 0.0001, loss: 0.8218 ||:  26%|##6       | 426/1635 [00:30<01:21, 14.81it/s]
2022-04-08 20:12:09,735 - INFO - tqdm - accuracy: 0.8580, batch_loss: 3.5005, loss: 0.8258 ||:  34%|###4      | 560/1635 [00:40<01:08, 15.63it/s]
2022-04-08 20:12:19,880 - INFO - tqdm - accuracy: 0.8546, batch_loss: 0.0001, loss: 0.8577 ||:  43%|####3     | 705/1635 [00:50<01:07, 13.86it/s]
2022-04-08 20:12:29,964 - INFO - tqdm - accuracy: 0.8546, batch_loss: 2.5336, loss: 0.8629 ||:  52%|#####1    | 846/1635 [01:00<00:50, 15.55it/s]
2022-04-08 20:12:40,042 - INFO - tqdm - accuracy: 0.8581, batch_loss: 1.3654, loss: 0.8203 ||:  60%|######    | 983/1635 [01:10<00:37, 17.31it/s]
2022-04-08 20:12:50,077 - INFO - tqdm - accuracy: 0.8566, batch_loss: 0.0001, loss: 0.8311 ||:  69%|######8   | 1126/1635 [01:20<00:33, 15.01it/s]
2022-04-08 20:13:00,175 - INFO - tqdm - accuracy: 0.8514, batch_loss: 0.0002, loss: 0.8753 ||:  78%|#######7  | 1272/1635 [01:30<00:26, 13.86it/s]
2022-04-08 20:13:10,208 - INFO - tqdm - accuracy: 0.8530, batch_loss: 0.0001, loss: 0.8533 ||:  86%|########6 | 1412/1635 [01:40<00:17, 12.91it/s]
2022-04-08 20:13:20,365 - INFO - tqdm - accuracy: 0.8538, batch_loss: 4.5939, loss: 0.8484 ||:  95%|#########5| 1559/1635 [01:51<00:05, 14.37it/s]
2022-04-08 20:13:25,190 - INFO - tqdm - accuracy: 0.8504, batch_loss: 0.6107, loss: 0.8641 ||: 100%|#########9| 1628/1635 [01:55<00:00, 10.48it/s]
2022-04-08 20:13:25,315 - INFO - tqdm - accuracy: 0.8503, batch_loss: 2.5738, loss: 0.8646 ||: 100%|#########9| 1630/1635 [01:56<00:00, 11.66it/s]
2022-04-08 20:13:25,449 - INFO - tqdm - accuracy: 0.8502, batch_loss: 4.7374, loss: 0.8665 ||: 100%|#########9| 1632/1635 [01:56<00:00, 12.46it/s]
2022-04-08 20:13:25,632 - INFO - tqdm - accuracy: 0.8504, batch_loss: 0.0006, loss: 0.8654 ||: 100%|#########9| 1634/1635 [01:56<00:00, 11.97it/s]
2022-04-08 20:13:25,688 - INFO - tqdm - accuracy: 0.8502, batch_loss: 2.9455, loss: 0.8667 ||: 100%|##########| 1635/1635 [01:56<00:00, 14.05it/s]
2022-04-08 20:13:25,688 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-08 20:13:25,689 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.995  |     0.850
2022-04-08 20:13:25,689 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9604.118  |       N/A
2022-04-08 20:13:25,690 - INFO - allennlp.training.tensorboard_writer - loss               |     0.014  |     0.867
2022-04-08 20:13:25,691 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  7239.301  |       N/A
2022-04-08 20:13:30,189 - INFO - allennlp.training.trainer - Epoch duration: 0:35:15.254064
2022-04-08 20:13:30,189 - INFO - allennlp.training.trainer - Estimated training time remaining: 1:10:32
2022-04-08 20:13:30,189 - INFO - allennlp.training.trainer - Epoch 8/9
2022-04-08 20:13:30,190 - INFO - allennlp.training.trainer - Worker 0 memory usage: 7.1G
2022-04-08 20:13:30,190 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-08 20:13:30,191 - INFO - allennlp.training.trainer - Training
2022-04-08 20:13:30,192 - INFO - tqdm - 0%|          | 0/590 [00:00<?, ?it/s]
2022-04-08 20:13:41,843 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0011, loss: 0.0009 ||:   1%|          | 4/590 [00:11<29:00,  2.97s/it]
2022-04-08 20:13:54,224 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0085, loss: 0.0019 ||:   1%|1         | 8/590 [00:24<29:24,  3.03s/it]
2022-04-08 20:14:04,246 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0122, loss: 0.0035 ||:   2%|1         | 11/590 [00:34<30:44,  3.19s/it]
2022-04-08 20:14:16,273 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0010, loss: 0.0034 ||:   3%|2         | 15/590 [00:46<29:09,  3.04s/it]
2022-04-08 20:14:26,461 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0194, loss: 0.0040 ||:   3%|3         | 18/590 [00:56<30:55,  3.24s/it]
2022-04-08 20:14:37,196 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0007, loss: 0.0050 ||:   4%|3         | 21/590 [01:07<33:12,  3.50s/it]
2022-04-08 20:14:49,188 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0115, loss: 0.0048 ||:   4%|4         | 25/590 [01:18<29:14,  3.11s/it]
2022-04-08 20:15:02,325 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0005, loss: 0.0044 ||:   5%|4         | 29/590 [01:32<30:11,  3.23s/it]
2022-04-08 20:15:15,190 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0063, loss: 0.0048 ||:   6%|5         | 33/590 [01:44<29:57,  3.23s/it]
2022-04-08 20:15:25,948 - INFO - tqdm - accuracy: 0.9974, batch_loss: 0.1255, loss: 0.0094 ||:   6%|6         | 36/590 [01:55<32:05,  3.48s/it]
2022-04-08 20:15:38,543 - INFO - tqdm - accuracy: 0.9977, batch_loss: 0.0022, loss: 0.0086 ||:   7%|6         | 40/590 [02:08<29:48,  3.25s/it]
2022-04-08 20:15:51,750 - INFO - tqdm - accuracy: 0.9979, batch_loss: 0.0056, loss: 0.0081 ||:   7%|7         | 44/590 [02:21<29:53,  3.28s/it]
2022-04-08 20:16:04,924 - INFO - tqdm - accuracy: 0.9980, batch_loss: 0.0008, loss: 0.0079 ||:   8%|8         | 48/590 [02:34<30:24,  3.37s/it]
2022-04-08 20:16:15,550 - INFO - tqdm - accuracy: 0.9982, batch_loss: 0.0003, loss: 0.0075 ||:   9%|8         | 51/590 [02:45<31:01,  3.45s/it]
2022-04-08 20:16:29,034 - INFO - tqdm - accuracy: 0.9977, batch_loss: 0.0010, loss: 0.0085 ||:   9%|9         | 55/590 [02:58<31:19,  3.51s/it]
2022-04-08 20:16:42,024 - INFO - tqdm - accuracy: 0.9979, batch_loss: 0.0027, loss: 0.0080 ||:  10%|#         | 59/590 [03:11<29:46,  3.36s/it]
2022-04-08 20:16:52,489 - INFO - tqdm - accuracy: 0.9980, batch_loss: 0.0003, loss: 0.0077 ||:  11%|#         | 62/590 [03:22<30:23,  3.45s/it]
2022-04-08 20:17:05,114 - INFO - tqdm - accuracy: 0.9981, batch_loss: 0.0014, loss: 0.0073 ||:  11%|#1        | 66/590 [03:34<28:28,  3.26s/it]
2022-04-08 20:17:18,454 - INFO - tqdm - accuracy: 0.9982, batch_loss: 0.0092, loss: 0.0070 ||:  12%|#1        | 70/590 [03:48<29:19,  3.38s/it]
2022-04-08 20:17:29,110 - INFO - tqdm - accuracy: 0.9983, batch_loss: 0.0004, loss: 0.0069 ||:  12%|#2        | 73/590 [03:58<30:20,  3.52s/it]
2022-04-08 20:17:39,315 - INFO - tqdm - accuracy: 0.9979, batch_loss: 0.0004, loss: 0.0088 ||:  13%|#2        | 76/590 [04:09<29:20,  3.42s/it]
2022-04-08 20:17:53,006 - INFO - tqdm - accuracy: 0.9980, batch_loss: 0.0025, loss: 0.0086 ||:  14%|#3        | 80/590 [04:22<29:53,  3.52s/it]
2022-04-08 20:18:05,714 - INFO - tqdm - accuracy: 0.9981, batch_loss: 0.0004, loss: 0.0082 ||:  14%|#4        | 84/590 [04:35<27:36,  3.27s/it]
2022-04-08 20:18:16,400 - INFO - tqdm - accuracy: 0.9982, batch_loss: 0.0021, loss: 0.0080 ||:  15%|#4        | 87/590 [04:46<28:26,  3.39s/it]
2022-04-08 20:18:29,155 - INFO - tqdm - accuracy: 0.9983, batch_loss: 0.0004, loss: 0.0077 ||:  15%|#5        | 91/590 [04:58<27:15,  3.28s/it]
2022-04-08 20:18:39,408 - INFO - tqdm - accuracy: 0.9980, batch_loss: 0.0478, loss: 0.0080 ||:  16%|#5        | 94/590 [05:09<27:42,  3.35s/it]
2022-04-08 20:18:49,928 - INFO - tqdm - accuracy: 0.9981, batch_loss: 0.0129, loss: 0.0081 ||:  16%|#6        | 97/590 [05:19<28:43,  3.50s/it]
2022-04-08 20:19:02,186 - INFO - tqdm - accuracy: 0.9981, batch_loss: 0.0003, loss: 0.0078 ||:  17%|#7        | 101/590 [05:31<25:48,  3.17s/it]
2022-04-08 20:19:12,259 - INFO - tqdm - accuracy: 0.9982, batch_loss: 0.0088, loss: 0.0077 ||:  18%|#7        | 104/590 [05:42<26:33,  3.28s/it]
2022-04-08 20:19:22,602 - INFO - tqdm - accuracy: 0.9982, batch_loss: 0.0008, loss: 0.0076 ||:  18%|#8        | 107/590 [05:52<26:43,  3.32s/it]
2022-04-08 20:19:35,491 - INFO - tqdm - accuracy: 0.9983, batch_loss: 0.0004, loss: 0.0073 ||:  19%|#8        | 111/590 [06:05<25:40,  3.22s/it]
2022-04-08 20:19:46,014 - INFO - tqdm - accuracy: 0.9984, batch_loss: 0.0008, loss: 0.0072 ||:  19%|#9        | 114/590 [06:15<27:32,  3.47s/it]
2022-04-08 20:19:57,307 - INFO - tqdm - accuracy: 0.9984, batch_loss: 0.0017, loss: 0.0071 ||:  20%|#9        | 117/590 [06:27<28:45,  3.65s/it]
2022-04-08 20:20:10,041 - INFO - tqdm - accuracy: 0.9982, batch_loss: 0.0050, loss: 0.0072 ||:  21%|##        | 121/590 [06:39<25:39,  3.28s/it]
2022-04-08 20:20:20,632 - INFO - tqdm - accuracy: 0.9982, batch_loss: 0.0022, loss: 0.0071 ||:  21%|##1       | 124/590 [06:50<26:56,  3.47s/it]
2022-04-08 20:20:31,553 - INFO - tqdm - accuracy: 0.9983, batch_loss: 0.0022, loss: 0.0069 ||:  22%|##1       | 127/590 [07:01<27:20,  3.54s/it]
2022-04-08 20:20:45,141 - INFO - tqdm - accuracy: 0.9981, batch_loss: 0.0011, loss: 0.0075 ||:  22%|##2       | 131/590 [07:14<26:36,  3.48s/it]
2022-04-08 20:20:55,556 - INFO - tqdm - accuracy: 0.9981, batch_loss: 0.0018, loss: 0.0074 ||:  23%|##2       | 134/590 [07:25<26:19,  3.46s/it]
2022-04-08 20:21:06,776 - INFO - tqdm - accuracy: 0.9979, batch_loss: 0.0002, loss: 0.0077 ||:  23%|##3       | 137/590 [07:36<27:27,  3.64s/it]
2022-04-08 20:21:19,009 - INFO - tqdm - accuracy: 0.9980, batch_loss: 0.0005, loss: 0.0076 ||:  24%|##3       | 141/590 [07:48<23:37,  3.16s/it]
2022-04-08 20:21:32,163 - INFO - tqdm - accuracy: 0.9978, batch_loss: 0.0338, loss: 0.0077 ||:  25%|##4       | 145/590 [08:01<24:21,  3.28s/it]
2022-04-08 20:21:45,305 - INFO - tqdm - accuracy: 0.9977, batch_loss: 0.0005, loss: 0.0086 ||:  25%|##5       | 149/590 [08:15<24:40,  3.36s/it]
2022-04-08 20:21:57,889 - INFO - tqdm - accuracy: 0.9978, batch_loss: 0.0002, loss: 0.0084 ||:  26%|##5       | 153/590 [08:27<22:59,  3.16s/it]
2022-04-08 20:22:11,216 - INFO - tqdm - accuracy: 0.9970, batch_loss: 0.0563, loss: 0.0101 ||:  27%|##6       | 157/590 [08:41<23:37,  3.27s/it]
2022-04-08 20:22:23,711 - INFO - tqdm - accuracy: 0.9971, batch_loss: 0.0067, loss: 0.0099 ||:  27%|##7       | 161/590 [08:53<22:39,  3.17s/it]
2022-04-08 20:22:35,100 - INFO - tqdm - accuracy: 0.9970, batch_loss: 0.0003, loss: 0.0100 ||:  28%|##7       | 164/590 [09:04<25:25,  3.58s/it]
2022-04-08 20:22:45,289 - INFO - tqdm - accuracy: 0.9968, batch_loss: 0.0006, loss: 0.0103 ||:  28%|##8       | 167/590 [09:15<24:21,  3.46s/it]
2022-04-08 20:22:59,054 - INFO - tqdm - accuracy: 0.9967, batch_loss: 0.0008, loss: 0.0103 ||:  29%|##8       | 171/590 [09:28<24:30,  3.51s/it]
2022-04-08 20:23:09,306 - INFO - tqdm - accuracy: 0.9968, batch_loss: 0.0067, loss: 0.0102 ||:  29%|##9       | 174/590 [09:39<23:31,  3.39s/it]
2022-04-08 20:23:19,417 - INFO - tqdm - accuracy: 0.9968, batch_loss: 0.0004, loss: 0.0101 ||:  30%|###       | 177/590 [09:49<22:58,  3.34s/it]
2022-04-08 20:23:30,867 - INFO - tqdm - accuracy: 0.9969, batch_loss: 0.0033, loss: 0.0100 ||:  31%|###       | 180/590 [10:00<25:08,  3.68s/it]
2022-04-08 20:23:42,114 - INFO - tqdm - accuracy: 0.9969, batch_loss: 0.0023, loss: 0.0099 ||:  31%|###1      | 183/590 [10:11<24:54,  3.67s/it]
2022-04-08 20:23:54,608 - INFO - tqdm - accuracy: 0.9970, batch_loss: 0.0003, loss: 0.0098 ||:  32%|###1      | 187/590 [10:24<22:51,  3.40s/it]
2022-04-08 20:24:07,423 - INFO - tqdm - accuracy: 0.9969, batch_loss: 0.0398, loss: 0.0098 ||:  32%|###2      | 191/590 [10:37<21:47,  3.28s/it]
2022-04-08 20:24:18,319 - INFO - tqdm - accuracy: 0.9969, batch_loss: 0.0005, loss: 0.0097 ||:  33%|###2      | 194/590 [10:48<23:16,  3.53s/it]
2022-04-08 20:24:28,585 - INFO - tqdm - accuracy: 0.9970, batch_loss: 0.0007, loss: 0.0096 ||:  33%|###3      | 197/590 [10:58<22:41,  3.46s/it]
2022-04-08 20:24:41,861 - INFO - tqdm - accuracy: 0.9969, batch_loss: 0.0007, loss: 0.0096 ||:  34%|###4      | 201/590 [11:11<21:52,  3.38s/it]
2022-04-08 20:24:55,221 - INFO - tqdm - accuracy: 0.9970, batch_loss: 0.0005, loss: 0.0095 ||:  35%|###4      | 205/590 [11:25<21:52,  3.41s/it]
2022-04-08 20:25:08,280 - INFO - tqdm - accuracy: 0.9970, batch_loss: 0.0035, loss: 0.0093 ||:  35%|###5      | 209/590 [11:38<20:59,  3.31s/it]
2022-04-08 20:25:18,456 - INFO - tqdm - accuracy: 0.9968, batch_loss: 0.0948, loss: 0.0101 ||:  36%|###5      | 212/590 [11:48<21:06,  3.35s/it]
2022-04-08 20:25:29,619 - INFO - tqdm - accuracy: 0.9968, batch_loss: 0.0135, loss: 0.0100 ||:  36%|###6      | 215/590 [11:59<22:25,  3.59s/it]
2022-04-08 20:25:42,186 - INFO - tqdm - accuracy: 0.9969, batch_loss: 0.0002, loss: 0.0098 ||:  37%|###7      | 219/590 [12:11<20:10,  3.26s/it]
2022-04-08 20:25:52,267 - INFO - tqdm - accuracy: 0.9969, batch_loss: 0.0014, loss: 0.0097 ||:  38%|###7      | 222/590 [12:22<20:30,  3.34s/it]
2022-04-08 20:26:03,316 - INFO - tqdm - accuracy: 0.9968, batch_loss: 0.0003, loss: 0.0097 ||:  38%|###8      | 225/590 [12:33<21:40,  3.56s/it]
2022-04-08 20:26:13,483 - INFO - tqdm - accuracy: 0.9968, batch_loss: 0.0003, loss: 0.0096 ||:  39%|###8      | 228/590 [12:43<20:27,  3.39s/it]
2022-04-08 20:26:23,533 - INFO - tqdm - accuracy: 0.9969, batch_loss: 0.0003, loss: 0.0095 ||:  39%|###9      | 231/590 [12:53<20:15,  3.39s/it]
2022-04-08 20:26:37,891 - INFO - tqdm - accuracy: 0.9969, batch_loss: 0.0041, loss: 0.0094 ||:  40%|###9      | 235/590 [13:07<22:08,  3.74s/it]
2022-04-08 20:26:48,383 - INFO - tqdm - accuracy: 0.9970, batch_loss: 0.0002, loss: 0.0093 ||:  40%|####      | 238/590 [13:18<21:13,  3.62s/it]
2022-04-08 20:27:02,155 - INFO - tqdm - accuracy: 0.9970, batch_loss: 0.0016, loss: 0.0092 ||:  41%|####1     | 242/590 [13:31<20:28,  3.53s/it]
2022-04-08 20:27:14,581 - INFO - tqdm - accuracy: 0.9971, batch_loss: 0.0004, loss: 0.0091 ||:  42%|####1     | 246/590 [13:44<18:00,  3.14s/it]
2022-04-08 20:27:26,929 - INFO - tqdm - accuracy: 0.9971, batch_loss: 0.0058, loss: 0.0090 ||:  42%|####2     | 250/590 [13:56<17:48,  3.14s/it]
2022-04-08 20:27:37,274 - INFO - tqdm - accuracy: 0.9972, batch_loss: 0.0017, loss: 0.0089 ||:  43%|####2     | 253/590 [14:07<18:29,  3.29s/it]
2022-04-08 20:27:47,680 - INFO - tqdm - accuracy: 0.9971, batch_loss: 0.0503, loss: 0.0090 ||:  43%|####3     | 256/590 [14:17<19:04,  3.43s/it]
2022-04-08 20:28:01,090 - INFO - tqdm - accuracy: 0.9971, batch_loss: 0.0002, loss: 0.0088 ||:  44%|####4     | 260/590 [14:30<18:43,  3.40s/it]
2022-04-08 20:28:11,832 - INFO - tqdm - accuracy: 0.9971, batch_loss: 0.0003, loss: 0.0088 ||:  45%|####4     | 263/590 [14:41<19:01,  3.49s/it]
2022-04-08 20:28:21,923 - INFO - tqdm - accuracy: 0.9971, batch_loss: 0.0005, loss: 0.0088 ||:  45%|####5     | 266/590 [14:51<18:23,  3.41s/it]
2022-04-08 20:28:32,601 - INFO - tqdm - accuracy: 0.9970, batch_loss: 0.0003, loss: 0.0090 ||:  46%|####5     | 269/590 [15:02<19:04,  3.57s/it]
2022-04-08 20:28:45,185 - INFO - tqdm - accuracy: 0.9970, batch_loss: 0.0013, loss: 0.0089 ||:  46%|####6     | 273/590 [15:14<17:22,  3.29s/it]
2022-04-08 20:28:58,337 - INFO - tqdm - accuracy: 0.9970, batch_loss: 0.0002, loss: 0.0090 ||:  47%|####6     | 277/590 [15:28<16:48,  3.22s/it]
2022-04-08 20:29:09,295 - INFO - tqdm - accuracy: 0.9969, batch_loss: 0.0072, loss: 0.0092 ||:  47%|####7     | 280/590 [15:39<18:20,  3.55s/it]
2022-04-08 20:29:22,122 - INFO - tqdm - accuracy: 0.9969, batch_loss: 0.0011, loss: 0.0091 ||:  48%|####8     | 284/590 [15:51<16:48,  3.30s/it]
2022-04-08 20:29:34,956 - INFO - tqdm - accuracy: 0.9969, batch_loss: 0.0430, loss: 0.0092 ||:  49%|####8     | 288/590 [16:04<15:55,  3.16s/it]
2022-04-08 20:29:48,078 - INFO - tqdm - accuracy: 0.9967, batch_loss: 0.0005, loss: 0.0097 ||:  49%|####9     | 292/590 [16:17<15:53,  3.20s/it]
2022-04-08 20:30:01,533 - INFO - tqdm - accuracy: 0.9967, batch_loss: 0.0006, loss: 0.0096 ||:  50%|#####     | 296/590 [16:31<16:25,  3.35s/it]
2022-04-08 20:30:12,258 - INFO - tqdm - accuracy: 0.9968, batch_loss: 0.0008, loss: 0.0095 ||:  51%|#####     | 299/590 [16:42<16:43,  3.45s/it]
2022-04-08 20:30:26,007 - INFO - tqdm - accuracy: 0.9968, batch_loss: 0.0014, loss: 0.0094 ||:  51%|#####1    | 303/590 [16:55<16:42,  3.49s/it]
2022-04-08 20:30:40,277 - INFO - tqdm - accuracy: 0.9968, batch_loss: 0.0006, loss: 0.0093 ||:  52%|#####2    | 307/590 [17:10<17:19,  3.67s/it]
2022-04-08 20:30:51,480 - INFO - tqdm - accuracy: 0.9969, batch_loss: 0.0004, loss: 0.0092 ||:  53%|#####2    | 310/590 [17:21<17:38,  3.78s/it]
2022-04-08 20:31:02,315 - INFO - tqdm - accuracy: 0.9969, batch_loss: 0.0008, loss: 0.0092 ||:  53%|#####3    | 313/590 [17:32<16:50,  3.65s/it]
2022-04-08 20:31:13,924 - INFO - tqdm - accuracy: 0.9969, batch_loss: 0.0003, loss: 0.0091 ||:  54%|#####3    | 317/590 [17:43<13:58,  3.07s/it]
2022-04-08 20:31:24,262 - INFO - tqdm - accuracy: 0.9970, batch_loss: 0.0008, loss: 0.0091 ||:  54%|#####4    | 320/590 [17:54<14:51,  3.30s/it]
2022-04-08 20:31:37,414 - INFO - tqdm - accuracy: 0.9970, batch_loss: 0.0004, loss: 0.0090 ||:  55%|#####4    | 324/590 [18:07<14:28,  3.27s/it]
2022-04-08 20:31:48,126 - INFO - tqdm - accuracy: 0.9970, batch_loss: 0.0002, loss: 0.0090 ||:  55%|#####5    | 327/590 [18:17<15:13,  3.47s/it]
2022-04-08 20:32:00,847 - INFO - tqdm - accuracy: 0.9970, batch_loss: 0.0651, loss: 0.0090 ||:  56%|#####6    | 331/590 [18:30<14:16,  3.31s/it]
2022-04-08 20:32:11,199 - INFO - tqdm - accuracy: 0.9970, batch_loss: 0.0005, loss: 0.0090 ||:  57%|#####6    | 334/590 [18:41<14:34,  3.42s/it]
2022-04-08 20:32:22,302 - INFO - tqdm - accuracy: 0.9970, batch_loss: 0.0005, loss: 0.0089 ||:  57%|#####7    | 337/590 [18:52<14:48,  3.51s/it]
2022-04-08 20:32:35,523 - INFO - tqdm - accuracy: 0.9970, batch_loss: 0.0002, loss: 0.0090 ||:  58%|#####7    | 341/590 [19:05<13:55,  3.36s/it]
2022-04-08 20:32:45,848 - INFO - tqdm - accuracy: 0.9969, batch_loss: 0.0137, loss: 0.0090 ||:  58%|#####8    | 344/590 [19:15<13:42,  3.35s/it]
2022-04-08 20:32:57,444 - INFO - tqdm - accuracy: 0.9968, batch_loss: 0.0879, loss: 0.0092 ||:  59%|#####8    | 347/590 [19:27<14:36,  3.61s/it]
2022-04-08 20:33:07,951 - INFO - tqdm - accuracy: 0.9969, batch_loss: 0.0004, loss: 0.0091 ||:  59%|#####9    | 350/590 [19:37<14:18,  3.58s/it]
2022-04-08 20:33:19,977 - INFO - tqdm - accuracy: 0.9969, batch_loss: 0.0005, loss: 0.0091 ||:  60%|######    | 354/590 [19:49<12:39,  3.22s/it]
2022-04-08 20:33:30,651 - INFO - tqdm - accuracy: 0.9969, batch_loss: 0.0003, loss: 0.0090 ||:  61%|######    | 357/590 [20:00<13:02,  3.36s/it]
2022-04-08 20:33:42,103 - INFO - tqdm - accuracy: 0.9969, batch_loss: 0.1767, loss: 0.0095 ||:  61%|######1   | 360/590 [20:11<13:59,  3.65s/it]
2022-04-08 20:33:54,952 - INFO - tqdm - accuracy: 0.9969, batch_loss: 0.0107, loss: 0.0094 ||:  62%|######1   | 364/590 [20:24<12:21,  3.28s/it]
2022-04-08 20:34:07,518 - INFO - tqdm - accuracy: 0.9969, batch_loss: 0.0009, loss: 0.0100 ||:  62%|######2   | 368/590 [20:37<11:55,  3.22s/it]
2022-04-08 20:34:20,555 - INFO - tqdm - accuracy: 0.9968, batch_loss: 0.0002, loss: 0.0105 ||:  63%|######3   | 372/590 [20:50<11:42,  3.22s/it]
2022-04-08 20:34:30,564 - INFO - tqdm - accuracy: 0.9968, batch_loss: 0.0017, loss: 0.0105 ||:  64%|######3   | 375/590 [21:00<12:00,  3.35s/it]
2022-04-08 20:34:40,965 - INFO - tqdm - accuracy: 0.9969, batch_loss: 0.0006, loss: 0.0104 ||:  64%|######4   | 378/590 [21:10<12:07,  3.43s/it]
2022-04-08 20:34:54,055 - INFO - tqdm - accuracy: 0.9969, batch_loss: 0.0024, loss: 0.0103 ||:  65%|######4   | 382/590 [21:23<11:36,  3.35s/it]
2022-04-08 20:35:04,706 - INFO - tqdm - accuracy: 0.9969, batch_loss: 0.0003, loss: 0.0103 ||:  65%|######5   | 385/590 [21:34<11:58,  3.51s/it]
2022-04-08 20:35:15,873 - INFO - tqdm - accuracy: 0.9969, batch_loss: 0.0003, loss: 0.0103 ||:  66%|######5   | 388/590 [21:45<12:29,  3.71s/it]
2022-04-08 20:35:29,187 - INFO - tqdm - accuracy: 0.9969, batch_loss: 0.0015, loss: 0.0103 ||:  66%|######6   | 392/590 [21:58<11:21,  3.44s/it]
2022-04-08 20:35:42,315 - INFO - tqdm - accuracy: 0.9969, batch_loss: 0.0003, loss: 0.0102 ||:  67%|######7   | 396/590 [22:12<10:51,  3.36s/it]
2022-04-08 20:35:52,873 - INFO - tqdm - accuracy: 0.9969, batch_loss: 0.0012, loss: 0.0101 ||:  68%|######7   | 399/590 [22:22<10:59,  3.45s/it]
2022-04-08 20:36:03,541 - INFO - tqdm - accuracy: 0.9969, batch_loss: 0.0521, loss: 0.0102 ||:  68%|######8   | 402/590 [22:33<11:10,  3.57s/it]
2022-04-08 20:36:13,859 - INFO - tqdm - accuracy: 0.9969, batch_loss: 0.0004, loss: 0.0101 ||:  69%|######8   | 405/590 [22:43<10:47,  3.50s/it]
2022-04-08 20:36:26,898 - INFO - tqdm - accuracy: 0.9969, batch_loss: 0.0014, loss: 0.0101 ||:  69%|######9   | 409/590 [22:56<10:10,  3.37s/it]
2022-04-08 20:36:37,268 - INFO - tqdm - accuracy: 0.9970, batch_loss: 0.0158, loss: 0.0101 ||:  70%|######9   | 412/590 [23:07<10:09,  3.42s/it]
2022-04-08 20:36:49,509 - INFO - tqdm - accuracy: 0.9968, batch_loss: 0.0010, loss: 0.0107 ||:  71%|#######   | 416/590 [23:19<09:13,  3.18s/it]
2022-04-08 20:36:59,896 - INFO - tqdm - accuracy: 0.9969, batch_loss: 0.0002, loss: 0.0106 ||:  71%|#######1  | 419/590 [23:29<09:43,  3.41s/it]
2022-04-08 20:37:10,004 - INFO - tqdm - accuracy: 0.9969, batch_loss: 0.0011, loss: 0.0106 ||:  72%|#######1  | 422/590 [23:39<09:31,  3.40s/it]
2022-04-08 20:37:20,493 - INFO - tqdm - accuracy: 0.9968, batch_loss: 0.0006, loss: 0.0106 ||:  72%|#######2  | 425/590 [23:50<09:33,  3.47s/it]
2022-04-08 20:37:31,031 - INFO - tqdm - accuracy: 0.9969, batch_loss: 0.0008, loss: 0.0105 ||:  73%|#######2  | 428/590 [24:00<09:28,  3.51s/it]
2022-04-08 20:37:44,519 - INFO - tqdm - accuracy: 0.9969, batch_loss: 0.0020, loss: 0.0105 ||:  73%|#######3  | 432/590 [24:14<09:03,  3.44s/it]
2022-04-08 20:37:54,854 - INFO - tqdm - accuracy: 0.9968, batch_loss: 0.0003, loss: 0.0105 ||:  74%|#######3  | 435/590 [24:24<08:55,  3.46s/it]
2022-04-08 20:38:05,125 - INFO - tqdm - accuracy: 0.9968, batch_loss: 0.0145, loss: 0.0107 ||:  74%|#######4  | 438/590 [24:34<08:47,  3.47s/it]
2022-04-08 20:38:15,195 - INFO - tqdm - accuracy: 0.9968, batch_loss: 0.0021, loss: 0.0107 ||:  75%|#######4  | 441/590 [24:45<08:12,  3.31s/it]
2022-04-08 20:38:28,118 - INFO - tqdm - accuracy: 0.9968, batch_loss: 0.0005, loss: 0.0106 ||:  75%|#######5  | 445/590 [24:57<07:44,  3.20s/it]
2022-04-08 20:38:41,578 - INFO - tqdm - accuracy: 0.9967, batch_loss: 0.0013, loss: 0.0107 ||:  76%|#######6  | 449/590 [25:11<08:03,  3.43s/it]
2022-04-08 20:38:53,534 - INFO - tqdm - accuracy: 0.9967, batch_loss: 0.0037, loss: 0.0110 ||:  77%|#######6  | 453/590 [25:23<07:05,  3.11s/it]
2022-04-08 20:39:04,822 - INFO - tqdm - accuracy: 0.9966, batch_loss: 0.0003, loss: 0.0111 ||:  77%|#######7  | 456/590 [25:34<07:58,  3.57s/it]
2022-04-08 20:39:18,029 - INFO - tqdm - accuracy: 0.9967, batch_loss: 0.0012, loss: 0.0110 ||:  78%|#######7  | 460/590 [25:47<07:25,  3.43s/it]
2022-04-08 20:39:28,904 - INFO - tqdm - accuracy: 0.9967, batch_loss: 0.0034, loss: 0.0110 ||:  78%|#######8  | 463/590 [25:58<07:30,  3.55s/it]
2022-04-08 20:39:42,149 - INFO - tqdm - accuracy: 0.9967, batch_loss: 0.0013, loss: 0.0109 ||:  79%|#######9  | 467/590 [26:11<06:57,  3.39s/it]
2022-04-08 20:39:54,672 - INFO - tqdm - accuracy: 0.9965, batch_loss: 0.0002, loss: 0.0114 ||:  80%|#######9  | 471/590 [26:24<06:30,  3.28s/it]
2022-04-08 20:40:05,284 - INFO - tqdm - accuracy: 0.9966, batch_loss: 0.0008, loss: 0.0113 ||:  80%|########  | 474/590 [26:35<06:46,  3.51s/it]
2022-04-08 20:40:15,344 - INFO - tqdm - accuracy: 0.9966, batch_loss: 0.0021, loss: 0.0113 ||:  81%|########  | 477/590 [26:45<06:22,  3.39s/it]
2022-04-08 20:40:26,582 - INFO - tqdm - accuracy: 0.9966, batch_loss: 0.0006, loss: 0.0112 ||:  81%|########1 | 480/590 [26:56<06:36,  3.61s/it]
2022-04-08 20:40:36,985 - INFO - tqdm - accuracy: 0.9966, batch_loss: 0.0065, loss: 0.0113 ||:  82%|########1 | 483/590 [27:06<06:22,  3.57s/it]
2022-04-08 20:40:49,322 - INFO - tqdm - accuracy: 0.9966, batch_loss: 0.0007, loss: 0.0112 ||:  83%|########2 | 487/590 [27:19<05:39,  3.29s/it]
2022-04-08 20:41:01,663 - INFO - tqdm - accuracy: 0.9966, batch_loss: 0.0080, loss: 0.0112 ||:  83%|########3 | 491/590 [27:31<05:14,  3.18s/it]
2022-04-08 20:41:15,495 - INFO - tqdm - accuracy: 0.9966, batch_loss: 0.0003, loss: 0.0111 ||:  84%|########3 | 495/590 [27:45<05:25,  3.43s/it]
2022-04-08 20:41:25,897 - INFO - tqdm - accuracy: 0.9966, batch_loss: 0.0017, loss: 0.0110 ||:  84%|########4 | 498/590 [27:55<05:23,  3.52s/it]
2022-04-08 20:41:36,726 - INFO - tqdm - accuracy: 0.9966, batch_loss: 0.0111, loss: 0.0110 ||:  85%|########4 | 501/590 [28:06<05:13,  3.53s/it]
2022-04-08 20:41:50,025 - INFO - tqdm - accuracy: 0.9967, batch_loss: 0.0008, loss: 0.0109 ||:  86%|########5 | 505/590 [28:19<04:52,  3.45s/it]
2022-04-08 20:42:03,436 - INFO - tqdm - accuracy: 0.9966, batch_loss: 0.0245, loss: 0.0109 ||:  86%|########6 | 509/590 [28:33<04:38,  3.44s/it]
2022-04-08 20:42:14,574 - INFO - tqdm - accuracy: 0.9966, batch_loss: 0.0004, loss: 0.0108 ||:  87%|########6 | 512/590 [28:44<04:37,  3.56s/it]
2022-04-08 20:42:25,122 - INFO - tqdm - accuracy: 0.9967, batch_loss: 0.0004, loss: 0.0108 ||:  87%|########7 | 515/590 [28:54<04:22,  3.51s/it]
2022-04-08 20:42:35,183 - INFO - tqdm - accuracy: 0.9966, batch_loss: 0.0371, loss: 0.0110 ||:  88%|########7 | 518/590 [29:04<04:05,  3.41s/it]
2022-04-08 20:42:45,243 - INFO - tqdm - accuracy: 0.9965, batch_loss: 0.0359, loss: 0.0110 ||:  88%|########8 | 521/590 [29:15<03:55,  3.41s/it]
2022-04-08 20:42:55,268 - INFO - tqdm - accuracy: 0.9965, batch_loss: 0.0010, loss: 0.0110 ||:  89%|########8 | 524/590 [29:25<03:41,  3.36s/it]
2022-04-08 20:43:05,812 - INFO - tqdm - accuracy: 0.9966, batch_loss: 0.0018, loss: 0.0109 ||:  89%|########9 | 527/590 [29:35<03:34,  3.41s/it]
2022-04-08 20:43:15,956 - INFO - tqdm - accuracy: 0.9966, batch_loss: 0.0018, loss: 0.0109 ||:  90%|########9 | 530/590 [29:45<03:25,  3.42s/it]
2022-04-08 20:43:26,095 - INFO - tqdm - accuracy: 0.9965, batch_loss: 0.1309, loss: 0.0111 ||:  90%|######### | 533/590 [29:55<03:13,  3.39s/it]
2022-04-08 20:43:36,448 - INFO - tqdm - accuracy: 0.9965, batch_loss: 0.0061, loss: 0.0111 ||:  91%|######### | 536/590 [30:06<03:07,  3.47s/it]
2022-04-08 20:43:49,423 - INFO - tqdm - accuracy: 0.9965, batch_loss: 0.0003, loss: 0.0111 ||:  92%|#########1| 540/590 [30:19<02:46,  3.34s/it]
2022-04-08 20:44:00,266 - INFO - tqdm - accuracy: 0.9965, batch_loss: 0.0003, loss: 0.0110 ||:  92%|#########2| 543/590 [30:30<02:47,  3.56s/it]
2022-04-08 20:44:12,654 - INFO - tqdm - accuracy: 0.9966, batch_loss: 0.0157, loss: 0.0110 ||:  93%|#########2| 547/590 [30:42<02:17,  3.20s/it]
2022-04-08 20:44:26,302 - INFO - tqdm - accuracy: 0.9965, batch_loss: 0.0010, loss: 0.0110 ||:  93%|#########3| 551/590 [30:56<02:14,  3.44s/it]
2022-04-08 20:44:36,689 - INFO - tqdm - accuracy: 0.9966, batch_loss: 0.0008, loss: 0.0109 ||:  94%|#########3| 554/590 [31:06<02:04,  3.47s/it]
2022-04-08 20:44:46,900 - INFO - tqdm - accuracy: 0.9966, batch_loss: 0.0006, loss: 0.0109 ||:  94%|#########4| 557/590 [31:16<01:52,  3.41s/it]
2022-04-08 20:44:57,628 - INFO - tqdm - accuracy: 0.9966, batch_loss: 0.0062, loss: 0.0109 ||:  95%|#########4| 560/590 [31:27<01:45,  3.53s/it]
2022-04-08 20:45:08,082 - INFO - tqdm - accuracy: 0.9966, batch_loss: 0.0070, loss: 0.0108 ||:  95%|#########5| 563/590 [31:37<01:33,  3.46s/it]
2022-04-08 20:45:18,404 - INFO - tqdm - accuracy: 0.9966, batch_loss: 0.0009, loss: 0.0108 ||:  96%|#########5| 566/590 [31:48<01:22,  3.43s/it]
2022-04-08 20:45:28,439 - INFO - tqdm - accuracy: 0.9966, batch_loss: 0.0014, loss: 0.0107 ||:  96%|#########6| 569/590 [31:58<01:09,  3.31s/it]
2022-04-08 20:45:41,518 - INFO - tqdm - accuracy: 0.9966, batch_loss: 0.0002, loss: 0.0107 ||:  97%|#########7| 573/590 [32:11<00:55,  3.24s/it]
2022-04-08 20:45:52,504 - INFO - tqdm - accuracy: 0.9966, batch_loss: 0.1517, loss: 0.0109 ||:  98%|#########7| 576/590 [32:22<00:50,  3.61s/it]
2022-04-08 20:46:05,077 - INFO - tqdm - accuracy: 0.9966, batch_loss: 0.0081, loss: 0.0109 ||:  98%|#########8| 579/590 [32:34<00:42,  3.85s/it]
2022-04-08 20:46:15,084 - INFO - tqdm - accuracy: 0.9966, batch_loss: 0.0566, loss: 0.0110 ||:  99%|#########8| 582/590 [32:44<00:27,  3.48s/it]
2022-04-08 20:46:26,534 - INFO - tqdm - accuracy: 0.9965, batch_loss: 0.0477, loss: 0.0110 ||:  99%|#########9| 585/590 [32:56<00:18,  3.74s/it]
2022-04-08 20:46:37,513 - INFO - tqdm - accuracy: 0.9965, batch_loss: 0.0029, loss: 0.0113 ||: 100%|#########9| 588/590 [33:07<00:07,  3.70s/it]
2022-04-08 20:46:40,915 - INFO - tqdm - accuracy: 0.9965, batch_loss: 0.0037, loss: 0.0113 ||: 100%|#########9| 589/590 [33:10<00:03,  3.61s/it]
2022-04-08 20:46:41,565 - INFO - tqdm - accuracy: 0.9965, batch_loss: 0.0072, loss: 0.0113 ||: 100%|##########| 590/590 [33:11<00:00,  2.72s/it]
2022-04-08 20:46:41,566 - INFO - tqdm - accuracy: 0.9965, batch_loss: 0.0072, loss: 0.0113 ||: 100%|##########| 590/590 [33:11<00:00,  3.38s/it]
2022-04-08 20:46:43,834 - INFO - allennlp.training.trainer - Validating
2022-04-08 20:46:43,836 - INFO - tqdm - 0%|          | 0/1635 [00:00<?, ?it/s]
2022-04-08 20:46:53,924 - INFO - tqdm - accuracy: 0.8379, batch_loss: 3.3674, loss: 0.9478 ||:   9%|8         | 145/1635 [00:10<02:03, 12.07it/s]
2022-04-08 20:47:03,979 - INFO - tqdm - accuracy: 0.8530, batch_loss: 0.0001, loss: 0.8503 ||:  18%|#8        | 296/1635 [00:20<01:27, 15.24it/s]
2022-04-08 20:47:13,986 - INFO - tqdm - accuracy: 0.8511, batch_loss: 4.5170, loss: 0.9071 ||:  26%|##5       | 423/1635 [00:30<01:34, 12.86it/s]
2022-04-08 20:47:24,013 - INFO - tqdm - accuracy: 0.8408, batch_loss: 1.8879, loss: 0.9820 ||:  34%|###4      | 559/1635 [00:40<00:59, 18.07it/s]
2022-04-08 20:47:34,129 - INFO - tqdm - accuracy: 0.8462, batch_loss: 4.7207, loss: 0.9594 ||:  43%|####2     | 699/1635 [00:50<01:03, 14.64it/s]
2022-04-08 20:47:44,140 - INFO - tqdm - accuracy: 0.8494, batch_loss: 0.0001, loss: 0.9476 ||:  51%|#####1    | 840/1635 [01:00<01:01, 12.89it/s]
2022-04-08 20:47:54,233 - INFO - tqdm - accuracy: 0.8476, batch_loss: 3.7968, loss: 0.9579 ||:  60%|#####9    | 978/1635 [01:10<00:44, 14.80it/s]
2022-04-08 20:48:04,382 - INFO - tqdm - accuracy: 0.8498, batch_loss: 0.0312, loss: 0.9450 ||:  69%|######8   | 1122/1635 [01:20<00:38, 13.48it/s]
2022-04-08 20:48:14,405 - INFO - tqdm - accuracy: 0.8519, batch_loss: 0.1199, loss: 0.9444 ||:  78%|#######7  | 1273/1635 [01:30<00:20, 17.28it/s]
2022-04-08 20:48:24,425 - INFO - tqdm - accuracy: 0.8496, batch_loss: 0.0013, loss: 0.9615 ||:  87%|########6 | 1420/1635 [01:40<00:17, 12.02it/s]
2022-04-08 20:48:34,539 - INFO - tqdm - accuracy: 0.8511, batch_loss: 0.0001, loss: 0.9573 ||:  96%|#########5| 1565/1635 [01:50<00:04, 14.02it/s]
2022-04-08 20:48:39,332 - INFO - tqdm - accuracy: 0.8526, batch_loss: 0.0001, loss: 0.9482 ||: 100%|#########9| 1628/1635 [01:55<00:00, 13.85it/s]
2022-04-08 20:48:39,460 - INFO - tqdm - accuracy: 0.8528, batch_loss: 0.0001, loss: 0.9470 ||: 100%|#########9| 1630/1635 [01:55<00:00, 14.31it/s]
2022-04-08 20:48:39,630 - INFO - tqdm - accuracy: 0.8523, batch_loss: 4.7012, loss: 0.9490 ||: 100%|#########9| 1632/1635 [01:55<00:00, 13.47it/s]
2022-04-08 20:48:39,810 - INFO - tqdm - accuracy: 0.8525, batch_loss: 0.0001, loss: 0.9478 ||: 100%|#########9| 1634/1635 [01:55<00:00, 12.67it/s]
2022-04-08 20:48:39,885 - INFO - tqdm - accuracy: 0.8526, batch_loss: 0.0002, loss: 0.9472 ||: 100%|##########| 1635/1635 [01:56<00:00, 14.09it/s]
2022-04-08 20:48:39,885 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-08 20:48:39,886 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.996  |     0.853
2022-04-08 20:48:39,887 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9604.118  |       N/A
2022-04-08 20:48:39,887 - INFO - allennlp.training.tensorboard_writer - loss               |     0.011  |     0.947
2022-04-08 20:48:39,888 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  7239.301  |       N/A
2022-04-08 20:48:44,382 - INFO - allennlp.training.trainer - Epoch duration: 0:35:14.192367
2022-04-08 20:48:44,382 - INFO - allennlp.training.trainer - Estimated training time remaining: 0:35:16
2022-04-08 20:48:44,382 - INFO - allennlp.training.trainer - Epoch 9/9
2022-04-08 20:48:44,382 - INFO - allennlp.training.trainer - Worker 0 memory usage: 7.1G
2022-04-08 20:48:44,382 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-08 20:48:44,384 - INFO - allennlp.training.trainer - Training
2022-04-08 20:48:44,384 - INFO - tqdm - 0%|          | 0/590 [00:00<?, ?it/s]
2022-04-08 20:48:55,379 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0095, loss: 0.0055 ||:   1%|          | 3/590 [00:10<35:23,  3.62s/it]
2022-04-08 20:49:08,107 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0009, loss: 0.0026 ||:   1%|1         | 7/590 [00:23<31:30,  3.24s/it]
2022-04-08 20:49:18,445 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0015, loss: 0.0026 ||:   2%|1         | 10/590 [00:34<32:51,  3.40s/it]
2022-04-08 20:49:29,555 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0004, loss: 0.0037 ||:   2%|2         | 13/590 [00:45<35:31,  3.69s/it]
2022-04-08 20:49:39,645 - INFO - tqdm - accuracy: 0.9980, batch_loss: 0.0003, loss: 0.0046 ||:   3%|2         | 16/590 [00:55<33:23,  3.49s/it]
2022-04-08 20:49:52,328 - INFO - tqdm - accuracy: 0.9969, batch_loss: 0.0013, loss: 0.0078 ||:   3%|3         | 20/590 [01:07<30:40,  3.23s/it]
2022-04-08 20:50:02,437 - INFO - tqdm - accuracy: 0.9973, batch_loss: 0.0005, loss: 0.0068 ||:   4%|3         | 23/590 [01:18<31:00,  3.28s/it]
2022-04-08 20:50:13,034 - INFO - tqdm - accuracy: 0.9976, batch_loss: 0.0035, loss: 0.0065 ||:   4%|4         | 26/590 [01:28<32:22,  3.44s/it]
2022-04-08 20:50:23,231 - INFO - tqdm - accuracy: 0.9968, batch_loss: 0.0305, loss: 0.0074 ||:   5%|4         | 29/590 [01:38<31:36,  3.38s/it]
2022-04-08 20:50:35,867 - INFO - tqdm - accuracy: 0.9972, batch_loss: 0.0005, loss: 0.0070 ||:   6%|5         | 33/590 [01:51<29:35,  3.19s/it]
2022-04-08 20:50:45,888 - INFO - tqdm - accuracy: 0.9974, batch_loss: 0.0003, loss: 0.0067 ||:   6%|6         | 36/590 [02:01<30:33,  3.31s/it]
2022-04-08 20:50:57,342 - INFO - tqdm - accuracy: 0.9976, batch_loss: 0.0002, loss: 0.0062 ||:   7%|6         | 39/590 [02:12<33:03,  3.60s/it]
2022-04-08 20:51:10,505 - INFO - tqdm - accuracy: 0.9978, batch_loss: 0.0002, loss: 0.0057 ||:   7%|7         | 43/590 [02:26<30:50,  3.38s/it]
2022-04-08 20:51:21,994 - INFO - tqdm - accuracy: 0.9980, batch_loss: 0.0064, loss: 0.0055 ||:   8%|7         | 46/590 [02:37<33:18,  3.67s/it]
2022-04-08 20:51:35,002 - INFO - tqdm - accuracy: 0.9981, batch_loss: 0.0005, loss: 0.0051 ||:   8%|8         | 50/590 [02:50<30:37,  3.40s/it]
2022-04-08 20:51:45,307 - INFO - tqdm - accuracy: 0.9982, batch_loss: 0.0022, loss: 0.0049 ||:   9%|8         | 53/590 [03:00<30:39,  3.43s/it]
2022-04-08 20:51:58,602 - INFO - tqdm - accuracy: 0.9984, batch_loss: 0.0019, loss: 0.0046 ||:  10%|9         | 57/590 [03:14<30:05,  3.39s/it]
2022-04-08 20:52:08,728 - INFO - tqdm - accuracy: 0.9984, batch_loss: 0.0004, loss: 0.0044 ||:  10%|#         | 60/590 [03:24<29:44,  3.37s/it]
2022-04-08 20:52:21,740 - INFO - tqdm - accuracy: 0.9985, batch_loss: 0.0028, loss: 0.0043 ||:  11%|#         | 64/590 [03:37<29:08,  3.32s/it]
2022-04-08 20:52:32,388 - INFO - tqdm - accuracy: 0.9981, batch_loss: 0.0005, loss: 0.0051 ||:  11%|#1        | 67/590 [03:48<30:50,  3.54s/it]
2022-04-08 20:52:45,734 - INFO - tqdm - accuracy: 0.9982, batch_loss: 0.0016, loss: 0.0048 ||:  12%|#2        | 71/590 [04:01<29:20,  3.39s/it]
2022-04-08 20:52:58,288 - INFO - tqdm - accuracy: 0.9983, batch_loss: 0.0017, loss: 0.0048 ||:  13%|#2        | 75/590 [04:13<27:49,  3.24s/it]
2022-04-08 20:53:08,641 - INFO - tqdm - accuracy: 0.9984, batch_loss: 0.0012, loss: 0.0047 ||:  13%|#3        | 78/590 [04:24<29:05,  3.41s/it]
2022-04-08 20:53:18,741 - INFO - tqdm - accuracy: 0.9981, batch_loss: 0.0287, loss: 0.0050 ||:  14%|#3        | 81/590 [04:34<28:59,  3.42s/it]
2022-04-08 20:53:31,722 - INFO - tqdm - accuracy: 0.9982, batch_loss: 0.0004, loss: 0.0048 ||:  14%|#4        | 85/590 [04:47<27:52,  3.31s/it]
2022-04-08 20:53:44,961 - INFO - tqdm - accuracy: 0.9982, batch_loss: 0.0013, loss: 0.0047 ||:  15%|#5        | 89/590 [05:00<27:27,  3.29s/it]
2022-04-08 20:53:55,090 - INFO - tqdm - accuracy: 0.9983, batch_loss: 0.0004, loss: 0.0046 ||:  16%|#5        | 92/590 [05:10<27:03,  3.26s/it]
2022-04-08 20:54:08,291 - INFO - tqdm - accuracy: 0.9984, batch_loss: 0.0002, loss: 0.0046 ||:  16%|#6        | 96/590 [05:23<27:11,  3.30s/it]
2022-04-08 20:54:18,766 - INFO - tqdm - accuracy: 0.9981, batch_loss: 0.0467, loss: 0.0049 ||:  17%|#6        | 99/590 [05:34<28:01,  3.42s/it]
2022-04-08 20:54:29,545 - INFO - tqdm - accuracy: 0.9982, batch_loss: 0.0003, loss: 0.0048 ||:  17%|#7        | 102/590 [05:45<28:45,  3.54s/it]
2022-04-08 20:54:42,094 - INFO - tqdm - accuracy: 0.9982, batch_loss: 0.0113, loss: 0.0047 ||:  18%|#7        | 106/590 [05:57<26:23,  3.27s/it]
2022-04-08 20:54:52,358 - INFO - tqdm - accuracy: 0.9983, batch_loss: 0.0002, loss: 0.0046 ||:  18%|#8        | 109/590 [06:07<26:50,  3.35s/it]
2022-04-08 20:55:02,996 - INFO - tqdm - accuracy: 0.9978, batch_loss: 0.0392, loss: 0.0051 ||:  19%|#8        | 112/590 [06:18<27:45,  3.48s/it]
2022-04-08 20:55:13,533 - INFO - tqdm - accuracy: 0.9978, batch_loss: 0.0020, loss: 0.0050 ||:  19%|#9        | 115/590 [06:29<27:37,  3.49s/it]
2022-04-08 20:55:26,718 - INFO - tqdm - accuracy: 0.9979, batch_loss: 0.0002, loss: 0.0049 ||:  20%|##        | 119/590 [06:42<26:09,  3.33s/it]
2022-04-08 20:55:39,721 - INFO - tqdm - accuracy: 0.9977, batch_loss: 0.0085, loss: 0.0050 ||:  21%|##        | 123/590 [06:55<25:02,  3.22s/it]
2022-04-08 20:55:52,862 - INFO - tqdm - accuracy: 0.9978, batch_loss: 0.0074, loss: 0.0049 ||:  22%|##1       | 127/590 [07:08<25:33,  3.31s/it]
2022-04-08 20:56:03,161 - INFO - tqdm - accuracy: 0.9976, batch_loss: 0.0001, loss: 0.0050 ||:  22%|##2       | 130/590 [07:18<26:23,  3.44s/it]
2022-04-08 20:56:13,352 - INFO - tqdm - accuracy: 0.9977, batch_loss: 0.0004, loss: 0.0049 ||:  23%|##2       | 133/590 [07:28<26:13,  3.44s/it]
2022-04-08 20:56:24,288 - INFO - tqdm - accuracy: 0.9977, batch_loss: 0.0002, loss: 0.0048 ||:  23%|##3       | 136/590 [07:39<27:57,  3.69s/it]
2022-04-08 20:56:37,762 - INFO - tqdm - accuracy: 0.9978, batch_loss: 0.0092, loss: 0.0048 ||:  24%|##3       | 140/590 [07:53<26:07,  3.48s/it]
2022-04-08 20:56:47,789 - INFO - tqdm - accuracy: 0.9974, batch_loss: 0.1134, loss: 0.0062 ||:  24%|##4       | 143/590 [08:03<24:59,  3.35s/it]
2022-04-08 20:56:59,348 - INFO - tqdm - accuracy: 0.9974, batch_loss: 0.0004, loss: 0.0060 ||:  25%|##4       | 147/590 [08:14<22:02,  2.99s/it]
2022-04-08 20:57:13,205 - INFO - tqdm - accuracy: 0.9975, batch_loss: 0.0005, loss: 0.0059 ||:  26%|##5       | 151/590 [08:28<25:12,  3.45s/it]
2022-04-08 20:57:26,378 - INFO - tqdm - accuracy: 0.9976, batch_loss: 0.0003, loss: 0.0058 ||:  26%|##6       | 155/590 [08:41<23:54,  3.30s/it]
2022-04-08 20:57:39,616 - INFO - tqdm - accuracy: 0.9976, batch_loss: 0.0005, loss: 0.0056 ||:  27%|##6       | 159/590 [08:55<23:59,  3.34s/it]
2022-04-08 20:57:50,365 - INFO - tqdm - accuracy: 0.9977, batch_loss: 0.0054, loss: 0.0056 ||:  27%|##7       | 162/590 [09:05<24:44,  3.47s/it]
2022-04-08 20:58:00,616 - INFO - tqdm - accuracy: 0.9977, batch_loss: 0.0018, loss: 0.0055 ||:  28%|##7       | 165/590 [09:16<24:47,  3.50s/it]
2022-04-08 20:58:11,080 - INFO - tqdm - accuracy: 0.9978, batch_loss: 0.0009, loss: 0.0054 ||:  28%|##8       | 168/590 [09:26<25:09,  3.58s/it]
2022-04-08 20:58:21,744 - INFO - tqdm - accuracy: 0.9978, batch_loss: 0.0002, loss: 0.0053 ||:  29%|##8       | 171/590 [09:37<25:09,  3.60s/it]
2022-04-08 20:58:34,163 - INFO - tqdm - accuracy: 0.9977, batch_loss: 0.0055, loss: 0.0057 ||:  30%|##9       | 175/590 [09:49<21:57,  3.17s/it]
2022-04-08 20:58:44,554 - INFO - tqdm - accuracy: 0.9975, batch_loss: 0.0385, loss: 0.0059 ||:  30%|###       | 178/590 [10:00<23:11,  3.38s/it]
2022-04-08 20:58:57,818 - INFO - tqdm - accuracy: 0.9976, batch_loss: 0.0082, loss: 0.0058 ||:  31%|###       | 182/590 [10:13<22:46,  3.35s/it]
2022-04-08 20:59:07,943 - INFO - tqdm - accuracy: 0.9975, batch_loss: 0.0001, loss: 0.0058 ||:  31%|###1      | 185/590 [10:23<22:57,  3.40s/it]
2022-04-08 20:59:20,544 - INFO - tqdm - accuracy: 0.9975, batch_loss: 0.0003, loss: 0.0057 ||:  32%|###2      | 189/590 [10:36<21:24,  3.20s/it]
2022-04-08 20:59:33,645 - INFO - tqdm - accuracy: 0.9974, batch_loss: 0.0002, loss: 0.0070 ||:  33%|###2      | 193/590 [10:49<21:25,  3.24s/it]
2022-04-08 20:59:44,030 - INFO - tqdm - accuracy: 0.9974, batch_loss: 0.0002, loss: 0.0069 ||:  33%|###3      | 196/590 [10:59<21:56,  3.34s/it]
2022-04-08 20:59:54,428 - INFO - tqdm - accuracy: 0.9973, batch_loss: 0.0272, loss: 0.0070 ||:  34%|###3      | 199/590 [11:10<22:59,  3.53s/it]
2022-04-08 21:00:07,282 - INFO - tqdm - accuracy: 0.9974, batch_loss: 0.0006, loss: 0.0068 ||:  34%|###4      | 203/590 [11:22<21:41,  3.36s/it]
2022-04-08 21:00:20,057 - INFO - tqdm - accuracy: 0.9974, batch_loss: 0.0002, loss: 0.0068 ||:  35%|###5      | 207/590 [11:35<20:26,  3.20s/it]
2022-04-08 21:00:30,223 - INFO - tqdm - accuracy: 0.9973, batch_loss: 0.0026, loss: 0.0068 ||:  36%|###5      | 210/590 [11:45<20:54,  3.30s/it]
2022-04-08 21:00:42,455 - INFO - tqdm - accuracy: 0.9974, batch_loss: 0.0006, loss: 0.0067 ||:  36%|###6      | 214/590 [11:58<19:32,  3.12s/it]
2022-04-08 21:00:54,634 - INFO - tqdm - accuracy: 0.9970, batch_loss: 0.1089, loss: 0.0078 ||:  37%|###6      | 218/590 [12:10<19:18,  3.11s/it]
2022-04-08 21:01:07,005 - INFO - tqdm - accuracy: 0.9970, batch_loss: 0.0041, loss: 0.0077 ||:  38%|###7      | 222/590 [12:22<19:16,  3.14s/it]
2022-04-08 21:01:17,206 - INFO - tqdm - accuracy: 0.9971, batch_loss: 0.0003, loss: 0.0076 ||:  38%|###8      | 225/590 [12:32<20:00,  3.29s/it]
2022-04-08 21:01:30,023 - INFO - tqdm - accuracy: 0.9971, batch_loss: 0.0003, loss: 0.0076 ||:  39%|###8      | 229/590 [12:45<19:11,  3.19s/it]
2022-04-08 21:01:40,782 - INFO - tqdm - accuracy: 0.9972, batch_loss: 0.0009, loss: 0.0075 ||:  39%|###9      | 232/590 [12:56<20:33,  3.44s/it]
2022-04-08 21:01:50,914 - INFO - tqdm - accuracy: 0.9972, batch_loss: 0.0187, loss: 0.0075 ||:  40%|###9      | 235/590 [13:06<20:14,  3.42s/it]
2022-04-08 21:02:01,139 - INFO - tqdm - accuracy: 0.9972, batch_loss: 0.0199, loss: 0.0075 ||:  40%|####      | 238/590 [13:16<20:13,  3.45s/it]
2022-04-08 21:02:12,085 - INFO - tqdm - accuracy: 0.9973, batch_loss: 0.0002, loss: 0.0074 ||:  41%|####      | 241/590 [13:27<20:47,  3.58s/it]
2022-04-08 21:02:25,419 - INFO - tqdm - accuracy: 0.9973, batch_loss: 0.0013, loss: 0.0073 ||:  42%|####1     | 245/590 [13:41<19:21,  3.37s/it]
2022-04-08 21:02:35,850 - INFO - tqdm - accuracy: 0.9974, batch_loss: 0.0003, loss: 0.0072 ||:  42%|####2     | 248/590 [13:51<19:41,  3.45s/it]
2022-04-08 21:02:46,603 - INFO - tqdm - accuracy: 0.9974, batch_loss: 0.0002, loss: 0.0071 ||:  43%|####2     | 251/590 [14:02<19:57,  3.53s/it]
2022-04-08 21:02:59,720 - INFO - tqdm - accuracy: 0.9974, batch_loss: 0.0010, loss: 0.0070 ||:  43%|####3     | 255/590 [14:15<18:49,  3.37s/it]
2022-04-08 21:03:13,320 - INFO - tqdm - accuracy: 0.9975, batch_loss: 0.0002, loss: 0.0069 ||:  44%|####3     | 259/590 [14:28<19:36,  3.56s/it]
2022-04-08 21:03:25,934 - INFO - tqdm - accuracy: 0.9975, batch_loss: 0.0009, loss: 0.0068 ||:  45%|####4     | 263/590 [14:41<17:22,  3.19s/it]
2022-04-08 21:03:36,336 - INFO - tqdm - accuracy: 0.9975, batch_loss: 0.0013, loss: 0.0068 ||:  45%|####5     | 266/590 [14:51<18:34,  3.44s/it]
2022-04-08 21:03:49,276 - INFO - tqdm - accuracy: 0.9976, batch_loss: 0.0089, loss: 0.0067 ||:  46%|####5     | 270/590 [15:04<17:44,  3.33s/it]
2022-04-08 21:03:59,753 - INFO - tqdm - accuracy: 0.9976, batch_loss: 0.0011, loss: 0.0067 ||:  46%|####6     | 273/590 [15:15<17:59,  3.41s/it]
2022-04-08 21:04:10,231 - INFO - tqdm - accuracy: 0.9975, batch_loss: 0.0179, loss: 0.0069 ||:  47%|####6     | 276/590 [15:25<18:06,  3.46s/it]
2022-04-08 21:04:20,263 - INFO - tqdm - accuracy: 0.9974, batch_loss: 0.0008, loss: 0.0069 ||:  47%|####7     | 279/590 [15:35<17:44,  3.42s/it]
2022-04-08 21:04:30,972 - INFO - tqdm - accuracy: 0.9975, batch_loss: 0.0005, loss: 0.0069 ||:  48%|####7     | 282/590 [15:46<18:17,  3.56s/it]
2022-04-08 21:04:41,979 - INFO - tqdm - accuracy: 0.9975, batch_loss: 0.0003, loss: 0.0068 ||:  48%|####8     | 285/590 [15:57<18:49,  3.70s/it]
2022-04-08 21:04:53,847 - INFO - tqdm - accuracy: 0.9975, batch_loss: 0.0106, loss: 0.0067 ||:  49%|####8     | 289/590 [16:09<15:56,  3.18s/it]
2022-04-08 21:05:04,952 - INFO - tqdm - accuracy: 0.9975, batch_loss: 0.0004, loss: 0.0067 ||:  49%|####9     | 292/590 [16:20<17:17,  3.48s/it]
2022-04-08 21:05:15,582 - INFO - tqdm - accuracy: 0.9975, batch_loss: 0.0001, loss: 0.0069 ||:  50%|#####     | 295/590 [16:31<17:12,  3.50s/it]
2022-04-08 21:05:28,725 - INFO - tqdm - accuracy: 0.9975, batch_loss: 0.0142, loss: 0.0068 ||:  51%|#####     | 299/590 [16:44<16:14,  3.35s/it]
2022-04-08 21:05:40,709 - INFO - tqdm - accuracy: 0.9974, batch_loss: 0.0003, loss: 0.0068 ||:  51%|#####1    | 303/590 [16:56<15:05,  3.16s/it]
2022-04-08 21:05:50,892 - INFO - tqdm - accuracy: 0.9973, batch_loss: 0.0044, loss: 0.0072 ||:  52%|#####1    | 306/590 [17:06<15:29,  3.27s/it]
2022-04-08 21:06:03,442 - INFO - tqdm - accuracy: 0.9972, batch_loss: 0.0300, loss: 0.0074 ||:  53%|#####2    | 310/590 [17:19<14:41,  3.15s/it]
2022-04-08 21:06:16,138 - INFO - tqdm - accuracy: 0.9972, batch_loss: 0.0003, loss: 0.0073 ||:  53%|#####3    | 314/590 [17:31<14:17,  3.11s/it]
2022-04-08 21:06:26,393 - INFO - tqdm - accuracy: 0.9972, batch_loss: 0.0210, loss: 0.0073 ||:  54%|#####3    | 317/590 [17:42<14:57,  3.29s/it]
2022-04-08 21:06:37,075 - INFO - tqdm - accuracy: 0.9973, batch_loss: 0.0003, loss: 0.0072 ||:  54%|#####4    | 320/590 [17:52<15:33,  3.46s/it]
2022-04-08 21:06:49,911 - INFO - tqdm - accuracy: 0.9973, batch_loss: 0.0003, loss: 0.0071 ||:  55%|#####4    | 324/590 [18:05<14:49,  3.34s/it]
2022-04-08 21:07:00,735 - INFO - tqdm - accuracy: 0.9973, batch_loss: 0.0009, loss: 0.0071 ||:  55%|#####5    | 327/590 [18:16<15:21,  3.50s/it]
2022-04-08 21:07:10,812 - INFO - tqdm - accuracy: 0.9973, batch_loss: 0.0002, loss: 0.0070 ||:  56%|#####5    | 330/590 [18:26<14:48,  3.42s/it]
2022-04-08 21:07:23,953 - INFO - tqdm - accuracy: 0.9974, batch_loss: 0.0005, loss: 0.0070 ||:  57%|#####6    | 334/590 [18:39<14:15,  3.34s/it]
2022-04-08 21:07:36,939 - INFO - tqdm - accuracy: 0.9974, batch_loss: 0.0030, loss: 0.0069 ||:  57%|#####7    | 338/590 [18:52<13:33,  3.23s/it]
2022-04-08 21:07:50,040 - INFO - tqdm - accuracy: 0.9974, batch_loss: 0.0023, loss: 0.0072 ||:  58%|#####7    | 342/590 [19:05<13:42,  3.32s/it]
2022-04-08 21:08:00,439 - INFO - tqdm - accuracy: 0.9974, batch_loss: 0.0002, loss: 0.0071 ||:  58%|#####8    | 345/590 [19:16<14:06,  3.45s/it]
2022-04-08 21:08:13,449 - INFO - tqdm - accuracy: 0.9974, batch_loss: 0.0005, loss: 0.0071 ||:  59%|#####9    | 349/590 [19:29<13:20,  3.32s/it]
2022-04-08 21:08:24,095 - INFO - tqdm - accuracy: 0.9974, batch_loss: 0.0003, loss: 0.0070 ||:  60%|#####9    | 352/590 [19:39<13:34,  3.42s/it]
2022-04-08 21:08:36,971 - INFO - tqdm - accuracy: 0.9975, batch_loss: 0.0047, loss: 0.0069 ||:  60%|######    | 356/590 [19:52<12:39,  3.25s/it]
2022-04-08 21:08:48,029 - INFO - tqdm - accuracy: 0.9975, batch_loss: 0.0134, loss: 0.0069 ||:  61%|######    | 359/590 [20:03<13:24,  3.48s/it]
2022-04-08 21:09:01,115 - INFO - tqdm - accuracy: 0.9975, batch_loss: 0.0003, loss: 0.0069 ||:  62%|######1   | 363/590 [20:16<12:39,  3.34s/it]
2022-04-08 21:09:11,441 - INFO - tqdm - accuracy: 0.9975, batch_loss: 0.0002, loss: 0.0068 ||:  62%|######2   | 366/590 [20:27<12:42,  3.40s/it]
2022-04-08 21:09:22,868 - INFO - tqdm - accuracy: 0.9975, batch_loss: 0.0002, loss: 0.0068 ||:  63%|######2   | 369/590 [20:38<13:35,  3.69s/it]
2022-04-08 21:09:33,017 - INFO - tqdm - accuracy: 0.9976, batch_loss: 0.0006, loss: 0.0067 ||:  63%|######3   | 372/590 [20:48<12:42,  3.50s/it]
2022-04-08 21:09:46,394 - INFO - tqdm - accuracy: 0.9976, batch_loss: 0.0002, loss: 0.0067 ||:  64%|######3   | 376/590 [21:02<12:09,  3.41s/it]
2022-04-08 21:09:56,862 - INFO - tqdm - accuracy: 0.9976, batch_loss: 0.0018, loss: 0.0066 ||:  64%|######4   | 379/590 [21:12<12:02,  3.43s/it]
2022-04-08 21:10:10,284 - INFO - tqdm - accuracy: 0.9976, batch_loss: 0.0001, loss: 0.0066 ||:  65%|######4   | 383/590 [21:25<11:51,  3.44s/it]
2022-04-08 21:10:20,416 - INFO - tqdm - accuracy: 0.9977, batch_loss: 0.0004, loss: 0.0065 ||:  65%|######5   | 386/590 [21:36<11:26,  3.37s/it]
2022-04-08 21:10:34,239 - INFO - tqdm - accuracy: 0.9977, batch_loss: 0.0002, loss: 0.0065 ||:  66%|######6   | 390/590 [21:49<11:50,  3.55s/it]
2022-04-08 21:10:48,004 - INFO - tqdm - accuracy: 0.9977, batch_loss: 0.0002, loss: 0.0065 ||:  67%|######6   | 394/590 [22:03<11:34,  3.55s/it]
2022-04-08 21:10:59,313 - INFO - tqdm - accuracy: 0.9977, batch_loss: 0.0002, loss: 0.0064 ||:  67%|######7   | 397/590 [22:14<12:02,  3.74s/it]
2022-04-08 21:11:09,498 - INFO - tqdm - accuracy: 0.9977, batch_loss: 0.0051, loss: 0.0064 ||:  68%|######7   | 400/590 [22:25<11:04,  3.50s/it]
2022-04-08 21:11:19,736 - INFO - tqdm - accuracy: 0.9976, batch_loss: 0.0002, loss: 0.0065 ||:  68%|######8   | 403/590 [22:35<10:50,  3.48s/it]
2022-04-08 21:11:29,769 - INFO - tqdm - accuracy: 0.9976, batch_loss: 0.0002, loss: 0.0065 ||:  69%|######8   | 406/590 [22:45<10:14,  3.34s/it]
2022-04-08 21:11:39,818 - INFO - tqdm - accuracy: 0.9976, batch_loss: 0.0002, loss: 0.0064 ||:  69%|######9   | 409/590 [22:55<10:09,  3.37s/it]
2022-04-08 21:11:51,021 - INFO - tqdm - accuracy: 0.9976, batch_loss: 0.0004, loss: 0.0064 ||:  70%|######9   | 412/590 [23:06<10:45,  3.62s/it]
2022-04-08 21:12:01,781 - INFO - tqdm - accuracy: 0.9977, batch_loss: 0.0017, loss: 0.0064 ||:  70%|#######   | 415/590 [23:17<10:30,  3.60s/it]
2022-04-08 21:12:12,023 - INFO - tqdm - accuracy: 0.9977, batch_loss: 0.0001, loss: 0.0063 ||:  71%|#######   | 418/590 [23:27<09:55,  3.46s/it]
2022-04-08 21:12:22,105 - INFO - tqdm - accuracy: 0.9977, batch_loss: 0.0002, loss: 0.0063 ||:  71%|#######1  | 421/590 [23:37<09:38,  3.42s/it]
2022-04-08 21:12:34,999 - INFO - tqdm - accuracy: 0.9976, batch_loss: 0.0035, loss: 0.0064 ||:  72%|#######2  | 425/590 [23:50<08:55,  3.25s/it]
2022-04-08 21:12:45,204 - INFO - tqdm - accuracy: 0.9977, batch_loss: 0.0002, loss: 0.0063 ||:  73%|#######2  | 428/590 [24:00<08:59,  3.33s/it]
2022-04-08 21:12:55,319 - INFO - tqdm - accuracy: 0.9976, batch_loss: 0.0003, loss: 0.0064 ||:  73%|#######3  | 431/590 [24:10<08:52,  3.35s/it]
2022-04-08 21:13:06,766 - INFO - tqdm - accuracy: 0.9976, batch_loss: 0.0004, loss: 0.0063 ||:  74%|#######3  | 434/590 [24:22<09:47,  3.76s/it]
2022-04-08 21:13:18,043 - INFO - tqdm - accuracy: 0.9976, batch_loss: 0.0007, loss: 0.0063 ||:  74%|#######4  | 437/590 [24:33<09:37,  3.77s/it]
2022-04-08 21:13:28,914 - INFO - tqdm - accuracy: 0.9976, batch_loss: 0.0102, loss: 0.0063 ||:  75%|#######4  | 440/590 [24:44<09:01,  3.61s/it]
2022-04-08 21:13:39,038 - INFO - tqdm - accuracy: 0.9975, batch_loss: 0.0002, loss: 0.0066 ||:  75%|#######5  | 443/590 [24:54<08:27,  3.45s/it]
2022-04-08 21:13:49,111 - INFO - tqdm - accuracy: 0.9973, batch_loss: 0.0825, loss: 0.0070 ||:  76%|#######5  | 446/590 [25:04<08:08,  3.39s/it]
2022-04-08 21:14:00,311 - INFO - tqdm - accuracy: 0.9974, batch_loss: 0.0013, loss: 0.0070 ||:  76%|#######6  | 449/590 [25:15<08:19,  3.54s/it]
2022-04-08 21:14:10,527 - INFO - tqdm - accuracy: 0.9974, batch_loss: 0.0004, loss: 0.0069 ||:  77%|#######6  | 452/590 [25:26<08:02,  3.50s/it]
2022-04-08 21:14:23,582 - INFO - tqdm - accuracy: 0.9974, batch_loss: 0.0012, loss: 0.0069 ||:  77%|#######7  | 456/590 [25:39<07:26,  3.33s/it]
2022-04-08 21:14:36,735 - INFO - tqdm - accuracy: 0.9974, batch_loss: 0.0002, loss: 0.0068 ||:  78%|#######7  | 460/590 [25:52<07:10,  3.31s/it]
2022-04-08 21:14:47,179 - INFO - tqdm - accuracy: 0.9974, batch_loss: 0.0005, loss: 0.0069 ||:  78%|#######8  | 463/590 [26:02<07:09,  3.38s/it]
2022-04-08 21:14:57,470 - INFO - tqdm - accuracy: 0.9974, batch_loss: 0.0007, loss: 0.0069 ||:  79%|#######8  | 466/590 [26:13<06:47,  3.29s/it]
2022-04-08 21:15:07,630 - INFO - tqdm - accuracy: 0.9974, batch_loss: 0.0010, loss: 0.0069 ||:  79%|#######9  | 469/590 [26:23<06:46,  3.36s/it]
2022-04-08 21:15:20,015 - INFO - tqdm - accuracy: 0.9974, batch_loss: 0.0003, loss: 0.0068 ||:  80%|########  | 473/590 [26:35<06:09,  3.16s/it]
2022-04-08 21:15:30,659 - INFO - tqdm - accuracy: 0.9974, batch_loss: 0.0003, loss: 0.0068 ||:  81%|########  | 476/590 [26:46<06:32,  3.45s/it]
2022-04-08 21:15:41,481 - INFO - tqdm - accuracy: 0.9975, batch_loss: 0.0017, loss: 0.0068 ||:  81%|########1 | 479/590 [26:57<06:28,  3.50s/it]
2022-04-08 21:15:51,552 - INFO - tqdm - accuracy: 0.9975, batch_loss: 0.0069, loss: 0.0067 ||:  82%|########1 | 482/590 [27:07<06:08,  3.41s/it]
2022-04-08 21:16:05,095 - INFO - tqdm - accuracy: 0.9975, batch_loss: 0.0002, loss: 0.0067 ||:  82%|########2 | 486/590 [27:20<05:54,  3.41s/it]
2022-04-08 21:16:15,968 - INFO - tqdm - accuracy: 0.9974, batch_loss: 0.0073, loss: 0.0068 ||:  83%|########2 | 489/590 [27:31<06:04,  3.61s/it]
2022-04-08 21:16:27,991 - INFO - tqdm - accuracy: 0.9974, batch_loss: 0.0002, loss: 0.0070 ||:  83%|########3 | 492/590 [27:43<06:13,  3.81s/it]
2022-04-08 21:16:40,966 - INFO - tqdm - accuracy: 0.9974, batch_loss: 0.0003, loss: 0.0070 ||:  84%|########4 | 496/590 [27:56<05:12,  3.33s/it]
2022-04-08 21:16:51,768 - INFO - tqdm - accuracy: 0.9974, batch_loss: 0.0008, loss: 0.0070 ||:  85%|########4 | 499/590 [28:07<05:22,  3.55s/it]
2022-04-08 21:17:04,184 - INFO - tqdm - accuracy: 0.9973, batch_loss: 0.0002, loss: 0.0070 ||:  85%|########5 | 503/590 [28:19<04:35,  3.17s/it]
2022-04-08 21:17:17,545 - INFO - tqdm - accuracy: 0.9973, batch_loss: 0.0011, loss: 0.0069 ||:  86%|########5 | 507/590 [28:33<04:39,  3.37s/it]
2022-04-08 21:17:29,405 - INFO - tqdm - accuracy: 0.9974, batch_loss: 0.0001, loss: 0.0069 ||:  86%|########6 | 510/590 [28:45<05:07,  3.85s/it]
2022-04-08 21:17:43,615 - INFO - tqdm - accuracy: 0.9974, batch_loss: 0.0008, loss: 0.0068 ||:  87%|########7 | 514/590 [28:59<04:45,  3.75s/it]
2022-04-08 21:17:55,598 - INFO - tqdm - accuracy: 0.9974, batch_loss: 0.0013, loss: 0.0068 ||:  88%|########7 | 518/590 [29:11<03:46,  3.15s/it]
2022-04-08 21:18:09,219 - INFO - tqdm - accuracy: 0.9974, batch_loss: 0.0007, loss: 0.0067 ||:  88%|########8 | 522/590 [29:24<03:48,  3.35s/it]
2022-04-08 21:18:19,440 - INFO - tqdm - accuracy: 0.9974, batch_loss: 0.0031, loss: 0.0069 ||:  89%|########8 | 525/590 [29:35<03:38,  3.36s/it]
2022-04-08 21:18:29,913 - INFO - tqdm - accuracy: 0.9973, batch_loss: 0.0305, loss: 0.0069 ||:  89%|########9 | 528/590 [29:45<03:33,  3.45s/it]
2022-04-08 21:18:42,838 - INFO - tqdm - accuracy: 0.9974, batch_loss: 0.0038, loss: 0.0069 ||:  90%|######### | 532/590 [29:58<03:14,  3.36s/it]
2022-04-08 21:18:56,503 - INFO - tqdm - accuracy: 0.9974, batch_loss: 0.0002, loss: 0.0069 ||:  91%|######### | 536/590 [30:12<03:08,  3.49s/it]
2022-04-08 21:19:09,548 - INFO - tqdm - accuracy: 0.9974, batch_loss: 0.0051, loss: 0.0068 ||:  92%|#########1| 540/590 [30:25<02:48,  3.37s/it]
2022-04-08 21:19:20,613 - INFO - tqdm - accuracy: 0.9973, batch_loss: 0.0002, loss: 0.0069 ||:  92%|#########2| 543/590 [30:36<02:44,  3.51s/it]
2022-04-08 21:19:34,310 - INFO - tqdm - accuracy: 0.9973, batch_loss: 0.0003, loss: 0.0068 ||:  93%|#########2| 547/590 [30:49<02:31,  3.52s/it]
2022-04-08 21:19:45,052 - INFO - tqdm - accuracy: 0.9973, batch_loss: 0.0002, loss: 0.0068 ||:  93%|#########3| 550/590 [31:00<02:20,  3.52s/it]
2022-04-08 21:19:55,154 - INFO - tqdm - accuracy: 0.9973, batch_loss: 0.0032, loss: 0.0069 ||:  94%|#########3| 553/590 [31:10<02:07,  3.44s/it]
2022-04-08 21:20:08,106 - INFO - tqdm - accuracy: 0.9974, batch_loss: 0.0004, loss: 0.0068 ||:  94%|#########4| 557/590 [31:23<01:51,  3.37s/it]
2022-04-08 21:20:21,485 - INFO - tqdm - accuracy: 0.9974, batch_loss: 0.0008, loss: 0.0068 ||:  95%|#########5| 561/590 [31:37<01:39,  3.42s/it]
2022-04-08 21:20:34,392 - INFO - tqdm - accuracy: 0.9973, batch_loss: 0.0002, loss: 0.0068 ||:  96%|#########5| 565/590 [31:50<01:20,  3.23s/it]
2022-04-08 21:20:44,627 - INFO - tqdm - accuracy: 0.9974, batch_loss: 0.0012, loss: 0.0068 ||:  96%|#########6| 568/590 [32:00<01:14,  3.38s/it]
2022-04-08 21:20:56,976 - INFO - tqdm - accuracy: 0.9974, batch_loss: 0.0004, loss: 0.0067 ||:  97%|#########6| 572/590 [32:12<00:57,  3.17s/it]
2022-04-08 21:21:07,076 - INFO - tqdm - accuracy: 0.9974, batch_loss: 0.0002, loss: 0.0067 ||:  97%|#########7| 575/590 [32:22<00:50,  3.35s/it]
2022-04-08 21:21:20,682 - INFO - tqdm - accuracy: 0.9974, batch_loss: 0.0011, loss: 0.0066 ||:  98%|#########8| 579/590 [32:36<00:37,  3.44s/it]
2022-04-08 21:21:33,182 - INFO - tqdm - accuracy: 0.9974, batch_loss: 0.0466, loss: 0.0067 ||:  99%|#########8| 583/590 [32:48<00:22,  3.21s/it]
2022-04-08 21:21:43,515 - INFO - tqdm - accuracy: 0.9974, batch_loss: 0.0019, loss: 0.0066 ||:  99%|#########9| 586/590 [32:59<00:13,  3.36s/it]
2022-04-08 21:21:49,660 - INFO - tqdm - accuracy: 0.9974, batch_loss: 0.0002, loss: 0.0066 ||: 100%|#########9| 588/590 [33:05<00:06,  3.21s/it]
2022-04-08 21:21:52,638 - INFO - tqdm - accuracy: 0.9974, batch_loss: 0.0003, loss: 0.0066 ||: 100%|#########9| 589/590 [33:08<00:03,  3.14s/it]
2022-04-08 21:21:53,840 - INFO - tqdm - accuracy: 0.9974, batch_loss: 0.0001, loss: 0.0066 ||: 100%|##########| 590/590 [33:09<00:00,  2.56s/it]
2022-04-08 21:21:53,841 - INFO - tqdm - accuracy: 0.9974, batch_loss: 0.0001, loss: 0.0066 ||: 100%|##########| 590/590 [33:09<00:00,  3.37s/it]
2022-04-08 21:21:56,127 - INFO - allennlp.training.trainer - Validating
2022-04-08 21:21:56,129 - INFO - tqdm - 0%|          | 0/1635 [00:00<?, ?it/s]
2022-04-08 21:22:06,202 - INFO - tqdm - accuracy: 0.8415, batch_loss: 1.1327, loss: 0.9034 ||:   9%|8         | 142/1635 [00:10<02:22, 10.50it/s]
2022-04-08 21:22:16,342 - INFO - tqdm - accuracy: 0.8497, batch_loss: 0.0220, loss: 0.9801 ||:  18%|#8        | 296/1635 [00:20<01:30, 14.87it/s]
2022-04-08 21:22:26,361 - INFO - tqdm - accuracy: 0.8605, batch_loss: 0.0134, loss: 0.9307 ||:  27%|##6       | 441/1635 [00:30<01:30, 13.23it/s]
2022-04-08 21:22:36,403 - INFO - tqdm - accuracy: 0.8625, batch_loss: 0.0001, loss: 0.9488 ||:  35%|###5      | 578/1635 [00:40<01:08, 15.37it/s]
2022-04-08 21:22:46,515 - INFO - tqdm - accuracy: 0.8556, batch_loss: 0.0001, loss: 1.0042 ||:  44%|####3     | 717/1635 [00:50<01:00, 15.24it/s]
2022-04-08 21:22:56,516 - INFO - tqdm - accuracy: 0.8556, batch_loss: 0.0000, loss: 0.9957 ||:  52%|#####2    | 855/1635 [01:00<00:54, 14.26it/s]
2022-04-08 21:23:06,712 - INFO - tqdm - accuracy: 0.8509, batch_loss: 0.0001, loss: 1.0339 ||:  61%|######1   | 999/1635 [01:10<00:56, 11.18it/s]
2022-04-08 21:23:16,917 - INFO - tqdm - accuracy: 0.8482, batch_loss: 0.0084, loss: 1.0411 ||:  69%|######9   | 1136/1635 [01:20<00:41, 12.00it/s]
2022-04-08 21:23:27,031 - INFO - tqdm - accuracy: 0.8499, batch_loss: 0.0119, loss: 1.0325 ||:  78%|#######8  | 1279/1635 [01:30<00:31, 11.34it/s]
2022-04-08 21:23:37,042 - INFO - tqdm - accuracy: 0.8479, batch_loss: 0.0021, loss: 1.0496 ||:  87%|########7 | 1423/1635 [01:40<00:14, 14.65it/s]
2022-04-08 21:23:47,141 - INFO - tqdm - accuracy: 0.8506, batch_loss: 0.2609, loss: 1.0313 ||:  96%|#########6| 1570/1635 [01:51<00:04, 13.23it/s]
2022-04-08 21:23:51,349 - INFO - tqdm - accuracy: 0.8543, batch_loss: 0.0000, loss: 1.0067 ||: 100%|#########9| 1627/1635 [01:55<00:00, 15.89it/s]
2022-04-08 21:23:51,512 - INFO - tqdm - accuracy: 0.8543, batch_loss: 4.9654, loss: 1.0078 ||: 100%|#########9| 1630/1635 [01:55<00:00, 16.71it/s]
2022-04-08 21:23:51,700 - INFO - tqdm - accuracy: 0.8545, batch_loss: 0.0000, loss: 1.0066 ||: 100%|#########9| 1632/1635 [01:55<00:00, 14.59it/s]
2022-04-08 21:23:51,856 - INFO - tqdm - accuracy: 0.8543, batch_loss: 0.3737, loss: 1.0056 ||: 100%|#########9| 1634/1635 [01:55<00:00, 14.05it/s]
2022-04-08 21:23:51,909 - INFO - tqdm - accuracy: 0.8544, batch_loss: 0.0001, loss: 1.0050 ||: 100%|##########| 1635/1635 [01:55<00:00, 14.12it/s]
2022-04-08 21:23:51,910 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-08 21:23:51,910 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.997  |     0.854
2022-04-08 21:23:51,911 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9604.118  |       N/A
2022-04-08 21:23:51,912 - INFO - allennlp.training.tensorboard_writer - loss               |     0.007  |     1.005
2022-04-08 21:23:51,912 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  7239.301  |       N/A
2022-04-08 21:23:56,437 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to './output_newboolq/best.th'.
2022-04-08 21:24:07,311 - INFO - allennlp.training.trainer - Epoch duration: 0:35:22.928664
2022-04-08 21:24:07,311 - INFO - allennlp.training.checkpointer - loading best weights
2022-04-08 21:24:08,073 - INFO - allennlp.common.util - Metrics: {
  "best_epoch": 9,
  "peak_worker_0_memory_MB": 7239.30078125,
  "peak_gpu_0_memory_MB": 9604.11767578125,
  "training_duration": "5:52:32.695944",
  "training_start_epoch": 0,
  "training_epochs": 9,
  "epoch": 9,
  "training_accuracy": 0.9974010819985148,
  "training_loss": 0.006600519212935007,
  "training_worker_0_memory_MB": 7239.30078125,
  "training_gpu_0_memory_MB": 9604.11767578125,
  "validation_accuracy": 0.854434250764526,
  "validation_loss": 1.0049913044728964,
  "best_validation_accuracy": 0.854434250764526,
  "best_validation_loss": 1.0049913044728964
}
2022-04-08 21:24:08,073 - INFO - allennlp.models.archival - archiving weights and vocabulary to ./output_newboolq/model.tar.gz
