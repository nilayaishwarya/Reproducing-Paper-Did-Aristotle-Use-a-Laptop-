2022-04-11 10:31:25,916 - INFO - allennlp.common.params - random_seed = 42
2022-04-11 10:31:25,916 - INFO - allennlp.common.params - numpy_seed = 42
2022-04-11 10:31:25,916 - INFO - allennlp.common.params - pytorch_seed = 42
2022-04-11 10:31:26,596 - INFO - allennlp.common.checks - Pytorch version: 1.7.1
2022-04-11 10:31:26,596 - INFO - allennlp.common.params - type = default
2022-04-11 10:31:26,597 - INFO - allennlp.common.params - dataset_reader.type = strategy_qa_reader
2022-04-11 10:31:26,597 - INFO - allennlp.common.params - dataset_reader.lazy = False
2022-04-11 10:31:26,597 - INFO - allennlp.common.params - dataset_reader.cache_directory = None
2022-04-11 10:31:26,597 - INFO - allennlp.common.params - dataset_reader.max_instances = None
2022-04-11 10:31:26,597 - INFO - allennlp.common.params - dataset_reader.manual_distributed_sharding = False
2022-04-11 10:31:26,597 - INFO - allennlp.common.params - dataset_reader.manual_multi_process_sharding = False
2022-04-11 10:31:26,597 - INFO - allennlp.common.params - dataset_reader.tokenizer_wrapper.type = default
2022-04-11 10:31:26,597 - INFO - allennlp.common.params - dataset_reader.tokenizer_wrapper.pretrained_model = roberta-large
2022-04-11 10:31:26,598 - INFO - allennlp.common.params - dataset_reader.tokenizer_wrapper.call_kwargs.return_offsets_mapping = True
2022-04-11 10:31:26,598 - INFO - allennlp.common.params - dataset_reader.tokenizer_wrapper.call_kwargs.return_special_tokens_mask = True
2022-04-11 10:31:26,598 - INFO - allennlp.common.params - dataset_reader.tokenizer_wrapper.call_kwargs.truncation = only_first
2022-04-11 10:31:27,780 - INFO - allennlp.common.params - dataset_reader.save_tokenizer = True
2022-04-11 10:31:27,780 - INFO - allennlp.common.params - dataset_reader.is_training = True
2022-04-11 10:31:27,781 - INFO - allennlp.common.params - dataset_reader.pickle.action = None
2022-04-11 10:31:27,781 - INFO - allennlp.common.params - dataset_reader.pickle.file_name = None
2022-04-11 10:31:27,781 - INFO - allennlp.common.params - dataset_reader.pickle.path = ../pickle
2022-04-11 10:31:27,781 - INFO - allennlp.common.params - dataset_reader.pickle.save_even_when_max_instances = False
2022-04-11 10:31:27,781 - INFO - allennlp.common.params - dataset_reader.paragraphs_source = IR-ORA-D
2022-04-11 10:31:27,781 - INFO - allennlp.common.params - dataset_reader.answer_last_decomposition_step = False
2022-04-11 10:31:27,781 - INFO - allennlp.common.params - dataset_reader.skip_if_context_missing = True
2022-04-11 10:31:27,781 - INFO - allennlp.common.params - dataset_reader.paragraphs_limit = 10
2022-04-11 10:31:27,781 - INFO - allennlp.common.params - dataset_reader.generated_decompositions_paths = None
2022-04-11 10:31:27,781 - INFO - allennlp.common.params - dataset_reader.save_elasticsearch_cache = True
2022-04-11 10:31:28,233 - INFO - filelock - Lock 140139027584336 acquired on cache.lock
2022-04-11 10:31:28,914 - INFO - filelock - Lock 140139027584336 released on cache.lock
2022-04-11 10:31:28,915 - INFO - allennlp.common.params - train_data_path = data/strategyqa/train.json
2022-04-11 10:31:28,915 - INFO - allennlp.common.params - vocabulary = <allennlp.common.lazy.Lazy object at 0x7f74b3b7e090>
2022-04-11 10:31:28,915 - INFO - allennlp.common.params - datasets_for_vocab_creation = None
2022-04-11 10:31:28,916 - INFO - allennlp.common.params - validation_dataset_reader.type = strategy_qa_reader
2022-04-11 10:31:28,916 - INFO - allennlp.common.params - validation_dataset_reader.lazy = False
2022-04-11 10:31:28,916 - INFO - allennlp.common.params - validation_dataset_reader.cache_directory = None
2022-04-11 10:31:28,916 - INFO - allennlp.common.params - validation_dataset_reader.max_instances = None
2022-04-11 10:31:28,917 - INFO - allennlp.common.params - validation_dataset_reader.manual_distributed_sharding = False
2022-04-11 10:31:28,917 - INFO - allennlp.common.params - validation_dataset_reader.manual_multi_process_sharding = False
2022-04-11 10:31:28,917 - INFO - allennlp.common.params - validation_dataset_reader.tokenizer_wrapper.type = default
2022-04-11 10:31:28,917 - INFO - allennlp.common.params - validation_dataset_reader.tokenizer_wrapper.pretrained_model = roberta-large
2022-04-11 10:31:28,917 - INFO - allennlp.common.params - validation_dataset_reader.tokenizer_wrapper.call_kwargs.return_offsets_mapping = True
2022-04-11 10:31:28,917 - INFO - allennlp.common.params - validation_dataset_reader.tokenizer_wrapper.call_kwargs.return_special_tokens_mask = True
2022-04-11 10:31:28,917 - INFO - allennlp.common.params - validation_dataset_reader.tokenizer_wrapper.call_kwargs.truncation = only_first
2022-04-11 10:31:30,114 - INFO - allennlp.common.params - validation_dataset_reader.save_tokenizer = False
2022-04-11 10:31:30,115 - INFO - allennlp.common.params - validation_dataset_reader.is_training = False
2022-04-11 10:31:30,115 - INFO - allennlp.common.params - validation_dataset_reader.pickle.action = None
2022-04-11 10:31:30,115 - INFO - allennlp.common.params - validation_dataset_reader.pickle.file_name = None
2022-04-11 10:31:30,115 - INFO - allennlp.common.params - validation_dataset_reader.pickle.path = ../pickle
2022-04-11 10:31:30,115 - INFO - allennlp.common.params - validation_dataset_reader.pickle.save_even_when_max_instances = False
2022-04-11 10:31:30,115 - INFO - allennlp.common.params - validation_dataset_reader.paragraphs_source = IR-ORA-D
2022-04-11 10:31:30,115 - INFO - allennlp.common.params - validation_dataset_reader.answer_last_decomposition_step = False
2022-04-11 10:31:30,115 - INFO - allennlp.common.params - validation_dataset_reader.skip_if_context_missing = True
2022-04-11 10:31:30,115 - INFO - allennlp.common.params - validation_dataset_reader.paragraphs_limit = 10
2022-04-11 10:31:30,115 - INFO - allennlp.common.params - validation_dataset_reader.generated_decompositions_paths = None
2022-04-11 10:31:30,115 - INFO - allennlp.common.params - validation_dataset_reader.save_elasticsearch_cache = True
2022-04-11 10:31:30,115 - INFO - filelock - Lock 140139207564240 acquired on cache.lock
2022-04-11 10:31:30,926 - INFO - filelock - Lock 140139207564240 released on cache.lock
2022-04-11 10:31:30,927 - INFO - allennlp.common.params - validation_data_path = data/strategyqa/dev.json
2022-04-11 10:31:30,928 - INFO - allennlp.common.params - validation_data_loader = None
2022-04-11 10:31:30,928 - INFO - allennlp.common.params - test_data_path = None
2022-04-11 10:31:30,928 - INFO - allennlp.common.params - evaluate_on_test = True
2022-04-11 10:31:30,928 - INFO - allennlp.common.params - batch_weight_key = 
2022-04-11 10:31:30,928 - INFO - allennlp.training.util - Reading training data from data/strategyqa/train.json
2022-04-11 10:31:30,929 - INFO - tqdm - reading instances: 0it [00:00, ?it/s]
2022-04-11 10:31:30,929 - INFO - src.data.dataset_readers.base.base_dataset_reader - Reading the dataset from scratch
2022-04-11 10:31:30,929 - INFO - src.data.dataset_readers.strategy_qa_reader - Reading the dataset:
2022-04-11 10:31:30,929 - INFO - src.data.dataset_readers.strategy_qa_reader - Reading file at data/strategyqa/train.json
2022-04-11 10:31:38,318 - INFO - filelock - Lock 140138368808912 acquired on cache.lock
2022-04-11 10:31:39,111 - INFO - filelock - Lock 140138368808912 released on cache.lock
2022-04-11 10:31:39,148 - INFO - allennlp.training.util - Reading validation data from data/strategyqa/dev.json
2022-04-11 10:31:39,149 - INFO - tqdm - reading instances: 0it [00:00, ?it/s]
2022-04-11 10:31:39,149 - INFO - src.data.dataset_readers.base.base_dataset_reader - Reading the dataset from scratch
2022-04-11 10:31:39,149 - INFO - src.data.dataset_readers.strategy_qa_reader - Reading the dataset:
2022-04-11 10:31:39,149 - INFO - src.data.dataset_readers.strategy_qa_reader - Reading file at data/strategyqa/dev.json
2022-04-11 10:31:39,596 - INFO - src.data.dataset_readers.utils.elasticsearch_utils - Empty results from Elasticsearch for: actions commercial pilots take concerning engine noise arriving departing Orange County, California?
2022-04-11 10:31:39,922 - INFO - filelock - Lock 140138376226256 acquired on cache.lock
2022-04-11 10:31:40,719 - INFO - filelock - Lock 140138376226256 released on cache.lock
2022-04-11 10:31:40,754 - INFO - allennlp.common.params - type = from_instances
2022-04-11 10:31:40,755 - INFO - allennlp.common.params - min_count = None
2022-04-11 10:31:40,755 - INFO - allennlp.common.params - max_vocab_size = None
2022-04-11 10:31:40,755 - INFO - allennlp.common.params - non_padded_namespaces = ('*tags', '*labels')
2022-04-11 10:31:40,755 - INFO - allennlp.common.params - pretrained_files = None
2022-04-11 10:31:40,755 - INFO - allennlp.common.params - only_include_pretrained_words = False
2022-04-11 10:31:40,755 - INFO - allennlp.common.params - tokens_to_add = None
2022-04-11 10:31:40,756 - INFO - allennlp.common.params - min_pretrained_embeddings = None
2022-04-11 10:31:40,756 - INFO - allennlp.common.params - padding_token = @@PADDING@@
2022-04-11 10:31:40,756 - INFO - allennlp.common.params - oov_token = @@UNKNOWN@@
2022-04-11 10:31:40,756 - INFO - allennlp.data.vocabulary - Fitting token dictionary from dataset.
2022-04-11 10:31:40,756 - INFO - tqdm - building vocab: 0it [00:00, ?it/s]
2022-04-11 10:31:40,762 - INFO - allennlp.common.params - model.type = hf_classifier
2022-04-11 10:31:40,762 - INFO - allennlp.common.params - model.regularizer = None
2022-04-11 10:31:40,762 - INFO - allennlp.common.params - model.pretrained_model = roberta-large
2022-04-11 10:31:40,763 - INFO - allennlp.common.params - model.tokenizer_wrapper.type = default
2022-04-11 10:31:40,763 - INFO - allennlp.common.params - model.tokenizer_wrapper.pretrained_model = roberta-large
2022-04-11 10:31:40,763 - INFO - allennlp.common.params - model.tokenizer_wrapper.call_kwargs.return_offsets_mapping = True
2022-04-11 10:31:40,763 - INFO - allennlp.common.params - model.tokenizer_wrapper.call_kwargs.return_special_tokens_mask = True
2022-04-11 10:31:40,763 - INFO - allennlp.common.params - model.tokenizer_wrapper.call_kwargs.truncation = only_first
2022-04-11 10:31:41,947 - INFO - allennlp.common.params - model.num_labels = 2
2022-04-11 10:31:41,947 - INFO - allennlp.common.params - model.label_namespace = labels
2022-04-11 10:31:41,947 - INFO - allennlp.common.params - model.transformer_weights_path = None
2022-04-11 10:31:41,948 - INFO - allennlp.common.params - model.initializer.regexes.0.1.type = pretrained
2022-04-11 10:31:41,948 - INFO - allennlp.common.params - model.initializer.regexes.0.1.weights_file_path = output_roberta-star/model.tar.gz
2022-04-11 10:31:41,949 - INFO - allennlp.models.archival - extracting archive file output_roberta-star/model.tar.gz to temp dir /tmp/tmpmvr1aq1l
2022-04-11 10:31:53,618 - INFO - allennlp.models.archival - removing temporary unarchived model dir at /tmp/tmpmvr1aq1l
2022-04-11 10:31:53,794 - INFO - allennlp.common.params - model.initializer.prevent_regexes = None
2022-04-11 10:32:08,760 - INFO - allennlp.nn.initializers - Initializing parameters
2022-04-11 10:32:08,760 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.embeddings.word_embeddings.weight using .* initializer
2022-04-11 10:32:08,769 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.embeddings.position_embeddings.weight using .* initializer
2022-04-11 10:32:08,770 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.embeddings.token_type_embeddings.weight using .* initializer
2022-04-11 10:32:08,770 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.embeddings.LayerNorm.weight using .* initializer
2022-04-11 10:32:08,770 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.embeddings.LayerNorm.bias using .* initializer
2022-04-11 10:32:08,770 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.self.query.weight using .* initializer
2022-04-11 10:32:08,771 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.self.query.bias using .* initializer
2022-04-11 10:32:08,771 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.self.key.weight using .* initializer
2022-04-11 10:32:08,771 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.self.key.bias using .* initializer
2022-04-11 10:32:08,771 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.self.value.weight using .* initializer
2022-04-11 10:32:08,772 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.self.value.bias using .* initializer
2022-04-11 10:32:08,772 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.output.dense.weight using .* initializer
2022-04-11 10:32:08,772 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.output.dense.bias using .* initializer
2022-04-11 10:32:08,772 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.output.LayerNorm.weight using .* initializer
2022-04-11 10:32:08,772 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.output.LayerNorm.bias using .* initializer
2022-04-11 10:32:08,773 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.intermediate.dense.weight using .* initializer
2022-04-11 10:32:08,773 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.intermediate.dense.bias using .* initializer
2022-04-11 10:32:08,774 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.output.dense.weight using .* initializer
2022-04-11 10:32:08,775 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.output.dense.bias using .* initializer
2022-04-11 10:32:08,775 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.output.LayerNorm.weight using .* initializer
2022-04-11 10:32:08,775 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.output.LayerNorm.bias using .* initializer
2022-04-11 10:32:08,775 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.self.query.weight using .* initializer
2022-04-11 10:32:08,775 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.self.query.bias using .* initializer
2022-04-11 10:32:08,776 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.self.key.weight using .* initializer
2022-04-11 10:32:08,776 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.self.key.bias using .* initializer
2022-04-11 10:32:08,776 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.self.value.weight using .* initializer
2022-04-11 10:32:08,776 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.self.value.bias using .* initializer
2022-04-11 10:32:08,777 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.output.dense.weight using .* initializer
2022-04-11 10:32:08,777 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.output.dense.bias using .* initializer
2022-04-11 10:32:08,777 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.output.LayerNorm.weight using .* initializer
2022-04-11 10:32:08,777 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.output.LayerNorm.bias using .* initializer
2022-04-11 10:32:08,777 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.intermediate.dense.weight using .* initializer
2022-04-11 10:32:08,778 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.intermediate.dense.bias using .* initializer
2022-04-11 10:32:08,778 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.output.dense.weight using .* initializer
2022-04-11 10:32:08,779 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.output.dense.bias using .* initializer
2022-04-11 10:32:08,779 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.output.LayerNorm.weight using .* initializer
2022-04-11 10:32:08,780 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.output.LayerNorm.bias using .* initializer
2022-04-11 10:32:08,780 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.self.query.weight using .* initializer
2022-04-11 10:32:08,780 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.self.query.bias using .* initializer
2022-04-11 10:32:08,780 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.self.key.weight using .* initializer
2022-04-11 10:32:08,781 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.self.key.bias using .* initializer
2022-04-11 10:32:08,781 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.self.value.weight using .* initializer
2022-04-11 10:32:08,781 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.self.value.bias using .* initializer
2022-04-11 10:32:08,781 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.output.dense.weight using .* initializer
2022-04-11 10:32:08,782 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.output.dense.bias using .* initializer
2022-04-11 10:32:08,782 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.output.LayerNorm.weight using .* initializer
2022-04-11 10:32:08,782 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.output.LayerNorm.bias using .* initializer
2022-04-11 10:32:08,782 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.intermediate.dense.weight using .* initializer
2022-04-11 10:32:08,783 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.intermediate.dense.bias using .* initializer
2022-04-11 10:32:08,783 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.output.dense.weight using .* initializer
2022-04-11 10:32:08,784 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.output.dense.bias using .* initializer
2022-04-11 10:32:08,784 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.output.LayerNorm.weight using .* initializer
2022-04-11 10:32:08,784 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.output.LayerNorm.bias using .* initializer
2022-04-11 10:32:08,784 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.self.query.weight using .* initializer
2022-04-11 10:32:08,785 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.self.query.bias using .* initializer
2022-04-11 10:32:08,785 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.self.key.weight using .* initializer
2022-04-11 10:32:08,785 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.self.key.bias using .* initializer
2022-04-11 10:32:08,785 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.self.value.weight using .* initializer
2022-04-11 10:32:08,786 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.self.value.bias using .* initializer
2022-04-11 10:32:08,786 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.output.dense.weight using .* initializer
2022-04-11 10:32:08,786 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.output.dense.bias using .* initializer
2022-04-11 10:32:08,786 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.output.LayerNorm.weight using .* initializer
2022-04-11 10:32:08,786 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.output.LayerNorm.bias using .* initializer
2022-04-11 10:32:08,787 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.intermediate.dense.weight using .* initializer
2022-04-11 10:32:08,788 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.intermediate.dense.bias using .* initializer
2022-04-11 10:32:08,788 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.output.dense.weight using .* initializer
2022-04-11 10:32:08,789 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.output.dense.bias using .* initializer
2022-04-11 10:32:08,789 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.output.LayerNorm.weight using .* initializer
2022-04-11 10:32:08,789 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.output.LayerNorm.bias using .* initializer
2022-04-11 10:32:08,789 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.self.query.weight using .* initializer
2022-04-11 10:32:08,789 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.self.query.bias using .* initializer
2022-04-11 10:32:08,790 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.self.key.weight using .* initializer
2022-04-11 10:32:08,790 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.self.key.bias using .* initializer
2022-04-11 10:32:08,790 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.self.value.weight using .* initializer
2022-04-11 10:32:08,790 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.self.value.bias using .* initializer
2022-04-11 10:32:08,791 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.output.dense.weight using .* initializer
2022-04-11 10:32:08,791 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.output.dense.bias using .* initializer
2022-04-11 10:32:08,791 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.output.LayerNorm.weight using .* initializer
2022-04-11 10:32:08,791 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.output.LayerNorm.bias using .* initializer
2022-04-11 10:32:08,792 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.intermediate.dense.weight using .* initializer
2022-04-11 10:32:08,793 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.intermediate.dense.bias using .* initializer
2022-04-11 10:32:08,793 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.output.dense.weight using .* initializer
2022-04-11 10:32:08,794 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.output.dense.bias using .* initializer
2022-04-11 10:32:08,794 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.output.LayerNorm.weight using .* initializer
2022-04-11 10:32:08,794 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.output.LayerNorm.bias using .* initializer
2022-04-11 10:32:08,794 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.self.query.weight using .* initializer
2022-04-11 10:32:08,795 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.self.query.bias using .* initializer
2022-04-11 10:32:08,795 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.self.key.weight using .* initializer
2022-04-11 10:32:08,795 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.self.key.bias using .* initializer
2022-04-11 10:32:08,795 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.self.value.weight using .* initializer
2022-04-11 10:32:08,796 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.self.value.bias using .* initializer
2022-04-11 10:32:08,796 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.output.dense.weight using .* initializer
2022-04-11 10:32:08,796 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.output.dense.bias using .* initializer
2022-04-11 10:32:08,797 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.output.LayerNorm.weight using .* initializer
2022-04-11 10:32:08,797 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.output.LayerNorm.bias using .* initializer
2022-04-11 10:32:08,797 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.intermediate.dense.weight using .* initializer
2022-04-11 10:32:08,798 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.intermediate.dense.bias using .* initializer
2022-04-11 10:32:08,798 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.output.dense.weight using .* initializer
2022-04-11 10:32:08,799 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.output.dense.bias using .* initializer
2022-04-11 10:32:08,799 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.output.LayerNorm.weight using .* initializer
2022-04-11 10:32:08,799 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.output.LayerNorm.bias using .* initializer
2022-04-11 10:32:08,800 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.self.query.weight using .* initializer
2022-04-11 10:32:08,800 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.self.query.bias using .* initializer
2022-04-11 10:32:08,800 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.self.key.weight using .* initializer
2022-04-11 10:32:08,801 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.self.key.bias using .* initializer
2022-04-11 10:32:08,801 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.self.value.weight using .* initializer
2022-04-11 10:32:08,801 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.self.value.bias using .* initializer
2022-04-11 10:32:08,801 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.output.dense.weight using .* initializer
2022-04-11 10:32:08,802 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.output.dense.bias using .* initializer
2022-04-11 10:32:08,802 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.output.LayerNorm.weight using .* initializer
2022-04-11 10:32:08,802 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.output.LayerNorm.bias using .* initializer
2022-04-11 10:32:08,802 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.intermediate.dense.weight using .* initializer
2022-04-11 10:32:08,803 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.intermediate.dense.bias using .* initializer
2022-04-11 10:32:08,803 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.output.dense.weight using .* initializer
2022-04-11 10:32:08,804 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.output.dense.bias using .* initializer
2022-04-11 10:32:08,805 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.output.LayerNorm.weight using .* initializer
2022-04-11 10:32:08,805 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.output.LayerNorm.bias using .* initializer
2022-04-11 10:32:08,805 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.self.query.weight using .* initializer
2022-04-11 10:32:08,805 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.self.query.bias using .* initializer
2022-04-11 10:32:08,806 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.self.key.weight using .* initializer
2022-04-11 10:32:08,806 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.self.key.bias using .* initializer
2022-04-11 10:32:08,806 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.self.value.weight using .* initializer
2022-04-11 10:32:08,807 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.self.value.bias using .* initializer
2022-04-11 10:32:08,807 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.output.dense.weight using .* initializer
2022-04-11 10:32:08,807 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.output.dense.bias using .* initializer
2022-04-11 10:32:08,807 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.output.LayerNorm.weight using .* initializer
2022-04-11 10:32:08,808 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.output.LayerNorm.bias using .* initializer
2022-04-11 10:32:08,808 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.intermediate.dense.weight using .* initializer
2022-04-11 10:32:08,809 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.intermediate.dense.bias using .* initializer
2022-04-11 10:32:08,809 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.output.dense.weight using .* initializer
2022-04-11 10:32:08,810 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.output.dense.bias using .* initializer
2022-04-11 10:32:08,810 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.output.LayerNorm.weight using .* initializer
2022-04-11 10:32:08,810 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.output.LayerNorm.bias using .* initializer
2022-04-11 10:32:08,810 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.self.query.weight using .* initializer
2022-04-11 10:32:08,811 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.self.query.bias using .* initializer
2022-04-11 10:32:08,811 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.self.key.weight using .* initializer
2022-04-11 10:32:08,811 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.self.key.bias using .* initializer
2022-04-11 10:32:08,812 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.self.value.weight using .* initializer
2022-04-11 10:32:08,812 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.self.value.bias using .* initializer
2022-04-11 10:32:08,812 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.output.dense.weight using .* initializer
2022-04-11 10:32:08,813 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.output.dense.bias using .* initializer
2022-04-11 10:32:08,813 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.output.LayerNorm.weight using .* initializer
2022-04-11 10:32:08,813 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.output.LayerNorm.bias using .* initializer
2022-04-11 10:32:08,813 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.intermediate.dense.weight using .* initializer
2022-04-11 10:32:08,814 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.intermediate.dense.bias using .* initializer
2022-04-11 10:32:08,814 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.output.dense.weight using .* initializer
2022-04-11 10:32:08,815 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.output.dense.bias using .* initializer
2022-04-11 10:32:08,815 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.output.LayerNorm.weight using .* initializer
2022-04-11 10:32:08,815 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.output.LayerNorm.bias using .* initializer
2022-04-11 10:32:08,816 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.self.query.weight using .* initializer
2022-04-11 10:32:08,816 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.self.query.bias using .* initializer
2022-04-11 10:32:08,816 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.self.key.weight using .* initializer
2022-04-11 10:32:08,816 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.self.key.bias using .* initializer
2022-04-11 10:32:08,817 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.self.value.weight using .* initializer
2022-04-11 10:32:08,817 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.self.value.bias using .* initializer
2022-04-11 10:32:08,817 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.output.dense.weight using .* initializer
2022-04-11 10:32:08,817 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.output.dense.bias using .* initializer
2022-04-11 10:32:08,818 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.output.LayerNorm.weight using .* initializer
2022-04-11 10:32:08,818 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.output.LayerNorm.bias using .* initializer
2022-04-11 10:32:08,818 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.intermediate.dense.weight using .* initializer
2022-04-11 10:32:08,819 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.intermediate.dense.bias using .* initializer
2022-04-11 10:32:08,819 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.output.dense.weight using .* initializer
2022-04-11 10:32:08,820 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.output.dense.bias using .* initializer
2022-04-11 10:32:08,820 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.output.LayerNorm.weight using .* initializer
2022-04-11 10:32:08,820 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.output.LayerNorm.bias using .* initializer
2022-04-11 10:32:08,820 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.self.query.weight using .* initializer
2022-04-11 10:32:08,821 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.self.query.bias using .* initializer
2022-04-11 10:32:08,821 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.self.key.weight using .* initializer
2022-04-11 10:32:08,821 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.self.key.bias using .* initializer
2022-04-11 10:32:08,821 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.self.value.weight using .* initializer
2022-04-11 10:32:08,822 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.self.value.bias using .* initializer
2022-04-11 10:32:08,822 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.output.dense.weight using .* initializer
2022-04-11 10:32:08,822 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.output.dense.bias using .* initializer
2022-04-11 10:32:08,822 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.output.LayerNorm.weight using .* initializer
2022-04-11 10:32:08,822 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.output.LayerNorm.bias using .* initializer
2022-04-11 10:32:08,823 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.intermediate.dense.weight using .* initializer
2022-04-11 10:32:08,824 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.intermediate.dense.bias using .* initializer
2022-04-11 10:32:08,824 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.output.dense.weight using .* initializer
2022-04-11 10:32:08,825 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.output.dense.bias using .* initializer
2022-04-11 10:32:08,825 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.output.LayerNorm.weight using .* initializer
2022-04-11 10:32:08,825 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.output.LayerNorm.bias using .* initializer
2022-04-11 10:32:08,825 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.self.query.weight using .* initializer
2022-04-11 10:32:08,826 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.self.query.bias using .* initializer
2022-04-11 10:32:08,826 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.self.key.weight using .* initializer
2022-04-11 10:32:08,826 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.self.key.bias using .* initializer
2022-04-11 10:32:08,826 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.self.value.weight using .* initializer
2022-04-11 10:32:08,827 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.self.value.bias using .* initializer
2022-04-11 10:32:08,827 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.output.dense.weight using .* initializer
2022-04-11 10:32:08,827 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.output.dense.bias using .* initializer
2022-04-11 10:32:08,828 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.output.LayerNorm.weight using .* initializer
2022-04-11 10:32:08,828 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.output.LayerNorm.bias using .* initializer
2022-04-11 10:32:08,828 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.intermediate.dense.weight using .* initializer
2022-04-11 10:32:08,829 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.intermediate.dense.bias using .* initializer
2022-04-11 10:32:08,829 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.output.dense.weight using .* initializer
2022-04-11 10:32:08,830 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.output.dense.bias using .* initializer
2022-04-11 10:32:08,830 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.output.LayerNorm.weight using .* initializer
2022-04-11 10:32:08,830 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.output.LayerNorm.bias using .* initializer
2022-04-11 10:32:08,831 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.self.query.weight using .* initializer
2022-04-11 10:32:08,831 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.self.query.bias using .* initializer
2022-04-11 10:32:08,831 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.self.key.weight using .* initializer
2022-04-11 10:32:08,832 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.self.key.bias using .* initializer
2022-04-11 10:32:08,832 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.self.value.weight using .* initializer
2022-04-11 10:32:08,832 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.self.value.bias using .* initializer
2022-04-11 10:32:08,832 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.output.dense.weight using .* initializer
2022-04-11 10:32:08,833 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.output.dense.bias using .* initializer
2022-04-11 10:32:08,833 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.output.LayerNorm.weight using .* initializer
2022-04-11 10:32:08,833 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.output.LayerNorm.bias using .* initializer
2022-04-11 10:32:08,833 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.intermediate.dense.weight using .* initializer
2022-04-11 10:32:08,834 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.intermediate.dense.bias using .* initializer
2022-04-11 10:32:08,834 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.output.dense.weight using .* initializer
2022-04-11 10:32:08,835 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.output.dense.bias using .* initializer
2022-04-11 10:32:08,836 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.output.LayerNorm.weight using .* initializer
2022-04-11 10:32:08,836 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.output.LayerNorm.bias using .* initializer
2022-04-11 10:32:08,836 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.self.query.weight using .* initializer
2022-04-11 10:32:08,836 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.self.query.bias using .* initializer
2022-04-11 10:32:08,837 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.self.key.weight using .* initializer
2022-04-11 10:32:08,837 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.self.key.bias using .* initializer
2022-04-11 10:32:08,837 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.self.value.weight using .* initializer
2022-04-11 10:32:08,838 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.self.value.bias using .* initializer
2022-04-11 10:32:08,838 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.output.dense.weight using .* initializer
2022-04-11 10:32:08,838 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.output.dense.bias using .* initializer
2022-04-11 10:32:08,838 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.output.LayerNorm.weight using .* initializer
2022-04-11 10:32:08,839 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.output.LayerNorm.bias using .* initializer
2022-04-11 10:32:08,839 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.intermediate.dense.weight using .* initializer
2022-04-11 10:32:08,840 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.intermediate.dense.bias using .* initializer
2022-04-11 10:32:08,840 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.output.dense.weight using .* initializer
2022-04-11 10:32:08,841 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.output.dense.bias using .* initializer
2022-04-11 10:32:08,841 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.output.LayerNorm.weight using .* initializer
2022-04-11 10:32:08,841 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.output.LayerNorm.bias using .* initializer
2022-04-11 10:32:08,841 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.self.query.weight using .* initializer
2022-04-11 10:32:08,842 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.self.query.bias using .* initializer
2022-04-11 10:32:08,842 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.self.key.weight using .* initializer
2022-04-11 10:32:08,842 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.self.key.bias using .* initializer
2022-04-11 10:32:08,842 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.self.value.weight using .* initializer
2022-04-11 10:32:08,843 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.self.value.bias using .* initializer
2022-04-11 10:32:08,843 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.output.dense.weight using .* initializer
2022-04-11 10:32:08,843 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.output.dense.bias using .* initializer
2022-04-11 10:32:08,844 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.output.LayerNorm.weight using .* initializer
2022-04-11 10:32:08,844 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.output.LayerNorm.bias using .* initializer
2022-04-11 10:32:08,844 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.intermediate.dense.weight using .* initializer
2022-04-11 10:32:08,845 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.intermediate.dense.bias using .* initializer
2022-04-11 10:32:08,845 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.output.dense.weight using .* initializer
2022-04-11 10:32:08,846 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.output.dense.bias using .* initializer
2022-04-11 10:32:08,846 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.output.LayerNorm.weight using .* initializer
2022-04-11 10:32:08,846 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.output.LayerNorm.bias using .* initializer
2022-04-11 10:32:08,847 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.self.query.weight using .* initializer
2022-04-11 10:32:08,847 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.self.query.bias using .* initializer
2022-04-11 10:32:08,847 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.self.key.weight using .* initializer
2022-04-11 10:32:08,848 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.self.key.bias using .* initializer
2022-04-11 10:32:08,848 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.self.value.weight using .* initializer
2022-04-11 10:32:08,848 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.self.value.bias using .* initializer
2022-04-11 10:32:08,849 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.output.dense.weight using .* initializer
2022-04-11 10:32:08,849 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.output.dense.bias using .* initializer
2022-04-11 10:32:08,849 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.output.LayerNorm.weight using .* initializer
2022-04-11 10:32:08,849 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.output.LayerNorm.bias using .* initializer
2022-04-11 10:32:08,849 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.intermediate.dense.weight using .* initializer
2022-04-11 10:32:08,850 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.intermediate.dense.bias using .* initializer
2022-04-11 10:32:08,851 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.output.dense.weight using .* initializer
2022-04-11 10:32:08,852 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.output.dense.bias using .* initializer
2022-04-11 10:32:08,852 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.output.LayerNorm.weight using .* initializer
2022-04-11 10:32:08,852 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.output.LayerNorm.bias using .* initializer
2022-04-11 10:32:08,852 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.self.query.weight using .* initializer
2022-04-11 10:32:08,852 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.self.query.bias using .* initializer
2022-04-11 10:32:08,853 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.self.key.weight using .* initializer
2022-04-11 10:32:08,853 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.self.key.bias using .* initializer
2022-04-11 10:32:08,853 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.self.value.weight using .* initializer
2022-04-11 10:32:08,854 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.self.value.bias using .* initializer
2022-04-11 10:32:08,854 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.output.dense.weight using .* initializer
2022-04-11 10:32:08,854 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.output.dense.bias using .* initializer
2022-04-11 10:32:08,854 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.output.LayerNorm.weight using .* initializer
2022-04-11 10:32:08,854 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.output.LayerNorm.bias using .* initializer
2022-04-11 10:32:08,855 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.intermediate.dense.weight using .* initializer
2022-04-11 10:32:08,856 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.intermediate.dense.bias using .* initializer
2022-04-11 10:32:08,856 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.output.dense.weight using .* initializer
2022-04-11 10:32:08,857 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.output.dense.bias using .* initializer
2022-04-11 10:32:08,857 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.output.LayerNorm.weight using .* initializer
2022-04-11 10:32:08,857 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.output.LayerNorm.bias using .* initializer
2022-04-11 10:32:08,857 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.self.query.weight using .* initializer
2022-04-11 10:32:08,858 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.self.query.bias using .* initializer
2022-04-11 10:32:08,858 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.self.key.weight using .* initializer
2022-04-11 10:32:08,858 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.self.key.bias using .* initializer
2022-04-11 10:32:08,858 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.self.value.weight using .* initializer
2022-04-11 10:32:08,859 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.self.value.bias using .* initializer
2022-04-11 10:32:08,859 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.output.dense.weight using .* initializer
2022-04-11 10:32:08,859 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.output.dense.bias using .* initializer
2022-04-11 10:32:08,860 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.output.LayerNorm.weight using .* initializer
2022-04-11 10:32:08,860 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.output.LayerNorm.bias using .* initializer
2022-04-11 10:32:08,860 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.intermediate.dense.weight using .* initializer
2022-04-11 10:32:08,861 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.intermediate.dense.bias using .* initializer
2022-04-11 10:32:08,861 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.output.dense.weight using .* initializer
2022-04-11 10:32:08,862 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.output.dense.bias using .* initializer
2022-04-11 10:32:08,863 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.output.LayerNorm.weight using .* initializer
2022-04-11 10:32:08,863 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.output.LayerNorm.bias using .* initializer
2022-04-11 10:32:08,863 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.self.query.weight using .* initializer
2022-04-11 10:32:08,863 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.self.query.bias using .* initializer
2022-04-11 10:32:08,863 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.self.key.weight using .* initializer
2022-04-11 10:32:08,864 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.self.key.bias using .* initializer
2022-04-11 10:32:08,864 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.self.value.weight using .* initializer
2022-04-11 10:32:08,864 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.self.value.bias using .* initializer
2022-04-11 10:32:08,865 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.output.dense.weight using .* initializer
2022-04-11 10:32:08,865 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.output.dense.bias using .* initializer
2022-04-11 10:32:08,865 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.output.LayerNorm.weight using .* initializer
2022-04-11 10:32:08,865 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.output.LayerNorm.bias using .* initializer
2022-04-11 10:32:08,865 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.intermediate.dense.weight using .* initializer
2022-04-11 10:32:08,866 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.intermediate.dense.bias using .* initializer
2022-04-11 10:32:08,867 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.output.dense.weight using .* initializer
2022-04-11 10:32:08,868 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.output.dense.bias using .* initializer
2022-04-11 10:32:08,868 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.output.LayerNorm.weight using .* initializer
2022-04-11 10:32:08,868 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.output.LayerNorm.bias using .* initializer
2022-04-11 10:32:08,868 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.self.query.weight using .* initializer
2022-04-11 10:32:08,868 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.self.query.bias using .* initializer
2022-04-11 10:32:08,869 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.self.key.weight using .* initializer
2022-04-11 10:32:08,869 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.self.key.bias using .* initializer
2022-04-11 10:32:08,869 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.self.value.weight using .* initializer
2022-04-11 10:32:08,870 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.self.value.bias using .* initializer
2022-04-11 10:32:08,870 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.output.dense.weight using .* initializer
2022-04-11 10:32:08,870 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.output.dense.bias using .* initializer
2022-04-11 10:32:08,870 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.output.LayerNorm.weight using .* initializer
2022-04-11 10:32:08,871 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.output.LayerNorm.bias using .* initializer
2022-04-11 10:32:08,871 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.intermediate.dense.weight using .* initializer
2022-04-11 10:32:08,872 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.intermediate.dense.bias using .* initializer
2022-04-11 10:32:08,872 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.output.dense.weight using .* initializer
2022-04-11 10:32:08,873 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.output.dense.bias using .* initializer
2022-04-11 10:32:08,873 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.output.LayerNorm.weight using .* initializer
2022-04-11 10:32:08,873 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.output.LayerNorm.bias using .* initializer
2022-04-11 10:32:08,873 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.self.query.weight using .* initializer
2022-04-11 10:32:08,874 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.self.query.bias using .* initializer
2022-04-11 10:32:08,874 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.self.key.weight using .* initializer
2022-04-11 10:32:08,874 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.self.key.bias using .* initializer
2022-04-11 10:32:08,874 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.self.value.weight using .* initializer
2022-04-11 10:32:08,875 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.self.value.bias using .* initializer
2022-04-11 10:32:08,875 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.output.dense.weight using .* initializer
2022-04-11 10:32:08,875 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.output.dense.bias using .* initializer
2022-04-11 10:32:08,876 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.output.LayerNorm.weight using .* initializer
2022-04-11 10:32:08,876 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.output.LayerNorm.bias using .* initializer
2022-04-11 10:32:08,876 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.intermediate.dense.weight using .* initializer
2022-04-11 10:32:08,877 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.intermediate.dense.bias using .* initializer
2022-04-11 10:32:08,877 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.output.dense.weight using .* initializer
2022-04-11 10:32:08,878 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.output.dense.bias using .* initializer
2022-04-11 10:32:08,878 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.output.LayerNorm.weight using .* initializer
2022-04-11 10:32:08,878 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.output.LayerNorm.bias using .* initializer
2022-04-11 10:32:08,879 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.self.query.weight using .* initializer
2022-04-11 10:32:08,879 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.self.query.bias using .* initializer
2022-04-11 10:32:08,879 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.self.key.weight using .* initializer
2022-04-11 10:32:08,880 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.self.key.bias using .* initializer
2022-04-11 10:32:08,880 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.self.value.weight using .* initializer
2022-04-11 10:32:08,880 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.self.value.bias using .* initializer
2022-04-11 10:32:08,880 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.output.dense.weight using .* initializer
2022-04-11 10:32:08,881 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.output.dense.bias using .* initializer
2022-04-11 10:32:08,881 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.output.LayerNorm.weight using .* initializer
2022-04-11 10:32:08,881 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.output.LayerNorm.bias using .* initializer
2022-04-11 10:32:08,881 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.intermediate.dense.weight using .* initializer
2022-04-11 10:32:08,882 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.intermediate.dense.bias using .* initializer
2022-04-11 10:32:08,882 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.output.dense.weight using .* initializer
2022-04-11 10:32:08,883 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.output.dense.bias using .* initializer
2022-04-11 10:32:08,884 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.output.LayerNorm.weight using .* initializer
2022-04-11 10:32:08,884 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.output.LayerNorm.bias using .* initializer
2022-04-11 10:32:08,884 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.self.query.weight using .* initializer
2022-04-11 10:32:08,884 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.self.query.bias using .* initializer
2022-04-11 10:32:08,884 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.self.key.weight using .* initializer
2022-04-11 10:32:08,885 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.self.key.bias using .* initializer
2022-04-11 10:32:08,885 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.self.value.weight using .* initializer
2022-04-11 10:32:08,885 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.self.value.bias using .* initializer
2022-04-11 10:32:08,886 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.output.dense.weight using .* initializer
2022-04-11 10:32:08,886 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.output.dense.bias using .* initializer
2022-04-11 10:32:08,886 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.output.LayerNorm.weight using .* initializer
2022-04-11 10:32:08,887 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.output.LayerNorm.bias using .* initializer
2022-04-11 10:32:08,887 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.intermediate.dense.weight using .* initializer
2022-04-11 10:32:08,888 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.intermediate.dense.bias using .* initializer
2022-04-11 10:32:08,888 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.output.dense.weight using .* initializer
2022-04-11 10:32:08,889 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.output.dense.bias using .* initializer
2022-04-11 10:32:08,889 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.output.LayerNorm.weight using .* initializer
2022-04-11 10:32:08,889 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.output.LayerNorm.bias using .* initializer
2022-04-11 10:32:08,889 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.self.query.weight using .* initializer
2022-04-11 10:32:08,890 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.self.query.bias using .* initializer
2022-04-11 10:32:08,890 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.self.key.weight using .* initializer
2022-04-11 10:32:08,890 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.self.key.bias using .* initializer
2022-04-11 10:32:08,890 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.self.value.weight using .* initializer
2022-04-11 10:32:08,891 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.self.value.bias using .* initializer
2022-04-11 10:32:08,891 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.output.dense.weight using .* initializer
2022-04-11 10:32:08,891 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.output.dense.bias using .* initializer
2022-04-11 10:32:08,892 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.output.LayerNorm.weight using .* initializer
2022-04-11 10:32:08,892 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.output.LayerNorm.bias using .* initializer
2022-04-11 10:32:08,892 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.intermediate.dense.weight using .* initializer
2022-04-11 10:32:08,893 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.intermediate.dense.bias using .* initializer
2022-04-11 10:32:08,893 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.output.dense.weight using .* initializer
2022-04-11 10:32:08,894 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.output.dense.bias using .* initializer
2022-04-11 10:32:08,894 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.output.LayerNorm.weight using .* initializer
2022-04-11 10:32:08,894 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.output.LayerNorm.bias using .* initializer
2022-04-11 10:32:08,894 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.pooler.dense.weight using .* initializer
2022-04-11 10:32:08,895 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.pooler.dense.bias using .* initializer
2022-04-11 10:32:08,895 - INFO - allennlp.nn.initializers - Initializing _classifier.classifier.dense.weight using .* initializer
2022-04-11 10:32:08,896 - INFO - allennlp.nn.initializers - Initializing _classifier.classifier.dense.bias using .* initializer
2022-04-11 10:32:08,896 - INFO - allennlp.nn.initializers - Initializing _classifier.classifier.out_proj.weight using .* initializer
2022-04-11 10:32:08,896 - INFO - allennlp.nn.initializers - Initializing _classifier.classifier.out_proj.bias using .* initializer
2022-04-11 10:32:08,896 - INFO - allennlp.nn.initializers - Done initializing parameters; the following parameters are using their default initialization from their code
2022-04-11 10:32:09,446 - INFO - filelock - Lock 140138368808912 acquired on ./output_ir-ora-d_hyper6/vocabulary/.lock
2022-04-11 10:32:09,447 - INFO - filelock - Lock 140138368808912 released on ./output_ir-ora-d_hyper6/vocabulary/.lock
2022-04-11 10:32:09,447 - INFO - allennlp.common.params - data_loader.type = pytorch_dataloader
2022-04-11 10:32:09,448 - INFO - allennlp.common.params - data_loader.batch_size = 1
2022-04-11 10:32:09,448 - INFO - allennlp.common.params - data_loader.shuffle = False
2022-04-11 10:32:09,448 - INFO - allennlp.common.params - data_loader.sampler = None
2022-04-11 10:32:09,448 - INFO - allennlp.common.params - data_loader.num_workers = 0
2022-04-11 10:32:09,448 - INFO - allennlp.common.params - data_loader.pin_memory = False
2022-04-11 10:32:09,448 - INFO - allennlp.common.params - data_loader.drop_last = False
2022-04-11 10:32:09,448 - INFO - allennlp.common.params - data_loader.timeout = 0
2022-04-11 10:32:09,449 - INFO - allennlp.common.params - data_loader.worker_init_fn = None
2022-04-11 10:32:09,449 - INFO - allennlp.common.params - data_loader.multiprocessing_context = None
2022-04-11 10:32:09,449 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None
2022-04-11 10:32:09,449 - INFO - allennlp.common.params - data_loader.batch_sampler.type = bucket
2022-04-11 10:32:09,449 - INFO - allennlp.common.params - data_loader.batch_sampler.batch_size = 2
2022-04-11 10:32:09,450 - INFO - allennlp.common.params - data_loader.batch_sampler.sorting_keys = None
2022-04-11 10:32:09,450 - INFO - allennlp.common.params - data_loader.batch_sampler.padding_noise = 0.1
2022-04-11 10:32:09,450 - INFO - allennlp.common.params - data_loader.batch_sampler.drop_last = False
2022-04-11 10:32:09,450 - INFO - allennlp.common.params - data_loader.type = pytorch_dataloader
2022-04-11 10:32:09,451 - INFO - allennlp.common.params - data_loader.batch_size = 1
2022-04-11 10:32:09,451 - INFO - allennlp.common.params - data_loader.shuffle = False
2022-04-11 10:32:09,451 - INFO - allennlp.common.params - data_loader.sampler = None
2022-04-11 10:32:09,451 - INFO - allennlp.common.params - data_loader.num_workers = 0
2022-04-11 10:32:09,451 - INFO - allennlp.common.params - data_loader.pin_memory = False
2022-04-11 10:32:09,452 - INFO - allennlp.common.params - data_loader.drop_last = False
2022-04-11 10:32:09,452 - INFO - allennlp.common.params - data_loader.timeout = 0
2022-04-11 10:32:09,452 - INFO - allennlp.common.params - data_loader.worker_init_fn = None
2022-04-11 10:32:09,452 - INFO - allennlp.common.params - data_loader.multiprocessing_context = None
2022-04-11 10:32:09,452 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None
2022-04-11 10:32:09,452 - INFO - allennlp.common.params - data_loader.batch_sampler.type = bucket
2022-04-11 10:32:09,453 - INFO - allennlp.common.params - data_loader.batch_sampler.batch_size = 2
2022-04-11 10:32:09,453 - INFO - allennlp.common.params - data_loader.batch_sampler.sorting_keys = None
2022-04-11 10:32:09,453 - INFO - allennlp.common.params - data_loader.batch_sampler.padding_noise = 0.1
2022-04-11 10:32:09,453 - INFO - allennlp.common.params - data_loader.batch_sampler.drop_last = False
2022-04-11 10:32:09,453 - INFO - allennlp.common.params - trainer.type = gradient_descent
2022-04-11 10:32:09,454 - INFO - allennlp.common.params - trainer.patience = None
2022-04-11 10:32:09,454 - INFO - allennlp.common.params - trainer.validation_metric = +accuracy
2022-04-11 10:32:09,454 - INFO - allennlp.common.params - trainer.num_epochs = 15
2022-04-11 10:32:09,454 - INFO - allennlp.common.params - trainer.cuda_device = 0
2022-04-11 10:32:09,454 - INFO - allennlp.common.params - trainer.grad_norm = None
2022-04-11 10:32:09,454 - INFO - allennlp.common.params - trainer.grad_clipping = None
2022-04-11 10:32:09,455 - INFO - allennlp.common.params - trainer.distributed = False
2022-04-11 10:32:09,455 - INFO - allennlp.common.params - trainer.world_size = 1
2022-04-11 10:32:09,455 - INFO - allennlp.common.params - trainer.num_gradient_accumulation_steps = 16
2022-04-11 10:32:09,455 - INFO - allennlp.common.params - trainer.use_amp = False
2022-04-11 10:32:09,455 - INFO - allennlp.common.params - trainer.no_grad = None
2022-04-11 10:32:09,456 - INFO - allennlp.common.params - trainer.momentum_scheduler = None
2022-04-11 10:32:09,456 - INFO - allennlp.common.params - trainer.tensorboard_writer = <allennlp.common.lazy.Lazy object at 0x7f74a972c390>
2022-04-11 10:32:09,456 - INFO - allennlp.common.params - trainer.moving_average = None
2022-04-11 10:32:09,456 - INFO - allennlp.common.params - trainer.checkpointer = <allennlp.common.lazy.Lazy object at 0x7f74a972c410>
2022-04-11 10:32:09,456 - INFO - allennlp.common.params - trainer.batch_callbacks = None
2022-04-11 10:32:09,456 - INFO - allennlp.common.params - trainer.epoch_callbacks = None
2022-04-11 10:32:09,456 - INFO - allennlp.common.params - trainer.end_callbacks = None
2022-04-11 10:32:09,456 - INFO - allennlp.common.params - trainer.trainer_callbacks = None
2022-04-11 10:32:12,253 - INFO - allennlp.common.params - trainer.optimizer.type = huggingface_adamw
2022-04-11 10:32:12,253 - INFO - allennlp.common.params - trainer.optimizer.parameter_groups = None
2022-04-11 10:32:12,253 - INFO - allennlp.common.params - trainer.optimizer.lr = 1e-05
2022-04-11 10:32:12,254 - INFO - allennlp.common.params - trainer.optimizer.betas = [0.9, 0.98]
2022-04-11 10:32:12,254 - INFO - allennlp.common.params - trainer.optimizer.eps = 1e-06
2022-04-11 10:32:12,254 - INFO - allennlp.common.params - trainer.optimizer.weight_decay = 0.1
2022-04-11 10:32:12,254 - INFO - allennlp.common.params - trainer.optimizer.correct_bias = True
2022-04-11 10:32:12,255 - INFO - allennlp.training.optimizers - Number of trainable parameters: 356411394
2022-04-11 10:32:12,257 - INFO - allennlp.common.util - The following parameters are Frozen (without gradient):
2022-04-11 10:32:12,260 - INFO - allennlp.common.util - The following parameters are Tunable (with gradient):
2022-04-11 10:32:12,260 - INFO - allennlp.common.util - _classifier.roberta.embeddings.word_embeddings.weight
2022-04-11 10:32:12,260 - INFO - allennlp.common.util - _classifier.roberta.embeddings.position_embeddings.weight
2022-04-11 10:32:12,260 - INFO - allennlp.common.util - _classifier.roberta.embeddings.token_type_embeddings.weight
2022-04-11 10:32:12,260 - INFO - allennlp.common.util - _classifier.roberta.embeddings.LayerNorm.weight
2022-04-11 10:32:12,260 - INFO - allennlp.common.util - _classifier.roberta.embeddings.LayerNorm.bias
2022-04-11 10:32:12,260 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.self.query.weight
2022-04-11 10:32:12,261 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.self.query.bias
2022-04-11 10:32:12,261 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.self.key.weight
2022-04-11 10:32:12,261 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.self.key.bias
2022-04-11 10:32:12,261 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.self.value.weight
2022-04-11 10:32:12,261 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.self.value.bias
2022-04-11 10:32:12,261 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.output.dense.weight
2022-04-11 10:32:12,261 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.output.dense.bias
2022-04-11 10:32:12,261 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.output.LayerNorm.weight
2022-04-11 10:32:12,262 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.output.LayerNorm.bias
2022-04-11 10:32:12,262 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.intermediate.dense.weight
2022-04-11 10:32:12,262 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.intermediate.dense.bias
2022-04-11 10:32:12,262 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.output.dense.weight
2022-04-11 10:32:12,262 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.output.dense.bias
2022-04-11 10:32:12,262 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.output.LayerNorm.weight
2022-04-11 10:32:12,262 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.output.LayerNorm.bias
2022-04-11 10:32:12,262 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.self.query.weight
2022-04-11 10:32:12,262 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.self.query.bias
2022-04-11 10:32:12,263 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.self.key.weight
2022-04-11 10:32:12,263 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.self.key.bias
2022-04-11 10:32:12,263 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.self.value.weight
2022-04-11 10:32:12,263 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.self.value.bias
2022-04-11 10:32:12,263 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.output.dense.weight
2022-04-11 10:32:12,263 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.output.dense.bias
2022-04-11 10:32:12,263 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.output.LayerNorm.weight
2022-04-11 10:32:12,263 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.output.LayerNorm.bias
2022-04-11 10:32:12,263 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.intermediate.dense.weight
2022-04-11 10:32:12,264 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.intermediate.dense.bias
2022-04-11 10:32:12,264 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.output.dense.weight
2022-04-11 10:32:12,264 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.output.dense.bias
2022-04-11 10:32:12,264 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.output.LayerNorm.weight
2022-04-11 10:32:12,264 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.output.LayerNorm.bias
2022-04-11 10:32:12,264 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.self.query.weight
2022-04-11 10:32:12,264 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.self.query.bias
2022-04-11 10:32:12,264 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.self.key.weight
2022-04-11 10:32:12,264 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.self.key.bias
2022-04-11 10:32:12,265 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.self.value.weight
2022-04-11 10:32:12,265 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.self.value.bias
2022-04-11 10:32:12,265 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.output.dense.weight
2022-04-11 10:32:12,265 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.output.dense.bias
2022-04-11 10:32:12,265 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.output.LayerNorm.weight
2022-04-11 10:32:12,265 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.output.LayerNorm.bias
2022-04-11 10:32:12,265 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.intermediate.dense.weight
2022-04-11 10:32:12,265 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.intermediate.dense.bias
2022-04-11 10:32:12,265 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.output.dense.weight
2022-04-11 10:32:12,266 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.output.dense.bias
2022-04-11 10:32:12,266 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.output.LayerNorm.weight
2022-04-11 10:32:12,266 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.output.LayerNorm.bias
2022-04-11 10:32:12,266 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.self.query.weight
2022-04-11 10:32:12,266 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.self.query.bias
2022-04-11 10:32:12,266 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.self.key.weight
2022-04-11 10:32:12,266 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.self.key.bias
2022-04-11 10:32:12,267 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.self.value.weight
2022-04-11 10:32:12,267 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.self.value.bias
2022-04-11 10:32:12,267 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.output.dense.weight
2022-04-11 10:32:12,267 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.output.dense.bias
2022-04-11 10:32:12,267 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.output.LayerNorm.weight
2022-04-11 10:32:12,267 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.output.LayerNorm.bias
2022-04-11 10:32:12,267 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.intermediate.dense.weight
2022-04-11 10:32:12,267 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.intermediate.dense.bias
2022-04-11 10:32:12,267 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.output.dense.weight
2022-04-11 10:32:12,268 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.output.dense.bias
2022-04-11 10:32:12,268 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.output.LayerNorm.weight
2022-04-11 10:32:12,268 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.output.LayerNorm.bias
2022-04-11 10:32:12,268 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.self.query.weight
2022-04-11 10:32:12,268 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.self.query.bias
2022-04-11 10:32:12,268 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.self.key.weight
2022-04-11 10:32:12,268 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.self.key.bias
2022-04-11 10:32:12,268 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.self.value.weight
2022-04-11 10:32:12,269 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.self.value.bias
2022-04-11 10:32:12,269 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.output.dense.weight
2022-04-11 10:32:12,269 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.output.dense.bias
2022-04-11 10:32:12,269 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.output.LayerNorm.weight
2022-04-11 10:32:12,269 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.output.LayerNorm.bias
2022-04-11 10:32:12,269 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.intermediate.dense.weight
2022-04-11 10:32:12,269 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.intermediate.dense.bias
2022-04-11 10:32:12,269 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.output.dense.weight
2022-04-11 10:32:12,269 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.output.dense.bias
2022-04-11 10:32:12,270 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.output.LayerNorm.weight
2022-04-11 10:32:12,270 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.output.LayerNorm.bias
2022-04-11 10:32:12,270 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.self.query.weight
2022-04-11 10:32:12,270 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.self.query.bias
2022-04-11 10:32:12,270 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.self.key.weight
2022-04-11 10:32:12,270 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.self.key.bias
2022-04-11 10:32:12,270 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.self.value.weight
2022-04-11 10:32:12,270 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.self.value.bias
2022-04-11 10:32:12,270 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.output.dense.weight
2022-04-11 10:32:12,271 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.output.dense.bias
2022-04-11 10:32:12,271 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.output.LayerNorm.weight
2022-04-11 10:32:12,271 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.output.LayerNorm.bias
2022-04-11 10:32:12,271 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.intermediate.dense.weight
2022-04-11 10:32:12,271 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.intermediate.dense.bias
2022-04-11 10:32:12,271 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.output.dense.weight
2022-04-11 10:32:12,271 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.output.dense.bias
2022-04-11 10:32:12,271 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.output.LayerNorm.weight
2022-04-11 10:32:12,271 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.output.LayerNorm.bias
2022-04-11 10:32:12,272 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.self.query.weight
2022-04-11 10:32:12,272 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.self.query.bias
2022-04-11 10:32:12,272 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.self.key.weight
2022-04-11 10:32:12,272 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.self.key.bias
2022-04-11 10:32:12,272 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.self.value.weight
2022-04-11 10:32:12,272 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.self.value.bias
2022-04-11 10:32:12,272 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.output.dense.weight
2022-04-11 10:32:12,272 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.output.dense.bias
2022-04-11 10:32:12,272 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.output.LayerNorm.weight
2022-04-11 10:32:12,273 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.output.LayerNorm.bias
2022-04-11 10:32:12,273 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.intermediate.dense.weight
2022-04-11 10:32:12,273 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.intermediate.dense.bias
2022-04-11 10:32:12,273 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.output.dense.weight
2022-04-11 10:32:12,273 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.output.dense.bias
2022-04-11 10:32:12,273 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.output.LayerNorm.weight
2022-04-11 10:32:12,273 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.output.LayerNorm.bias
2022-04-11 10:32:12,273 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.self.query.weight
2022-04-11 10:32:12,274 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.self.query.bias
2022-04-11 10:32:12,274 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.self.key.weight
2022-04-11 10:32:12,274 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.self.key.bias
2022-04-11 10:32:12,274 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.self.value.weight
2022-04-11 10:32:12,274 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.self.value.bias
2022-04-11 10:32:12,274 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.output.dense.weight
2022-04-11 10:32:12,274 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.output.dense.bias
2022-04-11 10:32:12,274 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.output.LayerNorm.weight
2022-04-11 10:32:12,275 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.output.LayerNorm.bias
2022-04-11 10:32:12,275 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.intermediate.dense.weight
2022-04-11 10:32:12,275 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.intermediate.dense.bias
2022-04-11 10:32:12,275 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.output.dense.weight
2022-04-11 10:32:12,275 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.output.dense.bias
2022-04-11 10:32:12,275 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.output.LayerNorm.weight
2022-04-11 10:32:12,275 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.output.LayerNorm.bias
2022-04-11 10:32:12,275 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.self.query.weight
2022-04-11 10:32:12,275 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.self.query.bias
2022-04-11 10:32:12,276 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.self.key.weight
2022-04-11 10:32:12,276 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.self.key.bias
2022-04-11 10:32:12,276 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.self.value.weight
2022-04-11 10:32:12,276 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.self.value.bias
2022-04-11 10:32:12,276 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.output.dense.weight
2022-04-11 10:32:12,276 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.output.dense.bias
2022-04-11 10:32:12,276 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.output.LayerNorm.weight
2022-04-11 10:32:12,277 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.output.LayerNorm.bias
2022-04-11 10:32:12,277 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.intermediate.dense.weight
2022-04-11 10:32:12,277 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.intermediate.dense.bias
2022-04-11 10:32:12,277 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.output.dense.weight
2022-04-11 10:32:12,277 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.output.dense.bias
2022-04-11 10:32:12,277 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.output.LayerNorm.weight
2022-04-11 10:32:12,277 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.output.LayerNorm.bias
2022-04-11 10:32:12,277 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.self.query.weight
2022-04-11 10:32:12,278 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.self.query.bias
2022-04-11 10:32:12,278 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.self.key.weight
2022-04-11 10:32:12,278 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.self.key.bias
2022-04-11 10:32:12,278 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.self.value.weight
2022-04-11 10:32:12,278 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.self.value.bias
2022-04-11 10:32:12,278 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.output.dense.weight
2022-04-11 10:32:12,278 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.output.dense.bias
2022-04-11 10:32:12,278 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.output.LayerNorm.weight
2022-04-11 10:32:12,278 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.output.LayerNorm.bias
2022-04-11 10:32:12,278 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.intermediate.dense.weight
2022-04-11 10:32:12,278 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.intermediate.dense.bias
2022-04-11 10:32:12,278 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.output.dense.weight
2022-04-11 10:32:12,278 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.output.dense.bias
2022-04-11 10:32:12,278 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.output.LayerNorm.weight
2022-04-11 10:32:12,278 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.output.LayerNorm.bias
2022-04-11 10:32:12,278 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.self.query.weight
2022-04-11 10:32:12,278 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.self.query.bias
2022-04-11 10:32:12,279 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.self.key.weight
2022-04-11 10:32:12,279 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.self.key.bias
2022-04-11 10:32:12,279 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.self.value.weight
2022-04-11 10:32:12,279 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.self.value.bias
2022-04-11 10:32:12,279 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.output.dense.weight
2022-04-11 10:32:12,279 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.output.dense.bias
2022-04-11 10:32:12,279 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.output.LayerNorm.weight
2022-04-11 10:32:12,279 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.output.LayerNorm.bias
2022-04-11 10:32:12,279 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.intermediate.dense.weight
2022-04-11 10:32:12,279 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.intermediate.dense.bias
2022-04-11 10:32:12,279 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.output.dense.weight
2022-04-11 10:32:12,279 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.output.dense.bias
2022-04-11 10:32:12,279 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.output.LayerNorm.weight
2022-04-11 10:32:12,279 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.output.LayerNorm.bias
2022-04-11 10:32:12,279 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.self.query.weight
2022-04-11 10:32:12,279 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.self.query.bias
2022-04-11 10:32:12,279 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.self.key.weight
2022-04-11 10:32:12,279 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.self.key.bias
2022-04-11 10:32:12,279 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.self.value.weight
2022-04-11 10:32:12,279 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.self.value.bias
2022-04-11 10:32:12,280 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.output.dense.weight
2022-04-11 10:32:12,280 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.output.dense.bias
2022-04-11 10:32:12,280 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.output.LayerNorm.weight
2022-04-11 10:32:12,280 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.output.LayerNorm.bias
2022-04-11 10:32:12,280 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.intermediate.dense.weight
2022-04-11 10:32:12,280 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.intermediate.dense.bias
2022-04-11 10:32:12,280 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.output.dense.weight
2022-04-11 10:32:12,280 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.output.dense.bias
2022-04-11 10:32:12,280 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.output.LayerNorm.weight
2022-04-11 10:32:12,280 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.output.LayerNorm.bias
2022-04-11 10:32:12,280 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.self.query.weight
2022-04-11 10:32:12,280 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.self.query.bias
2022-04-11 10:32:12,280 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.self.key.weight
2022-04-11 10:32:12,280 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.self.key.bias
2022-04-11 10:32:12,280 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.self.value.weight
2022-04-11 10:32:12,280 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.self.value.bias
2022-04-11 10:32:12,280 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.output.dense.weight
2022-04-11 10:32:12,280 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.output.dense.bias
2022-04-11 10:32:12,280 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.output.LayerNorm.weight
2022-04-11 10:32:12,281 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.output.LayerNorm.bias
2022-04-11 10:32:12,281 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.intermediate.dense.weight
2022-04-11 10:32:12,281 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.intermediate.dense.bias
2022-04-11 10:32:12,281 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.output.dense.weight
2022-04-11 10:32:12,281 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.output.dense.bias
2022-04-11 10:32:12,281 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.output.LayerNorm.weight
2022-04-11 10:32:12,281 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.output.LayerNorm.bias
2022-04-11 10:32:12,281 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.self.query.weight
2022-04-11 10:32:12,281 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.self.query.bias
2022-04-11 10:32:12,281 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.self.key.weight
2022-04-11 10:32:12,281 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.self.key.bias
2022-04-11 10:32:12,281 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.self.value.weight
2022-04-11 10:32:12,281 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.self.value.bias
2022-04-11 10:32:12,281 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.output.dense.weight
2022-04-11 10:32:12,281 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.output.dense.bias
2022-04-11 10:32:12,281 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.output.LayerNorm.weight
2022-04-11 10:32:12,281 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.output.LayerNorm.bias
2022-04-11 10:32:12,281 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.intermediate.dense.weight
2022-04-11 10:32:12,281 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.intermediate.dense.bias
2022-04-11 10:32:12,281 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.output.dense.weight
2022-04-11 10:32:12,282 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.output.dense.bias
2022-04-11 10:32:12,282 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.output.LayerNorm.weight
2022-04-11 10:32:12,282 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.output.LayerNorm.bias
2022-04-11 10:32:12,282 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.self.query.weight
2022-04-11 10:32:12,282 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.self.query.bias
2022-04-11 10:32:12,282 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.self.key.weight
2022-04-11 10:32:12,282 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.self.key.bias
2022-04-11 10:32:12,282 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.self.value.weight
2022-04-11 10:32:12,282 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.self.value.bias
2022-04-11 10:32:12,282 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.output.dense.weight
2022-04-11 10:32:12,282 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.output.dense.bias
2022-04-11 10:32:12,282 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.output.LayerNorm.weight
2022-04-11 10:32:12,282 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.output.LayerNorm.bias
2022-04-11 10:32:12,282 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.intermediate.dense.weight
2022-04-11 10:32:12,282 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.intermediate.dense.bias
2022-04-11 10:32:12,282 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.output.dense.weight
2022-04-11 10:32:12,282 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.output.dense.bias
2022-04-11 10:32:12,282 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.output.LayerNorm.weight
2022-04-11 10:32:12,283 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.output.LayerNorm.bias
2022-04-11 10:32:12,283 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.self.query.weight
2022-04-11 10:32:12,283 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.self.query.bias
2022-04-11 10:32:12,283 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.self.key.weight
2022-04-11 10:32:12,283 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.self.key.bias
2022-04-11 10:32:12,283 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.self.value.weight
2022-04-11 10:32:12,283 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.self.value.bias
2022-04-11 10:32:12,283 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.output.dense.weight
2022-04-11 10:32:12,283 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.output.dense.bias
2022-04-11 10:32:12,283 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.output.LayerNorm.weight
2022-04-11 10:32:12,283 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.output.LayerNorm.bias
2022-04-11 10:32:12,283 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.intermediate.dense.weight
2022-04-11 10:32:12,283 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.intermediate.dense.bias
2022-04-11 10:32:12,283 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.output.dense.weight
2022-04-11 10:32:12,283 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.output.dense.bias
2022-04-11 10:32:12,283 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.output.LayerNorm.weight
2022-04-11 10:32:12,283 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.output.LayerNorm.bias
2022-04-11 10:32:12,283 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.self.query.weight
2022-04-11 10:32:12,283 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.self.query.bias
2022-04-11 10:32:12,283 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.self.key.weight
2022-04-11 10:32:12,284 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.self.key.bias
2022-04-11 10:32:12,284 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.self.value.weight
2022-04-11 10:32:12,284 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.self.value.bias
2022-04-11 10:32:12,284 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.output.dense.weight
2022-04-11 10:32:12,284 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.output.dense.bias
2022-04-11 10:32:12,284 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.output.LayerNorm.weight
2022-04-11 10:32:12,284 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.output.LayerNorm.bias
2022-04-11 10:32:12,284 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.intermediate.dense.weight
2022-04-11 10:32:12,284 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.intermediate.dense.bias
2022-04-11 10:32:12,284 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.output.dense.weight
2022-04-11 10:32:12,284 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.output.dense.bias
2022-04-11 10:32:12,284 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.output.LayerNorm.weight
2022-04-11 10:32:12,284 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.output.LayerNorm.bias
2022-04-11 10:32:12,284 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.self.query.weight
2022-04-11 10:32:12,284 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.self.query.bias
2022-04-11 10:32:12,284 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.self.key.weight
2022-04-11 10:32:12,284 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.self.key.bias
2022-04-11 10:32:12,284 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.self.value.weight
2022-04-11 10:32:12,284 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.self.value.bias
2022-04-11 10:32:12,285 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.output.dense.weight
2022-04-11 10:32:12,285 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.output.dense.bias
2022-04-11 10:32:12,285 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.output.LayerNorm.weight
2022-04-11 10:32:12,285 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.output.LayerNorm.bias
2022-04-11 10:32:12,285 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.intermediate.dense.weight
2022-04-11 10:32:12,285 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.intermediate.dense.bias
2022-04-11 10:32:12,285 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.output.dense.weight
2022-04-11 10:32:12,285 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.output.dense.bias
2022-04-11 10:32:12,285 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.output.LayerNorm.weight
2022-04-11 10:32:12,285 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.output.LayerNorm.bias
2022-04-11 10:32:12,285 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.self.query.weight
2022-04-11 10:32:12,285 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.self.query.bias
2022-04-11 10:32:12,285 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.self.key.weight
2022-04-11 10:32:12,285 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.self.key.bias
2022-04-11 10:32:12,285 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.self.value.weight
2022-04-11 10:32:12,285 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.self.value.bias
2022-04-11 10:32:12,285 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.output.dense.weight
2022-04-11 10:32:12,285 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.output.dense.bias
2022-04-11 10:32:12,286 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.output.LayerNorm.weight
2022-04-11 10:32:12,286 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.output.LayerNorm.bias
2022-04-11 10:32:12,286 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.intermediate.dense.weight
2022-04-11 10:32:12,286 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.intermediate.dense.bias
2022-04-11 10:32:12,286 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.output.dense.weight
2022-04-11 10:32:12,286 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.output.dense.bias
2022-04-11 10:32:12,286 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.output.LayerNorm.weight
2022-04-11 10:32:12,286 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.output.LayerNorm.bias
2022-04-11 10:32:12,286 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.self.query.weight
2022-04-11 10:32:12,286 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.self.query.bias
2022-04-11 10:32:12,286 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.self.key.weight
2022-04-11 10:32:12,286 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.self.key.bias
2022-04-11 10:32:12,286 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.self.value.weight
2022-04-11 10:32:12,286 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.self.value.bias
2022-04-11 10:32:12,286 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.output.dense.weight
2022-04-11 10:32:12,287 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.output.dense.bias
2022-04-11 10:32:12,287 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.output.LayerNorm.weight
2022-04-11 10:32:12,287 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.output.LayerNorm.bias
2022-04-11 10:32:12,287 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.intermediate.dense.weight
2022-04-11 10:32:12,287 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.intermediate.dense.bias
2022-04-11 10:32:12,287 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.output.dense.weight
2022-04-11 10:32:12,287 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.output.dense.bias
2022-04-11 10:32:12,287 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.output.LayerNorm.weight
2022-04-11 10:32:12,287 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.output.LayerNorm.bias
2022-04-11 10:32:12,287 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.self.query.weight
2022-04-11 10:32:12,287 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.self.query.bias
2022-04-11 10:32:12,287 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.self.key.weight
2022-04-11 10:32:12,287 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.self.key.bias
2022-04-11 10:32:12,287 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.self.value.weight
2022-04-11 10:32:12,287 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.self.value.bias
2022-04-11 10:32:12,287 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.output.dense.weight
2022-04-11 10:32:12,287 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.output.dense.bias
2022-04-11 10:32:12,288 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.output.LayerNorm.weight
2022-04-11 10:32:12,288 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.output.LayerNorm.bias
2022-04-11 10:32:12,288 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.intermediate.dense.weight
2022-04-11 10:32:12,288 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.intermediate.dense.bias
2022-04-11 10:32:12,288 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.output.dense.weight
2022-04-11 10:32:12,288 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.output.dense.bias
2022-04-11 10:32:12,288 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.output.LayerNorm.weight
2022-04-11 10:32:12,288 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.output.LayerNorm.bias
2022-04-11 10:32:12,288 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.self.query.weight
2022-04-11 10:32:12,288 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.self.query.bias
2022-04-11 10:32:12,288 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.self.key.weight
2022-04-11 10:32:12,288 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.self.key.bias
2022-04-11 10:32:12,288 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.self.value.weight
2022-04-11 10:32:12,288 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.self.value.bias
2022-04-11 10:32:12,288 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.output.dense.weight
2022-04-11 10:32:12,288 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.output.dense.bias
2022-04-11 10:32:12,288 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.output.LayerNorm.weight
2022-04-11 10:32:12,288 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.output.LayerNorm.bias
2022-04-11 10:32:12,289 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.intermediate.dense.weight
2022-04-11 10:32:12,289 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.intermediate.dense.bias
2022-04-11 10:32:12,289 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.output.dense.weight
2022-04-11 10:32:12,289 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.output.dense.bias
2022-04-11 10:32:12,289 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.output.LayerNorm.weight
2022-04-11 10:32:12,289 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.output.LayerNorm.bias
2022-04-11 10:32:12,289 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.self.query.weight
2022-04-11 10:32:12,289 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.self.query.bias
2022-04-11 10:32:12,289 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.self.key.weight
2022-04-11 10:32:12,289 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.self.key.bias
2022-04-11 10:32:12,289 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.self.value.weight
2022-04-11 10:32:12,289 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.self.value.bias
2022-04-11 10:32:12,289 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.output.dense.weight
2022-04-11 10:32:12,289 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.output.dense.bias
2022-04-11 10:32:12,289 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.output.LayerNorm.weight
2022-04-11 10:32:12,289 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.output.LayerNorm.bias
2022-04-11 10:32:12,289 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.intermediate.dense.weight
2022-04-11 10:32:12,289 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.intermediate.dense.bias
2022-04-11 10:32:12,289 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.output.dense.weight
2022-04-11 10:32:12,289 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.output.dense.bias
2022-04-11 10:32:12,290 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.output.LayerNorm.weight
2022-04-11 10:32:12,290 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.output.LayerNorm.bias
2022-04-11 10:32:12,290 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.self.query.weight
2022-04-11 10:32:12,290 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.self.query.bias
2022-04-11 10:32:12,290 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.self.key.weight
2022-04-11 10:32:12,290 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.self.key.bias
2022-04-11 10:32:12,290 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.self.value.weight
2022-04-11 10:32:12,290 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.self.value.bias
2022-04-11 10:32:12,290 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.output.dense.weight
2022-04-11 10:32:12,290 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.output.dense.bias
2022-04-11 10:32:12,290 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.output.LayerNorm.weight
2022-04-11 10:32:12,290 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.output.LayerNorm.bias
2022-04-11 10:32:12,290 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.intermediate.dense.weight
2022-04-11 10:32:12,290 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.intermediate.dense.bias
2022-04-11 10:32:12,290 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.output.dense.weight
2022-04-11 10:32:12,290 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.output.dense.bias
2022-04-11 10:32:12,290 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.output.LayerNorm.weight
2022-04-11 10:32:12,290 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.output.LayerNorm.bias
2022-04-11 10:32:12,290 - INFO - allennlp.common.util - _classifier.roberta.pooler.dense.weight
2022-04-11 10:32:12,291 - INFO - allennlp.common.util - _classifier.roberta.pooler.dense.bias
2022-04-11 10:32:12,291 - INFO - allennlp.common.util - _classifier.classifier.dense.weight
2022-04-11 10:32:12,291 - INFO - allennlp.common.util - _classifier.classifier.dense.bias
2022-04-11 10:32:12,291 - INFO - allennlp.common.util - _classifier.classifier.out_proj.weight
2022-04-11 10:32:12,291 - INFO - allennlp.common.util - _classifier.classifier.out_proj.bias
2022-04-11 10:32:12,291 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.type = slanted_triangular
2022-04-11 10:32:12,291 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.cut_frac = 0.12
2022-04-11 10:32:12,291 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.ratio = 32
2022-04-11 10:32:12,291 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.last_epoch = -1
2022-04-11 10:32:12,291 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.gradual_unfreezing = False
2022-04-11 10:32:12,291 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.discriminative_fine_tuning = False
2022-04-11 10:32:12,291 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.decay_factor = 0.38
2022-04-11 10:32:12,292 - INFO - allennlp.common.params - type = default
2022-04-11 10:32:12,292 - INFO - allennlp.common.params - keep_serialized_model_every_num_seconds = None
2022-04-11 10:32:12,292 - INFO - allennlp.common.params - num_serialized_models_to_keep = 2
2022-04-11 10:32:12,292 - INFO - allennlp.common.params - model_save_interval = None
2022-04-11 10:32:12,292 - INFO - allennlp.common.params - summary_interval = 100
2022-04-11 10:32:12,292 - INFO - allennlp.common.params - histogram_interval = None
2022-04-11 10:32:12,292 - INFO - allennlp.common.params - batch_size_interval = None
2022-04-11 10:32:12,292 - INFO - allennlp.common.params - should_log_parameter_statistics = True
2022-04-11 10:32:12,292 - INFO - allennlp.common.params - should_log_learning_rate = False
2022-04-11 10:32:12,292 - INFO - allennlp.common.params - get_batch_num_total = None
2022-04-11 10:32:12,295 - WARNING - allennlp.training.trainer - You provided a validation dataset but patience was set to None, meaning that early stopping is disabled
2022-04-11 10:32:12,390 - INFO - allennlp.training.trainer - Beginning training.
2022-04-11 10:32:12,390 - INFO - allennlp.training.trainer - Epoch 0/14
2022-04-11 10:32:12,390 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-11 10:32:12,390 - INFO - allennlp.training.trainer - GPU 0 memory usage: 1.3G
2022-04-11 10:32:12,392 - INFO - allennlp.training.trainer - Training
2022-04-11 10:32:12,392 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-11 10:32:12,392 - INFO - allennlp.data.samplers.bucket_batch_sampler - No sorting keys given; trying to guess a good one
2022-04-11 10:32:12,392 - INFO - allennlp.data.samplers.bucket_batch_sampler - Using ['tokens'] as the sorting keys
2022-04-11 10:32:22,525 - INFO - tqdm - accuracy: 0.4688, batch_loss: 1.5490, loss: 1.5490 ||:   2%|1         | 1/65 [00:10<10:48, 10.13s/it]
2022-04-11 10:32:32,748 - INFO - tqdm - accuracy: 0.5156, batch_loss: 1.5521, loss: 1.5506 ||:   3%|3         | 2/65 [00:20<10:41, 10.19s/it]
2022-04-11 10:32:43,062 - INFO - tqdm - accuracy: 0.5208, batch_loss: 2.0205, loss: 1.7072 ||:   5%|4         | 3/65 [00:30<10:35, 10.24s/it]
2022-04-11 10:32:53,484 - INFO - tqdm - accuracy: 0.5469, batch_loss: 1.3520, loss: 1.6184 ||:   6%|6         | 4/65 [00:41<10:29, 10.31s/it]
2022-04-11 10:33:03,975 - INFO - tqdm - accuracy: 0.5312, batch_loss: 1.2386, loss: 1.5424 ||:   8%|7         | 5/65 [00:51<10:22, 10.38s/it]
2022-04-11 10:33:14,615 - INFO - tqdm - accuracy: 0.5156, batch_loss: 1.7361, loss: 1.5747 ||:   9%|9         | 6/65 [01:02<10:17, 10.47s/it]
2022-04-11 10:33:25,388 - INFO - tqdm - accuracy: 0.5179, batch_loss: 1.4990, loss: 1.5639 ||:  11%|#         | 7/65 [01:12<10:12, 10.57s/it]
2022-04-11 10:33:36,319 - INFO - tqdm - accuracy: 0.5078, batch_loss: 1.5804, loss: 1.5660 ||:  12%|#2        | 8/65 [01:23<10:08, 10.68s/it]
2022-04-11 10:33:47,410 - INFO - tqdm - accuracy: 0.5000, batch_loss: 1.7803, loss: 1.5898 ||:  14%|#3        | 9/65 [01:35<10:05, 10.81s/it]
2022-04-11 10:33:58,613 - INFO - tqdm - accuracy: 0.4906, batch_loss: 1.6203, loss: 1.5928 ||:  15%|#5        | 10/65 [01:46<10:01, 10.93s/it]
2022-04-11 10:34:10,142 - INFO - tqdm - accuracy: 0.4886, batch_loss: 1.5361, loss: 1.5877 ||:  17%|#6        | 11/65 [01:57<10:00, 11.11s/it]
2022-04-11 10:34:21,867 - INFO - tqdm - accuracy: 0.4974, batch_loss: 1.0748, loss: 1.5449 ||:  18%|#8        | 12/65 [02:09<09:58, 11.30s/it]
2022-04-11 10:34:33,518 - INFO - tqdm - accuracy: 0.5048, batch_loss: 1.0115, loss: 1.5039 ||:  20%|##        | 13/65 [02:21<09:53, 11.41s/it]
2022-04-11 10:34:44,812 - INFO - tqdm - accuracy: 0.5045, batch_loss: 1.5864, loss: 1.5098 ||:  22%|##1       | 14/65 [02:32<09:39, 11.37s/it]
2022-04-11 10:34:56,182 - INFO - tqdm - accuracy: 0.5083, batch_loss: 1.2314, loss: 1.4912 ||:  23%|##3       | 15/65 [02:43<09:28, 11.37s/it]
2022-04-11 10:35:07,603 - INFO - tqdm - accuracy: 0.5020, batch_loss: 1.2174, loss: 1.4741 ||:  25%|##4       | 16/65 [02:55<09:17, 11.39s/it]
2022-04-11 10:35:19,168 - INFO - tqdm - accuracy: 0.5074, batch_loss: 0.6735, loss: 1.4270 ||:  26%|##6       | 17/65 [03:06<09:09, 11.44s/it]
2022-04-11 10:35:30,758 - INFO - tqdm - accuracy: 0.5122, batch_loss: 0.9429, loss: 1.4001 ||:  28%|##7       | 18/65 [03:18<08:59, 11.49s/it]
2022-04-11 10:35:42,311 - INFO - tqdm - accuracy: 0.5148, batch_loss: 0.9964, loss: 1.3789 ||:  29%|##9       | 19/65 [03:29<08:49, 11.51s/it]
2022-04-11 10:35:53,561 - INFO - tqdm - accuracy: 0.5156, batch_loss: 0.7993, loss: 1.3499 ||:  31%|###       | 20/65 [03:41<08:34, 11.43s/it]
2022-04-11 10:36:05,058 - INFO - tqdm - accuracy: 0.5074, batch_loss: 1.3627, loss: 1.3505 ||:  32%|###2      | 21/65 [03:52<08:23, 11.45s/it]
2022-04-11 10:36:16,436 - INFO - tqdm - accuracy: 0.5085, batch_loss: 0.9626, loss: 1.3329 ||:  34%|###3      | 22/65 [04:04<08:11, 11.43s/it]
2022-04-11 10:36:27,981 - INFO - tqdm - accuracy: 0.5109, batch_loss: 0.9538, loss: 1.3164 ||:  35%|###5      | 23/65 [04:15<08:01, 11.46s/it]
2022-04-11 10:36:39,534 - INFO - tqdm - accuracy: 0.5091, batch_loss: 0.9173, loss: 1.2998 ||:  37%|###6      | 24/65 [04:27<07:51, 11.49s/it]
2022-04-11 10:36:51,101 - INFO - tqdm - accuracy: 0.5062, batch_loss: 1.0831, loss: 1.2911 ||:  38%|###8      | 25/65 [04:38<07:40, 11.51s/it]
2022-04-11 10:37:02,695 - INFO - tqdm - accuracy: 0.5000, batch_loss: 0.9148, loss: 1.2766 ||:  40%|####      | 26/65 [04:50<07:29, 11.54s/it]
2022-04-11 10:37:14,306 - INFO - tqdm - accuracy: 0.5035, batch_loss: 0.6829, loss: 1.2546 ||:  42%|####1     | 27/65 [05:01<07:19, 11.56s/it]
2022-04-11 10:37:25,919 - INFO - tqdm - accuracy: 0.5067, batch_loss: 0.7797, loss: 1.2377 ||:  43%|####3     | 28/65 [05:13<07:08, 11.58s/it]
2022-04-11 10:37:37,525 - INFO - tqdm - accuracy: 0.5032, batch_loss: 0.8504, loss: 1.2243 ||:  45%|####4     | 29/65 [05:25<06:57, 11.58s/it]
2022-04-11 10:37:49,121 - INFO - tqdm - accuracy: 0.5010, batch_loss: 0.9144, loss: 1.2140 ||:  46%|####6     | 30/65 [05:36<06:45, 11.59s/it]
2022-04-11 10:38:00,669 - INFO - tqdm - accuracy: 0.5060, batch_loss: 0.6201, loss: 1.1948 ||:  48%|####7     | 31/65 [05:48<06:33, 11.58s/it]
2022-04-11 10:38:12,173 - INFO - tqdm - accuracy: 0.5029, batch_loss: 0.9180, loss: 1.1862 ||:  49%|####9     | 32/65 [05:59<06:21, 11.55s/it]
2022-04-11 10:38:23,668 - INFO - tqdm - accuracy: 0.5085, batch_loss: 0.6254, loss: 1.1692 ||:  51%|#####     | 33/65 [06:11<06:09, 11.54s/it]
2022-04-11 10:38:35,172 - INFO - tqdm - accuracy: 0.5074, batch_loss: 0.7520, loss: 1.1569 ||:  52%|#####2    | 34/65 [06:22<05:57, 11.53s/it]
2022-04-11 10:38:46,672 - INFO - tqdm - accuracy: 0.5054, batch_loss: 0.7797, loss: 1.1461 ||:  54%|#####3    | 35/65 [06:34<05:45, 11.52s/it]
2022-04-11 10:38:58,189 - INFO - tqdm - accuracy: 0.5078, batch_loss: 0.6818, loss: 1.1332 ||:  55%|#####5    | 36/65 [06:45<05:34, 11.52s/it]
2022-04-11 10:39:09,723 - INFO - tqdm - accuracy: 0.5084, batch_loss: 0.7608, loss: 1.1232 ||:  57%|#####6    | 37/65 [06:57<05:22, 11.52s/it]
2022-04-11 10:39:21,281 - INFO - tqdm - accuracy: 0.5090, batch_loss: 0.6793, loss: 1.1115 ||:  58%|#####8    | 38/65 [07:08<05:11, 11.53s/it]
2022-04-11 10:39:32,841 - INFO - tqdm - accuracy: 0.5080, batch_loss: 0.7182, loss: 1.1014 ||:  60%|######    | 39/65 [07:20<05:00, 11.54s/it]
2022-04-11 10:39:44,412 - INFO - tqdm - accuracy: 0.5078, batch_loss: 0.6721, loss: 1.0907 ||:  62%|######1   | 40/65 [07:32<04:48, 11.55s/it]
2022-04-11 10:39:55,985 - INFO - tqdm - accuracy: 0.5069, batch_loss: 0.6880, loss: 1.0809 ||:  63%|######3   | 41/65 [07:43<04:37, 11.56s/it]
2022-04-11 10:40:07,580 - INFO - tqdm - accuracy: 0.5104, batch_loss: 0.6578, loss: 1.0708 ||:  65%|######4   | 42/65 [07:55<04:26, 11.57s/it]
2022-04-11 10:40:19,148 - INFO - tqdm - accuracy: 0.5080, batch_loss: 0.7922, loss: 1.0643 ||:  66%|######6   | 43/65 [08:06<04:14, 11.57s/it]
2022-04-11 10:40:30,691 - INFO - tqdm - accuracy: 0.5092, batch_loss: 0.6507, loss: 1.0549 ||:  68%|######7   | 44/65 [08:18<04:02, 11.56s/it]
2022-04-11 10:40:42,235 - INFO - tqdm - accuracy: 0.5104, batch_loss: 0.6810, loss: 1.0466 ||:  69%|######9   | 45/65 [08:29<03:51, 11.56s/it]
2022-04-11 10:40:53,773 - INFO - tqdm - accuracy: 0.5109, batch_loss: 0.6878, loss: 1.0388 ||:  71%|#######   | 46/65 [08:41<03:39, 11.55s/it]
2022-04-11 10:41:05,303 - INFO - tqdm - accuracy: 0.5126, batch_loss: 0.6686, loss: 1.0309 ||:  72%|#######2  | 47/65 [08:52<03:27, 11.54s/it]
2022-04-11 10:41:16,813 - INFO - tqdm - accuracy: 0.5130, batch_loss: 0.7408, loss: 1.0249 ||:  74%|#######3  | 48/65 [09:04<03:16, 11.53s/it]
2022-04-11 10:41:28,341 - INFO - tqdm - accuracy: 0.5147, batch_loss: 0.6854, loss: 1.0179 ||:  75%|#######5  | 49/65 [09:15<03:04, 11.53s/it]
2022-04-11 10:41:39,858 - INFO - tqdm - accuracy: 0.5156, batch_loss: 0.6641, loss: 1.0109 ||:  77%|#######6  | 50/65 [09:27<02:52, 11.53s/it]
2022-04-11 10:41:51,384 - INFO - tqdm - accuracy: 0.5159, batch_loss: 0.7366, loss: 1.0055 ||:  78%|#######8  | 51/65 [09:38<02:41, 11.53s/it]
2022-04-11 10:42:02,908 - INFO - tqdm - accuracy: 0.5198, batch_loss: 0.6110, loss: 0.9979 ||:  80%|########  | 52/65 [09:50<02:29, 11.53s/it]
2022-04-11 10:42:14,437 - INFO - tqdm - accuracy: 0.5195, batch_loss: 0.7044, loss: 0.9924 ||:  82%|########1 | 53/65 [10:02<02:18, 11.53s/it]
2022-04-11 10:42:25,653 - INFO - tqdm - accuracy: 0.5252, batch_loss: 0.5655, loss: 0.9845 ||:  83%|########3 | 54/65 [10:13<02:05, 11.43s/it]
2022-04-11 10:42:37,204 - INFO - tqdm - accuracy: 0.5230, batch_loss: 0.7518, loss: 0.9802 ||:  85%|########4 | 55/65 [10:24<01:54, 11.47s/it]
2022-04-11 10:42:48,748 - INFO - tqdm - accuracy: 0.5237, batch_loss: 0.6971, loss: 0.9752 ||:  86%|########6 | 56/65 [10:36<01:43, 11.49s/it]
2022-04-11 10:43:00,319 - INFO - tqdm - accuracy: 0.5200, batch_loss: 0.7978, loss: 0.9721 ||:  88%|########7 | 57/65 [10:47<01:32, 11.52s/it]
2022-04-11 10:43:11,868 - INFO - tqdm - accuracy: 0.5197, batch_loss: 0.6847, loss: 0.9671 ||:  89%|########9 | 58/65 [10:59<01:20, 11.53s/it]
2022-04-11 10:43:23,428 - INFO - tqdm - accuracy: 0.5193, batch_loss: 0.7217, loss: 0.9629 ||:  91%|######### | 59/65 [11:11<01:09, 11.54s/it]
2022-04-11 10:43:34,984 - INFO - tqdm - accuracy: 0.5195, batch_loss: 0.6632, loss: 0.9580 ||:  92%|#########2| 60/65 [11:22<00:57, 11.54s/it]
2022-04-11 10:43:46,544 - INFO - tqdm - accuracy: 0.5197, batch_loss: 0.6741, loss: 0.9533 ||:  94%|#########3| 61/65 [11:34<00:46, 11.55s/it]
2022-04-11 10:43:58,125 - INFO - tqdm - accuracy: 0.5229, batch_loss: 0.6745, loss: 0.9488 ||:  95%|#########5| 62/65 [11:45<00:34, 11.56s/it]
2022-04-11 10:44:09,706 - INFO - tqdm - accuracy: 0.5231, batch_loss: 0.6983, loss: 0.9448 ||:  97%|#########6| 63/65 [11:57<00:23, 11.56s/it]
2022-04-11 10:44:21,265 - INFO - tqdm - accuracy: 0.5222, batch_loss: 0.7171, loss: 0.9413 ||:  98%|#########8| 64/65 [12:08<00:11, 11.56s/it]
2022-04-11 10:44:26,401 - INFO - tqdm - accuracy: 0.5216, batch_loss: 0.7336, loss: 0.9381 ||: 100%|##########| 65/65 [12:14<00:00,  9.63s/it]
2022-04-11 10:44:26,402 - INFO - tqdm - accuracy: 0.5216, batch_loss: 0.7336, loss: 0.9381 ||: 100%|##########| 65/65 [12:14<00:00, 11.29s/it]
2022-04-11 10:44:29,384 - INFO - allennlp.training.trainer - Validating
2022-04-11 10:44:29,386 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-11 10:44:29,386 - INFO - allennlp.data.samplers.bucket_batch_sampler - No sorting keys given; trying to guess a good one
2022-04-11 10:44:29,386 - INFO - allennlp.data.samplers.bucket_batch_sampler - Using ['tokens'] as the sorting keys
2022-04-11 10:44:39,532 - INFO - tqdm - accuracy: 0.5000, batch_loss: 0.6253, loss: 0.7020 ||:  35%|###4      | 40/115 [00:10<00:19,  3.91it/s]
2022-04-11 10:44:49,691 - INFO - tqdm - accuracy: 0.4625, batch_loss: 0.6328, loss: 0.6993 ||:  70%|######9   | 80/115 [00:20<00:08,  3.95it/s]
2022-04-11 10:44:58,572 - INFO - tqdm - accuracy: 0.4803, batch_loss: 0.8247, loss: 0.6992 ||: 100%|##########| 115/115 [00:29<00:00,  3.91it/s]
2022-04-11 10:44:58,573 - INFO - tqdm - accuracy: 0.4803, batch_loss: 0.8247, loss: 0.6992 ||: 100%|##########| 115/115 [00:29<00:00,  3.94it/s]
2022-04-11 10:44:58,573 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-11 10:44:58,574 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.522  |     0.480
2022-04-11 10:44:58,575 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  1359.606  |       N/A
2022-04-11 10:44:58,576 - INFO - allennlp.training.tensorboard_writer - loss               |     0.938  |     0.699
2022-04-11 10:44:58,576 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5802.672  |       N/A
2022-04-11 10:45:04,612 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to './output_ir-ora-d_hyper6/best.th'.
2022-04-11 10:45:05,931 - INFO - allennlp.training.trainer - Epoch duration: 0:12:53.541258
2022-04-11 10:45:05,932 - INFO - allennlp.training.trainer - Estimated training time remaining: 3:00:29
2022-04-11 10:45:05,932 - INFO - allennlp.training.trainer - Epoch 1/14
2022-04-11 10:45:05,932 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-11 10:45:05,932 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-11 10:45:05,934 - INFO - allennlp.training.trainer - Training
2022-04-11 10:45:05,934 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-11 10:45:17,250 - INFO - tqdm - accuracy: 0.3750, batch_loss: 0.7183, loss: 0.7183 ||:   2%|1         | 1/65 [00:11<12:04, 11.32s/it]
2022-04-11 10:45:28,833 - INFO - tqdm - accuracy: 0.4844, batch_loss: 0.6827, loss: 0.7005 ||:   3%|3         | 2/65 [00:22<12:02, 11.47s/it]
2022-04-11 10:45:40,723 - INFO - tqdm - accuracy: 0.4479, batch_loss: 0.7330, loss: 0.7113 ||:   5%|4         | 3/65 [00:34<12:03, 11.66s/it]
2022-04-11 10:45:52,495 - INFO - tqdm - accuracy: 0.4766, batch_loss: 0.6771, loss: 0.7028 ||:   6%|6         | 4/65 [00:46<11:54, 11.71s/it]
2022-04-11 10:46:03,992 - INFO - tqdm - accuracy: 0.5000, batch_loss: 0.6793, loss: 0.6981 ||:   8%|7         | 5/65 [00:58<11:37, 11.63s/it]
2022-04-11 10:46:15,413 - INFO - tqdm - accuracy: 0.5156, batch_loss: 0.6736, loss: 0.6940 ||:   9%|9         | 6/65 [01:09<11:22, 11.56s/it]
2022-04-11 10:46:26,815 - INFO - tqdm - accuracy: 0.5089, batch_loss: 0.6950, loss: 0.6941 ||:  11%|#         | 7/65 [01:20<11:07, 11.51s/it]
2022-04-11 10:46:38,305 - INFO - tqdm - accuracy: 0.5078, batch_loss: 0.6923, loss: 0.6939 ||:  12%|#2        | 8/65 [01:32<10:55, 11.50s/it]
2022-04-11 10:46:49,920 - INFO - tqdm - accuracy: 0.5243, batch_loss: 0.6660, loss: 0.6908 ||:  14%|#3        | 9/65 [01:43<10:46, 11.54s/it]
2022-04-11 10:47:01,520 - INFO - tqdm - accuracy: 0.5312, batch_loss: 0.6565, loss: 0.6874 ||:  15%|#5        | 10/65 [01:55<10:35, 11.56s/it]
2022-04-11 10:47:13,059 - INFO - tqdm - accuracy: 0.5284, batch_loss: 0.7105, loss: 0.6895 ||:  17%|#6        | 11/65 [02:07<10:23, 11.55s/it]
2022-04-11 10:47:24,584 - INFO - tqdm - accuracy: 0.5365, batch_loss: 0.6703, loss: 0.6879 ||:  18%|#8        | 12/65 [02:18<10:11, 11.54s/it]
2022-04-11 10:47:36,065 - INFO - tqdm - accuracy: 0.5409, batch_loss: 0.6922, loss: 0.6882 ||:  20%|##        | 13/65 [02:30<09:59, 11.52s/it]
2022-04-11 10:47:47,480 - INFO - tqdm - accuracy: 0.5379, batch_loss: 0.7141, loss: 0.6901 ||:  22%|##1       | 14/65 [02:41<09:46, 11.49s/it]
2022-04-11 10:47:59,034 - INFO - tqdm - accuracy: 0.5333, batch_loss: 0.6969, loss: 0.6905 ||:  23%|##3       | 15/65 [02:53<09:35, 11.51s/it]
2022-04-11 10:48:10,627 - INFO - tqdm - accuracy: 0.5352, batch_loss: 0.6913, loss: 0.6906 ||:  25%|##4       | 16/65 [03:04<09:25, 11.54s/it]
2022-04-11 10:48:22,247 - INFO - tqdm - accuracy: 0.5331, batch_loss: 0.7342, loss: 0.6931 ||:  26%|##6       | 17/65 [03:16<09:14, 11.56s/it]
2022-04-11 10:48:33,837 - INFO - tqdm - accuracy: 0.5399, batch_loss: 0.6287, loss: 0.6896 ||:  28%|##7       | 18/65 [03:27<09:03, 11.57s/it]
2022-04-11 10:48:45,380 - INFO - tqdm - accuracy: 0.5378, batch_loss: 0.7001, loss: 0.6901 ||:  29%|##9       | 19/65 [03:39<08:51, 11.56s/it]
2022-04-11 10:48:56,922 - INFO - tqdm - accuracy: 0.5437, batch_loss: 0.6562, loss: 0.6884 ||:  31%|###       | 20/65 [03:50<08:40, 11.56s/it]
2022-04-11 10:49:08,443 - INFO - tqdm - accuracy: 0.5476, batch_loss: 0.6330, loss: 0.6858 ||:  32%|###2      | 21/65 [04:02<08:27, 11.55s/it]
2022-04-11 10:49:19,830 - INFO - tqdm - accuracy: 0.5483, batch_loss: 0.6835, loss: 0.6857 ||:  34%|###3      | 22/65 [04:13<08:14, 11.50s/it]
2022-04-11 10:49:31,357 - INFO - tqdm - accuracy: 0.5503, batch_loss: 0.6669, loss: 0.6849 ||:  35%|###5      | 23/65 [04:25<08:03, 11.51s/it]
2022-04-11 10:49:42,870 - INFO - tqdm - accuracy: 0.5482, batch_loss: 0.7000, loss: 0.6855 ||:  37%|###6      | 24/65 [04:36<07:51, 11.51s/it]
2022-04-11 10:49:54,363 - INFO - tqdm - accuracy: 0.5525, batch_loss: 0.6219, loss: 0.6829 ||:  38%|###8      | 25/65 [04:48<07:40, 11.50s/it]
2022-04-11 10:50:05,904 - INFO - tqdm - accuracy: 0.5553, batch_loss: 0.6769, loss: 0.6827 ||:  40%|####      | 26/65 [04:59<07:29, 11.52s/it]
2022-04-11 10:50:17,414 - INFO - tqdm - accuracy: 0.5579, batch_loss: 0.6498, loss: 0.6815 ||:  42%|####1     | 27/65 [05:11<07:17, 11.51s/it]
2022-04-11 10:50:28,930 - INFO - tqdm - accuracy: 0.5603, batch_loss: 0.6679, loss: 0.6810 ||:  43%|####3     | 28/65 [05:22<07:06, 11.51s/it]
2022-04-11 10:50:40,450 - INFO - tqdm - accuracy: 0.5625, batch_loss: 0.6522, loss: 0.6800 ||:  45%|####4     | 29/65 [05:34<06:54, 11.52s/it]
2022-04-11 10:50:51,996 - INFO - tqdm - accuracy: 0.5604, batch_loss: 0.7063, loss: 0.6809 ||:  46%|####6     | 30/65 [05:46<06:43, 11.52s/it]
2022-04-11 10:51:03,532 - INFO - tqdm - accuracy: 0.5605, batch_loss: 0.6719, loss: 0.6806 ||:  48%|####7     | 31/65 [05:57<06:31, 11.53s/it]
2022-04-11 10:51:15,074 - INFO - tqdm - accuracy: 0.5615, batch_loss: 0.6380, loss: 0.6793 ||:  49%|####9     | 32/65 [06:09<06:20, 11.53s/it]
2022-04-11 10:51:26,637 - INFO - tqdm - accuracy: 0.5597, batch_loss: 0.6911, loss: 0.6796 ||:  51%|#####     | 33/65 [06:20<06:09, 11.54s/it]
2022-04-11 10:51:38,177 - INFO - tqdm - accuracy: 0.5616, batch_loss: 0.6781, loss: 0.6796 ||:  52%|#####2    | 34/65 [06:32<05:57, 11.54s/it]
2022-04-11 10:51:50,103 - INFO - tqdm - accuracy: 0.5563, batch_loss: 0.6947, loss: 0.6800 ||:  54%|#####3    | 35/65 [06:44<05:49, 11.66s/it]
2022-04-11 10:52:01,642 - INFO - tqdm - accuracy: 0.5616, batch_loss: 0.6388, loss: 0.6789 ||:  55%|#####5    | 36/65 [06:55<05:37, 11.62s/it]
2022-04-11 10:52:13,192 - INFO - tqdm - accuracy: 0.5642, batch_loss: 0.5967, loss: 0.6766 ||:  57%|#####6    | 37/65 [07:07<05:24, 11.60s/it]
2022-04-11 10:52:24,758 - INFO - tqdm - accuracy: 0.5674, batch_loss: 0.6751, loss: 0.6766 ||:  58%|#####8    | 38/65 [07:18<05:12, 11.59s/it]
2022-04-11 10:52:36,299 - INFO - tqdm - accuracy: 0.5665, batch_loss: 0.6732, loss: 0.6765 ||:  60%|######    | 39/65 [07:30<05:00, 11.58s/it]
2022-04-11 10:52:47,870 - INFO - tqdm - accuracy: 0.5695, batch_loss: 0.6215, loss: 0.6751 ||:  62%|######1   | 40/65 [07:41<04:49, 11.57s/it]
2022-04-11 10:52:59,415 - INFO - tqdm - accuracy: 0.5694, batch_loss: 0.6316, loss: 0.6741 ||:  63%|######3   | 41/65 [07:53<04:37, 11.57s/it]
2022-04-11 10:53:10,991 - INFO - tqdm - accuracy: 0.5707, batch_loss: 0.6231, loss: 0.6729 ||:  65%|######4   | 42/65 [08:05<04:26, 11.57s/it]
2022-04-11 10:53:22,586 - INFO - tqdm - accuracy: 0.5727, batch_loss: 0.7169, loss: 0.6739 ||:  66%|######6   | 43/65 [08:16<04:14, 11.58s/it]
2022-04-11 10:53:34,169 - INFO - tqdm - accuracy: 0.5739, batch_loss: 0.6673, loss: 0.6737 ||:  68%|######7   | 44/65 [08:28<04:03, 11.58s/it]
2022-04-11 10:53:45,775 - INFO - tqdm - accuracy: 0.5750, batch_loss: 0.5865, loss: 0.6718 ||:  69%|######9   | 45/65 [08:39<03:51, 11.59s/it]
2022-04-11 10:53:57,375 - INFO - tqdm - accuracy: 0.5768, batch_loss: 0.6359, loss: 0.6710 ||:  71%|#######   | 46/65 [08:51<03:40, 11.59s/it]
2022-04-11 10:54:08,958 - INFO - tqdm - accuracy: 0.5785, batch_loss: 0.6751, loss: 0.6711 ||:  72%|#######2  | 47/65 [09:03<03:28, 11.59s/it]
2022-04-11 10:54:20,544 - INFO - tqdm - accuracy: 0.5775, batch_loss: 0.7244, loss: 0.6722 ||:  74%|#######3  | 48/65 [09:14<03:16, 11.59s/it]
2022-04-11 10:54:32,146 - INFO - tqdm - accuracy: 0.5778, batch_loss: 0.7836, loss: 0.6745 ||:  75%|#######5  | 49/65 [09:26<03:05, 11.59s/it]
2022-04-11 10:54:43,729 - INFO - tqdm - accuracy: 0.5806, batch_loss: 0.6624, loss: 0.6743 ||:  77%|#######6  | 50/65 [09:37<02:53, 11.59s/it]
2022-04-11 10:54:55,317 - INFO - tqdm - accuracy: 0.5772, batch_loss: 0.7886, loss: 0.6765 ||:  78%|#######8  | 51/65 [09:49<02:42, 11.59s/it]
2022-04-11 10:55:06,890 - INFO - tqdm - accuracy: 0.5763, batch_loss: 0.7246, loss: 0.6774 ||:  80%|########  | 52/65 [10:00<02:30, 11.58s/it]
2022-04-11 10:55:18,456 - INFO - tqdm - accuracy: 0.5772, batch_loss: 0.6309, loss: 0.6765 ||:  82%|########1 | 53/65 [10:12<02:18, 11.58s/it]
2022-04-11 10:55:30,036 - INFO - tqdm - accuracy: 0.5775, batch_loss: 0.7473, loss: 0.6779 ||:  83%|########3 | 54/65 [10:24<02:07, 11.58s/it]
2022-04-11 10:55:41,612 - INFO - tqdm - accuracy: 0.5795, batch_loss: 0.5734, loss: 0.6760 ||:  85%|########4 | 55/65 [10:35<01:55, 11.58s/it]
2022-04-11 10:55:53,195 - INFO - tqdm - accuracy: 0.5809, batch_loss: 0.6201, loss: 0.6750 ||:  86%|########6 | 56/65 [10:47<01:44, 11.58s/it]
2022-04-11 10:56:04,494 - INFO - tqdm - accuracy: 0.5822, batch_loss: 0.6218, loss: 0.6740 ||:  88%|########7 | 57/65 [10:58<01:31, 11.50s/it]
2022-04-11 10:56:16,055 - INFO - tqdm - accuracy: 0.5808, batch_loss: 0.7260, loss: 0.6749 ||:  89%|########9 | 58/65 [11:10<01:20, 11.52s/it]
2022-04-11 10:56:27,603 - INFO - tqdm - accuracy: 0.5826, batch_loss: 0.6224, loss: 0.6740 ||:  91%|######### | 59/65 [11:21<01:09, 11.52s/it]
2022-04-11 10:56:39,144 - INFO - tqdm - accuracy: 0.5823, batch_loss: 0.7461, loss: 0.6752 ||:  92%|#########2| 60/65 [11:33<00:57, 11.53s/it]
2022-04-11 10:56:50,662 - INFO - tqdm - accuracy: 0.5840, batch_loss: 0.5745, loss: 0.6736 ||:  94%|#########3| 61/65 [11:44<00:46, 11.53s/it]
2022-04-11 10:57:02,217 - INFO - tqdm - accuracy: 0.5857, batch_loss: 0.5165, loss: 0.6710 ||:  95%|#########5| 62/65 [11:56<00:34, 11.53s/it]
2022-04-11 10:57:13,744 - INFO - tqdm - accuracy: 0.5863, batch_loss: 0.6397, loss: 0.6705 ||:  97%|#########6| 63/65 [12:07<00:23, 11.53s/it]
2022-04-11 10:57:24,915 - INFO - tqdm - accuracy: 0.5857, batch_loss: 0.7153, loss: 0.6712 ||:  98%|#########8| 64/65 [12:18<00:11, 11.42s/it]
2022-04-11 10:57:30,039 - INFO - tqdm - accuracy: 0.5866, batch_loss: 0.6650, loss: 0.6712 ||: 100%|##########| 65/65 [12:24<00:00,  9.53s/it]
2022-04-11 10:57:30,040 - INFO - tqdm - accuracy: 0.5866, batch_loss: 0.6650, loss: 0.6712 ||: 100%|##########| 65/65 [12:24<00:00, 11.45s/it]
2022-04-11 10:57:32,927 - INFO - allennlp.training.trainer - Validating
2022-04-11 10:57:32,929 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-11 10:57:42,938 - INFO - tqdm - accuracy: 0.6875, batch_loss: 0.7034, loss: 0.6367 ||:  35%|###4      | 40/115 [00:10<00:19,  3.94it/s]
2022-04-11 10:57:53,179 - INFO - tqdm - accuracy: 0.6500, batch_loss: 0.7224, loss: 0.6507 ||:  70%|######9   | 80/115 [00:20<00:09,  3.88it/s]
2022-04-11 10:58:02,116 - INFO - tqdm - accuracy: 0.6376, batch_loss: 0.5967, loss: 0.6667 ||: 100%|##########| 115/115 [00:29<00:00,  3.85it/s]
2022-04-11 10:58:02,117 - INFO - tqdm - accuracy: 0.6376, batch_loss: 0.5967, loss: 0.6667 ||: 100%|##########| 115/115 [00:29<00:00,  3.94it/s]
2022-04-11 10:58:02,117 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-11 10:58:02,118 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.587  |     0.638
2022-04-11 10:58:02,119 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-11 10:58:02,119 - INFO - allennlp.training.tensorboard_writer - loss               |     0.671  |     0.667
2022-04-11 10:58:02,120 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5802.672  |       N/A
2022-04-11 10:58:07,951 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to './output_ir-ora-d_hyper6/best.th'.
2022-04-11 10:58:22,751 - INFO - allennlp.training.trainer - Epoch duration: 0:13:16.819474
2022-04-11 10:58:22,752 - INFO - allennlp.training.trainer - Estimated training time remaining: 2:50:07
2022-04-11 10:58:22,752 - INFO - allennlp.training.trainer - Epoch 2/14
2022-04-11 10:58:22,752 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-11 10:58:22,752 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-11 10:58:22,754 - INFO - allennlp.training.trainer - Training
2022-04-11 10:58:22,754 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-11 10:58:33,932 - INFO - tqdm - accuracy: 0.5938, batch_loss: 0.6738, loss: 0.6738 ||:   2%|1         | 1/65 [00:11<11:55, 11.18s/it]
2022-04-11 10:58:45,417 - INFO - tqdm - accuracy: 0.7188, batch_loss: 0.5133, loss: 0.5936 ||:   3%|3         | 2/65 [00:22<11:55, 11.36s/it]
2022-04-11 10:58:57,178 - INFO - tqdm - accuracy: 0.7083, batch_loss: 0.5611, loss: 0.5827 ||:   5%|4         | 3/65 [00:34<11:55, 11.54s/it]
2022-04-11 10:59:09,090 - INFO - tqdm - accuracy: 0.7266, batch_loss: 0.5291, loss: 0.5693 ||:   6%|6         | 4/65 [00:46<11:52, 11.69s/it]
2022-04-11 10:59:20,612 - INFO - tqdm - accuracy: 0.7312, batch_loss: 0.5234, loss: 0.5602 ||:   8%|7         | 5/65 [00:57<11:37, 11.63s/it]
2022-04-11 10:59:31,866 - INFO - tqdm - accuracy: 0.7552, batch_loss: 0.5257, loss: 0.5544 ||:   9%|9         | 6/65 [01:09<11:18, 11.50s/it]
2022-04-11 10:59:43,289 - INFO - tqdm - accuracy: 0.7589, batch_loss: 0.5537, loss: 0.5543 ||:  11%|#         | 7/65 [01:20<11:05, 11.48s/it]
2022-04-11 10:59:54,892 - INFO - tqdm - accuracy: 0.7383, batch_loss: 0.6223, loss: 0.5628 ||:  12%|#2        | 8/65 [01:32<10:56, 11.52s/it]
2022-04-11 11:00:06,588 - INFO - tqdm - accuracy: 0.7431, batch_loss: 0.5092, loss: 0.5568 ||:  14%|#3        | 9/65 [01:43<10:48, 11.57s/it]
2022-04-11 11:00:18,168 - INFO - tqdm - accuracy: 0.7406, batch_loss: 0.5664, loss: 0.5578 ||:  15%|#5        | 10/65 [01:55<10:36, 11.57s/it]
2022-04-11 11:00:29,693 - INFO - tqdm - accuracy: 0.7273, batch_loss: 0.6019, loss: 0.5618 ||:  17%|#6        | 11/65 [02:06<10:24, 11.56s/it]
2022-04-11 11:00:41,184 - INFO - tqdm - accuracy: 0.7240, batch_loss: 0.5594, loss: 0.5616 ||:  18%|#8        | 12/65 [02:18<10:11, 11.54s/it]
2022-04-11 11:00:52,344 - INFO - tqdm - accuracy: 0.7036, batch_loss: 0.7932, loss: 0.5794 ||:  20%|##        | 13/65 [02:29<09:54, 11.42s/it]
2022-04-11 11:01:03,893 - INFO - tqdm - accuracy: 0.7025, batch_loss: 0.5831, loss: 0.5797 ||:  22%|##1       | 14/65 [02:41<09:44, 11.46s/it]
2022-04-11 11:01:15,490 - INFO - tqdm - accuracy: 0.6952, batch_loss: 0.6991, loss: 0.5876 ||:  23%|##3       | 15/65 [02:52<09:35, 11.50s/it]
2022-04-11 11:01:27,089 - INFO - tqdm - accuracy: 0.7006, batch_loss: 0.4904, loss: 0.5816 ||:  25%|##4       | 16/65 [03:04<09:25, 11.53s/it]
2022-04-11 11:01:38,665 - INFO - tqdm - accuracy: 0.6961, batch_loss: 0.6600, loss: 0.5862 ||:  26%|##6       | 17/65 [03:15<09:14, 11.54s/it]
2022-04-11 11:01:50,239 - INFO - tqdm - accuracy: 0.6870, batch_loss: 0.8301, loss: 0.5997 ||:  28%|##7       | 18/65 [03:27<09:03, 11.55s/it]
2022-04-11 11:02:01,801 - INFO - tqdm - accuracy: 0.6886, batch_loss: 0.6060, loss: 0.6001 ||:  29%|##9       | 19/65 [03:39<08:51, 11.56s/it]
2022-04-11 11:02:13,378 - INFO - tqdm - accuracy: 0.6901, batch_loss: 0.5542, loss: 0.5978 ||:  31%|###       | 20/65 [03:50<08:40, 11.56s/it]
2022-04-11 11:02:24,925 - INFO - tqdm - accuracy: 0.6855, batch_loss: 0.6405, loss: 0.5998 ||:  32%|###2      | 21/65 [04:02<08:28, 11.56s/it]
2022-04-11 11:02:36,516 - INFO - tqdm - accuracy: 0.6842, batch_loss: 0.5637, loss: 0.5982 ||:  34%|###3      | 22/65 [04:13<08:17, 11.57s/it]
2022-04-11 11:02:48,082 - INFO - tqdm - accuracy: 0.6844, batch_loss: 0.5611, loss: 0.5965 ||:  35%|###5      | 23/65 [04:25<08:05, 11.57s/it]
2022-04-11 11:02:59,538 - INFO - tqdm - accuracy: 0.6858, batch_loss: 0.5776, loss: 0.5958 ||:  37%|###6      | 24/65 [04:36<07:52, 11.53s/it]
2022-04-11 11:03:11,127 - INFO - tqdm - accuracy: 0.6884, batch_loss: 0.5535, loss: 0.5941 ||:  38%|###8      | 25/65 [04:48<07:42, 11.55s/it]
2022-04-11 11:03:22,695 - INFO - tqdm - accuracy: 0.6907, batch_loss: 0.5408, loss: 0.5920 ||:  40%|####      | 26/65 [04:59<07:30, 11.56s/it]
2022-04-11 11:03:34,258 - INFO - tqdm - accuracy: 0.6952, batch_loss: 0.4818, loss: 0.5879 ||:  42%|####1     | 27/65 [05:11<07:19, 11.56s/it]
2022-04-11 11:03:45,843 - INFO - tqdm - accuracy: 0.6994, batch_loss: 0.4741, loss: 0.5839 ||:  43%|####3     | 28/65 [05:23<07:07, 11.57s/it]
2022-04-11 11:03:57,440 - INFO - tqdm - accuracy: 0.6990, batch_loss: 0.5927, loss: 0.5842 ||:  45%|####4     | 29/65 [05:34<06:56, 11.58s/it]
2022-04-11 11:04:09,032 - INFO - tqdm - accuracy: 0.7007, batch_loss: 0.5177, loss: 0.5820 ||:  46%|####6     | 30/65 [05:46<06:45, 11.58s/it]
2022-04-11 11:04:20,585 - INFO - tqdm - accuracy: 0.6983, batch_loss: 0.5426, loss: 0.5807 ||:  48%|####7     | 31/65 [05:57<06:33, 11.57s/it]
2022-04-11 11:04:32,095 - INFO - tqdm - accuracy: 0.6989, batch_loss: 0.5401, loss: 0.5794 ||:  49%|####9     | 32/65 [06:09<06:21, 11.55s/it]
2022-04-11 11:04:43,583 - INFO - tqdm - accuracy: 0.6986, batch_loss: 0.6632, loss: 0.5820 ||:  51%|#####     | 33/65 [06:20<06:09, 11.53s/it]
2022-04-11 11:04:55,062 - INFO - tqdm - accuracy: 0.6973, batch_loss: 0.6561, loss: 0.5841 ||:  52%|#####2    | 34/65 [06:32<05:57, 11.52s/it]
2022-04-11 11:05:06,583 - INFO - tqdm - accuracy: 0.6979, batch_loss: 0.4807, loss: 0.5812 ||:  54%|#####3    | 35/65 [06:43<05:45, 11.52s/it]
2022-04-11 11:05:18,133 - INFO - tqdm - accuracy: 0.6994, batch_loss: 0.5336, loss: 0.5799 ||:  55%|#####5    | 36/65 [06:55<05:34, 11.53s/it]
2022-04-11 11:05:29,708 - INFO - tqdm - accuracy: 0.6974, batch_loss: 0.5677, loss: 0.5795 ||:  57%|#####6    | 37/65 [07:06<05:23, 11.54s/it]
2022-04-11 11:05:41,292 - INFO - tqdm - accuracy: 0.7004, batch_loss: 0.3680, loss: 0.5740 ||:  58%|#####8    | 38/65 [07:18<05:11, 11.55s/it]
2022-04-11 11:05:52,875 - INFO - tqdm - accuracy: 0.7001, batch_loss: 0.5298, loss: 0.5728 ||:  60%|######    | 39/65 [07:30<05:00, 11.56s/it]
2022-04-11 11:06:04,440 - INFO - tqdm - accuracy: 0.6990, batch_loss: 0.6174, loss: 0.5739 ||:  62%|######1   | 40/65 [07:41<04:49, 11.56s/it]
2022-04-11 11:06:15,976 - INFO - tqdm - accuracy: 0.6995, batch_loss: 0.4618, loss: 0.5712 ||:  63%|######3   | 41/65 [07:53<04:37, 11.56s/it]
2022-04-11 11:06:27,393 - INFO - tqdm - accuracy: 0.7014, batch_loss: 0.4676, loss: 0.5687 ||:  65%|######4   | 42/65 [08:04<04:24, 11.51s/it]
2022-04-11 11:06:38,879 - INFO - tqdm - accuracy: 0.7011, batch_loss: 0.6117, loss: 0.5697 ||:  66%|######6   | 43/65 [08:16<04:13, 11.51s/it]
2022-04-11 11:06:50,387 - INFO - tqdm - accuracy: 0.7015, batch_loss: 0.4918, loss: 0.5680 ||:  68%|######7   | 44/65 [08:27<04:01, 11.51s/it]
2022-04-11 11:07:01,887 - INFO - tqdm - accuracy: 0.7026, batch_loss: 0.5668, loss: 0.5679 ||:  69%|######9   | 45/65 [08:39<03:50, 11.50s/it]
2022-04-11 11:07:13,366 - INFO - tqdm - accuracy: 0.7036, batch_loss: 0.4381, loss: 0.5651 ||:  71%|#######   | 46/65 [08:50<03:38, 11.50s/it]
2022-04-11 11:07:24,873 - INFO - tqdm - accuracy: 0.7033, batch_loss: 0.6111, loss: 0.5661 ||:  72%|#######2  | 47/65 [09:02<03:26, 11.50s/it]
2022-04-11 11:07:36,396 - INFO - tqdm - accuracy: 0.7016, batch_loss: 0.7210, loss: 0.5693 ||:  74%|#######3  | 48/65 [09:13<03:15, 11.51s/it]
2022-04-11 11:07:47,963 - INFO - tqdm - accuracy: 0.6994, batch_loss: 0.5491, loss: 0.5689 ||:  75%|#######5  | 49/65 [09:25<03:04, 11.52s/it]
2022-04-11 11:07:59,540 - INFO - tqdm - accuracy: 0.6986, batch_loss: 0.6103, loss: 0.5697 ||:  77%|#######6  | 50/65 [09:36<02:53, 11.54s/it]
2022-04-11 11:08:11,090 - INFO - tqdm - accuracy: 0.6959, batch_loss: 0.6795, loss: 0.5719 ||:  78%|#######8  | 51/65 [09:48<02:41, 11.54s/it]
2022-04-11 11:08:22,629 - INFO - tqdm - accuracy: 0.6945, batch_loss: 0.5278, loss: 0.5710 ||:  80%|########  | 52/65 [09:59<02:30, 11.54s/it]
2022-04-11 11:08:34,138 - INFO - tqdm - accuracy: 0.6962, batch_loss: 0.4969, loss: 0.5696 ||:  82%|########1 | 53/65 [10:11<02:18, 11.53s/it]
2022-04-11 11:08:45,625 - INFO - tqdm - accuracy: 0.6966, batch_loss: 0.5559, loss: 0.5694 ||:  83%|########3 | 54/65 [10:22<02:06, 11.52s/it]
2022-04-11 11:08:57,110 - INFO - tqdm - accuracy: 0.6964, batch_loss: 0.6355, loss: 0.5706 ||:  85%|########4 | 55/65 [10:34<01:55, 11.51s/it]
2022-04-11 11:09:08,633 - INFO - tqdm - accuracy: 0.6957, batch_loss: 0.6154, loss: 0.5714 ||:  86%|########6 | 56/65 [10:45<01:43, 11.51s/it]
2022-04-11 11:09:20,143 - INFO - tqdm - accuracy: 0.6945, batch_loss: 0.7266, loss: 0.5741 ||:  88%|########7 | 57/65 [10:57<01:32, 11.51s/it]
2022-04-11 11:09:31,433 - INFO - tqdm - accuracy: 0.6943, batch_loss: 0.5128, loss: 0.5731 ||:  89%|########9 | 58/65 [11:08<01:20, 11.45s/it]
2022-04-11 11:09:42,986 - INFO - tqdm - accuracy: 0.6963, batch_loss: 0.4624, loss: 0.5712 ||:  91%|######### | 59/65 [11:20<01:08, 11.48s/it]
2022-04-11 11:09:54,581 - INFO - tqdm - accuracy: 0.6946, batch_loss: 0.6076, loss: 0.5718 ||:  92%|#########2| 60/65 [11:31<00:57, 11.51s/it]
2022-04-11 11:10:06,130 - INFO - tqdm - accuracy: 0.6950, batch_loss: 0.4741, loss: 0.5702 ||:  94%|#########3| 61/65 [11:43<00:46, 11.52s/it]
2022-04-11 11:10:17,695 - INFO - tqdm - accuracy: 0.6934, batch_loss: 0.6414, loss: 0.5713 ||:  95%|#########5| 62/65 [11:54<00:34, 11.54s/it]
2022-04-11 11:10:29,265 - INFO - tqdm - accuracy: 0.6923, batch_loss: 0.5524, loss: 0.5710 ||:  97%|#########6| 63/65 [12:06<00:23, 11.55s/it]
2022-04-11 11:10:40,836 - INFO - tqdm - accuracy: 0.6893, batch_loss: 0.6883, loss: 0.5729 ||:  98%|#########8| 64/65 [12:18<00:11, 11.55s/it]
2022-04-11 11:10:45,971 - INFO - tqdm - accuracy: 0.6890, batch_loss: 0.6520, loss: 0.5741 ||: 100%|##########| 65/65 [12:23<00:00,  9.63s/it]
2022-04-11 11:10:45,972 - INFO - tqdm - accuracy: 0.6890, batch_loss: 0.6520, loss: 0.5741 ||: 100%|##########| 65/65 [12:23<00:00, 11.43s/it]
2022-04-11 11:10:48,849 - INFO - allennlp.training.trainer - Validating
2022-04-11 11:10:48,850 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-11 11:10:59,005 - INFO - tqdm - accuracy: 0.6000, batch_loss: 0.9507, loss: 0.7390 ||:  35%|###4      | 40/115 [00:10<00:19,  3.91it/s]
2022-04-11 11:11:09,007 - INFO - tqdm - accuracy: 0.6289, batch_loss: 0.7774, loss: 0.6686 ||:  70%|######9   | 80/115 [00:20<00:08,  4.00it/s]
2022-04-11 11:11:17,985 - INFO - tqdm - accuracy: 0.6419, batch_loss: 1.3014, loss: 0.6854 ||: 100%|##########| 115/115 [00:29<00:00,  3.90it/s]
2022-04-11 11:11:17,985 - INFO - tqdm - accuracy: 0.6419, batch_loss: 1.3014, loss: 0.6854 ||: 100%|##########| 115/115 [00:29<00:00,  3.95it/s]
2022-04-11 11:11:17,986 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-11 11:11:17,986 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.689  |     0.642
2022-04-11 11:11:17,987 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-11 11:11:17,988 - INFO - allennlp.training.tensorboard_writer - loss               |     0.574  |     0.685
2022-04-11 11:11:17,988 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5802.672  |       N/A
2022-04-11 11:11:23,824 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to './output_ir-ora-d_hyper6/best.th'.
2022-04-11 11:11:38,275 - INFO - allennlp.training.trainer - Epoch duration: 0:13:15.522895
2022-04-11 11:11:38,275 - INFO - allennlp.training.trainer - Estimated training time remaining: 2:37:43
2022-04-11 11:11:38,275 - INFO - allennlp.training.trainer - Epoch 3/14
2022-04-11 11:11:38,275 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-11 11:11:38,275 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-11 11:11:38,277 - INFO - allennlp.training.trainer - Training
2022-04-11 11:11:38,277 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-11 11:11:49,520 - INFO - tqdm - accuracy: 0.8438, batch_loss: 0.4639, loss: 0.4639 ||:   2%|1         | 1/65 [00:11<11:59, 11.24s/it]
2022-04-11 11:12:01,136 - INFO - tqdm - accuracy: 0.7344, batch_loss: 0.6187, loss: 0.5413 ||:   3%|3         | 2/65 [00:22<12:02, 11.46s/it]
2022-04-11 11:12:13,084 - INFO - tqdm - accuracy: 0.7708, batch_loss: 0.4724, loss: 0.5184 ||:   5%|4         | 3/65 [00:34<12:04, 11.68s/it]
2022-04-11 11:12:24,790 - INFO - tqdm - accuracy: 0.7812, batch_loss: 0.4262, loss: 0.4953 ||:   6%|6         | 4/65 [00:46<11:53, 11.69s/it]
2022-04-11 11:12:36,701 - INFO - tqdm - accuracy: 0.7937, batch_loss: 0.4040, loss: 0.4771 ||:   8%|7         | 5/65 [00:58<11:46, 11.77s/it]
2022-04-11 11:12:48,012 - INFO - tqdm - accuracy: 0.8021, batch_loss: 0.4070, loss: 0.4654 ||:   9%|9         | 6/65 [01:09<11:25, 11.61s/it]
2022-04-11 11:12:59,483 - INFO - tqdm - accuracy: 0.8170, batch_loss: 0.3802, loss: 0.4532 ||:  11%|#         | 7/65 [01:21<11:10, 11.57s/it]
2022-04-11 11:13:10,931 - INFO - tqdm - accuracy: 0.8320, batch_loss: 0.3102, loss: 0.4353 ||:  12%|#2        | 8/65 [01:32<10:57, 11.53s/it]
2022-04-11 11:13:22,561 - INFO - tqdm - accuracy: 0.8333, batch_loss: 0.4037, loss: 0.4318 ||:  14%|#3        | 9/65 [01:44<10:47, 11.56s/it]
2022-04-11 11:13:34,118 - INFO - tqdm - accuracy: 0.8438, batch_loss: 0.3122, loss: 0.4199 ||:  15%|#5        | 10/65 [01:55<10:35, 11.56s/it]
2022-04-11 11:13:45,510 - INFO - tqdm - accuracy: 0.8438, batch_loss: 0.3549, loss: 0.4139 ||:  17%|#6        | 11/65 [02:07<10:21, 11.51s/it]
2022-04-11 11:13:56,973 - INFO - tqdm - accuracy: 0.8490, batch_loss: 0.2552, loss: 0.4007 ||:  18%|#8        | 12/65 [02:18<10:09, 11.49s/it]
2022-04-11 11:14:08,483 - INFO - tqdm - accuracy: 0.8558, batch_loss: 0.2262, loss: 0.3873 ||:  20%|##        | 13/65 [02:30<09:57, 11.50s/it]
2022-04-11 11:14:20,018 - INFO - tqdm - accuracy: 0.8482, batch_loss: 0.4373, loss: 0.3909 ||:  22%|##1       | 14/65 [02:41<09:47, 11.51s/it]
2022-04-11 11:14:31,555 - INFO - tqdm - accuracy: 0.8438, batch_loss: 0.4587, loss: 0.3954 ||:  23%|##3       | 15/65 [02:53<09:35, 11.52s/it]
2022-04-11 11:14:43,089 - INFO - tqdm - accuracy: 0.8438, batch_loss: 0.3477, loss: 0.3924 ||:  25%|##4       | 16/65 [03:04<09:24, 11.52s/it]
2022-04-11 11:14:54,642 - INFO - tqdm - accuracy: 0.8382, batch_loss: 0.5506, loss: 0.4017 ||:  26%|##6       | 17/65 [03:16<09:13, 11.53s/it]
2022-04-11 11:15:06,220 - INFO - tqdm - accuracy: 0.8455, batch_loss: 0.2049, loss: 0.3908 ||:  28%|##7       | 18/65 [03:27<09:02, 11.55s/it]
2022-04-11 11:15:17,789 - INFO - tqdm - accuracy: 0.8405, batch_loss: 0.5279, loss: 0.3980 ||:  29%|##9       | 19/65 [03:39<08:51, 11.55s/it]
2022-04-11 11:15:29,352 - INFO - tqdm - accuracy: 0.8328, batch_loss: 0.6131, loss: 0.4087 ||:  31%|###       | 20/65 [03:51<08:40, 11.56s/it]
2022-04-11 11:15:40,914 - INFO - tqdm - accuracy: 0.8378, batch_loss: 0.3030, loss: 0.4037 ||:  32%|###2      | 21/65 [04:02<08:28, 11.56s/it]
2022-04-11 11:15:52,482 - INFO - tqdm - accuracy: 0.8366, batch_loss: 0.4028, loss: 0.4037 ||:  34%|###3      | 22/65 [04:14<08:17, 11.56s/it]
2022-04-11 11:16:04,064 - INFO - tqdm - accuracy: 0.8383, batch_loss: 0.2288, loss: 0.3961 ||:  35%|###5      | 23/65 [04:25<08:05, 11.57s/it]
2022-04-11 11:16:15,634 - INFO - tqdm - accuracy: 0.8359, batch_loss: 0.3218, loss: 0.3930 ||:  37%|###6      | 24/65 [04:37<07:54, 11.57s/it]
2022-04-11 11:16:27,207 - INFO - tqdm - accuracy: 0.8387, batch_loss: 0.2721, loss: 0.3881 ||:  38%|###8      | 25/65 [04:48<07:42, 11.57s/it]
2022-04-11 11:16:38,771 - INFO - tqdm - accuracy: 0.8413, batch_loss: 0.2433, loss: 0.3826 ||:  40%|####      | 26/65 [05:00<07:31, 11.57s/it]
2022-04-11 11:16:50,339 - INFO - tqdm - accuracy: 0.8391, batch_loss: 0.3474, loss: 0.3813 ||:  42%|####1     | 27/65 [05:12<07:19, 11.57s/it]
2022-04-11 11:17:01,877 - INFO - tqdm - accuracy: 0.8393, batch_loss: 0.3401, loss: 0.3798 ||:  43%|####3     | 28/65 [05:23<07:07, 11.56s/it]
2022-04-11 11:17:13,410 - INFO - tqdm - accuracy: 0.8405, batch_loss: 0.2415, loss: 0.3750 ||:  45%|####4     | 29/65 [05:35<06:55, 11.55s/it]
2022-04-11 11:17:24,942 - INFO - tqdm - accuracy: 0.8406, batch_loss: 0.2785, loss: 0.3718 ||:  46%|####6     | 30/65 [05:46<06:44, 11.55s/it]
2022-04-11 11:17:36,453 - INFO - tqdm - accuracy: 0.8417, batch_loss: 0.4142, loss: 0.3732 ||:  48%|####7     | 31/65 [05:58<06:32, 11.54s/it]
2022-04-11 11:17:47,843 - INFO - tqdm - accuracy: 0.8389, batch_loss: 0.4373, loss: 0.3752 ||:  49%|####9     | 32/65 [06:09<06:19, 11.49s/it]
2022-04-11 11:17:59,361 - INFO - tqdm - accuracy: 0.8381, batch_loss: 0.3261, loss: 0.3737 ||:  51%|#####     | 33/65 [06:21<06:07, 11.50s/it]
2022-04-11 11:18:10,857 - INFO - tqdm - accuracy: 0.8373, batch_loss: 0.4519, loss: 0.3760 ||:  52%|#####2    | 34/65 [06:32<05:56, 11.50s/it]
2022-04-11 11:18:22,319 - INFO - tqdm - accuracy: 0.8366, batch_loss: 0.3388, loss: 0.3749 ||:  54%|#####3    | 35/65 [06:44<05:44, 11.49s/it]
2022-04-11 11:18:33,834 - INFO - tqdm - accuracy: 0.8333, batch_loss: 0.4994, loss: 0.3784 ||:  55%|#####5    | 36/65 [06:55<05:33, 11.50s/it]
2022-04-11 11:18:45,368 - INFO - tqdm - accuracy: 0.8328, batch_loss: 0.3263, loss: 0.3770 ||:  57%|#####6    | 37/65 [07:07<05:22, 11.51s/it]
2022-04-11 11:18:56,905 - INFO - tqdm - accuracy: 0.8331, batch_loss: 0.3101, loss: 0.3752 ||:  58%|#####8    | 38/65 [07:18<05:10, 11.52s/it]
2022-04-11 11:19:08,450 - INFO - tqdm - accuracy: 0.8317, batch_loss: 0.5538, loss: 0.3798 ||:  60%|######    | 39/65 [07:30<04:59, 11.52s/it]
2022-04-11 11:19:20,000 - INFO - tqdm - accuracy: 0.8320, batch_loss: 0.3405, loss: 0.3788 ||:  62%|######1   | 40/65 [07:41<04:48, 11.53s/it]
2022-04-11 11:19:31,566 - INFO - tqdm - accuracy: 0.8323, batch_loss: 0.3550, loss: 0.3782 ||:  63%|######3   | 41/65 [07:53<04:37, 11.54s/it]
2022-04-11 11:19:43,118 - INFO - tqdm - accuracy: 0.8333, batch_loss: 0.3777, loss: 0.3782 ||:  65%|######4   | 42/65 [08:04<04:25, 11.55s/it]
2022-04-11 11:19:54,649 - INFO - tqdm - accuracy: 0.8350, batch_loss: 0.3142, loss: 0.3767 ||:  66%|######6   | 43/65 [08:16<04:13, 11.54s/it]
2022-04-11 11:20:06,220 - INFO - tqdm - accuracy: 0.8338, batch_loss: 0.5229, loss: 0.3801 ||:  68%|######7   | 44/65 [08:27<04:02, 11.55s/it]
2022-04-11 11:20:17,796 - INFO - tqdm - accuracy: 0.8319, batch_loss: 0.4670, loss: 0.3820 ||:  69%|######9   | 45/65 [08:39<03:51, 11.56s/it]
2022-04-11 11:20:29,370 - INFO - tqdm - accuracy: 0.8302, batch_loss: 0.3893, loss: 0.3821 ||:  71%|#######   | 46/65 [08:51<03:39, 11.56s/it]
2022-04-11 11:20:40,949 - INFO - tqdm - accuracy: 0.8311, batch_loss: 0.3815, loss: 0.3821 ||:  72%|#######2  | 47/65 [09:02<03:28, 11.57s/it]
2022-04-11 11:20:52,542 - INFO - tqdm - accuracy: 0.8327, batch_loss: 0.1932, loss: 0.3782 ||:  74%|#######3  | 48/65 [09:14<03:16, 11.58s/it]
2022-04-11 11:21:04,130 - INFO - tqdm - accuracy: 0.8310, batch_loss: 0.4143, loss: 0.3789 ||:  75%|#######5  | 49/65 [09:25<03:05, 11.58s/it]
2022-04-11 11:21:15,702 - INFO - tqdm - accuracy: 0.8300, batch_loss: 0.4395, loss: 0.3801 ||:  77%|#######6  | 50/65 [09:37<02:53, 11.58s/it]
2022-04-11 11:21:27,256 - INFO - tqdm - accuracy: 0.8284, batch_loss: 0.5016, loss: 0.3825 ||:  78%|#######8  | 51/65 [09:48<02:41, 11.57s/it]
2022-04-11 11:21:38,568 - INFO - tqdm - accuracy: 0.8269, batch_loss: 0.4981, loss: 0.3848 ||:  80%|########  | 52/65 [10:00<02:29, 11.49s/it]
2022-04-11 11:21:50,141 - INFO - tqdm - accuracy: 0.8255, batch_loss: 0.4193, loss: 0.3854 ||:  82%|########1 | 53/65 [10:11<02:18, 11.52s/it]
2022-04-11 11:22:01,732 - INFO - tqdm - accuracy: 0.8247, batch_loss: 0.4927, loss: 0.3874 ||:  83%|########3 | 54/65 [10:23<02:06, 11.54s/it]
2022-04-11 11:22:13,307 - INFO - tqdm - accuracy: 0.8227, batch_loss: 0.4432, loss: 0.3884 ||:  85%|########4 | 55/65 [10:35<01:55, 11.55s/it]
2022-04-11 11:22:24,893 - INFO - tqdm - accuracy: 0.8231, batch_loss: 0.4197, loss: 0.3890 ||:  86%|########6 | 56/65 [10:46<01:44, 11.56s/it]
2022-04-11 11:22:36,438 - INFO - tqdm - accuracy: 0.8229, batch_loss: 0.4244, loss: 0.3896 ||:  88%|########7 | 57/65 [10:58<01:32, 11.56s/it]
2022-04-11 11:22:47,996 - INFO - tqdm - accuracy: 0.8244, batch_loss: 0.2627, loss: 0.3874 ||:  89%|########9 | 58/65 [11:09<01:20, 11.56s/it]
2022-04-11 11:22:59,173 - INFO - tqdm - accuracy: 0.8241, batch_loss: 0.3706, loss: 0.3871 ||:  91%|######### | 59/65 [11:20<01:08, 11.44s/it]
2022-04-11 11:23:10,698 - INFO - tqdm - accuracy: 0.8249, batch_loss: 0.3010, loss: 0.3857 ||:  92%|#########2| 60/65 [11:32<00:57, 11.47s/it]
2022-04-11 11:23:22,199 - INFO - tqdm - accuracy: 0.8257, batch_loss: 0.3081, loss: 0.3844 ||:  94%|#########3| 61/65 [11:43<00:45, 11.48s/it]
2022-04-11 11:23:33,675 - INFO - tqdm - accuracy: 0.8260, batch_loss: 0.3685, loss: 0.3841 ||:  95%|#########5| 62/65 [11:55<00:34, 11.48s/it]
2022-04-11 11:23:45,171 - INFO - tqdm - accuracy: 0.8253, batch_loss: 0.3590, loss: 0.3837 ||:  97%|#########6| 63/65 [12:06<00:22, 11.48s/it]
2022-04-11 11:23:56,669 - INFO - tqdm - accuracy: 0.8251, batch_loss: 0.3317, loss: 0.3829 ||:  98%|#########8| 64/65 [12:18<00:11, 11.49s/it]
2022-04-11 11:24:01,798 - INFO - tqdm - accuracy: 0.8248, batch_loss: 0.3637, loss: 0.3826 ||: 100%|##########| 65/65 [12:23<00:00,  9.58s/it]
2022-04-11 11:24:01,802 - INFO - tqdm - accuracy: 0.8248, batch_loss: 0.3637, loss: 0.3826 ||: 100%|##########| 65/65 [12:23<00:00, 11.44s/it]
2022-04-11 11:24:04,719 - INFO - allennlp.training.trainer - Validating
2022-04-11 11:24:04,720 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-11 11:24:14,828 - INFO - tqdm - accuracy: 0.6125, batch_loss: 0.0201, loss: 1.1392 ||:  35%|###4      | 40/115 [00:10<00:19,  3.95it/s]
2022-04-11 11:24:24,948 - INFO - tqdm - accuracy: 0.6562, batch_loss: 0.4203, loss: 0.9786 ||:  70%|######9   | 80/115 [00:20<00:09,  3.89it/s]
2022-04-11 11:24:33,823 - INFO - tqdm - accuracy: 0.6463, batch_loss: 0.0377, loss: 0.9372 ||: 100%|##########| 115/115 [00:29<00:00,  3.95it/s]
2022-04-11 11:24:33,823 - INFO - tqdm - accuracy: 0.6463, batch_loss: 0.0377, loss: 0.9372 ||: 100%|##########| 115/115 [00:29<00:00,  3.95it/s]
2022-04-11 11:24:33,824 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-11 11:24:33,824 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.825  |     0.646
2022-04-11 11:24:33,825 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-11 11:24:33,826 - INFO - allennlp.training.tensorboard_writer - loss               |     0.383  |     0.937
2022-04-11 11:24:33,826 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5802.672  |       N/A
2022-04-11 11:24:39,510 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to './output_ir-ora-d_hyper6/best.th'.
2022-04-11 11:24:56,018 - INFO - allennlp.training.trainer - Epoch duration: 0:13:17.743492
2022-04-11 11:24:56,019 - INFO - allennlp.training.trainer - Estimated training time remaining: 2:24:59
2022-04-11 11:24:56,019 - INFO - allennlp.training.trainer - Epoch 4/14
2022-04-11 11:24:56,019 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-11 11:24:56,019 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-11 11:24:56,021 - INFO - allennlp.training.trainer - Training
2022-04-11 11:24:56,021 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-11 11:25:07,234 - INFO - tqdm - accuracy: 0.9062, batch_loss: 0.2470, loss: 0.2470 ||:   2%|1         | 1/65 [00:11<11:57, 11.21s/it]
2022-04-11 11:25:18,775 - INFO - tqdm - accuracy: 0.9062, batch_loss: 0.1639, loss: 0.2054 ||:   3%|3         | 2/65 [00:22<11:58, 11.41s/it]
2022-04-11 11:25:30,642 - INFO - tqdm - accuracy: 0.8958, batch_loss: 0.3028, loss: 0.2379 ||:   5%|4         | 3/65 [00:34<12:00, 11.62s/it]
2022-04-11 11:25:42,479 - INFO - tqdm - accuracy: 0.9062, batch_loss: 0.2325, loss: 0.2365 ||:   6%|6         | 4/65 [00:46<11:53, 11.70s/it]
2022-04-11 11:25:53,945 - INFO - tqdm - accuracy: 0.9062, batch_loss: 0.2270, loss: 0.2346 ||:   8%|7         | 5/65 [00:57<11:37, 11.62s/it]
2022-04-11 11:26:05,315 - INFO - tqdm - accuracy: 0.9219, batch_loss: 0.1207, loss: 0.2156 ||:   9%|9         | 6/65 [01:09<11:20, 11.53s/it]
2022-04-11 11:26:16,739 - INFO - tqdm - accuracy: 0.9241, batch_loss: 0.1762, loss: 0.2100 ||:  11%|#         | 7/65 [01:20<11:06, 11.50s/it]
2022-04-11 11:26:28,261 - INFO - tqdm - accuracy: 0.9297, batch_loss: 0.1228, loss: 0.1991 ||:  12%|#2        | 8/65 [01:32<10:55, 11.51s/it]
2022-04-11 11:26:39,869 - INFO - tqdm - accuracy: 0.9271, batch_loss: 0.1989, loss: 0.1991 ||:  14%|#3        | 9/65 [01:43<10:46, 11.54s/it]
2022-04-11 11:26:51,470 - INFO - tqdm - accuracy: 0.9250, batch_loss: 0.1540, loss: 0.1946 ||:  15%|#5        | 10/65 [01:55<10:35, 11.56s/it]
2022-04-11 11:27:02,998 - INFO - tqdm - accuracy: 0.9205, batch_loss: 0.2453, loss: 0.1992 ||:  17%|#6        | 11/65 [02:06<10:23, 11.55s/it]
2022-04-11 11:27:14,424 - INFO - tqdm - accuracy: 0.9245, batch_loss: 0.1156, loss: 0.1922 ||:  18%|#8        | 12/65 [02:18<10:10, 11.51s/it]
2022-04-11 11:27:25,905 - INFO - tqdm - accuracy: 0.9231, batch_loss: 0.2276, loss: 0.1949 ||:  20%|##        | 13/65 [02:29<09:58, 11.50s/it]
2022-04-11 11:27:37,382 - INFO - tqdm - accuracy: 0.9286, batch_loss: 0.0589, loss: 0.1852 ||:  22%|##1       | 14/65 [02:41<09:46, 11.49s/it]
2022-04-11 11:27:48,874 - INFO - tqdm - accuracy: 0.9250, batch_loss: 0.2454, loss: 0.1892 ||:  23%|##3       | 15/65 [02:52<09:34, 11.49s/it]
2022-04-11 11:28:00,376 - INFO - tqdm - accuracy: 0.9277, batch_loss: 0.0996, loss: 0.1836 ||:  25%|##4       | 16/65 [03:04<09:23, 11.50s/it]
2022-04-11 11:28:11,888 - INFO - tqdm - accuracy: 0.9283, batch_loss: 0.1639, loss: 0.1825 ||:  26%|##6       | 17/65 [03:15<09:12, 11.50s/it]
2022-04-11 11:28:23,434 - INFO - tqdm - accuracy: 0.9253, batch_loss: 0.1897, loss: 0.1829 ||:  28%|##7       | 18/65 [03:27<09:01, 11.51s/it]
2022-04-11 11:28:34,973 - INFO - tqdm - accuracy: 0.9276, batch_loss: 0.0926, loss: 0.1781 ||:  29%|##9       | 19/65 [03:38<08:50, 11.52s/it]
2022-04-11 11:28:46,509 - INFO - tqdm - accuracy: 0.9250, batch_loss: 0.2948, loss: 0.1840 ||:  31%|###       | 20/65 [03:50<08:38, 11.53s/it]
2022-04-11 11:28:58,035 - INFO - tqdm - accuracy: 0.9256, batch_loss: 0.1411, loss: 0.1819 ||:  32%|###2      | 21/65 [04:02<08:27, 11.53s/it]
2022-04-11 11:29:09,571 - INFO - tqdm - accuracy: 0.9276, batch_loss: 0.1590, loss: 0.1809 ||:  34%|###3      | 22/65 [04:13<08:15, 11.53s/it]
2022-04-11 11:29:21,094 - INFO - tqdm - accuracy: 0.9212, batch_loss: 0.3956, loss: 0.1902 ||:  35%|###5      | 23/65 [04:25<08:04, 11.53s/it]
2022-04-11 11:29:32,618 - INFO - tqdm - accuracy: 0.9206, batch_loss: 0.1870, loss: 0.1901 ||:  37%|###6      | 24/65 [04:36<07:52, 11.53s/it]
2022-04-11 11:29:44,153 - INFO - tqdm - accuracy: 0.9150, batch_loss: 0.3294, loss: 0.1956 ||:  38%|###8      | 25/65 [04:48<07:41, 11.53s/it]
2022-04-11 11:29:55,686 - INFO - tqdm - accuracy: 0.9159, batch_loss: 0.1502, loss: 0.1939 ||:  40%|####      | 26/65 [04:59<07:29, 11.53s/it]
2022-04-11 11:30:07,252 - INFO - tqdm - accuracy: 0.9178, batch_loss: 0.0967, loss: 0.1903 ||:  42%|####1     | 27/65 [05:11<07:18, 11.54s/it]
2022-04-11 11:30:18,807 - INFO - tqdm - accuracy: 0.9163, batch_loss: 0.5053, loss: 0.2016 ||:  43%|####3     | 28/65 [05:22<07:07, 11.55s/it]
2022-04-11 11:30:30,353 - INFO - tqdm - accuracy: 0.9149, batch_loss: 0.2635, loss: 0.2037 ||:  45%|####4     | 29/65 [05:34<06:55, 11.55s/it]
2022-04-11 11:30:41,795 - INFO - tqdm - accuracy: 0.9146, batch_loss: 0.1828, loss: 0.2030 ||:  46%|####6     | 30/65 [05:45<06:42, 11.51s/it]
2022-04-11 11:30:53,351 - INFO - tqdm - accuracy: 0.9153, batch_loss: 0.0966, loss: 0.1996 ||:  48%|####7     | 31/65 [05:57<06:31, 11.53s/it]
2022-04-11 11:31:04,900 - INFO - tqdm - accuracy: 0.9170, batch_loss: 0.1783, loss: 0.1989 ||:  49%|####9     | 32/65 [06:08<06:20, 11.53s/it]
2022-04-11 11:31:16,441 - INFO - tqdm - accuracy: 0.9167, batch_loss: 0.2399, loss: 0.2001 ||:  51%|#####     | 33/65 [06:20<06:09, 11.54s/it]
2022-04-11 11:31:27,987 - INFO - tqdm - accuracy: 0.9164, batch_loss: 0.3277, loss: 0.2039 ||:  52%|#####2    | 34/65 [06:31<05:57, 11.54s/it]
2022-04-11 11:31:39,537 - INFO - tqdm - accuracy: 0.9161, batch_loss: 0.2772, loss: 0.2060 ||:  54%|#####3    | 35/65 [06:43<05:46, 11.54s/it]
2022-04-11 11:31:51,099 - INFO - tqdm - accuracy: 0.9167, batch_loss: 0.1949, loss: 0.2057 ||:  55%|#####5    | 36/65 [06:55<05:34, 11.55s/it]
2022-04-11 11:32:02,317 - INFO - tqdm - accuracy: 0.9163, batch_loss: 0.1711, loss: 0.2047 ||:  57%|#####6    | 37/65 [07:06<05:20, 11.45s/it]
2022-04-11 11:32:13,895 - INFO - tqdm - accuracy: 0.9177, batch_loss: 0.0664, loss: 0.2011 ||:  58%|#####8    | 38/65 [07:17<05:10, 11.49s/it]
2022-04-11 11:32:25,466 - INFO - tqdm - accuracy: 0.9174, batch_loss: 0.1708, loss: 0.2003 ||:  60%|######    | 39/65 [07:29<04:59, 11.51s/it]
2022-04-11 11:32:37,464 - INFO - tqdm - accuracy: 0.9163, batch_loss: 0.2175, loss: 0.2008 ||:  62%|######1   | 40/65 [07:41<04:51, 11.66s/it]
2022-04-11 11:32:48,982 - INFO - tqdm - accuracy: 0.9169, batch_loss: 0.1577, loss: 0.1997 ||:  63%|######3   | 41/65 [07:52<04:38, 11.62s/it]
2022-04-11 11:33:00,539 - INFO - tqdm - accuracy: 0.9151, batch_loss: 0.3184, loss: 0.2025 ||:  65%|######4   | 42/65 [08:04<04:26, 11.60s/it]
2022-04-11 11:33:12,095 - INFO - tqdm - accuracy: 0.9164, batch_loss: 0.1342, loss: 0.2009 ||:  66%|######6   | 43/65 [08:16<04:14, 11.59s/it]
2022-04-11 11:33:23,646 - INFO - tqdm - accuracy: 0.9154, batch_loss: 0.2746, loss: 0.2026 ||:  68%|######7   | 44/65 [08:27<04:03, 11.58s/it]
2022-04-11 11:33:35,058 - INFO - tqdm - accuracy: 0.9166, batch_loss: 0.1455, loss: 0.2013 ||:  69%|######9   | 45/65 [08:39<03:50, 11.53s/it]
2022-04-11 11:33:46,614 - INFO - tqdm - accuracy: 0.9150, batch_loss: 0.4218, loss: 0.2061 ||:  71%|#######   | 46/65 [08:50<03:39, 11.54s/it]
2022-04-11 11:33:58,160 - INFO - tqdm - accuracy: 0.9148, batch_loss: 0.1610, loss: 0.2052 ||:  72%|#######2  | 47/65 [09:02<03:27, 11.54s/it]
2022-04-11 11:34:09,708 - INFO - tqdm - accuracy: 0.9160, batch_loss: 0.1482, loss: 0.2040 ||:  74%|#######3  | 48/65 [09:13<03:16, 11.54s/it]
2022-04-11 11:34:21,282 - INFO - tqdm - accuracy: 0.9170, batch_loss: 0.0904, loss: 0.2017 ||:  75%|#######5  | 49/65 [09:25<03:04, 11.55s/it]
2022-04-11 11:34:32,837 - INFO - tqdm - accuracy: 0.9162, batch_loss: 0.3039, loss: 0.2037 ||:  77%|#######6  | 50/65 [09:36<02:53, 11.55s/it]
2022-04-11 11:34:44,406 - INFO - tqdm - accuracy: 0.9166, batch_loss: 0.1385, loss: 0.2024 ||:  78%|#######8  | 51/65 [09:48<02:41, 11.56s/it]
2022-04-11 11:34:55,981 - INFO - tqdm - accuracy: 0.9170, batch_loss: 0.1745, loss: 0.2019 ||:  80%|########  | 52/65 [09:59<02:30, 11.56s/it]
2022-04-11 11:35:07,553 - INFO - tqdm - accuracy: 0.9174, batch_loss: 0.2553, loss: 0.2029 ||:  82%|########1 | 53/65 [10:11<02:18, 11.57s/it]
2022-04-11 11:35:19,117 - INFO - tqdm - accuracy: 0.9166, batch_loss: 0.3532, loss: 0.2057 ||:  83%|########3 | 54/65 [10:23<02:07, 11.56s/it]
2022-04-11 11:35:30,670 - INFO - tqdm - accuracy: 0.9164, batch_loss: 0.2112, loss: 0.2058 ||:  85%|########4 | 55/65 [10:34<01:55, 11.56s/it]
2022-04-11 11:35:42,218 - INFO - tqdm - accuracy: 0.9174, batch_loss: 0.1283, loss: 0.2044 ||:  86%|########6 | 56/65 [10:46<01:44, 11.56s/it]
2022-04-11 11:35:53,762 - INFO - tqdm - accuracy: 0.9155, batch_loss: 0.3663, loss: 0.2072 ||:  88%|########7 | 57/65 [10:57<01:32, 11.55s/it]
2022-04-11 11:36:05,329 - INFO - tqdm - accuracy: 0.9143, batch_loss: 0.1845, loss: 0.2069 ||:  89%|########9 | 58/65 [11:09<01:20, 11.56s/it]
2022-04-11 11:36:16,885 - INFO - tqdm - accuracy: 0.9152, batch_loss: 0.0992, loss: 0.2050 ||:  91%|######### | 59/65 [11:20<01:09, 11.56s/it]
2022-04-11 11:36:28,462 - INFO - tqdm - accuracy: 0.9145, batch_loss: 0.2524, loss: 0.2058 ||:  92%|#########2| 60/65 [11:32<00:57, 11.56s/it]
2022-04-11 11:36:40,041 - INFO - tqdm - accuracy: 0.9144, batch_loss: 0.2415, loss: 0.2064 ||:  94%|#########3| 61/65 [11:44<00:46, 11.57s/it]
2022-04-11 11:36:51,366 - INFO - tqdm - accuracy: 0.9153, batch_loss: 0.1017, loss: 0.2047 ||:  95%|#########5| 62/65 [11:55<00:34, 11.49s/it]
2022-04-11 11:37:02,935 - INFO - tqdm - accuracy: 0.9161, batch_loss: 0.1277, loss: 0.2035 ||:  97%|#########6| 63/65 [12:06<00:23, 11.52s/it]
2022-04-11 11:37:14,478 - INFO - tqdm - accuracy: 0.9155, batch_loss: 0.2201, loss: 0.2038 ||:  98%|#########8| 64/65 [12:18<00:11, 11.52s/it]
2022-04-11 11:37:19,602 - INFO - tqdm - accuracy: 0.9156, batch_loss: 0.2095, loss: 0.2038 ||: 100%|##########| 65/65 [12:23<00:00,  9.60s/it]
2022-04-11 11:37:19,602 - INFO - tqdm - accuracy: 0.9156, batch_loss: 0.2095, loss: 0.2038 ||: 100%|##########| 65/65 [12:23<00:00, 11.44s/it]
2022-04-11 11:37:22,500 - INFO - allennlp.training.trainer - Validating
2022-04-11 11:37:22,502 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-11 11:37:32,649 - INFO - tqdm - accuracy: 0.6250, batch_loss: 0.0014, loss: 1.2310 ||:  35%|###4      | 40/115 [00:10<00:19,  3.92it/s]
2022-04-11 11:37:42,863 - INFO - tqdm - accuracy: 0.6312, batch_loss: 0.0039, loss: 1.2623 ||:  70%|######9   | 80/115 [00:20<00:08,  3.91it/s]
2022-04-11 11:37:51,612 - INFO - tqdm - accuracy: 0.6332, batch_loss: 0.0069, loss: 1.2666 ||: 100%|##########| 115/115 [00:29<00:00,  3.90it/s]
2022-04-11 11:37:51,613 - INFO - tqdm - accuracy: 0.6332, batch_loss: 0.0069, loss: 1.2666 ||: 100%|##########| 115/115 [00:29<00:00,  3.95it/s]
2022-04-11 11:37:51,613 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-11 11:37:51,613 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.916  |     0.633
2022-04-11 11:37:51,614 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-11 11:37:51,615 - INFO - allennlp.training.tensorboard_writer - loss               |     0.204  |     1.267
2022-04-11 11:37:51,616 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5802.672  |       N/A
2022-04-11 11:37:57,804 - INFO - allennlp.training.trainer - Epoch duration: 0:13:01.785478
2022-04-11 11:37:57,805 - INFO - allennlp.training.trainer - Estimated training time remaining: 2:11:30
2022-04-11 11:37:57,805 - INFO - allennlp.training.trainer - Epoch 5/14
2022-04-11 11:37:57,805 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-11 11:37:57,805 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-11 11:37:57,807 - INFO - allennlp.training.trainer - Training
2022-04-11 11:37:57,807 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-11 11:38:09,120 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.0942, loss: 0.0942 ||:   2%|1         | 1/65 [00:11<12:04, 11.31s/it]
2022-04-11 11:38:20,822 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.0816, loss: 0.0879 ||:   3%|3         | 2/65 [00:23<12:07, 11.54s/it]
2022-04-11 11:38:32,660 - INFO - tqdm - accuracy: 0.9583, batch_loss: 0.1655, loss: 0.1137 ||:   5%|4         | 3/65 [00:34<12:03, 11.68s/it]
2022-04-11 11:38:44,167 - INFO - tqdm - accuracy: 0.9609, batch_loss: 0.0992, loss: 0.1101 ||:   6%|6         | 4/65 [00:46<11:48, 11.61s/it]
2022-04-11 11:38:55,590 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.0863, loss: 0.1054 ||:   8%|7         | 5/65 [00:57<11:32, 11.54s/it]
2022-04-11 11:39:07,034 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.0974, loss: 0.1040 ||:   9%|9         | 6/65 [01:09<11:19, 11.51s/it]
2022-04-11 11:39:18,423 - INFO - tqdm - accuracy: 0.9732, batch_loss: 0.0476, loss: 0.0960 ||:  11%|#         | 7/65 [01:20<11:05, 11.47s/it]
2022-04-11 11:39:30,017 - INFO - tqdm - accuracy: 0.9727, batch_loss: 0.0810, loss: 0.0941 ||:  12%|#2        | 8/65 [01:32<10:56, 11.51s/it]
2022-04-11 11:39:41,638 - INFO - tqdm - accuracy: 0.9757, batch_loss: 0.0620, loss: 0.0905 ||:  14%|#3        | 9/65 [01:43<10:46, 11.54s/it]
2022-04-11 11:39:53,206 - INFO - tqdm - accuracy: 0.9781, batch_loss: 0.0506, loss: 0.0865 ||:  15%|#5        | 10/65 [01:55<10:35, 11.55s/it]
2022-04-11 11:40:04,740 - INFO - tqdm - accuracy: 0.9744, batch_loss: 0.1402, loss: 0.0914 ||:  17%|#6        | 11/65 [02:06<10:23, 11.55s/it]
2022-04-11 11:40:16,245 - INFO - tqdm - accuracy: 0.9740, batch_loss: 0.0628, loss: 0.0890 ||:  18%|#8        | 12/65 [02:18<10:11, 11.53s/it]
2022-04-11 11:40:27,729 - INFO - tqdm - accuracy: 0.9712, batch_loss: 0.1036, loss: 0.0902 ||:  20%|##        | 13/65 [02:29<09:58, 11.52s/it]
2022-04-11 11:40:39,173 - INFO - tqdm - accuracy: 0.9643, batch_loss: 0.3743, loss: 0.1105 ||:  22%|##1       | 14/65 [02:41<09:46, 11.50s/it]
2022-04-11 11:40:50,652 - INFO - tqdm - accuracy: 0.9667, batch_loss: 0.0236, loss: 0.1047 ||:  23%|##3       | 15/65 [02:52<09:34, 11.49s/it]
2022-04-11 11:41:01,839 - INFO - tqdm - accuracy: 0.9667, batch_loss: 0.0692, loss: 0.1024 ||:  25%|##4       | 16/65 [03:04<09:18, 11.40s/it]
2022-04-11 11:41:13,386 - INFO - tqdm - accuracy: 0.9687, batch_loss: 0.0265, loss: 0.0980 ||:  26%|##6       | 17/65 [03:15<09:09, 11.44s/it]
2022-04-11 11:41:24,946 - INFO - tqdm - accuracy: 0.9687, batch_loss: 0.1858, loss: 0.1029 ||:  28%|##7       | 18/65 [03:27<08:59, 11.48s/it]
2022-04-11 11:41:36,525 - INFO - tqdm - accuracy: 0.9621, batch_loss: 0.2338, loss: 0.1097 ||:  29%|##9       | 19/65 [03:38<08:49, 11.51s/it]
2022-04-11 11:41:48,082 - INFO - tqdm - accuracy: 0.9624, batch_loss: 0.0457, loss: 0.1065 ||:  31%|###       | 20/65 [03:50<08:38, 11.52s/it]
2022-04-11 11:41:59,622 - INFO - tqdm - accuracy: 0.9613, batch_loss: 0.1854, loss: 0.1103 ||:  32%|###2      | 21/65 [04:01<08:27, 11.53s/it]
2022-04-11 11:42:11,189 - INFO - tqdm - accuracy: 0.9616, batch_loss: 0.1520, loss: 0.1122 ||:  34%|###3      | 22/65 [04:13<08:16, 11.54s/it]
2022-04-11 11:42:22,761 - INFO - tqdm - accuracy: 0.9565, batch_loss: 0.4154, loss: 0.1254 ||:  35%|###5      | 23/65 [04:24<08:05, 11.55s/it]
2022-04-11 11:42:34,317 - INFO - tqdm - accuracy: 0.9557, batch_loss: 0.1351, loss: 0.1258 ||:  37%|###6      | 24/65 [04:36<07:53, 11.55s/it]
2022-04-11 11:42:45,875 - INFO - tqdm - accuracy: 0.9549, batch_loss: 0.1693, loss: 0.1275 ||:  38%|###8      | 25/65 [04:48<07:42, 11.55s/it]
2022-04-11 11:42:57,417 - INFO - tqdm - accuracy: 0.9555, batch_loss: 0.0923, loss: 0.1262 ||:  40%|####      | 26/65 [04:59<07:30, 11.55s/it]
2022-04-11 11:43:08,952 - INFO - tqdm - accuracy: 0.9560, batch_loss: 0.1079, loss: 0.1255 ||:  42%|####1     | 27/65 [05:11<07:18, 11.55s/it]
2022-04-11 11:43:20,506 - INFO - tqdm - accuracy: 0.9542, batch_loss: 0.0986, loss: 0.1245 ||:  43%|####3     | 28/65 [05:22<07:07, 11.55s/it]
2022-04-11 11:43:32,051 - INFO - tqdm - accuracy: 0.9558, batch_loss: 0.0268, loss: 0.1212 ||:  45%|####4     | 29/65 [05:34<06:55, 11.55s/it]
2022-04-11 11:43:43,572 - INFO - tqdm - accuracy: 0.9541, batch_loss: 0.1207, loss: 0.1211 ||:  46%|####6     | 30/65 [05:45<06:43, 11.54s/it]
2022-04-11 11:43:55,104 - INFO - tqdm - accuracy: 0.9556, batch_loss: 0.0267, loss: 0.1181 ||:  48%|####7     | 31/65 [05:57<06:32, 11.54s/it]
2022-04-11 11:44:06,630 - INFO - tqdm - accuracy: 0.9570, batch_loss: 0.0458, loss: 0.1158 ||:  49%|####9     | 32/65 [06:08<06:20, 11.53s/it]
2022-04-11 11:44:18,140 - INFO - tqdm - accuracy: 0.9583, batch_loss: 0.0632, loss: 0.1142 ||:  51%|#####     | 33/65 [06:20<06:08, 11.53s/it]
2022-04-11 11:44:29,661 - INFO - tqdm - accuracy: 0.9595, batch_loss: 0.0613, loss: 0.1127 ||:  52%|#####2    | 34/65 [06:31<05:57, 11.52s/it]
2022-04-11 11:44:41,188 - INFO - tqdm - accuracy: 0.9598, batch_loss: 0.0991, loss: 0.1123 ||:  54%|#####3    | 35/65 [06:43<05:45, 11.53s/it]
2022-04-11 11:44:52,703 - INFO - tqdm - accuracy: 0.9600, batch_loss: 0.0739, loss: 0.1112 ||:  55%|#####5    | 36/65 [06:54<05:34, 11.52s/it]
2022-04-11 11:45:04,212 - INFO - tqdm - accuracy: 0.9611, batch_loss: 0.0589, loss: 0.1098 ||:  57%|#####6    | 37/65 [07:06<05:22, 11.52s/it]
2022-04-11 11:45:15,704 - INFO - tqdm - accuracy: 0.9613, batch_loss: 0.0618, loss: 0.1086 ||:  58%|#####8    | 38/65 [07:17<05:10, 11.51s/it]
2022-04-11 11:45:27,221 - INFO - tqdm - accuracy: 0.9607, batch_loss: 0.1415, loss: 0.1094 ||:  60%|######    | 39/65 [07:29<04:59, 11.51s/it]
2022-04-11 11:45:38,724 - INFO - tqdm - accuracy: 0.9609, batch_loss: 0.1560, loss: 0.1106 ||:  62%|######1   | 40/65 [07:40<04:47, 11.51s/it]
2022-04-11 11:45:50,228 - INFO - tqdm - accuracy: 0.9619, batch_loss: 0.0312, loss: 0.1086 ||:  63%|######3   | 41/65 [07:52<04:36, 11.51s/it]
2022-04-11 11:46:01,749 - INFO - tqdm - accuracy: 0.9613, batch_loss: 0.1021, loss: 0.1085 ||:  65%|######4   | 42/65 [08:03<04:24, 11.51s/it]
2022-04-11 11:46:13,248 - INFO - tqdm - accuracy: 0.9615, batch_loss: 0.1397, loss: 0.1092 ||:  66%|######6   | 43/65 [08:15<04:13, 11.51s/it]
2022-04-11 11:46:24,755 - INFO - tqdm - accuracy: 0.9595, batch_loss: 0.2185, loss: 0.1117 ||:  68%|######7   | 44/65 [08:26<04:01, 11.51s/it]
2022-04-11 11:46:36,008 - INFO - tqdm - accuracy: 0.9597, batch_loss: 0.0625, loss: 0.1106 ||:  69%|######9   | 45/65 [08:38<03:48, 11.43s/it]
2022-04-11 11:46:47,530 - INFO - tqdm - accuracy: 0.9585, batch_loss: 0.1862, loss: 0.1122 ||:  71%|#######   | 46/65 [08:49<03:37, 11.46s/it]
2022-04-11 11:46:59,063 - INFO - tqdm - accuracy: 0.9581, batch_loss: 0.1097, loss: 0.1122 ||:  72%|#######2  | 47/65 [09:01<03:26, 11.48s/it]
2022-04-11 11:47:10,592 - INFO - tqdm - accuracy: 0.9577, batch_loss: 0.1382, loss: 0.1127 ||:  74%|#######3  | 48/65 [09:12<03:15, 11.50s/it]
2022-04-11 11:47:22,130 - INFO - tqdm - accuracy: 0.9579, batch_loss: 0.0360, loss: 0.1112 ||:  75%|#######5  | 49/65 [09:24<03:04, 11.51s/it]
2022-04-11 11:47:33,672 - INFO - tqdm - accuracy: 0.9581, batch_loss: 0.0680, loss: 0.1103 ||:  77%|#######6  | 50/65 [09:35<02:52, 11.52s/it]
2022-04-11 11:47:45,230 - INFO - tqdm - accuracy: 0.9583, batch_loss: 0.0682, loss: 0.1095 ||:  78%|#######8  | 51/65 [09:47<02:41, 11.53s/it]
2022-04-11 11:47:56,780 - INFO - tqdm - accuracy: 0.9573, batch_loss: 0.1390, loss: 0.1100 ||:  80%|########  | 52/65 [09:58<02:29, 11.54s/it]
2022-04-11 11:48:08,307 - INFO - tqdm - accuracy: 0.9581, batch_loss: 0.0189, loss: 0.1083 ||:  82%|########1 | 53/65 [10:10<02:18, 11.53s/it]
2022-04-11 11:48:19,836 - INFO - tqdm - accuracy: 0.9583, batch_loss: 0.0761, loss: 0.1077 ||:  83%|########3 | 54/65 [10:22<02:06, 11.53s/it]
2022-04-11 11:48:31,373 - INFO - tqdm - accuracy: 0.9585, batch_loss: 0.1031, loss: 0.1076 ||:  85%|########4 | 55/65 [10:33<01:55, 11.53s/it]
2022-04-11 11:48:42,911 - INFO - tqdm - accuracy: 0.9587, batch_loss: 0.0857, loss: 0.1072 ||:  86%|########6 | 56/65 [10:45<01:43, 11.53s/it]
2022-04-11 11:48:54,424 - INFO - tqdm - accuracy: 0.9589, batch_loss: 0.0732, loss: 0.1066 ||:  88%|########7 | 57/65 [10:56<01:32, 11.53s/it]
2022-04-11 11:49:05,945 - INFO - tqdm - accuracy: 0.9580, batch_loss: 0.1495, loss: 0.1074 ||:  89%|########9 | 58/65 [11:08<01:20, 11.53s/it]
2022-04-11 11:49:17,326 - INFO - tqdm - accuracy: 0.9576, batch_loss: 0.1359, loss: 0.1079 ||:  91%|######### | 59/65 [11:19<01:08, 11.48s/it]
2022-04-11 11:49:28,831 - INFO - tqdm - accuracy: 0.9573, batch_loss: 0.1109, loss: 0.1079 ||:  92%|#########2| 60/65 [11:31<00:57, 11.49s/it]
2022-04-11 11:49:40,343 - INFO - tqdm - accuracy: 0.9575, batch_loss: 0.0887, loss: 0.1076 ||:  94%|#########3| 61/65 [11:42<00:45, 11.50s/it]
2022-04-11 11:49:51,840 - INFO - tqdm - accuracy: 0.9571, batch_loss: 0.0948, loss: 0.1074 ||:  95%|#########5| 62/65 [11:54<00:34, 11.50s/it]
2022-04-11 11:50:03,357 - INFO - tqdm - accuracy: 0.9558, batch_loss: 0.2405, loss: 0.1095 ||:  97%|#########6| 63/65 [12:05<00:23, 11.50s/it]
2022-04-11 11:50:14,886 - INFO - tqdm - accuracy: 0.9551, batch_loss: 0.1841, loss: 0.1107 ||:  98%|#########8| 64/65 [12:17<00:11, 11.51s/it]
2022-04-11 11:50:20,011 - INFO - tqdm - accuracy: 0.9549, batch_loss: 0.2137, loss: 0.1123 ||: 100%|##########| 65/65 [12:22<00:00,  9.59s/it]
2022-04-11 11:50:20,011 - INFO - tqdm - accuracy: 0.9549, batch_loss: 0.2137, loss: 0.1123 ||: 100%|##########| 65/65 [12:22<00:00, 11.42s/it]
2022-04-11 11:50:22,868 - INFO - allennlp.training.trainer - Validating
2022-04-11 11:50:22,870 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-11 11:50:33,019 - INFO - tqdm - accuracy: 0.5750, batch_loss: 7.0590, loss: 1.8446 ||:  35%|###4      | 40/115 [00:10<00:19,  3.93it/s]
2022-04-11 11:50:43,029 - INFO - tqdm - accuracy: 0.6415, batch_loss: 0.1598, loss: 1.5265 ||:  70%|######9   | 80/115 [00:20<00:08,  4.11it/s]
2022-04-11 11:50:52,001 - INFO - tqdm - accuracy: 0.6245, batch_loss: 0.0033, loss: 1.5981 ||: 100%|##########| 115/115 [00:29<00:00,  3.90it/s]
2022-04-11 11:50:52,001 - INFO - tqdm - accuracy: 0.6245, batch_loss: 0.0033, loss: 1.5981 ||: 100%|##########| 115/115 [00:29<00:00,  3.95it/s]
2022-04-11 11:50:52,001 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-11 11:50:52,002 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.955  |     0.624
2022-04-11 11:50:52,003 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-11 11:50:52,004 - INFO - allennlp.training.tensorboard_writer - loss               |     0.112  |     1.598
2022-04-11 11:50:52,004 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5802.672  |       N/A
2022-04-11 11:50:58,151 - INFO - allennlp.training.trainer - Epoch duration: 0:13:00.346139
2022-04-11 11:50:58,151 - INFO - allennlp.training.trainer - Estimated training time remaining: 1:58:08
2022-04-11 11:50:58,151 - INFO - allennlp.training.trainer - Epoch 6/14
2022-04-11 11:50:58,151 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-11 11:50:58,152 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-11 11:50:58,153 - INFO - allennlp.training.trainer - Training
2022-04-11 11:50:58,153 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-11 11:51:09,565 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.0570, loss: 0.0570 ||:   2%|1         | 1/65 [00:11<12:10, 11.41s/it]
2022-04-11 11:51:21,205 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.0460, loss: 0.0515 ||:   3%|3         | 2/65 [00:23<12:07, 11.55s/it]
2022-04-11 11:51:33,040 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.0801, loss: 0.0610 ||:   5%|4         | 3/65 [00:34<12:04, 11.68s/it]
2022-04-11 11:51:44,602 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.1295, loss: 0.0782 ||:   6%|6         | 4/65 [00:46<11:49, 11.63s/it]
2022-04-11 11:51:56,037 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.0995, loss: 0.0824 ||:   8%|7         | 5/65 [00:57<11:33, 11.56s/it]
2022-04-11 11:52:07,492 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.0702, loss: 0.0804 ||:   9%|9         | 6/65 [01:09<11:19, 11.53s/it]
2022-04-11 11:52:19,012 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.1281, loss: 0.0872 ||:  11%|#         | 7/65 [01:20<11:08, 11.52s/it]
2022-04-11 11:52:30,575 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.0506, loss: 0.0826 ||:  12%|#2        | 8/65 [01:32<10:57, 11.54s/it]
2022-04-11 11:52:42,169 - INFO - tqdm - accuracy: 0.9722, batch_loss: 0.0272, loss: 0.0765 ||:  14%|#3        | 9/65 [01:44<10:47, 11.55s/it]
2022-04-11 11:52:54,120 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.2892, loss: 0.0977 ||:  15%|#5        | 10/65 [01:55<10:42, 11.68s/it]
2022-04-11 11:53:05,606 - INFO - tqdm - accuracy: 0.9659, batch_loss: 0.1465, loss: 0.1022 ||:  17%|#6        | 11/65 [02:07<10:27, 11.62s/it]
2022-04-11 11:53:17,063 - INFO - tqdm - accuracy: 0.9661, batch_loss: 0.0996, loss: 0.1020 ||:  18%|#8        | 12/65 [02:18<10:13, 11.57s/it]
2022-04-11 11:53:28,560 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.0314, loss: 0.0965 ||:  20%|##        | 13/65 [02:30<10:00, 11.55s/it]
2022-04-11 11:53:40,076 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.1228, loss: 0.0984 ||:  22%|##1       | 14/65 [02:41<09:48, 11.54s/it]
2022-04-11 11:53:51,599 - INFO - tqdm - accuracy: 0.9667, batch_loss: 0.0772, loss: 0.0970 ||:  23%|##3       | 15/65 [02:53<09:36, 11.53s/it]
2022-04-11 11:54:03,107 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.0231, loss: 0.0924 ||:  25%|##4       | 16/65 [03:04<09:24, 11.53s/it]
2022-04-11 11:54:14,642 - INFO - tqdm - accuracy: 0.9706, batch_loss: 0.0376, loss: 0.0892 ||:  26%|##6       | 17/65 [03:16<09:13, 11.53s/it]
2022-04-11 11:54:26,163 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.1713, loss: 0.0937 ||:  28%|##7       | 18/65 [03:28<09:01, 11.53s/it]
2022-04-11 11:54:37,720 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.0551, loss: 0.0917 ||:  29%|##9       | 19/65 [03:39<08:50, 11.54s/it]
2022-04-11 11:54:49,267 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.1277, loss: 0.0935 ||:  31%|###       | 20/65 [03:51<08:39, 11.54s/it]
2022-04-11 11:55:00,791 - INFO - tqdm - accuracy: 0.9702, batch_loss: 0.0417, loss: 0.0910 ||:  32%|###2      | 21/65 [04:02<08:27, 11.53s/it]
2022-04-11 11:55:12,335 - INFO - tqdm - accuracy: 0.9716, batch_loss: 0.0265, loss: 0.0881 ||:  34%|###3      | 22/65 [04:14<08:16, 11.54s/it]
2022-04-11 11:55:23,882 - INFO - tqdm - accuracy: 0.9728, batch_loss: 0.0540, loss: 0.0866 ||:  35%|###5      | 23/65 [04:25<08:04, 11.54s/it]
2022-04-11 11:55:35,441 - INFO - tqdm - accuracy: 0.9714, batch_loss: 0.2654, loss: 0.0940 ||:  37%|###6      | 24/65 [04:37<07:53, 11.55s/it]
2022-04-11 11:55:46,989 - INFO - tqdm - accuracy: 0.9700, batch_loss: 0.0626, loss: 0.0928 ||:  38%|###8      | 25/65 [04:48<07:41, 11.55s/it]
2022-04-11 11:55:58,533 - INFO - tqdm - accuracy: 0.9712, batch_loss: 0.0249, loss: 0.0902 ||:  40%|####      | 26/65 [05:00<07:30, 11.55s/it]
2022-04-11 11:56:10,106 - INFO - tqdm - accuracy: 0.9711, batch_loss: 0.0952, loss: 0.0904 ||:  42%|####1     | 27/65 [05:11<07:19, 11.55s/it]
2022-04-11 11:56:21,660 - INFO - tqdm - accuracy: 0.9721, batch_loss: 0.0476, loss: 0.0888 ||:  43%|####3     | 28/65 [05:23<07:07, 11.55s/it]
2022-04-11 11:56:33,211 - INFO - tqdm - accuracy: 0.9731, batch_loss: 0.0203, loss: 0.0865 ||:  45%|####4     | 29/65 [05:35<06:55, 11.55s/it]
2022-04-11 11:56:44,657 - INFO - tqdm - accuracy: 0.9729, batch_loss: 0.0770, loss: 0.0862 ||:  46%|####6     | 30/65 [05:46<06:43, 11.52s/it]
2022-04-11 11:56:56,245 - INFO - tqdm - accuracy: 0.9728, batch_loss: 0.0443, loss: 0.0848 ||:  48%|####7     | 31/65 [05:58<06:32, 11.54s/it]
2022-04-11 11:57:07,828 - INFO - tqdm - accuracy: 0.9736, batch_loss: 0.0262, loss: 0.0830 ||:  49%|####9     | 32/65 [06:09<06:21, 11.55s/it]
2022-04-11 11:57:19,408 - INFO - tqdm - accuracy: 0.9735, batch_loss: 0.1176, loss: 0.0840 ||:  51%|#####     | 33/65 [06:21<06:09, 11.56s/it]
2022-04-11 11:57:30,974 - INFO - tqdm - accuracy: 0.9743, batch_loss: 0.0257, loss: 0.0823 ||:  52%|#####2    | 34/65 [06:32<05:58, 11.56s/it]
2022-04-11 11:57:42,567 - INFO - tqdm - accuracy: 0.9750, batch_loss: 0.0242, loss: 0.0807 ||:  54%|#####3    | 35/65 [06:44<05:47, 11.57s/it]
2022-04-11 11:57:54,164 - INFO - tqdm - accuracy: 0.9757, batch_loss: 0.0206, loss: 0.0790 ||:  55%|#####5    | 36/65 [06:56<05:35, 11.58s/it]
2022-04-11 11:58:05,757 - INFO - tqdm - accuracy: 0.9764, batch_loss: 0.0109, loss: 0.0771 ||:  57%|#####6    | 37/65 [07:07<05:24, 11.58s/it]
2022-04-11 11:58:17,346 - INFO - tqdm - accuracy: 0.9770, batch_loss: 0.0577, loss: 0.0766 ||:  58%|#####8    | 38/65 [07:19<05:12, 11.59s/it]
2022-04-11 11:58:28,930 - INFO - tqdm - accuracy: 0.9760, batch_loss: 0.1995, loss: 0.0798 ||:  60%|######    | 39/65 [07:30<05:01, 11.58s/it]
2022-04-11 11:58:40,517 - INFO - tqdm - accuracy: 0.9750, batch_loss: 0.2164, loss: 0.0832 ||:  62%|######1   | 40/65 [07:42<04:49, 11.59s/it]
2022-04-11 11:58:51,733 - INFO - tqdm - accuracy: 0.9756, batch_loss: 0.0240, loss: 0.0818 ||:  63%|######3   | 41/65 [07:53<04:35, 11.47s/it]
2022-04-11 11:59:03,317 - INFO - tqdm - accuracy: 0.9754, batch_loss: 0.0563, loss: 0.0811 ||:  65%|######4   | 42/65 [08:05<04:24, 11.51s/it]
2022-04-11 11:59:14,655 - INFO - tqdm - accuracy: 0.9745, batch_loss: 0.0737, loss: 0.0810 ||:  66%|######6   | 43/65 [08:16<04:12, 11.46s/it]
2022-04-11 11:59:26,224 - INFO - tqdm - accuracy: 0.9744, batch_loss: 0.0463, loss: 0.0802 ||:  68%|######7   | 44/65 [08:28<04:01, 11.49s/it]
2022-04-11 11:59:37,774 - INFO - tqdm - accuracy: 0.9736, batch_loss: 0.0907, loss: 0.0804 ||:  69%|######9   | 45/65 [08:39<03:50, 11.51s/it]
2022-04-11 11:59:49,298 - INFO - tqdm - accuracy: 0.9742, batch_loss: 0.0598, loss: 0.0800 ||:  71%|#######   | 46/65 [08:51<03:38, 11.51s/it]
2022-04-11 12:00:00,814 - INFO - tqdm - accuracy: 0.9747, batch_loss: 0.0159, loss: 0.0786 ||:  72%|#######2  | 47/65 [09:02<03:27, 11.51s/it]
2022-04-11 12:00:12,316 - INFO - tqdm - accuracy: 0.9746, batch_loss: 0.0493, loss: 0.0780 ||:  74%|#######3  | 48/65 [09:14<03:15, 11.51s/it]
2022-04-11 12:00:23,831 - INFO - tqdm - accuracy: 0.9751, batch_loss: 0.0082, loss: 0.0766 ||:  75%|#######5  | 49/65 [09:25<03:04, 11.51s/it]
2022-04-11 12:00:35,333 - INFO - tqdm - accuracy: 0.9756, batch_loss: 0.0072, loss: 0.0752 ||:  77%|#######6  | 50/65 [09:37<02:52, 11.51s/it]
2022-04-11 12:00:46,835 - INFO - tqdm - accuracy: 0.9761, batch_loss: 0.0321, loss: 0.0743 ||:  78%|#######8  | 51/65 [09:48<02:41, 11.51s/it]
2022-04-11 12:00:58,206 - INFO - tqdm - accuracy: 0.9765, batch_loss: 0.0089, loss: 0.0731 ||:  80%|########  | 52/65 [10:00<02:29, 11.47s/it]
2022-04-11 12:01:09,701 - INFO - tqdm - accuracy: 0.9770, batch_loss: 0.0264, loss: 0.0722 ||:  82%|########1 | 53/65 [10:11<02:17, 11.47s/it]
2022-04-11 12:01:21,191 - INFO - tqdm - accuracy: 0.9774, batch_loss: 0.0210, loss: 0.0713 ||:  83%|########3 | 54/65 [10:23<02:06, 11.48s/it]
2022-04-11 12:01:32,685 - INFO - tqdm - accuracy: 0.9778, batch_loss: 0.0166, loss: 0.0703 ||:  85%|########4 | 55/65 [10:34<01:54, 11.48s/it]
2022-04-11 12:01:44,061 - INFO - tqdm - accuracy: 0.9765, batch_loss: 0.2663, loss: 0.0738 ||:  86%|########6 | 56/65 [10:45<01:43, 11.45s/it]
2022-04-11 12:01:55,560 - INFO - tqdm - accuracy: 0.9759, batch_loss: 0.1686, loss: 0.0754 ||:  88%|########7 | 57/65 [10:57<01:31, 11.47s/it]
2022-04-11 12:02:07,065 - INFO - tqdm - accuracy: 0.9763, batch_loss: 0.0064, loss: 0.0742 ||:  89%|########9 | 58/65 [11:08<01:20, 11.48s/it]
2022-04-11 12:02:18,545 - INFO - tqdm - accuracy: 0.9767, batch_loss: 0.0072, loss: 0.0731 ||:  91%|######### | 59/65 [11:20<01:08, 11.48s/it]
2022-04-11 12:02:30,065 - INFO - tqdm - accuracy: 0.9771, batch_loss: 0.0034, loss: 0.0719 ||:  92%|#########2| 60/65 [11:31<00:57, 11.49s/it]
2022-04-11 12:02:41,595 - INFO - tqdm - accuracy: 0.9769, batch_loss: 0.0423, loss: 0.0715 ||:  94%|#########3| 61/65 [11:43<00:46, 11.50s/it]
2022-04-11 12:02:53,133 - INFO - tqdm - accuracy: 0.9763, batch_loss: 0.2113, loss: 0.0737 ||:  95%|#########5| 62/65 [11:54<00:34, 11.51s/it]
2022-04-11 12:03:04,660 - INFO - tqdm - accuracy: 0.9762, batch_loss: 0.1322, loss: 0.0746 ||:  97%|#########6| 63/65 [12:06<00:23, 11.52s/it]
2022-04-11 12:03:16,191 - INFO - tqdm - accuracy: 0.9756, batch_loss: 0.1255, loss: 0.0754 ||:  98%|#########8| 64/65 [12:18<00:11, 11.52s/it]
2022-04-11 12:03:21,330 - INFO - tqdm - accuracy: 0.9753, batch_loss: 0.0585, loss: 0.0752 ||: 100%|##########| 65/65 [12:23<00:00,  9.61s/it]
2022-04-11 12:03:21,330 - INFO - tqdm - accuracy: 0.9753, batch_loss: 0.0585, loss: 0.0752 ||: 100%|##########| 65/65 [12:23<00:00, 11.43s/it]
2022-04-11 12:03:24,179 - INFO - allennlp.training.trainer - Validating
2022-04-11 12:03:24,181 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-11 12:03:34,218 - INFO - tqdm - accuracy: 0.6709, batch_loss: 4.9517, loss: 1.4235 ||:  35%|###4      | 40/115 [00:10<00:19,  3.94it/s]
2022-04-11 12:03:44,450 - INFO - tqdm - accuracy: 0.6478, batch_loss: 0.0005, loss: 1.6909 ||:  70%|######9   | 80/115 [00:20<00:09,  3.89it/s]
2022-04-11 12:03:53,327 - INFO - tqdm - accuracy: 0.6463, batch_loss: 2.3196, loss: 1.6843 ||: 100%|##########| 115/115 [00:29<00:00,  3.90it/s]
2022-04-11 12:03:53,327 - INFO - tqdm - accuracy: 0.6463, batch_loss: 2.3196, loss: 1.6843 ||: 100%|##########| 115/115 [00:29<00:00,  3.95it/s]
2022-04-11 12:03:53,328 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-11 12:03:53,329 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.975  |     0.646
2022-04-11 12:03:53,330 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-11 12:03:53,330 - INFO - allennlp.training.tensorboard_writer - loss               |     0.075  |     1.684
2022-04-11 12:03:53,331 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5802.672  |       N/A
2022-04-11 12:03:59,486 - INFO - allennlp.training.trainer - Epoch duration: 0:13:01.335056
2022-04-11 12:03:59,487 - INFO - allennlp.training.trainer - Estimated training time remaining: 1:44:53
2022-04-11 12:03:59,487 - INFO - allennlp.training.trainer - Epoch 7/14
2022-04-11 12:03:59,487 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-11 12:03:59,487 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-11 12:03:59,489 - INFO - allennlp.training.trainer - Training
2022-04-11 12:03:59,489 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-11 12:04:10,903 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0201, loss: 0.0201 ||:   2%|1         | 1/65 [00:11<12:10, 11.41s/it]
2022-04-11 12:04:22,524 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0445, loss: 0.0323 ||:   3%|3         | 2/65 [00:23<12:06, 11.54s/it]
2022-04-11 12:04:34,347 - INFO - tqdm - accuracy: 0.9792, batch_loss: 0.0709, loss: 0.0452 ||:   5%|4         | 3/65 [00:34<12:03, 11.67s/it]
2022-04-11 12:04:45,912 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0077, loss: 0.0358 ||:   6%|6         | 4/65 [00:46<11:49, 11.63s/it]
2022-04-11 12:04:57,363 - INFO - tqdm - accuracy: 0.9812, batch_loss: 0.1116, loss: 0.0510 ||:   8%|7         | 5/65 [00:57<11:33, 11.56s/it]
2022-04-11 12:05:08,830 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0022, loss: 0.0428 ||:   9%|9         | 6/65 [01:09<11:20, 11.53s/it]
2022-04-11 12:05:20,381 - INFO - tqdm - accuracy: 0.9821, batch_loss: 0.0344, loss: 0.0416 ||:  11%|#         | 7/65 [01:20<11:09, 11.54s/it]
2022-04-11 12:05:31,979 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0148, loss: 0.0383 ||:  12%|#2        | 8/65 [01:32<10:58, 11.56s/it]
2022-04-11 12:05:43,602 - INFO - tqdm - accuracy: 0.9861, batch_loss: 0.0073, loss: 0.0348 ||:  14%|#3        | 9/65 [01:44<10:48, 11.58s/it]
2022-04-11 12:05:55,117 - INFO - tqdm - accuracy: 0.9875, batch_loss: 0.0196, loss: 0.0333 ||:  15%|#5        | 10/65 [01:55<10:35, 11.56s/it]
2022-04-11 12:06:06,599 - INFO - tqdm - accuracy: 0.9886, batch_loss: 0.0223, loss: 0.0323 ||:  17%|#6        | 11/65 [02:07<10:22, 11.53s/it]
2022-04-11 12:06:18,096 - INFO - tqdm - accuracy: 0.9870, batch_loss: 0.0518, loss: 0.0339 ||:  18%|#8        | 12/65 [02:18<10:10, 11.52s/it]
2022-04-11 12:06:29,645 - INFO - tqdm - accuracy: 0.9880, batch_loss: 0.0126, loss: 0.0323 ||:  20%|##        | 13/65 [02:30<09:59, 11.53s/it]
2022-04-11 12:06:41,222 - INFO - tqdm - accuracy: 0.9866, batch_loss: 0.0493, loss: 0.0335 ||:  22%|##1       | 14/65 [02:41<09:48, 11.54s/it]
2022-04-11 12:06:52,684 - INFO - tqdm - accuracy: 0.9875, batch_loss: 0.0114, loss: 0.0320 ||:  23%|##3       | 15/65 [02:53<09:36, 11.52s/it]
2022-04-11 12:07:04,240 - INFO - tqdm - accuracy: 0.9863, batch_loss: 0.0668, loss: 0.0342 ||:  25%|##4       | 16/65 [03:04<09:25, 11.53s/it]
2022-04-11 12:07:15,631 - INFO - tqdm - accuracy: 0.9871, batch_loss: 0.0060, loss: 0.0326 ||:  26%|##6       | 17/65 [03:16<09:11, 11.49s/it]
2022-04-11 12:07:27,137 - INFO - tqdm - accuracy: 0.9878, batch_loss: 0.0069, loss: 0.0311 ||:  28%|##7       | 18/65 [03:27<09:00, 11.49s/it]
2022-04-11 12:07:38,639 - INFO - tqdm - accuracy: 0.9885, batch_loss: 0.0120, loss: 0.0301 ||:  29%|##9       | 19/65 [03:39<08:48, 11.50s/it]
2022-04-11 12:07:50,007 - INFO - tqdm - accuracy: 0.9875, batch_loss: 0.0881, loss: 0.0330 ||:  31%|###       | 20/65 [03:50<08:35, 11.46s/it]
2022-04-11 12:08:01,497 - INFO - tqdm - accuracy: 0.9881, batch_loss: 0.0262, loss: 0.0327 ||:  32%|###2      | 21/65 [04:02<08:24, 11.47s/it]
2022-04-11 12:08:13,025 - INFO - tqdm - accuracy: 0.9872, batch_loss: 0.0584, loss: 0.0339 ||:  34%|###3      | 22/65 [04:13<08:13, 11.49s/it]
2022-04-11 12:08:24,565 - INFO - tqdm - accuracy: 0.9878, batch_loss: 0.0225, loss: 0.0334 ||:  35%|###5      | 23/65 [04:25<08:03, 11.50s/it]
2022-04-11 12:08:36,111 - INFO - tqdm - accuracy: 0.9883, batch_loss: 0.0043, loss: 0.0322 ||:  37%|###6      | 24/65 [04:36<07:52, 11.52s/it]
2022-04-11 12:08:47,659 - INFO - tqdm - accuracy: 0.9888, batch_loss: 0.0076, loss: 0.0312 ||:  38%|###8      | 25/65 [04:48<07:40, 11.52s/it]
2022-04-11 12:08:59,180 - INFO - tqdm - accuracy: 0.9892, batch_loss: 0.0097, loss: 0.0303 ||:  40%|####      | 26/65 [04:59<07:29, 11.52s/it]
2022-04-11 12:09:10,730 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.0107, loss: 0.0296 ||:  42%|####1     | 27/65 [05:11<07:18, 11.53s/it]
2022-04-11 12:09:22,283 - INFO - tqdm - accuracy: 0.9900, batch_loss: 0.0047, loss: 0.0287 ||:  43%|####3     | 28/65 [05:22<07:06, 11.54s/it]
2022-04-11 12:09:33,848 - INFO - tqdm - accuracy: 0.9881, batch_loss: 0.0713, loss: 0.0302 ||:  45%|####4     | 29/65 [05:34<06:55, 11.55s/it]
2022-04-11 12:09:45,406 - INFO - tqdm - accuracy: 0.9885, batch_loss: 0.0228, loss: 0.0299 ||:  46%|####6     | 30/65 [05:45<06:44, 11.55s/it]
2022-04-11 12:09:56,957 - INFO - tqdm - accuracy: 0.9879, batch_loss: 0.0358, loss: 0.0301 ||:  48%|####7     | 31/65 [05:57<06:32, 11.55s/it]
2022-04-11 12:10:08,551 - INFO - tqdm - accuracy: 0.9883, batch_loss: 0.0135, loss: 0.0296 ||:  49%|####9     | 32/65 [06:09<06:21, 11.56s/it]
2022-04-11 12:10:20,133 - INFO - tqdm - accuracy: 0.9886, batch_loss: 0.0222, loss: 0.0294 ||:  51%|#####     | 33/65 [06:20<06:10, 11.57s/it]
2022-04-11 12:10:31,715 - INFO - tqdm - accuracy: 0.9871, batch_loss: 0.0809, loss: 0.0309 ||:  52%|#####2    | 34/65 [06:32<05:58, 11.57s/it]
2022-04-11 12:10:43,285 - INFO - tqdm - accuracy: 0.9875, batch_loss: 0.0390, loss: 0.0311 ||:  54%|#####3    | 35/65 [06:43<05:47, 11.57s/it]
2022-04-11 12:10:54,874 - INFO - tqdm - accuracy: 0.9878, batch_loss: 0.0097, loss: 0.0305 ||:  55%|#####5    | 36/65 [06:55<05:35, 11.58s/it]
2022-04-11 12:11:06,441 - INFO - tqdm - accuracy: 0.9882, batch_loss: 0.0134, loss: 0.0301 ||:  57%|#####6    | 37/65 [07:06<05:24, 11.57s/it]
2022-04-11 12:11:17,989 - INFO - tqdm - accuracy: 0.9885, batch_loss: 0.0084, loss: 0.0295 ||:  58%|#####8    | 38/65 [07:18<05:12, 11.57s/it]
2022-04-11 12:11:29,517 - INFO - tqdm - accuracy: 0.9888, batch_loss: 0.0132, loss: 0.0291 ||:  60%|######    | 39/65 [07:30<05:00, 11.55s/it]
2022-04-11 12:11:41,024 - INFO - tqdm - accuracy: 0.9883, batch_loss: 0.1032, loss: 0.0309 ||:  62%|######1   | 40/65 [07:41<04:48, 11.54s/it]
2022-04-11 12:11:52,540 - INFO - tqdm - accuracy: 0.9878, batch_loss: 0.0441, loss: 0.0313 ||:  63%|######3   | 41/65 [07:53<04:36, 11.53s/it]
2022-04-11 12:12:04,045 - INFO - tqdm - accuracy: 0.9874, batch_loss: 0.1903, loss: 0.0350 ||:  65%|######4   | 42/65 [08:04<04:25, 11.52s/it]
2022-04-11 12:12:15,560 - INFO - tqdm - accuracy: 0.9876, batch_loss: 0.0283, loss: 0.0349 ||:  66%|######6   | 43/65 [08:16<04:13, 11.52s/it]
2022-04-11 12:12:27,034 - INFO - tqdm - accuracy: 0.9879, batch_loss: 0.0212, loss: 0.0346 ||:  68%|######7   | 44/65 [08:27<04:01, 11.51s/it]
2022-04-11 12:12:38,913 - INFO - tqdm - accuracy: 0.9882, batch_loss: 0.0022, loss: 0.0339 ||:  69%|######9   | 45/65 [08:39<03:52, 11.62s/it]
2022-04-11 12:12:50,370 - INFO - tqdm - accuracy: 0.9885, batch_loss: 0.0134, loss: 0.0334 ||:  71%|#######   | 46/65 [08:50<03:39, 11.57s/it]
2022-04-11 12:13:01,851 - INFO - tqdm - accuracy: 0.9887, batch_loss: 0.0186, loss: 0.0331 ||:  72%|#######2  | 47/65 [09:02<03:27, 11.54s/it]
2022-04-11 12:13:13,356 - INFO - tqdm - accuracy: 0.9889, batch_loss: 0.0282, loss: 0.0330 ||:  74%|#######3  | 48/65 [09:13<03:16, 11.53s/it]
2022-04-11 12:13:24,883 - INFO - tqdm - accuracy: 0.9892, batch_loss: 0.0050, loss: 0.0324 ||:  75%|#######5  | 49/65 [09:25<03:04, 11.53s/it]
2022-04-11 12:13:36,404 - INFO - tqdm - accuracy: 0.9881, batch_loss: 0.0888, loss: 0.0335 ||:  77%|#######6  | 50/65 [09:36<02:52, 11.53s/it]
2022-04-11 12:13:47,932 - INFO - tqdm - accuracy: 0.9877, batch_loss: 0.0354, loss: 0.0336 ||:  78%|#######8  | 51/65 [09:48<02:41, 11.53s/it]
2022-04-11 12:13:59,477 - INFO - tqdm - accuracy: 0.9880, batch_loss: 0.0189, loss: 0.0333 ||:  80%|########  | 52/65 [09:59<02:29, 11.53s/it]
2022-04-11 12:14:11,030 - INFO - tqdm - accuracy: 0.9882, batch_loss: 0.0076, loss: 0.0328 ||:  82%|########1 | 53/65 [10:11<02:18, 11.54s/it]
2022-04-11 12:14:22,584 - INFO - tqdm - accuracy: 0.9884, batch_loss: 0.0283, loss: 0.0327 ||:  83%|########3 | 54/65 [10:23<02:06, 11.54s/it]
2022-04-11 12:14:34,143 - INFO - tqdm - accuracy: 0.9881, batch_loss: 0.1715, loss: 0.0353 ||:  85%|########4 | 55/65 [10:34<01:55, 11.55s/it]
2022-04-11 12:14:45,706 - INFO - tqdm - accuracy: 0.9877, batch_loss: 0.0622, loss: 0.0357 ||:  86%|########6 | 56/65 [10:46<01:43, 11.55s/it]
2022-04-11 12:14:57,295 - INFO - tqdm - accuracy: 0.9879, batch_loss: 0.0079, loss: 0.0352 ||:  88%|########7 | 57/65 [10:57<01:32, 11.56s/it]
2022-04-11 12:15:08,874 - INFO - tqdm - accuracy: 0.9876, batch_loss: 0.1359, loss: 0.0370 ||:  89%|########9 | 58/65 [11:09<01:20, 11.57s/it]
2022-04-11 12:15:19,856 - INFO - tqdm - accuracy: 0.9878, batch_loss: 0.0251, loss: 0.0368 ||:  91%|######### | 59/65 [11:20<01:08, 11.39s/it]
2022-04-11 12:15:31,437 - INFO - tqdm - accuracy: 0.9880, batch_loss: 0.0097, loss: 0.0363 ||:  92%|#########2| 60/65 [11:31<00:57, 11.45s/it]
2022-04-11 12:15:43,037 - INFO - tqdm - accuracy: 0.9882, batch_loss: 0.0120, loss: 0.0359 ||:  94%|#########3| 61/65 [11:43<00:45, 11.49s/it]
2022-04-11 12:15:54,612 - INFO - tqdm - accuracy: 0.9879, batch_loss: 0.0666, loss: 0.0364 ||:  95%|#########5| 62/65 [11:55<00:34, 11.52s/it]
2022-04-11 12:16:06,199 - INFO - tqdm - accuracy: 0.9881, batch_loss: 0.0032, loss: 0.0359 ||:  97%|#########6| 63/65 [12:06<00:23, 11.54s/it]
2022-04-11 12:16:17,761 - INFO - tqdm - accuracy: 0.9883, batch_loss: 0.0397, loss: 0.0360 ||:  98%|#########8| 64/65 [12:18<00:11, 11.55s/it]
2022-04-11 12:16:22,904 - INFO - tqdm - accuracy: 0.9884, batch_loss: 0.0054, loss: 0.0355 ||: 100%|##########| 65/65 [12:23<00:00,  9.63s/it]
2022-04-11 12:16:22,904 - INFO - tqdm - accuracy: 0.9884, batch_loss: 0.0054, loss: 0.0355 ||: 100%|##########| 65/65 [12:23<00:00, 11.44s/it]
2022-04-11 12:16:25,766 - INFO - allennlp.training.trainer - Validating
2022-04-11 12:16:25,768 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-11 12:16:35,830 - INFO - tqdm - accuracy: 0.7000, batch_loss: 3.4092, loss: 1.5724 ||:  35%|###4      | 40/115 [00:10<00:18,  4.00it/s]
2022-04-11 12:16:45,926 - INFO - tqdm - accuracy: 0.6541, batch_loss: 2.0923, loss: 1.8318 ||:  70%|######9   | 80/115 [00:20<00:08,  3.91it/s]
2022-04-11 12:16:54,913 - INFO - tqdm - accuracy: 0.6376, batch_loss: 4.8101, loss: 1.8648 ||: 100%|##########| 115/115 [00:29<00:00,  3.90it/s]
2022-04-11 12:16:54,914 - INFO - tqdm - accuracy: 0.6376, batch_loss: 4.8101, loss: 1.8648 ||: 100%|##########| 115/115 [00:29<00:00,  3.95it/s]
2022-04-11 12:16:54,914 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-11 12:16:54,914 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.988  |     0.638
2022-04-11 12:16:54,915 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-11 12:16:54,916 - INFO - allennlp.training.tensorboard_writer - loss               |     0.035  |     1.865
2022-04-11 12:16:54,916 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5802.672  |       N/A
2022-04-11 12:17:00,946 - INFO - allennlp.training.trainer - Epoch duration: 0:13:01.459469
2022-04-11 12:17:00,947 - INFO - allennlp.training.trainer - Estimated training time remaining: 1:31:42
2022-04-11 12:17:00,947 - INFO - allennlp.training.trainer - Epoch 8/14
2022-04-11 12:17:00,947 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-11 12:17:00,947 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-11 12:17:00,948 - INFO - allennlp.training.trainer - Training
2022-04-11 12:17:00,949 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-11 12:17:12,339 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.0812, loss: 0.0812 ||:   2%|1         | 1/65 [00:11<12:08, 11.39s/it]
2022-04-11 12:17:23,980 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0027, loss: 0.0420 ||:   3%|3         | 2/65 [00:23<12:06, 11.54s/it]
2022-04-11 12:17:35,821 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.0102, loss: 0.0314 ||:   5%|4         | 3/65 [00:34<12:03, 11.68s/it]
2022-04-11 12:17:47,367 - INFO - tqdm - accuracy: 0.9922, batch_loss: 0.0070, loss: 0.0253 ||:   6%|6         | 4/65 [00:46<11:49, 11.62s/it]
2022-04-11 12:17:58,811 - INFO - tqdm - accuracy: 0.9938, batch_loss: 0.0219, loss: 0.0246 ||:   8%|7         | 5/65 [00:57<11:33, 11.56s/it]
2022-04-11 12:18:10,268 - INFO - tqdm - accuracy: 0.9948, batch_loss: 0.0248, loss: 0.0246 ||:   9%|9         | 6/65 [01:09<11:19, 11.52s/it]
2022-04-11 12:18:21,797 - INFO - tqdm - accuracy: 0.9955, batch_loss: 0.0224, loss: 0.0243 ||:  11%|#         | 7/65 [01:20<11:08, 11.53s/it]
2022-04-11 12:18:33,380 - INFO - tqdm - accuracy: 0.9961, batch_loss: 0.0028, loss: 0.0216 ||:  12%|#2        | 8/65 [01:32<10:58, 11.54s/it]
2022-04-11 12:18:45,012 - INFO - tqdm - accuracy: 0.9965, batch_loss: 0.0087, loss: 0.0202 ||:  14%|#3        | 9/65 [01:44<10:48, 11.57s/it]
2022-04-11 12:18:56,576 - INFO - tqdm - accuracy: 0.9938, batch_loss: 0.0955, loss: 0.0277 ||:  15%|#5        | 10/65 [01:55<10:36, 11.57s/it]
2022-04-11 12:19:08,096 - INFO - tqdm - accuracy: 0.9943, batch_loss: 0.0251, loss: 0.0275 ||:  17%|#6        | 11/65 [02:07<10:23, 11.55s/it]
2022-04-11 12:19:19,585 - INFO - tqdm - accuracy: 0.9922, batch_loss: 0.0821, loss: 0.0320 ||:  18%|#8        | 12/65 [02:18<10:11, 11.53s/it]
2022-04-11 12:19:31,062 - INFO - tqdm - accuracy: 0.9880, batch_loss: 0.1893, loss: 0.0441 ||:  20%|##        | 13/65 [02:30<09:58, 11.52s/it]
2022-04-11 12:19:42,550 - INFO - tqdm - accuracy: 0.9888, batch_loss: 0.0032, loss: 0.0412 ||:  22%|##1       | 14/65 [02:41<09:46, 11.51s/it]
2022-04-11 12:19:54,074 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.0060, loss: 0.0389 ||:  23%|##3       | 15/65 [02:53<09:35, 11.51s/it]
2022-04-11 12:20:05,581 - INFO - tqdm - accuracy: 0.9902, batch_loss: 0.0153, loss: 0.0374 ||:  25%|##4       | 16/65 [03:04<09:24, 11.51s/it]
2022-04-11 12:20:17,093 - INFO - tqdm - accuracy: 0.9908, batch_loss: 0.0126, loss: 0.0359 ||:  26%|##6       | 17/65 [03:16<09:12, 11.51s/it]
2022-04-11 12:20:28,642 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.0753, loss: 0.0381 ||:  28%|##7       | 18/65 [03:27<09:01, 11.52s/it]
2022-04-11 12:20:40,184 - INFO - tqdm - accuracy: 0.9885, batch_loss: 0.0518, loss: 0.0388 ||:  29%|##9       | 19/65 [03:39<08:50, 11.53s/it]
2022-04-11 12:20:51,734 - INFO - tqdm - accuracy: 0.9891, batch_loss: 0.0131, loss: 0.0376 ||:  31%|###       | 20/65 [03:50<08:39, 11.54s/it]
2022-04-11 12:21:03,291 - INFO - tqdm - accuracy: 0.9866, batch_loss: 0.0981, loss: 0.0404 ||:  32%|###2      | 21/65 [04:02<08:27, 11.54s/it]
2022-04-11 12:21:14,865 - INFO - tqdm - accuracy: 0.9872, batch_loss: 0.0023, loss: 0.0387 ||:  34%|###3      | 22/65 [04:13<08:16, 11.55s/it]
2022-04-11 12:21:26,432 - INFO - tqdm - accuracy: 0.9878, batch_loss: 0.0025, loss: 0.0371 ||:  35%|###5      | 23/65 [04:25<08:05, 11.56s/it]
2022-04-11 12:21:38,017 - INFO - tqdm - accuracy: 0.9883, batch_loss: 0.0025, loss: 0.0357 ||:  37%|###6      | 24/65 [04:37<07:54, 11.56s/it]
2022-04-11 12:21:49,599 - INFO - tqdm - accuracy: 0.9875, batch_loss: 0.0601, loss: 0.0367 ||:  38%|###8      | 25/65 [04:48<07:42, 11.57s/it]
2022-04-11 12:22:01,200 - INFO - tqdm - accuracy: 0.9880, batch_loss: 0.0102, loss: 0.0356 ||:  40%|####      | 26/65 [05:00<07:31, 11.58s/it]
2022-04-11 12:22:12,557 - INFO - tqdm - accuracy: 0.9884, batch_loss: 0.0037, loss: 0.0345 ||:  42%|####1     | 27/65 [05:11<07:17, 11.51s/it]
2022-04-11 12:22:24,130 - INFO - tqdm - accuracy: 0.9888, batch_loss: 0.0075, loss: 0.0335 ||:  43%|####3     | 28/65 [05:23<07:06, 11.53s/it]
2022-04-11 12:22:35,669 - INFO - tqdm - accuracy: 0.9892, batch_loss: 0.0031, loss: 0.0324 ||:  45%|####4     | 29/65 [05:34<06:55, 11.53s/it]
2022-04-11 12:22:47,181 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.0227, loss: 0.0321 ||:  46%|####6     | 30/65 [05:46<06:43, 11.53s/it]
2022-04-11 12:22:58,686 - INFO - tqdm - accuracy: 0.9899, batch_loss: 0.0055, loss: 0.0313 ||:  48%|####7     | 31/65 [05:57<06:31, 11.52s/it]
2022-04-11 12:23:10,168 - INFO - tqdm - accuracy: 0.9902, batch_loss: 0.0057, loss: 0.0305 ||:  49%|####9     | 32/65 [06:09<06:19, 11.51s/it]
2022-04-11 12:23:21,672 - INFO - tqdm - accuracy: 0.9886, batch_loss: 0.2435, loss: 0.0369 ||:  51%|#####     | 33/65 [06:20<06:08, 11.51s/it]
2022-04-11 12:23:33,194 - INFO - tqdm - accuracy: 0.9881, batch_loss: 0.0583, loss: 0.0375 ||:  52%|#####2    | 34/65 [06:32<05:56, 11.51s/it]
2022-04-11 12:23:44,724 - INFO - tqdm - accuracy: 0.9884, batch_loss: 0.0021, loss: 0.0365 ||:  54%|#####3    | 35/65 [06:43<05:45, 11.52s/it]
2022-04-11 12:23:56,258 - INFO - tqdm - accuracy: 0.9887, batch_loss: 0.0021, loss: 0.0356 ||:  55%|#####5    | 36/65 [06:55<05:34, 11.52s/it]
2022-04-11 12:24:07,782 - INFO - tqdm - accuracy: 0.9890, batch_loss: 0.0348, loss: 0.0356 ||:  57%|#####6    | 37/65 [07:06<05:22, 11.52s/it]
2022-04-11 12:24:19,338 - INFO - tqdm - accuracy: 0.9893, batch_loss: 0.0025, loss: 0.0347 ||:  58%|#####8    | 38/65 [07:18<05:11, 11.53s/it]
2022-04-11 12:24:30,629 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.0239, loss: 0.0344 ||:  60%|######    | 39/65 [07:29<04:57, 11.46s/it]
2022-04-11 12:24:42,185 - INFO - tqdm - accuracy: 0.9898, batch_loss: 0.0040, loss: 0.0337 ||:  62%|######1   | 40/65 [07:41<04:47, 11.49s/it]
2022-04-11 12:24:53,761 - INFO - tqdm - accuracy: 0.9901, batch_loss: 0.0117, loss: 0.0331 ||:  63%|######3   | 41/65 [07:52<04:36, 11.52s/it]
2022-04-11 12:25:05,334 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.0810, loss: 0.0343 ||:  65%|######4   | 42/65 [08:04<04:25, 11.53s/it]
2022-04-11 12:25:16,894 - INFO - tqdm - accuracy: 0.9891, batch_loss: 0.1096, loss: 0.0360 ||:  66%|######6   | 43/65 [08:15<04:13, 11.54s/it]
2022-04-11 12:25:28,483 - INFO - tqdm - accuracy: 0.9872, batch_loss: 0.3273, loss: 0.0426 ||:  68%|######7   | 44/65 [08:27<04:02, 11.56s/it]
2022-04-11 12:25:40,080 - INFO - tqdm - accuracy: 0.9875, batch_loss: 0.0336, loss: 0.0424 ||:  69%|######9   | 45/65 [08:39<03:51, 11.57s/it]
2022-04-11 12:25:51,667 - INFO - tqdm - accuracy: 0.9878, batch_loss: 0.0072, loss: 0.0417 ||:  71%|#######   | 46/65 [08:50<03:39, 11.57s/it]
2022-04-11 12:26:03,243 - INFO - tqdm - accuracy: 0.9880, batch_loss: 0.0098, loss: 0.0410 ||:  72%|#######2  | 47/65 [09:02<03:28, 11.57s/it]
2022-04-11 12:26:14,801 - INFO - tqdm - accuracy: 0.9883, batch_loss: 0.0046, loss: 0.0402 ||:  74%|#######3  | 48/65 [09:13<03:16, 11.57s/it]
2022-04-11 12:26:26,370 - INFO - tqdm - accuracy: 0.9885, batch_loss: 0.0109, loss: 0.0396 ||:  75%|#######5  | 49/65 [09:25<03:05, 11.57s/it]
2022-04-11 12:26:37,893 - INFO - tqdm - accuracy: 0.9888, batch_loss: 0.0084, loss: 0.0390 ||:  77%|#######6  | 50/65 [09:36<02:53, 11.56s/it]
2022-04-11 12:26:49,440 - INFO - tqdm - accuracy: 0.9877, batch_loss: 0.0855, loss: 0.0399 ||:  78%|#######8  | 51/65 [09:48<02:41, 11.55s/it]
2022-04-11 12:27:00,955 - INFO - tqdm - accuracy: 0.9880, batch_loss: 0.0188, loss: 0.0395 ||:  80%|########  | 52/65 [10:00<02:30, 11.54s/it]
2022-04-11 12:27:12,469 - INFO - tqdm - accuracy: 0.9882, batch_loss: 0.0081, loss: 0.0389 ||:  82%|########1 | 53/65 [10:11<02:18, 11.53s/it]
2022-04-11 12:27:23,979 - INFO - tqdm - accuracy: 0.9884, batch_loss: 0.0071, loss: 0.0383 ||:  83%|########3 | 54/65 [10:23<02:06, 11.53s/it]
2022-04-11 12:27:35,474 - INFO - tqdm - accuracy: 0.9881, batch_loss: 0.0376, loss: 0.0383 ||:  85%|########4 | 55/65 [10:34<01:55, 11.52s/it]
2022-04-11 12:27:46,972 - INFO - tqdm - accuracy: 0.9872, batch_loss: 0.1223, loss: 0.0398 ||:  86%|########6 | 56/65 [10:46<01:43, 11.51s/it]
2022-04-11 12:27:58,469 - INFO - tqdm - accuracy: 0.9874, batch_loss: 0.0051, loss: 0.0392 ||:  88%|########7 | 57/65 [10:57<01:32, 11.51s/it]
2022-04-11 12:28:09,967 - INFO - tqdm - accuracy: 0.9876, batch_loss: 0.0076, loss: 0.0387 ||:  89%|########9 | 58/65 [11:09<01:20, 11.50s/it]
2022-04-11 12:28:21,457 - INFO - tqdm - accuracy: 0.9878, batch_loss: 0.0036, loss: 0.0381 ||:  91%|######### | 59/65 [11:20<01:08, 11.50s/it]
2022-04-11 12:28:32,594 - INFO - tqdm - accuracy: 0.9875, batch_loss: 0.0662, loss: 0.0385 ||:  92%|#########2| 60/65 [11:31<00:56, 11.39s/it]
2022-04-11 12:28:44,087 - INFO - tqdm - accuracy: 0.9877, batch_loss: 0.0184, loss: 0.0382 ||:  94%|#########3| 61/65 [11:43<00:45, 11.42s/it]
2022-04-11 12:28:55,564 - INFO - tqdm - accuracy: 0.9874, batch_loss: 0.0585, loss: 0.0385 ||:  95%|#########5| 62/65 [11:54<00:34, 11.44s/it]
2022-04-11 12:29:07,088 - INFO - tqdm - accuracy: 0.9876, batch_loss: 0.0026, loss: 0.0380 ||:  97%|#########6| 63/65 [12:06<00:22, 11.46s/it]
2022-04-11 12:29:18,616 - INFO - tqdm - accuracy: 0.9873, batch_loss: 0.0598, loss: 0.0383 ||:  98%|#########8| 64/65 [12:17<00:11, 11.48s/it]
2022-04-11 12:29:23,614 - INFO - tqdm - accuracy: 0.9874, batch_loss: 0.0026, loss: 0.0378 ||: 100%|##########| 65/65 [12:22<00:00,  9.54s/it]
2022-04-11 12:29:23,614 - INFO - tqdm - accuracy: 0.9874, batch_loss: 0.0026, loss: 0.0378 ||: 100%|##########| 65/65 [12:22<00:00, 11.43s/it]
2022-04-11 12:29:26,464 - INFO - allennlp.training.trainer - Validating
2022-04-11 12:29:26,466 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-11 12:29:36,600 - INFO - tqdm - accuracy: 0.6250, batch_loss: 0.8790, loss: 1.8566 ||:  35%|###4      | 40/115 [00:10<00:19,  3.92it/s]
2022-04-11 12:29:46,821 - INFO - tqdm - accuracy: 0.6375, batch_loss: 0.1845, loss: 1.8759 ||:  70%|######9   | 80/115 [00:20<00:08,  3.90it/s]
2022-04-11 12:29:55,570 - INFO - tqdm - accuracy: 0.6376, batch_loss: 0.0369, loss: 1.8335 ||: 100%|##########| 115/115 [00:29<00:00,  3.97it/s]
2022-04-11 12:29:55,570 - INFO - tqdm - accuracy: 0.6376, batch_loss: 0.0369, loss: 1.8335 ||: 100%|##########| 115/115 [00:29<00:00,  3.95it/s]
2022-04-11 12:29:55,570 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-11 12:29:55,571 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.987  |     0.638
2022-04-11 12:29:55,573 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-11 12:29:55,573 - INFO - allennlp.training.tensorboard_writer - loss               |     0.038  |     1.833
2022-04-11 12:29:55,573 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5802.672  |       N/A
2022-04-11 12:30:01,958 - INFO - allennlp.training.trainer - Epoch duration: 0:13:01.010851
2022-04-11 12:30:01,958 - INFO - allennlp.training.trainer - Estimated training time remaining: 1:18:33
2022-04-11 12:30:01,958 - INFO - allennlp.training.trainer - Epoch 9/14
2022-04-11 12:30:01,958 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-11 12:30:01,958 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-11 12:30:01,960 - INFO - allennlp.training.trainer - Training
2022-04-11 12:30:01,960 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-11 12:30:13,355 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0050, loss: 0.0050 ||:   2%|1         | 1/65 [00:11<12:09, 11.39s/it]
2022-04-11 12:30:25,015 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0030, loss: 0.0040 ||:   3%|3         | 2/65 [00:23<12:07, 11.55s/it]
2022-04-11 12:30:36,848 - INFO - tqdm - accuracy: 0.9792, batch_loss: 0.0670, loss: 0.0250 ||:   5%|4         | 3/65 [00:34<12:04, 11.68s/it]
2022-04-11 12:30:48,284 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0019, loss: 0.0192 ||:   6%|6         | 4/65 [00:46<11:46, 11.58s/it]
2022-04-11 12:30:59,716 - INFO - tqdm - accuracy: 0.9875, batch_loss: 0.0093, loss: 0.0172 ||:   8%|7         | 5/65 [00:57<11:31, 11.53s/it]
2022-04-11 12:31:11,167 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.0186, loss: 0.0174 ||:   9%|9         | 6/65 [01:09<11:18, 11.50s/it]
2022-04-11 12:31:22,675 - INFO - tqdm - accuracy: 0.9911, batch_loss: 0.0022, loss: 0.0153 ||:  11%|#         | 7/65 [01:20<11:07, 11.50s/it]
2022-04-11 12:31:34,230 - INFO - tqdm - accuracy: 0.9922, batch_loss: 0.0169, loss: 0.0155 ||:  12%|#2        | 8/65 [01:32<10:56, 11.52s/it]
2022-04-11 12:31:45,848 - INFO - tqdm - accuracy: 0.9931, batch_loss: 0.0011, loss: 0.0139 ||:  14%|#3        | 9/65 [01:43<10:46, 11.55s/it]
2022-04-11 12:31:57,384 - INFO - tqdm - accuracy: 0.9938, batch_loss: 0.0026, loss: 0.0128 ||:  15%|#5        | 10/65 [01:55<10:35, 11.55s/it]
2022-04-11 12:32:08,912 - INFO - tqdm - accuracy: 0.9915, batch_loss: 0.0937, loss: 0.0201 ||:  17%|#6        | 11/65 [02:06<10:23, 11.54s/it]
2022-04-11 12:32:20,398 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.0383, loss: 0.0216 ||:  18%|#8        | 12/65 [02:18<10:10, 11.52s/it]
2022-04-11 12:32:31,878 - INFO - tqdm - accuracy: 0.9904, batch_loss: 0.0072, loss: 0.0205 ||:  20%|##        | 13/65 [02:29<09:58, 11.51s/it]
2022-04-11 12:32:43,392 - INFO - tqdm - accuracy: 0.9911, batch_loss: 0.0049, loss: 0.0194 ||:  22%|##1       | 14/65 [02:41<09:47, 11.51s/it]
2022-04-11 12:32:55,304 - INFO - tqdm - accuracy: 0.9917, batch_loss: 0.0049, loss: 0.0184 ||:  23%|##3       | 15/65 [02:53<09:41, 11.63s/it]
2022-04-11 12:33:06,814 - INFO - tqdm - accuracy: 0.9922, batch_loss: 0.0018, loss: 0.0174 ||:  25%|##4       | 16/65 [03:04<09:28, 11.60s/it]
2022-04-11 12:33:18,396 - INFO - tqdm - accuracy: 0.9908, batch_loss: 0.0433, loss: 0.0189 ||:  26%|##6       | 17/65 [03:16<09:16, 11.59s/it]
2022-04-11 12:33:29,992 - INFO - tqdm - accuracy: 0.9913, batch_loss: 0.0036, loss: 0.0181 ||:  28%|##7       | 18/65 [03:28<09:04, 11.59s/it]
2022-04-11 12:33:41,566 - INFO - tqdm - accuracy: 0.9901, batch_loss: 0.1549, loss: 0.0253 ||:  29%|##9       | 19/65 [03:39<08:53, 11.59s/it]
2022-04-11 12:33:53,124 - INFO - tqdm - accuracy: 0.9891, batch_loss: 0.0796, loss: 0.0280 ||:  31%|###       | 20/65 [03:51<08:41, 11.58s/it]
2022-04-11 12:34:04,674 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.0183, loss: 0.0275 ||:  32%|###2      | 21/65 [04:02<08:29, 11.57s/it]
2022-04-11 12:34:16,241 - INFO - tqdm - accuracy: 0.9901, batch_loss: 0.0089, loss: 0.0267 ||:  34%|###3      | 22/65 [04:14<08:17, 11.57s/it]
2022-04-11 12:34:27,776 - INFO - tqdm - accuracy: 0.9891, batch_loss: 0.0497, loss: 0.0277 ||:  35%|###5      | 23/65 [04:25<08:05, 11.56s/it]
2022-04-11 12:34:39,324 - INFO - tqdm - accuracy: 0.9883, batch_loss: 0.0371, loss: 0.0281 ||:  37%|###6      | 24/65 [04:37<07:53, 11.56s/it]
2022-04-11 12:34:50,858 - INFO - tqdm - accuracy: 0.9888, batch_loss: 0.0024, loss: 0.0270 ||:  38%|###8      | 25/65 [04:48<07:41, 11.55s/it]
2022-04-11 12:35:02,400 - INFO - tqdm - accuracy: 0.9880, batch_loss: 0.0904, loss: 0.0295 ||:  40%|####      | 26/65 [05:00<07:30, 11.55s/it]
2022-04-11 12:35:13,925 - INFO - tqdm - accuracy: 0.9873, batch_loss: 0.0612, loss: 0.0307 ||:  42%|####1     | 27/65 [05:11<07:18, 11.54s/it]
2022-04-11 12:35:25,463 - INFO - tqdm - accuracy: 0.9877, batch_loss: 0.0058, loss: 0.0298 ||:  43%|####3     | 28/65 [05:23<07:06, 11.54s/it]
2022-04-11 12:35:36,989 - INFO - tqdm - accuracy: 0.9881, batch_loss: 0.0047, loss: 0.0289 ||:  45%|####4     | 29/65 [05:35<06:55, 11.54s/it]
2022-04-11 12:35:48,404 - INFO - tqdm - accuracy: 0.9885, batch_loss: 0.0069, loss: 0.0282 ||:  46%|####6     | 30/65 [05:46<06:42, 11.50s/it]
2022-04-11 12:35:59,929 - INFO - tqdm - accuracy: 0.9879, batch_loss: 0.0682, loss: 0.0295 ||:  48%|####7     | 31/65 [05:57<06:31, 11.51s/it]
2022-04-11 12:36:11,454 - INFO - tqdm - accuracy: 0.9883, batch_loss: 0.0024, loss: 0.0286 ||:  49%|####9     | 32/65 [06:09<06:19, 11.51s/it]
2022-04-11 12:36:22,963 - INFO - tqdm - accuracy: 0.9877, batch_loss: 0.0896, loss: 0.0305 ||:  51%|#####     | 33/65 [06:21<06:08, 11.51s/it]
2022-04-11 12:36:34,501 - INFO - tqdm - accuracy: 0.9871, batch_loss: 0.1030, loss: 0.0326 ||:  52%|#####2    | 34/65 [06:32<05:57, 11.52s/it]
2022-04-11 12:36:45,668 - INFO - tqdm - accuracy: 0.9866, batch_loss: 0.0348, loss: 0.0327 ||:  54%|#####3    | 35/65 [06:43<05:42, 11.41s/it]
2022-04-11 12:36:57,205 - INFO - tqdm - accuracy: 0.9870, batch_loss: 0.0186, loss: 0.0323 ||:  55%|#####5    | 36/65 [06:55<05:32, 11.45s/it]
2022-04-11 12:37:08,722 - INFO - tqdm - accuracy: 0.9873, batch_loss: 0.0040, loss: 0.0315 ||:  57%|#####6    | 37/65 [07:06<05:21, 11.47s/it]
2022-04-11 12:37:20,254 - INFO - tqdm - accuracy: 0.9877, batch_loss: 0.0189, loss: 0.0312 ||:  58%|#####8    | 38/65 [07:18<05:10, 11.49s/it]
2022-04-11 12:37:31,788 - INFO - tqdm - accuracy: 0.9880, batch_loss: 0.0019, loss: 0.0304 ||:  60%|######    | 39/65 [07:29<04:59, 11.50s/it]
2022-04-11 12:37:43,308 - INFO - tqdm - accuracy: 0.9883, batch_loss: 0.0042, loss: 0.0298 ||:  62%|######1   | 40/65 [07:41<04:47, 11.51s/it]
2022-04-11 12:37:54,846 - INFO - tqdm - accuracy: 0.9886, batch_loss: 0.0060, loss: 0.0292 ||:  63%|######3   | 41/65 [07:52<04:36, 11.52s/it]
2022-04-11 12:38:06,381 - INFO - tqdm - accuracy: 0.9888, batch_loss: 0.0036, loss: 0.0286 ||:  65%|######4   | 42/65 [08:04<04:25, 11.52s/it]
2022-04-11 12:38:17,922 - INFO - tqdm - accuracy: 0.9891, batch_loss: 0.0138, loss: 0.0282 ||:  66%|######6   | 43/65 [08:15<04:13, 11.53s/it]
2022-04-11 12:38:29,471 - INFO - tqdm - accuracy: 0.9893, batch_loss: 0.0105, loss: 0.0278 ||:  68%|######7   | 44/65 [08:27<04:02, 11.53s/it]
2022-04-11 12:38:41,009 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.0067, loss: 0.0274 ||:  69%|######9   | 45/65 [08:39<03:50, 11.54s/it]
2022-04-11 12:38:52,558 - INFO - tqdm - accuracy: 0.9898, batch_loss: 0.0030, loss: 0.0268 ||:  71%|#######   | 46/65 [08:50<03:39, 11.54s/it]
2022-04-11 12:39:04,106 - INFO - tqdm - accuracy: 0.9900, batch_loss: 0.0067, loss: 0.0264 ||:  72%|#######2  | 47/65 [09:02<03:27, 11.54s/it]
2022-04-11 12:39:15,640 - INFO - tqdm - accuracy: 0.9902, batch_loss: 0.0090, loss: 0.0260 ||:  74%|#######3  | 48/65 [09:13<03:16, 11.54s/it]
2022-04-11 12:39:27,158 - INFO - tqdm - accuracy: 0.9904, batch_loss: 0.0145, loss: 0.0258 ||:  75%|#######5  | 49/65 [09:25<03:04, 11.53s/it]
2022-04-11 12:39:38,421 - INFO - tqdm - accuracy: 0.9906, batch_loss: 0.0149, loss: 0.0256 ||:  77%|#######6  | 50/65 [09:36<02:51, 11.45s/it]
2022-04-11 12:39:49,956 - INFO - tqdm - accuracy: 0.9902, batch_loss: 0.1530, loss: 0.0281 ||:  78%|#######8  | 51/65 [09:47<02:40, 11.48s/it]
2022-04-11 12:40:01,507 - INFO - tqdm - accuracy: 0.9904, batch_loss: 0.0025, loss: 0.0276 ||:  80%|########  | 52/65 [09:59<02:29, 11.50s/it]
2022-04-11 12:40:13,061 - INFO - tqdm - accuracy: 0.9906, batch_loss: 0.0041, loss: 0.0271 ||:  82%|########1 | 53/65 [10:11<02:18, 11.52s/it]
2022-04-11 12:40:24,632 - INFO - tqdm - accuracy: 0.9907, batch_loss: 0.0121, loss: 0.0269 ||:  83%|########3 | 54/65 [10:22<02:06, 11.53s/it]
2022-04-11 12:40:36,189 - INFO - tqdm - accuracy: 0.9909, batch_loss: 0.0041, loss: 0.0265 ||:  85%|########4 | 55/65 [10:34<01:55, 11.54s/it]
2022-04-11 12:40:47,760 - INFO - tqdm - accuracy: 0.9911, batch_loss: 0.0209, loss: 0.0264 ||:  86%|########6 | 56/65 [10:45<01:43, 11.55s/it]
2022-04-11 12:40:59,348 - INFO - tqdm - accuracy: 0.9912, batch_loss: 0.0033, loss: 0.0260 ||:  88%|########7 | 57/65 [10:57<01:32, 11.56s/it]
2022-04-11 12:41:10,931 - INFO - tqdm - accuracy: 0.9914, batch_loss: 0.0173, loss: 0.0258 ||:  89%|########9 | 58/65 [11:08<01:20, 11.57s/it]
2022-04-11 12:41:22,523 - INFO - tqdm - accuracy: 0.9915, batch_loss: 0.0262, loss: 0.0258 ||:  91%|######### | 59/65 [11:20<01:09, 11.57s/it]
2022-04-11 12:41:34,093 - INFO - tqdm - accuracy: 0.9917, batch_loss: 0.0253, loss: 0.0258 ||:  92%|#########2| 60/65 [11:32<00:57, 11.57s/it]
2022-04-11 12:41:45,671 - INFO - tqdm - accuracy: 0.9918, batch_loss: 0.0077, loss: 0.0255 ||:  94%|#########3| 61/65 [11:43<00:46, 11.57s/it]
2022-04-11 12:41:57,258 - INFO - tqdm - accuracy: 0.9919, batch_loss: 0.0037, loss: 0.0252 ||:  95%|#########5| 62/65 [11:55<00:34, 11.58s/it]
2022-04-11 12:42:08,840 - INFO - tqdm - accuracy: 0.9916, batch_loss: 0.0383, loss: 0.0254 ||:  97%|#########6| 63/65 [12:06<00:23, 11.58s/it]
2022-04-11 12:42:20,414 - INFO - tqdm - accuracy: 0.9912, batch_loss: 0.0238, loss: 0.0253 ||:  98%|#########8| 64/65 [12:18<00:11, 11.58s/it]
2022-04-11 12:42:25,522 - INFO - tqdm - accuracy: 0.9913, batch_loss: 0.0143, loss: 0.0252 ||: 100%|##########| 65/65 [12:23<00:00,  9.64s/it]
2022-04-11 12:42:25,523 - INFO - tqdm - accuracy: 0.9913, batch_loss: 0.0143, loss: 0.0252 ||: 100%|##########| 65/65 [12:23<00:00, 11.44s/it]
2022-04-11 12:42:28,419 - INFO - allennlp.training.trainer - Validating
2022-04-11 12:42:28,421 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-11 12:42:38,484 - INFO - tqdm - accuracy: 0.6049, batch_loss: 2.5593, loss: 2.2047 ||:  36%|###5      | 41/115 [00:10<00:18,  3.99it/s]
2022-04-11 12:42:48,713 - INFO - tqdm - accuracy: 0.6319, batch_loss: 0.0027, loss: 2.0500 ||:  71%|#######1  | 82/115 [00:20<00:08,  4.01it/s]
2022-04-11 12:42:56,916 - INFO - tqdm - accuracy: 0.6419, batch_loss: 0.0439, loss: 1.8991 ||: 100%|##########| 115/115 [00:28<00:00,  4.03it/s]
2022-04-11 12:42:56,917 - INFO - tqdm - accuracy: 0.6419, batch_loss: 0.0439, loss: 1.8991 ||: 100%|##########| 115/115 [00:28<00:00,  4.04it/s]
2022-04-11 12:42:56,917 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-11 12:42:56,918 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.991  |     0.642
2022-04-11 12:42:56,919 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-11 12:42:56,919 - INFO - allennlp.training.tensorboard_writer - loss               |     0.025  |     1.899
2022-04-11 12:42:56,919 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5802.672  |       N/A
2022-04-11 12:43:03,139 - INFO - allennlp.training.trainer - Epoch duration: 0:13:01.180998
2022-04-11 12:43:03,139 - INFO - allennlp.training.trainer - Estimated training time remaining: 1:05:25
2022-04-11 12:43:03,139 - INFO - allennlp.training.trainer - Epoch 10/14
2022-04-11 12:43:03,140 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-11 12:43:03,140 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-11 12:43:03,141 - INFO - allennlp.training.trainer - Training
2022-04-11 12:43:03,141 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-11 12:43:14,182 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0021, loss: 0.0021 ||:   2%|1         | 1/65 [00:11<11:46, 11.04s/it]
2022-04-11 12:43:24,984 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0017, loss: 0.0019 ||:   3%|3         | 2/65 [00:21<11:26, 10.90s/it]
2022-04-11 12:43:36,071 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0052, loss: 0.0030 ||:   5%|4         | 3/65 [00:32<11:21, 10.99s/it]
2022-04-11 12:43:47,150 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0080, loss: 0.0043 ||:   6%|6         | 4/65 [00:44<11:12, 11.02s/it]
2022-04-11 12:43:58,093 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0033, loss: 0.0041 ||:   8%|7         | 5/65 [00:54<10:59, 10.99s/it]
2022-04-11 12:44:09,169 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0014, loss: 0.0036 ||:   9%|9         | 6/65 [01:06<10:50, 11.02s/it]
2022-04-11 12:44:20,228 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0229, loss: 0.0064 ||:  11%|#         | 7/65 [01:17<10:39, 11.03s/it]
2022-04-11 12:44:31,263 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0023, loss: 0.0059 ||:  12%|#2        | 8/65 [01:28<10:28, 11.03s/it]
2022-04-11 12:44:42,302 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0067, loss: 0.0060 ||:  14%|#3        | 9/65 [01:39<10:18, 11.04s/it]
2022-04-11 12:44:53,340 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0046, loss: 0.0058 ||:  15%|#5        | 10/65 [01:50<10:07, 11.04s/it]
2022-04-11 12:45:04,361 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0116, loss: 0.0063 ||:  17%|#6        | 11/65 [02:01<09:55, 11.03s/it]
2022-04-11 12:45:15,364 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0171, loss: 0.0072 ||:  18%|#8        | 12/65 [02:12<09:44, 11.02s/it]
2022-04-11 12:45:26,362 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0027, loss: 0.0069 ||:  20%|##        | 13/65 [02:23<09:32, 11.02s/it]
2022-04-11 12:45:37,364 - INFO - tqdm - accuracy: 0.9978, batch_loss: 0.0606, loss: 0.0107 ||:  22%|##1       | 14/65 [02:34<09:21, 11.01s/it]
2022-04-11 12:45:48,389 - INFO - tqdm - accuracy: 0.9979, batch_loss: 0.0084, loss: 0.0106 ||:  23%|##3       | 15/65 [02:45<09:10, 11.02s/it]
2022-04-11 12:45:59,382 - INFO - tqdm - accuracy: 0.9980, batch_loss: 0.0033, loss: 0.0101 ||:  25%|##4       | 16/65 [02:56<08:59, 11.01s/it]
2022-04-11 12:46:10,386 - INFO - tqdm - accuracy: 0.9982, batch_loss: 0.0008, loss: 0.0096 ||:  26%|##6       | 17/65 [03:07<08:48, 11.01s/it]
2022-04-11 12:46:21,370 - INFO - tqdm - accuracy: 0.9983, batch_loss: 0.0024, loss: 0.0092 ||:  28%|##7       | 18/65 [03:18<08:37, 11.00s/it]
2022-04-11 12:46:32,368 - INFO - tqdm - accuracy: 0.9984, batch_loss: 0.0042, loss: 0.0089 ||:  29%|##9       | 19/65 [03:29<08:25, 11.00s/it]
2022-04-11 12:46:43,340 - INFO - tqdm - accuracy: 0.9984, batch_loss: 0.0050, loss: 0.0087 ||:  31%|###       | 20/65 [03:40<08:14, 10.99s/it]
2022-04-11 12:46:54,311 - INFO - tqdm - accuracy: 0.9985, batch_loss: 0.0011, loss: 0.0084 ||:  32%|###2      | 21/65 [03:51<08:03, 10.99s/it]
2022-04-11 12:47:05,302 - INFO - tqdm - accuracy: 0.9986, batch_loss: 0.0058, loss: 0.0082 ||:  34%|###3      | 22/65 [04:02<07:52, 10.99s/it]
2022-04-11 12:47:15,936 - INFO - tqdm - accuracy: 0.9986, batch_loss: 0.0040, loss: 0.0081 ||:  35%|###5      | 23/65 [04:12<07:37, 10.88s/it]
2022-04-11 12:47:26,848 - INFO - tqdm - accuracy: 0.9987, batch_loss: 0.0014, loss: 0.0078 ||:  37%|###6      | 24/65 [04:23<07:26, 10.89s/it]
2022-04-11 12:47:37,785 - INFO - tqdm - accuracy: 0.9987, batch_loss: 0.0099, loss: 0.0079 ||:  38%|###8      | 25/65 [04:34<07:16, 10.90s/it]
2022-04-11 12:47:48,609 - INFO - tqdm - accuracy: 0.9976, batch_loss: 0.0692, loss: 0.0102 ||:  40%|####      | 26/65 [04:45<07:04, 10.88s/it]
2022-04-11 12:47:59,542 - INFO - tqdm - accuracy: 0.9977, batch_loss: 0.0035, loss: 0.0100 ||:  42%|####1     | 27/65 [04:56<06:54, 10.90s/it]
2022-04-11 12:48:10,497 - INFO - tqdm - accuracy: 0.9978, batch_loss: 0.0067, loss: 0.0099 ||:  43%|####3     | 28/65 [05:07<06:43, 10.91s/it]
2022-04-11 12:48:21,444 - INFO - tqdm - accuracy: 0.9978, batch_loss: 0.0139, loss: 0.0100 ||:  45%|####4     | 29/65 [05:18<06:33, 10.92s/it]
2022-04-11 12:48:32,404 - INFO - tqdm - accuracy: 0.9979, batch_loss: 0.0149, loss: 0.0102 ||:  46%|####6     | 30/65 [05:29<06:22, 10.93s/it]
2022-04-11 12:48:43,359 - INFO - tqdm - accuracy: 0.9980, batch_loss: 0.0138, loss: 0.0103 ||:  48%|####7     | 31/65 [05:40<06:11, 10.94s/it]
2022-04-11 12:48:54,308 - INFO - tqdm - accuracy: 0.9980, batch_loss: 0.0022, loss: 0.0100 ||:  49%|####9     | 32/65 [05:51<06:01, 10.94s/it]
2022-04-11 12:49:05,264 - INFO - tqdm - accuracy: 0.9981, batch_loss: 0.0065, loss: 0.0099 ||:  51%|#####     | 33/65 [06:02<05:50, 10.95s/it]
2022-04-11 12:49:16,228 - INFO - tqdm - accuracy: 0.9982, batch_loss: 0.0044, loss: 0.0098 ||:  52%|#####2    | 34/65 [06:13<05:39, 10.95s/it]
2022-04-11 12:49:27,187 - INFO - tqdm - accuracy: 0.9982, batch_loss: 0.0017, loss: 0.0095 ||:  54%|#####3    | 35/65 [06:24<05:28, 10.95s/it]
2022-04-11 12:49:38,157 - INFO - tqdm - accuracy: 0.9983, batch_loss: 0.0061, loss: 0.0094 ||:  55%|#####5    | 36/65 [06:35<05:17, 10.96s/it]
2022-04-11 12:49:49,131 - INFO - tqdm - accuracy: 0.9983, batch_loss: 0.0010, loss: 0.0092 ||:  57%|#####6    | 37/65 [06:45<05:06, 10.96s/it]
2022-04-11 12:50:00,087 - INFO - tqdm - accuracy: 0.9984, batch_loss: 0.0060, loss: 0.0091 ||:  58%|#####8    | 38/65 [06:56<04:55, 10.96s/it]
2022-04-11 12:50:11,052 - INFO - tqdm - accuracy: 0.9984, batch_loss: 0.0013, loss: 0.0089 ||:  60%|######    | 39/65 [07:07<04:45, 10.96s/it]
2022-04-11 12:50:22,009 - INFO - tqdm - accuracy: 0.9984, batch_loss: 0.0022, loss: 0.0087 ||:  62%|######1   | 40/65 [07:18<04:34, 10.96s/it]
2022-04-11 12:50:32,985 - INFO - tqdm - accuracy: 0.9985, batch_loss: 0.0115, loss: 0.0088 ||:  63%|######3   | 41/65 [07:29<04:23, 10.97s/it]
2022-04-11 12:50:43,910 - INFO - tqdm - accuracy: 0.9985, batch_loss: 0.0034, loss: 0.0087 ||:  65%|######4   | 42/65 [07:40<04:11, 10.95s/it]
2022-04-11 12:50:54,838 - INFO - tqdm - accuracy: 0.9985, batch_loss: 0.0016, loss: 0.0085 ||:  66%|######6   | 43/65 [07:51<04:00, 10.95s/it]
2022-04-11 12:51:05,761 - INFO - tqdm - accuracy: 0.9986, batch_loss: 0.0010, loss: 0.0083 ||:  68%|######7   | 44/65 [08:02<03:49, 10.94s/it]
2022-04-11 12:51:16,689 - INFO - tqdm - accuracy: 0.9986, batch_loss: 0.0101, loss: 0.0084 ||:  69%|######9   | 45/65 [08:13<03:38, 10.94s/it]
2022-04-11 12:51:27,596 - INFO - tqdm - accuracy: 0.9980, batch_loss: 0.1158, loss: 0.0107 ||:  71%|#######   | 46/65 [08:24<03:27, 10.93s/it]
2022-04-11 12:51:38,491 - INFO - tqdm - accuracy: 0.9980, batch_loss: 0.0134, loss: 0.0108 ||:  72%|#######2  | 47/65 [08:35<03:16, 10.92s/it]
2022-04-11 12:51:49,414 - INFO - tqdm - accuracy: 0.9980, batch_loss: 0.0035, loss: 0.0106 ||:  74%|#######3  | 48/65 [08:46<03:05, 10.92s/it]
2022-04-11 12:52:00,353 - INFO - tqdm - accuracy: 0.9981, batch_loss: 0.0010, loss: 0.0104 ||:  75%|#######5  | 49/65 [08:57<02:54, 10.93s/it]
2022-04-11 12:52:11,777 - INFO - tqdm - accuracy: 0.9981, batch_loss: 0.0013, loss: 0.0102 ||:  77%|#######6  | 50/65 [09:08<02:46, 11.07s/it]
2022-04-11 12:52:22,666 - INFO - tqdm - accuracy: 0.9982, batch_loss: 0.0008, loss: 0.0101 ||:  78%|#######8  | 51/65 [09:19<02:34, 11.02s/it]
2022-04-11 12:52:33,591 - INFO - tqdm - accuracy: 0.9982, batch_loss: 0.0044, loss: 0.0100 ||:  80%|########  | 52/65 [09:30<02:22, 10.99s/it]
2022-04-11 12:52:44,520 - INFO - tqdm - accuracy: 0.9982, batch_loss: 0.0011, loss: 0.0098 ||:  82%|########1 | 53/65 [09:41<02:11, 10.97s/it]
2022-04-11 12:52:55,460 - INFO - tqdm - accuracy: 0.9977, batch_loss: 0.0276, loss: 0.0101 ||:  83%|########3 | 54/65 [09:52<02:00, 10.96s/it]
2022-04-11 12:53:06,386 - INFO - tqdm - accuracy: 0.9977, batch_loss: 0.0009, loss: 0.0100 ||:  85%|########4 | 55/65 [10:03<01:49, 10.95s/it]
2022-04-11 12:53:17,322 - INFO - tqdm - accuracy: 0.9978, batch_loss: 0.0007, loss: 0.0098 ||:  86%|########6 | 56/65 [10:14<01:38, 10.95s/it]
2022-04-11 12:53:28,265 - INFO - tqdm - accuracy: 0.9978, batch_loss: 0.0018, loss: 0.0096 ||:  88%|########7 | 57/65 [10:25<01:27, 10.95s/it]
2022-04-11 12:53:39,217 - INFO - tqdm - accuracy: 0.9973, batch_loss: 0.0518, loss: 0.0104 ||:  89%|########9 | 58/65 [10:36<01:16, 10.95s/it]
2022-04-11 12:53:50,158 - INFO - tqdm - accuracy: 0.9974, batch_loss: 0.0039, loss: 0.0103 ||:  91%|######### | 59/65 [10:47<01:05, 10.95s/it]
2022-04-11 12:54:01,102 - INFO - tqdm - accuracy: 0.9974, batch_loss: 0.0009, loss: 0.0101 ||:  92%|#########2| 60/65 [10:57<00:54, 10.95s/it]
2022-04-11 12:54:12,061 - INFO - tqdm - accuracy: 0.9974, batch_loss: 0.0020, loss: 0.0100 ||:  94%|#########3| 61/65 [11:08<00:43, 10.95s/it]
2022-04-11 12:54:23,015 - INFO - tqdm - accuracy: 0.9975, batch_loss: 0.0015, loss: 0.0098 ||:  95%|#########5| 62/65 [11:19<00:32, 10.95s/it]
2022-04-11 12:54:33,972 - INFO - tqdm - accuracy: 0.9975, batch_loss: 0.0064, loss: 0.0098 ||:  97%|#########6| 63/65 [11:30<00:21, 10.95s/it]
2022-04-11 12:54:44,921 - INFO - tqdm - accuracy: 0.9976, batch_loss: 0.0073, loss: 0.0097 ||:  98%|#########8| 64/65 [11:41<00:10, 10.95s/it]
2022-04-11 12:54:49,789 - INFO - tqdm - accuracy: 0.9976, batch_loss: 0.0009, loss: 0.0096 ||: 100%|##########| 65/65 [11:46<00:00,  9.13s/it]
2022-04-11 12:54:49,790 - INFO - tqdm - accuracy: 0.9976, batch_loss: 0.0009, loss: 0.0096 ||: 100%|##########| 65/65 [11:46<00:00, 10.87s/it]
2022-04-11 12:54:52,751 - INFO - allennlp.training.trainer - Validating
2022-04-11 12:54:52,753 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-11 12:55:02,812 - INFO - tqdm - accuracy: 0.7024, batch_loss: 0.0334, loss: 1.5699 ||:  37%|###6      | 42/115 [00:10<00:17,  4.17it/s]
2022-04-11 12:55:12,956 - INFO - tqdm - accuracy: 0.6450, batch_loss: 0.0024, loss: 2.0647 ||:  74%|#######3  | 85/115 [00:20<00:07,  4.15it/s]
2022-04-11 12:55:20,175 - INFO - tqdm - accuracy: 0.6463, batch_loss: 4.4093, loss: 2.1627 ||: 100%|##########| 115/115 [00:27<00:00,  4.16it/s]
2022-04-11 12:55:20,175 - INFO - tqdm - accuracy: 0.6463, batch_loss: 4.4093, loss: 2.1627 ||: 100%|##########| 115/115 [00:27<00:00,  4.19it/s]
2022-04-11 12:55:20,176 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-11 12:55:20,176 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.998  |     0.646
2022-04-11 12:55:20,177 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-11 12:55:20,177 - INFO - allennlp.training.tensorboard_writer - loss               |     0.010  |     2.163
2022-04-11 12:55:20,178 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5802.672  |       N/A
2022-04-11 12:55:26,525 - INFO - allennlp.training.trainer - Epoch duration: 0:12:23.385666
2022-04-11 12:55:26,525 - INFO - allennlp.training.trainer - Estimated training time remaining: 0:52:05
2022-04-11 12:55:26,526 - INFO - allennlp.training.trainer - Epoch 11/14
2022-04-11 12:55:26,526 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-11 12:55:26,526 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-11 12:55:26,527 - INFO - allennlp.training.trainer - Training
2022-04-11 12:55:26,528 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-11 12:55:37,329 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0006, loss: 0.0006 ||:   2%|1         | 1/65 [00:10<11:31, 10.80s/it]
2022-04-11 12:55:48,164 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0015, loss: 0.0011 ||:   3%|3         | 2/65 [00:21<11:21, 10.82s/it]
2022-04-11 12:55:59,027 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0061, loss: 0.0027 ||:   5%|4         | 3/65 [00:32<11:12, 10.84s/it]
2022-04-11 12:56:09,871 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0212, loss: 0.0073 ||:   6%|6         | 4/65 [00:43<11:01, 10.84s/it]
2022-04-11 12:56:20,743 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0044, loss: 0.0068 ||:   8%|7         | 5/65 [00:54<10:51, 10.85s/it]
2022-04-11 12:56:31,625 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0005, loss: 0.0057 ||:   9%|9         | 6/65 [01:05<10:40, 10.86s/it]
2022-04-11 12:56:42,505 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0023, loss: 0.0052 ||:  11%|#         | 7/65 [01:15<10:30, 10.87s/it]
2022-04-11 12:56:53,364 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0008, loss: 0.0047 ||:  12%|#2        | 8/65 [01:26<10:19, 10.87s/it]
2022-04-11 12:57:04,252 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0008, loss: 0.0043 ||:  14%|#3        | 9/65 [01:37<10:08, 10.87s/it]
2022-04-11 12:57:15,137 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0007, loss: 0.0039 ||:  15%|#5        | 10/65 [01:48<09:58, 10.88s/it]
2022-04-11 12:57:26,027 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0015, loss: 0.0037 ||:  17%|#6        | 11/65 [01:59<09:47, 10.88s/it]
2022-04-11 12:57:36,939 - INFO - tqdm - accuracy: 0.9974, batch_loss: 0.1139, loss: 0.0129 ||:  18%|#8        | 12/65 [02:10<09:37, 10.89s/it]
2022-04-11 12:57:47,873 - INFO - tqdm - accuracy: 0.9976, batch_loss: 0.0051, loss: 0.0123 ||:  20%|##        | 13/65 [02:21<09:26, 10.90s/it]
2022-04-11 12:57:58,794 - INFO - tqdm - accuracy: 0.9978, batch_loss: 0.0006, loss: 0.0114 ||:  22%|##1       | 14/65 [02:32<09:16, 10.91s/it]
2022-04-11 12:58:09,731 - INFO - tqdm - accuracy: 0.9979, batch_loss: 0.0012, loss: 0.0108 ||:  23%|##3       | 15/65 [02:43<09:05, 10.92s/it]
2022-04-11 12:58:20,561 - INFO - tqdm - accuracy: 0.9980, batch_loss: 0.0008, loss: 0.0101 ||:  25%|##4       | 16/65 [02:54<08:53, 10.89s/it]
2022-04-11 12:58:31,494 - INFO - tqdm - accuracy: 0.9982, batch_loss: 0.0006, loss: 0.0096 ||:  26%|##6       | 17/65 [03:04<08:43, 10.90s/it]
2022-04-11 12:58:42,419 - INFO - tqdm - accuracy: 0.9983, batch_loss: 0.0019, loss: 0.0091 ||:  28%|##7       | 18/65 [03:15<08:32, 10.91s/it]
2022-04-11 12:58:53,377 - INFO - tqdm - accuracy: 0.9984, batch_loss: 0.0124, loss: 0.0093 ||:  29%|##9       | 19/65 [03:26<08:22, 10.92s/it]
2022-04-11 12:59:04,329 - INFO - tqdm - accuracy: 0.9969, batch_loss: 0.1641, loss: 0.0171 ||:  31%|###       | 20/65 [03:37<08:11, 10.93s/it]
2022-04-11 12:59:15,279 - INFO - tqdm - accuracy: 0.9970, batch_loss: 0.0017, loss: 0.0163 ||:  32%|###2      | 21/65 [03:48<08:01, 10.94s/it]
2022-04-11 12:59:26,232 - INFO - tqdm - accuracy: 0.9972, batch_loss: 0.0084, loss: 0.0160 ||:  34%|###3      | 22/65 [03:59<07:50, 10.94s/it]
2022-04-11 12:59:37,204 - INFO - tqdm - accuracy: 0.9973, batch_loss: 0.0046, loss: 0.0155 ||:  35%|###5      | 23/65 [04:10<07:39, 10.95s/it]
2022-04-11 12:59:48,166 - INFO - tqdm - accuracy: 0.9961, batch_loss: 0.0413, loss: 0.0165 ||:  37%|###6      | 24/65 [04:21<07:29, 10.95s/it]
2022-04-11 12:59:59,130 - INFO - tqdm - accuracy: 0.9962, batch_loss: 0.0008, loss: 0.0159 ||:  38%|###8      | 25/65 [04:32<07:18, 10.96s/it]
2022-04-11 13:00:10,104 - INFO - tqdm - accuracy: 0.9964, batch_loss: 0.0010, loss: 0.0153 ||:  40%|####      | 26/65 [04:43<07:07, 10.96s/it]
2022-04-11 13:00:21,074 - INFO - tqdm - accuracy: 0.9965, batch_loss: 0.0227, loss: 0.0156 ||:  42%|####1     | 27/65 [04:54<06:56, 10.96s/it]
2022-04-11 13:00:32,035 - INFO - tqdm - accuracy: 0.9967, batch_loss: 0.0012, loss: 0.0151 ||:  43%|####3     | 28/65 [05:05<06:45, 10.96s/it]
2022-04-11 13:00:43,001 - INFO - tqdm - accuracy: 0.9968, batch_loss: 0.0055, loss: 0.0148 ||:  45%|####4     | 29/65 [05:16<06:34, 10.96s/it]
2022-04-11 13:00:53,985 - INFO - tqdm - accuracy: 0.9969, batch_loss: 0.0032, loss: 0.0144 ||:  46%|####6     | 30/65 [05:27<06:23, 10.97s/it]
2022-04-11 13:01:04,963 - INFO - tqdm - accuracy: 0.9970, batch_loss: 0.0015, loss: 0.0140 ||:  48%|####7     | 31/65 [05:38<06:13, 10.97s/it]
2022-04-11 13:01:15,923 - INFO - tqdm - accuracy: 0.9971, batch_loss: 0.0148, loss: 0.0140 ||:  49%|####9     | 32/65 [05:49<06:01, 10.97s/it]
2022-04-11 13:01:26,850 - INFO - tqdm - accuracy: 0.9972, batch_loss: 0.0012, loss: 0.0136 ||:  51%|#####     | 33/65 [06:00<05:50, 10.96s/it]
2022-04-11 13:01:37,811 - INFO - tqdm - accuracy: 0.9972, batch_loss: 0.0013, loss: 0.0132 ||:  52%|#####2    | 34/65 [06:11<05:39, 10.96s/it]
2022-04-11 13:01:48,746 - INFO - tqdm - accuracy: 0.9973, batch_loss: 0.0013, loss: 0.0129 ||:  54%|#####3    | 35/65 [06:22<05:28, 10.95s/it]
2022-04-11 13:01:59,687 - INFO - tqdm - accuracy: 0.9974, batch_loss: 0.0008, loss: 0.0126 ||:  55%|#####5    | 36/65 [06:33<05:17, 10.95s/it]
2022-04-11 13:02:10,622 - INFO - tqdm - accuracy: 0.9975, batch_loss: 0.0051, loss: 0.0124 ||:  57%|#####6    | 37/65 [06:44<05:06, 10.94s/it]
2022-04-11 13:02:21,578 - INFO - tqdm - accuracy: 0.9967, batch_loss: 0.0570, loss: 0.0135 ||:  58%|#####8    | 38/65 [06:55<04:55, 10.95s/it]
2022-04-11 13:02:32,532 - INFO - tqdm - accuracy: 0.9968, batch_loss: 0.0018, loss: 0.0132 ||:  60%|######    | 39/65 [07:06<04:44, 10.95s/it]
2022-04-11 13:02:43,470 - INFO - tqdm - accuracy: 0.9969, batch_loss: 0.0042, loss: 0.0130 ||:  62%|######1   | 40/65 [07:16<04:33, 10.95s/it]
2022-04-11 13:02:54,429 - INFO - tqdm - accuracy: 0.9970, batch_loss: 0.0057, loss: 0.0128 ||:  63%|######3   | 41/65 [07:27<04:22, 10.95s/it]
2022-04-11 13:03:05,393 - INFO - tqdm - accuracy: 0.9970, batch_loss: 0.0013, loss: 0.0126 ||:  65%|######4   | 42/65 [07:38<04:11, 10.95s/it]
2022-04-11 13:03:16,243 - INFO - tqdm - accuracy: 0.9971, batch_loss: 0.0007, loss: 0.0123 ||:  66%|######6   | 43/65 [07:49<04:00, 10.92s/it]
2022-04-11 13:03:27,215 - INFO - tqdm - accuracy: 0.9972, batch_loss: 0.0046, loss: 0.0121 ||:  68%|######7   | 44/65 [08:00<03:49, 10.94s/it]
2022-04-11 13:03:37,851 - INFO - tqdm - accuracy: 0.9972, batch_loss: 0.0015, loss: 0.0119 ||:  69%|######9   | 45/65 [08:11<03:36, 10.85s/it]
2022-04-11 13:03:48,817 - INFO - tqdm - accuracy: 0.9973, batch_loss: 0.0027, loss: 0.0117 ||:  71%|#######   | 46/65 [08:22<03:26, 10.88s/it]
2022-04-11 13:03:59,739 - INFO - tqdm - accuracy: 0.9973, batch_loss: 0.0012, loss: 0.0115 ||:  72%|#######2  | 47/65 [08:33<03:16, 10.89s/it]
2022-04-11 13:04:10,678 - INFO - tqdm - accuracy: 0.9974, batch_loss: 0.0029, loss: 0.0113 ||:  74%|#######3  | 48/65 [08:44<03:05, 10.91s/it]
2022-04-11 13:04:21,620 - INFO - tqdm - accuracy: 0.9974, batch_loss: 0.0049, loss: 0.0111 ||:  75%|#######5  | 49/65 [08:55<02:54, 10.92s/it]
2022-04-11 13:04:32,554 - INFO - tqdm - accuracy: 0.9975, batch_loss: 0.0007, loss: 0.0109 ||:  77%|#######6  | 50/65 [09:06<02:43, 10.92s/it]
2022-04-11 13:04:43,492 - INFO - tqdm - accuracy: 0.9975, batch_loss: 0.0033, loss: 0.0108 ||:  78%|#######8  | 51/65 [09:16<02:32, 10.93s/it]
2022-04-11 13:04:54,423 - INFO - tqdm - accuracy: 0.9976, batch_loss: 0.0007, loss: 0.0106 ||:  80%|########  | 52/65 [09:27<02:22, 10.93s/it]
2022-04-11 13:05:05,361 - INFO - tqdm - accuracy: 0.9976, batch_loss: 0.0039, loss: 0.0105 ||:  82%|########1 | 53/65 [09:38<02:11, 10.93s/it]
2022-04-11 13:05:16,260 - INFO - tqdm - accuracy: 0.9971, batch_loss: 0.0518, loss: 0.0112 ||:  83%|########3 | 54/65 [09:49<02:00, 10.92s/it]
2022-04-11 13:05:27,167 - INFO - tqdm - accuracy: 0.9972, batch_loss: 0.0033, loss: 0.0111 ||:  85%|########4 | 55/65 [10:00<01:49, 10.92s/it]
2022-04-11 13:05:38,065 - INFO - tqdm - accuracy: 0.9972, batch_loss: 0.0012, loss: 0.0109 ||:  86%|########6 | 56/65 [10:11<01:38, 10.91s/it]
2022-04-11 13:05:48,759 - INFO - tqdm - accuracy: 0.9973, batch_loss: 0.0085, loss: 0.0109 ||:  88%|########7 | 57/65 [10:22<01:26, 10.85s/it]
2022-04-11 13:05:59,651 - INFO - tqdm - accuracy: 0.9973, batch_loss: 0.0048, loss: 0.0108 ||:  89%|########9 | 58/65 [10:33<01:16, 10.86s/it]
2022-04-11 13:06:10,563 - INFO - tqdm - accuracy: 0.9974, batch_loss: 0.0006, loss: 0.0106 ||:  91%|######### | 59/65 [10:44<01:05, 10.88s/it]
2022-04-11 13:06:21,477 - INFO - tqdm - accuracy: 0.9974, batch_loss: 0.0017, loss: 0.0104 ||:  92%|#########2| 60/65 [10:54<00:54, 10.89s/it]
2022-04-11 13:06:32,379 - INFO - tqdm - accuracy: 0.9974, batch_loss: 0.0006, loss: 0.0103 ||:  94%|#########3| 61/65 [11:05<00:43, 10.89s/it]
2022-04-11 13:06:43,283 - INFO - tqdm - accuracy: 0.9975, batch_loss: 0.0006, loss: 0.0101 ||:  95%|#########5| 62/65 [11:16<00:32, 10.90s/it]
2022-04-11 13:06:54,199 - INFO - tqdm - accuracy: 0.9975, batch_loss: 0.0006, loss: 0.0100 ||:  97%|#########6| 63/65 [11:27<00:21, 10.90s/it]
2022-04-11 13:07:05,118 - INFO - tqdm - accuracy: 0.9976, batch_loss: 0.0019, loss: 0.0098 ||:  98%|#########8| 64/65 [11:38<00:10, 10.91s/it]
2022-04-11 13:07:09,971 - INFO - tqdm - accuracy: 0.9976, batch_loss: 0.0050, loss: 0.0098 ||: 100%|##########| 65/65 [11:43<00:00,  9.09s/it]
2022-04-11 13:07:09,971 - INFO - tqdm - accuracy: 0.9976, batch_loss: 0.0050, loss: 0.0098 ||: 100%|##########| 65/65 [11:43<00:00, 10.82s/it]
2022-04-11 13:07:12,835 - INFO - allennlp.training.trainer - Validating
2022-04-11 13:07:12,836 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-11 13:07:22,865 - INFO - tqdm - accuracy: 0.6071, batch_loss: 0.0005, loss: 2.4802 ||:  37%|###6      | 42/115 [00:10<00:17,  4.21it/s]
2022-04-11 13:07:33,069 - INFO - tqdm - accuracy: 0.6213, batch_loss: 0.4409, loss: 2.2750 ||:  74%|#######3  | 85/115 [00:20<00:07,  4.27it/s]
2022-04-11 13:07:40,188 - INFO - tqdm - accuracy: 0.6288, batch_loss: 2.4277, loss: 2.2635 ||: 100%|##########| 115/115 [00:27<00:00,  4.17it/s]
2022-04-11 13:07:40,189 - INFO - tqdm - accuracy: 0.6288, batch_loss: 2.4277, loss: 2.2635 ||: 100%|##########| 115/115 [00:27<00:00,  4.20it/s]
2022-04-11 13:07:40,189 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-11 13:07:40,190 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.998  |     0.629
2022-04-11 13:07:40,191 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-11 13:07:40,191 - INFO - allennlp.training.tensorboard_writer - loss               |     0.010  |     2.264
2022-04-11 13:07:40,192 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5802.672  |       N/A
2022-04-11 13:07:46,641 - INFO - allennlp.training.trainer - Epoch duration: 0:12:20.115908
2022-04-11 13:07:46,642 - INFO - allennlp.training.trainer - Estimated training time remaining: 0:38:53
2022-04-11 13:07:46,642 - INFO - allennlp.training.trainer - Epoch 12/14
2022-04-11 13:07:46,642 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-11 13:07:46,642 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-11 13:07:46,644 - INFO - allennlp.training.trainer - Training
2022-04-11 13:07:46,644 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-11 13:07:57,458 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.0516, loss: 0.0516 ||:   2%|1         | 1/65 [00:10<11:32, 10.81s/it]
2022-04-11 13:08:08,312 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.0556, loss: 0.0536 ||:   3%|3         | 2/65 [00:21<11:22, 10.84s/it]
2022-04-11 13:08:19,056 - INFO - tqdm - accuracy: 0.9792, batch_loss: 0.0007, loss: 0.0360 ||:   5%|4         | 3/65 [00:32<11:09, 10.79s/it]
2022-04-11 13:08:29,922 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0047, loss: 0.0281 ||:   6%|6         | 4/65 [00:43<11:00, 10.82s/it]
2022-04-11 13:08:40,801 - INFO - tqdm - accuracy: 0.9875, batch_loss: 0.0046, loss: 0.0234 ||:   8%|7         | 5/65 [00:54<10:50, 10.84s/it]
2022-04-11 13:08:51,676 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.0018, loss: 0.0198 ||:   9%|9         | 6/65 [01:05<10:40, 10.85s/it]
2022-04-11 13:09:02,553 - INFO - tqdm - accuracy: 0.9911, batch_loss: 0.0018, loss: 0.0172 ||:  11%|#         | 7/65 [01:15<10:29, 10.86s/it]
2022-04-11 13:09:13,417 - INFO - tqdm - accuracy: 0.9922, batch_loss: 0.0010, loss: 0.0152 ||:  12%|#2        | 8/65 [01:26<10:19, 10.86s/it]
2022-04-11 13:09:24,304 - INFO - tqdm - accuracy: 0.9931, batch_loss: 0.0018, loss: 0.0137 ||:  14%|#3        | 9/65 [01:37<10:08, 10.87s/it]
2022-04-11 13:09:35,207 - INFO - tqdm - accuracy: 0.9938, batch_loss: 0.0007, loss: 0.0124 ||:  15%|#5        | 10/65 [01:48<09:58, 10.88s/it]
2022-04-11 13:09:46,113 - INFO - tqdm - accuracy: 0.9943, batch_loss: 0.0013, loss: 0.0114 ||:  17%|#6        | 11/65 [01:59<09:47, 10.89s/it]
2022-04-11 13:09:56,992 - INFO - tqdm - accuracy: 0.9948, batch_loss: 0.0012, loss: 0.0106 ||:  18%|#8        | 12/65 [02:10<09:36, 10.89s/it]
2022-04-11 13:10:07,564 - INFO - tqdm - accuracy: 0.9952, batch_loss: 0.0005, loss: 0.0098 ||:  20%|##        | 13/65 [02:20<09:21, 10.79s/it]
2022-04-11 13:10:18,487 - INFO - tqdm - accuracy: 0.9955, batch_loss: 0.0041, loss: 0.0094 ||:  22%|##1       | 14/65 [02:31<09:12, 10.83s/it]
2022-04-11 13:10:29,384 - INFO - tqdm - accuracy: 0.9958, batch_loss: 0.0005, loss: 0.0088 ||:  23%|##3       | 15/65 [02:42<09:02, 10.85s/it]
2022-04-11 13:10:40,296 - INFO - tqdm - accuracy: 0.9961, batch_loss: 0.0012, loss: 0.0083 ||:  25%|##4       | 16/65 [02:53<08:52, 10.87s/it]
2022-04-11 13:10:51,207 - INFO - tqdm - accuracy: 0.9963, batch_loss: 0.0004, loss: 0.0078 ||:  26%|##6       | 17/65 [03:04<08:42, 10.88s/it]
2022-04-11 13:11:02,150 - INFO - tqdm - accuracy: 0.9965, batch_loss: 0.0039, loss: 0.0076 ||:  28%|##7       | 18/65 [03:15<08:32, 10.90s/it]
2022-04-11 13:11:13,100 - INFO - tqdm - accuracy: 0.9951, batch_loss: 0.0333, loss: 0.0090 ||:  29%|##9       | 19/65 [03:26<08:22, 10.91s/it]
2022-04-11 13:11:24,468 - INFO - tqdm - accuracy: 0.9953, batch_loss: 0.0011, loss: 0.0086 ||:  31%|###       | 20/65 [03:37<08:17, 11.05s/it]
2022-04-11 13:11:35,395 - INFO - tqdm - accuracy: 0.9955, batch_loss: 0.0011, loss: 0.0082 ||:  32%|###2      | 21/65 [03:48<08:04, 11.01s/it]
2022-04-11 13:11:46,318 - INFO - tqdm - accuracy: 0.9943, batch_loss: 0.1920, loss: 0.0166 ||:  34%|###3      | 22/65 [03:59<07:52, 10.99s/it]
2022-04-11 13:11:57,267 - INFO - tqdm - accuracy: 0.9946, batch_loss: 0.0096, loss: 0.0163 ||:  35%|###5      | 23/65 [04:10<07:40, 10.98s/it]
2022-04-11 13:12:08,220 - INFO - tqdm - accuracy: 0.9948, batch_loss: 0.0022, loss: 0.0157 ||:  37%|###6      | 24/65 [04:21<07:29, 10.97s/it]
2022-04-11 13:12:18,928 - INFO - tqdm - accuracy: 0.9937, batch_loss: 0.0886, loss: 0.0186 ||:  38%|###8      | 25/65 [04:32<07:15, 10.89s/it]
2022-04-11 13:12:29,868 - INFO - tqdm - accuracy: 0.9940, batch_loss: 0.0039, loss: 0.0180 ||:  40%|####      | 26/65 [04:43<07:05, 10.91s/it]
2022-04-11 13:12:40,799 - INFO - tqdm - accuracy: 0.9942, batch_loss: 0.0007, loss: 0.0174 ||:  42%|####1     | 27/65 [04:54<06:54, 10.91s/it]
2022-04-11 13:12:51,745 - INFO - tqdm - accuracy: 0.9944, batch_loss: 0.0029, loss: 0.0169 ||:  43%|####3     | 28/65 [05:05<06:44, 10.92s/it]
2022-04-11 13:13:02,666 - INFO - tqdm - accuracy: 0.9946, batch_loss: 0.0006, loss: 0.0163 ||:  45%|####4     | 29/65 [05:16<06:33, 10.92s/it]
2022-04-11 13:13:13,594 - INFO - tqdm - accuracy: 0.9948, batch_loss: 0.0027, loss: 0.0159 ||:  46%|####6     | 30/65 [05:26<06:22, 10.92s/it]
2022-04-11 13:13:24,535 - INFO - tqdm - accuracy: 0.9950, batch_loss: 0.0004, loss: 0.0154 ||:  48%|####7     | 31/65 [05:37<06:11, 10.93s/it]
2022-04-11 13:13:35,367 - INFO - tqdm - accuracy: 0.9941, batch_loss: 0.0318, loss: 0.0159 ||:  49%|####9     | 32/65 [05:48<05:59, 10.90s/it]
2022-04-11 13:13:46,308 - INFO - tqdm - accuracy: 0.9943, batch_loss: 0.0007, loss: 0.0154 ||:  51%|#####     | 33/65 [05:59<05:49, 10.91s/it]
2022-04-11 13:13:57,263 - INFO - tqdm - accuracy: 0.9945, batch_loss: 0.0011, loss: 0.0150 ||:  52%|#####2    | 34/65 [06:10<05:38, 10.93s/it]
2022-04-11 13:14:08,218 - INFO - tqdm - accuracy: 0.9937, batch_loss: 0.0893, loss: 0.0171 ||:  54%|#####3    | 35/65 [06:21<05:28, 10.93s/it]
2022-04-11 13:14:19,179 - INFO - tqdm - accuracy: 0.9930, batch_loss: 0.0710, loss: 0.0186 ||:  55%|#####5    | 36/65 [06:32<05:17, 10.94s/it]
2022-04-11 13:14:30,142 - INFO - tqdm - accuracy: 0.9932, batch_loss: 0.0005, loss: 0.0181 ||:  57%|#####6    | 37/65 [06:43<05:06, 10.95s/it]
2022-04-11 13:14:41,117 - INFO - tqdm - accuracy: 0.9934, batch_loss: 0.0044, loss: 0.0178 ||:  58%|#####8    | 38/65 [06:54<04:55, 10.96s/it]
2022-04-11 13:14:52,090 - INFO - tqdm - accuracy: 0.9936, batch_loss: 0.0011, loss: 0.0173 ||:  60%|######    | 39/65 [07:05<04:44, 10.96s/it]
2022-04-11 13:15:03,050 - INFO - tqdm - accuracy: 0.9937, batch_loss: 0.0049, loss: 0.0170 ||:  62%|######1   | 40/65 [07:16<04:34, 10.96s/it]
2022-04-11 13:15:14,035 - INFO - tqdm - accuracy: 0.9939, batch_loss: 0.0014, loss: 0.0166 ||:  63%|######3   | 41/65 [07:27<04:23, 10.97s/it]
2022-04-11 13:15:24,994 - INFO - tqdm - accuracy: 0.9940, batch_loss: 0.0024, loss: 0.0163 ||:  65%|######4   | 42/65 [07:38<04:12, 10.97s/it]
2022-04-11 13:15:35,969 - INFO - tqdm - accuracy: 0.9942, batch_loss: 0.0005, loss: 0.0159 ||:  66%|######6   | 43/65 [07:49<04:01, 10.97s/it]
2022-04-11 13:15:46,955 - INFO - tqdm - accuracy: 0.9943, batch_loss: 0.0011, loss: 0.0156 ||:  68%|######7   | 44/65 [08:00<03:50, 10.97s/it]
2022-04-11 13:15:57,934 - INFO - tqdm - accuracy: 0.9944, batch_loss: 0.0034, loss: 0.0153 ||:  69%|######9   | 45/65 [08:11<03:39, 10.98s/it]
2022-04-11 13:16:08,914 - INFO - tqdm - accuracy: 0.9946, batch_loss: 0.0007, loss: 0.0150 ||:  71%|#######   | 46/65 [08:22<03:28, 10.98s/it]
2022-04-11 13:16:19,904 - INFO - tqdm - accuracy: 0.9947, batch_loss: 0.0006, loss: 0.0147 ||:  72%|#######2  | 47/65 [08:33<03:17, 10.98s/it]
2022-04-11 13:16:30,870 - INFO - tqdm - accuracy: 0.9948, batch_loss: 0.0068, loss: 0.0145 ||:  74%|#######3  | 48/65 [08:44<03:06, 10.98s/it]
2022-04-11 13:16:41,850 - INFO - tqdm - accuracy: 0.9949, batch_loss: 0.0005, loss: 0.0143 ||:  75%|#######5  | 49/65 [08:55<02:55, 10.98s/it]
2022-04-11 13:16:52,834 - INFO - tqdm - accuracy: 0.9950, batch_loss: 0.0007, loss: 0.0140 ||:  77%|#######6  | 50/65 [09:06<02:44, 10.98s/it]
2022-04-11 13:17:03,795 - INFO - tqdm - accuracy: 0.9951, batch_loss: 0.0136, loss: 0.0140 ||:  78%|#######8  | 51/65 [09:17<02:33, 10.97s/it]
2022-04-11 13:17:14,760 - INFO - tqdm - accuracy: 0.9952, batch_loss: 0.0009, loss: 0.0137 ||:  80%|########  | 52/65 [09:28<02:22, 10.97s/it]
2022-04-11 13:17:25,737 - INFO - tqdm - accuracy: 0.9953, batch_loss: 0.0014, loss: 0.0135 ||:  82%|########1 | 53/65 [09:39<02:11, 10.97s/it]
2022-04-11 13:17:36,703 - INFO - tqdm - accuracy: 0.9954, batch_loss: 0.0013, loss: 0.0133 ||:  83%|########3 | 54/65 [09:50<02:00, 10.97s/it]
2022-04-11 13:17:47,659 - INFO - tqdm - accuracy: 0.9955, batch_loss: 0.0005, loss: 0.0130 ||:  85%|########4 | 55/65 [10:01<01:49, 10.97s/it]
2022-04-11 13:17:58,619 - INFO - tqdm - accuracy: 0.9955, batch_loss: 0.0015, loss: 0.0128 ||:  86%|########6 | 56/65 [10:11<01:38, 10.96s/it]
2022-04-11 13:18:09,569 - INFO - tqdm - accuracy: 0.9956, batch_loss: 0.0029, loss: 0.0127 ||:  88%|########7 | 57/65 [10:22<01:27, 10.96s/it]
2022-04-11 13:18:20,503 - INFO - tqdm - accuracy: 0.9957, batch_loss: 0.0147, loss: 0.0127 ||:  89%|########9 | 58/65 [10:33<01:16, 10.95s/it]
2022-04-11 13:18:31,442 - INFO - tqdm - accuracy: 0.9958, batch_loss: 0.0011, loss: 0.0125 ||:  91%|######### | 59/65 [10:44<01:05, 10.95s/it]
2022-04-11 13:18:42,382 - INFO - tqdm - accuracy: 0.9958, batch_loss: 0.0182, loss: 0.0126 ||:  92%|#########2| 60/65 [10:55<00:54, 10.95s/it]
2022-04-11 13:18:53,319 - INFO - tqdm - accuracy: 0.9959, batch_loss: 0.0005, loss: 0.0124 ||:  94%|#########3| 61/65 [11:06<00:43, 10.94s/it]
2022-04-11 13:19:04,245 - INFO - tqdm - accuracy: 0.9960, batch_loss: 0.0042, loss: 0.0123 ||:  95%|#########5| 62/65 [11:17<00:32, 10.94s/it]
2022-04-11 13:19:15,186 - INFO - tqdm - accuracy: 0.9955, batch_loss: 0.1025, loss: 0.0137 ||:  97%|#########6| 63/65 [11:28<00:21, 10.94s/it]
2022-04-11 13:19:26,020 - INFO - tqdm - accuracy: 0.9956, batch_loss: 0.0127, loss: 0.0137 ||:  98%|#########8| 64/65 [11:39<00:10, 10.91s/it]
2022-04-11 13:19:30,881 - INFO - tqdm - accuracy: 0.9956, batch_loss: 0.0006, loss: 0.0135 ||: 100%|##########| 65/65 [11:44<00:00,  9.09s/it]
2022-04-11 13:19:30,881 - INFO - tqdm - accuracy: 0.9956, batch_loss: 0.0006, loss: 0.0135 ||: 100%|##########| 65/65 [11:44<00:00, 10.83s/it]
2022-04-11 13:19:33,745 - INFO - allennlp.training.trainer - Validating
2022-04-11 13:19:33,746 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-11 13:19:43,867 - INFO - tqdm - accuracy: 0.7529, batch_loss: 4.9774, loss: 1.5461 ||:  37%|###7      | 43/115 [00:10<00:17,  4.17it/s]
2022-04-11 13:19:53,976 - INFO - tqdm - accuracy: 0.6864, batch_loss: 0.0016, loss: 1.8353 ||:  74%|#######3  | 85/115 [00:20<00:07,  4.17it/s]
2022-04-11 13:20:01,204 - INFO - tqdm - accuracy: 0.6550, batch_loss: 0.2374, loss: 2.0729 ||: 100%|##########| 115/115 [00:27<00:00,  4.15it/s]
2022-04-11 13:20:01,205 - INFO - tqdm - accuracy: 0.6550, batch_loss: 0.2374, loss: 2.0729 ||: 100%|##########| 115/115 [00:27<00:00,  4.19it/s]
2022-04-11 13:20:01,205 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-11 13:20:01,206 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.996  |     0.655
2022-04-11 13:20:01,207 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-11 13:20:01,207 - INFO - allennlp.training.tensorboard_writer - loss               |     0.013  |     2.073
2022-04-11 13:20:01,208 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5802.672  |       N/A
2022-04-11 13:20:07,049 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to './output_ir-ora-d_hyper6/best.th'.
2022-04-11 13:20:25,164 - INFO - allennlp.training.trainer - Epoch duration: 0:12:38.522457
2022-04-11 13:20:25,165 - INFO - allennlp.training.trainer - Estimated training time remaining: 0:25:52
2022-04-11 13:20:25,165 - INFO - allennlp.training.trainer - Epoch 13/14
2022-04-11 13:20:25,165 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-11 13:20:25,165 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-11 13:20:25,166 - INFO - allennlp.training.trainer - Training
2022-04-11 13:20:25,167 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-11 13:20:35,742 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0010, loss: 0.0010 ||:   2%|1         | 1/65 [00:10<11:16, 10.57s/it]
2022-04-11 13:20:46,369 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0285, loss: 0.0147 ||:   3%|3         | 2/65 [00:21<11:08, 10.61s/it]
2022-04-11 13:20:57,034 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.0008, loss: 0.0101 ||:   5%|4         | 3/65 [00:31<10:59, 10.63s/it]
2022-04-11 13:21:07,737 - INFO - tqdm - accuracy: 0.9922, batch_loss: 0.0011, loss: 0.0078 ||:   6%|6         | 4/65 [00:42<10:50, 10.66s/it]
2022-04-11 13:21:18,459 - INFO - tqdm - accuracy: 0.9938, batch_loss: 0.0014, loss: 0.0065 ||:   8%|7         | 5/65 [00:53<10:40, 10.68s/it]
2022-04-11 13:21:29,221 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.0251, loss: 0.0096 ||:   9%|9         | 6/65 [01:04<10:31, 10.71s/it]
2022-04-11 13:21:39,997 - INFO - tqdm - accuracy: 0.9911, batch_loss: 0.0037, loss: 0.0088 ||:  11%|#         | 7/65 [01:14<10:22, 10.73s/it]
2022-04-11 13:21:50,789 - INFO - tqdm - accuracy: 0.9922, batch_loss: 0.0064, loss: 0.0085 ||:  12%|#2        | 8/65 [01:25<10:12, 10.75s/it]
2022-04-11 13:22:01,581 - INFO - tqdm - accuracy: 0.9931, batch_loss: 0.0010, loss: 0.0077 ||:  14%|#3        | 9/65 [01:36<10:02, 10.76s/it]
2022-04-11 13:22:12,400 - INFO - tqdm - accuracy: 0.9938, batch_loss: 0.0054, loss: 0.0074 ||:  15%|#5        | 10/65 [01:47<09:52, 10.78s/it]
2022-04-11 13:22:23,217 - INFO - tqdm - accuracy: 0.9943, batch_loss: 0.0027, loss: 0.0070 ||:  17%|#6        | 11/65 [01:58<09:42, 10.79s/it]
2022-04-11 13:22:34,053 - INFO - tqdm - accuracy: 0.9948, batch_loss: 0.0049, loss: 0.0068 ||:  18%|#8        | 12/65 [02:08<09:32, 10.81s/it]
2022-04-11 13:22:44,898 - INFO - tqdm - accuracy: 0.9952, batch_loss: 0.0008, loss: 0.0064 ||:  20%|##        | 13/65 [02:19<09:22, 10.82s/it]
2022-04-11 13:22:55,748 - INFO - tqdm - accuracy: 0.9955, batch_loss: 0.0135, loss: 0.0069 ||:  22%|##1       | 14/65 [02:30<09:12, 10.83s/it]
2022-04-11 13:23:06,623 - INFO - tqdm - accuracy: 0.9958, batch_loss: 0.0004, loss: 0.0064 ||:  23%|##3       | 15/65 [02:41<09:02, 10.84s/it]
2022-04-11 13:23:17,523 - INFO - tqdm - accuracy: 0.9961, batch_loss: 0.0006, loss: 0.0061 ||:  25%|##4       | 16/65 [02:52<08:52, 10.86s/it]
2022-04-11 13:23:28,418 - INFO - tqdm - accuracy: 0.9963, batch_loss: 0.0018, loss: 0.0058 ||:  26%|##6       | 17/65 [03:03<08:41, 10.87s/it]
2022-04-11 13:23:39,016 - INFO - tqdm - accuracy: 0.9965, batch_loss: 0.0010, loss: 0.0056 ||:  28%|##7       | 18/65 [03:13<08:27, 10.79s/it]
2022-04-11 13:23:49,931 - INFO - tqdm - accuracy: 0.9967, batch_loss: 0.0092, loss: 0.0057 ||:  29%|##9       | 19/65 [03:24<08:18, 10.83s/it]
2022-04-11 13:24:00,855 - INFO - tqdm - accuracy: 0.9969, batch_loss: 0.0012, loss: 0.0055 ||:  31%|###       | 20/65 [03:35<08:08, 10.86s/it]
2022-04-11 13:24:11,785 - INFO - tqdm - accuracy: 0.9970, batch_loss: 0.0040, loss: 0.0054 ||:  32%|###2      | 21/65 [03:46<07:58, 10.88s/it]
2022-04-11 13:24:22,732 - INFO - tqdm - accuracy: 0.9957, batch_loss: 0.0485, loss: 0.0074 ||:  34%|###3      | 22/65 [03:57<07:48, 10.90s/it]
2022-04-11 13:24:33,681 - INFO - tqdm - accuracy: 0.9959, batch_loss: 0.0012, loss: 0.0071 ||:  35%|###5      | 23/65 [04:08<07:38, 10.91s/it]
2022-04-11 13:24:44,608 - INFO - tqdm - accuracy: 0.9961, batch_loss: 0.0004, loss: 0.0069 ||:  37%|###6      | 24/65 [04:19<07:27, 10.92s/it]
2022-04-11 13:24:55,563 - INFO - tqdm - accuracy: 0.9962, batch_loss: 0.0010, loss: 0.0066 ||:  38%|###8      | 25/65 [04:30<07:17, 10.93s/it]
2022-04-11 13:25:06,527 - INFO - tqdm - accuracy: 0.9952, batch_loss: 0.0408, loss: 0.0079 ||:  40%|####      | 26/65 [04:41<07:06, 10.94s/it]
2022-04-11 13:25:17,484 - INFO - tqdm - accuracy: 0.9942, batch_loss: 0.0395, loss: 0.0091 ||:  42%|####1     | 27/65 [04:52<06:55, 10.94s/it]
2022-04-11 13:25:28,427 - INFO - tqdm - accuracy: 0.9933, batch_loss: 0.1437, loss: 0.0139 ||:  43%|####3     | 28/65 [05:03<06:44, 10.94s/it]
2022-04-11 13:25:39,363 - INFO - tqdm - accuracy: 0.9935, batch_loss: 0.0026, loss: 0.0135 ||:  45%|####4     | 29/65 [05:14<06:33, 10.94s/it]
2022-04-11 13:25:50,305 - INFO - tqdm - accuracy: 0.9927, batch_loss: 0.1492, loss: 0.0180 ||:  46%|####6     | 30/65 [05:25<06:22, 10.94s/it]
2022-04-11 13:26:01,276 - INFO - tqdm - accuracy: 0.9929, batch_loss: 0.0006, loss: 0.0175 ||:  48%|####7     | 31/65 [05:36<06:12, 10.95s/it]
2022-04-11 13:26:12,220 - INFO - tqdm - accuracy: 0.9932, batch_loss: 0.0165, loss: 0.0174 ||:  49%|####9     | 32/65 [05:47<06:01, 10.95s/it]
2022-04-11 13:26:23,153 - INFO - tqdm - accuracy: 0.9934, batch_loss: 0.0200, loss: 0.0175 ||:  51%|#####     | 33/65 [05:57<05:50, 10.94s/it]
2022-04-11 13:26:33,967 - INFO - tqdm - accuracy: 0.9936, batch_loss: 0.0061, loss: 0.0172 ||:  52%|#####2    | 34/65 [06:08<05:38, 10.90s/it]
2022-04-11 13:26:44,899 - INFO - tqdm - accuracy: 0.9929, batch_loss: 0.0625, loss: 0.0185 ||:  54%|#####3    | 35/65 [06:19<05:27, 10.91s/it]
2022-04-11 13:26:55,837 - INFO - tqdm - accuracy: 0.9922, batch_loss: 0.1134, loss: 0.0211 ||:  55%|#####5    | 36/65 [06:30<05:16, 10.92s/it]
2022-04-11 13:27:06,771 - INFO - tqdm - accuracy: 0.9924, batch_loss: 0.0004, loss: 0.0206 ||:  57%|#####6    | 37/65 [06:41<05:05, 10.92s/it]
2022-04-11 13:27:17,696 - INFO - tqdm - accuracy: 0.9918, batch_loss: 0.1294, loss: 0.0234 ||:  58%|#####8    | 38/65 [06:52<04:54, 10.92s/it]
2022-04-11 13:27:28,616 - INFO - tqdm - accuracy: 0.9920, batch_loss: 0.0090, loss: 0.0231 ||:  60%|######    | 39/65 [07:03<04:44, 10.92s/it]
2022-04-11 13:27:39,530 - INFO - tqdm - accuracy: 0.9922, batch_loss: 0.0036, loss: 0.0226 ||:  62%|######1   | 40/65 [07:14<04:33, 10.92s/it]
2022-04-11 13:27:50,449 - INFO - tqdm - accuracy: 0.9924, batch_loss: 0.0005, loss: 0.0220 ||:  63%|######3   | 41/65 [07:25<04:22, 10.92s/it]
2022-04-11 13:28:01,407 - INFO - tqdm - accuracy: 0.9926, batch_loss: 0.0009, loss: 0.0215 ||:  65%|######4   | 42/65 [07:36<04:11, 10.93s/it]
2022-04-11 13:28:12,328 - INFO - tqdm - accuracy: 0.9927, batch_loss: 0.0006, loss: 0.0210 ||:  66%|######6   | 43/65 [07:47<04:00, 10.93s/it]
2022-04-11 13:28:23,269 - INFO - tqdm - accuracy: 0.9929, batch_loss: 0.0023, loss: 0.0206 ||:  68%|######7   | 44/65 [07:58<03:49, 10.93s/it]
2022-04-11 13:28:34,233 - INFO - tqdm - accuracy: 0.9931, batch_loss: 0.0123, loss: 0.0204 ||:  69%|######9   | 45/65 [08:09<03:38, 10.94s/it]
2022-04-11 13:28:44,942 - INFO - tqdm - accuracy: 0.9932, batch_loss: 0.0012, loss: 0.0200 ||:  71%|#######   | 46/65 [08:19<03:26, 10.87s/it]
2022-04-11 13:28:55,920 - INFO - tqdm - accuracy: 0.9933, batch_loss: 0.0009, loss: 0.0196 ||:  72%|#######2  | 47/65 [08:30<03:16, 10.90s/it]
2022-04-11 13:29:06,870 - INFO - tqdm - accuracy: 0.9935, batch_loss: 0.0016, loss: 0.0192 ||:  74%|#######3  | 48/65 [08:41<03:05, 10.92s/it]
2022-04-11 13:29:17,831 - INFO - tqdm - accuracy: 0.9936, batch_loss: 0.0123, loss: 0.0191 ||:  75%|#######5  | 49/65 [08:52<02:54, 10.93s/it]
2022-04-11 13:29:28,685 - INFO - tqdm - accuracy: 0.9937, batch_loss: 0.0053, loss: 0.0188 ||:  77%|#######6  | 50/65 [09:03<02:43, 10.91s/it]
2022-04-11 13:29:39,653 - INFO - tqdm - accuracy: 0.9939, batch_loss: 0.0029, loss: 0.0185 ||:  78%|#######8  | 51/65 [09:14<02:32, 10.93s/it]
2022-04-11 13:29:50,604 - INFO - tqdm - accuracy: 0.9940, batch_loss: 0.0016, loss: 0.0182 ||:  80%|########  | 52/65 [09:25<02:22, 10.93s/it]
2022-04-11 13:30:01,578 - INFO - tqdm - accuracy: 0.9935, batch_loss: 0.0647, loss: 0.0191 ||:  82%|########1 | 53/65 [09:36<02:11, 10.95s/it]
2022-04-11 13:30:12,518 - INFO - tqdm - accuracy: 0.9936, batch_loss: 0.0007, loss: 0.0187 ||:  83%|########3 | 54/65 [09:47<02:00, 10.94s/it]
2022-04-11 13:30:23,872 - INFO - tqdm - accuracy: 0.9937, batch_loss: 0.0044, loss: 0.0185 ||:  85%|########4 | 55/65 [09:58<01:50, 11.07s/it]
2022-04-11 13:30:34,772 - INFO - tqdm - accuracy: 0.9939, batch_loss: 0.0010, loss: 0.0181 ||:  86%|########6 | 56/65 [10:09<01:39, 11.02s/it]
2022-04-11 13:30:45,699 - INFO - tqdm - accuracy: 0.9940, batch_loss: 0.0015, loss: 0.0179 ||:  88%|########7 | 57/65 [10:20<01:27, 10.99s/it]
2022-04-11 13:30:56,632 - INFO - tqdm - accuracy: 0.9941, batch_loss: 0.0038, loss: 0.0176 ||:  89%|########9 | 58/65 [10:31<01:16, 10.97s/it]
2022-04-11 13:31:07,577 - INFO - tqdm - accuracy: 0.9942, batch_loss: 0.0018, loss: 0.0173 ||:  91%|######### | 59/65 [10:42<01:05, 10.96s/it]
2022-04-11 13:31:18,529 - INFO - tqdm - accuracy: 0.9943, batch_loss: 0.0004, loss: 0.0171 ||:  92%|#########2| 60/65 [10:53<00:54, 10.96s/it]
2022-04-11 13:31:29,486 - INFO - tqdm - accuracy: 0.9944, batch_loss: 0.0011, loss: 0.0168 ||:  94%|#########3| 61/65 [11:04<00:43, 10.96s/it]
2022-04-11 13:31:40,431 - INFO - tqdm - accuracy: 0.9945, batch_loss: 0.0017, loss: 0.0166 ||:  95%|#########5| 62/65 [11:15<00:32, 10.96s/it]
2022-04-11 13:31:51,410 - INFO - tqdm - accuracy: 0.9945, batch_loss: 0.0004, loss: 0.0163 ||:  97%|#########6| 63/65 [11:26<00:21, 10.96s/it]
2022-04-11 13:32:02,372 - INFO - tqdm - accuracy: 0.9946, batch_loss: 0.0023, loss: 0.0161 ||:  98%|#########8| 64/65 [11:37<00:10, 10.96s/it]
2022-04-11 13:32:07,249 - INFO - tqdm - accuracy: 0.9947, batch_loss: 0.0431, loss: 0.0165 ||: 100%|##########| 65/65 [11:42<00:00,  9.14s/it]
2022-04-11 13:32:07,249 - INFO - tqdm - accuracy: 0.9947, batch_loss: 0.0431, loss: 0.0165 ||: 100%|##########| 65/65 [11:42<00:00, 10.80s/it]
2022-04-11 13:32:10,111 - INFO - allennlp.training.trainer - Validating
2022-04-11 13:32:10,113 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-11 13:32:20,338 - INFO - tqdm - accuracy: 0.7059, batch_loss: 1.4375, loss: 1.6266 ||:  37%|###7      | 43/115 [00:10<00:17,  4.13it/s]
2022-04-11 13:32:30,464 - INFO - tqdm - accuracy: 0.6331, batch_loss: 0.0272, loss: 2.2048 ||:  74%|#######3  | 85/115 [00:20<00:07,  4.13it/s]
2022-04-11 13:32:37,623 - INFO - tqdm - accuracy: 0.6376, batch_loss: 0.1956, loss: 2.1054 ||: 100%|##########| 115/115 [00:27<00:00,  4.14it/s]
2022-04-11 13:32:37,623 - INFO - tqdm - accuracy: 0.6376, batch_loss: 0.1956, loss: 2.1054 ||: 100%|##########| 115/115 [00:27<00:00,  4.18it/s]
2022-04-11 13:32:37,623 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-11 13:32:37,624 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.995  |     0.638
2022-04-11 13:32:37,625 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-11 13:32:37,625 - INFO - allennlp.training.tensorboard_writer - loss               |     0.016  |     2.105
2022-04-11 13:32:37,626 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5802.672  |       N/A
2022-04-11 13:32:43,776 - INFO - allennlp.training.trainer - Epoch duration: 0:12:18.611047
2022-04-11 13:32:43,776 - INFO - allennlp.training.trainer - Estimated training time remaining: 0:12:53
2022-04-11 13:32:43,776 - INFO - allennlp.training.trainer - Epoch 14/14
2022-04-11 13:32:43,776 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-11 13:32:43,777 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-11 13:32:43,778 - INFO - allennlp.training.trainer - Training
2022-04-11 13:32:43,778 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-11 13:32:54,601 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0006, loss: 0.0006 ||:   2%|1         | 1/65 [00:10<11:32, 10.82s/it]
2022-04-11 13:33:05,468 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0018, loss: 0.0012 ||:   3%|3         | 2/65 [00:21<11:23, 10.85s/it]
2022-04-11 13:33:16,332 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0013, loss: 0.0012 ||:   5%|4         | 3/65 [00:32<11:13, 10.86s/it]
2022-04-11 13:33:27,223 - INFO - tqdm - accuracy: 0.9922, batch_loss: 0.0520, loss: 0.0139 ||:   6%|6         | 4/65 [00:43<11:03, 10.87s/it]
2022-04-11 13:33:38,125 - INFO - tqdm - accuracy: 0.9938, batch_loss: 0.0010, loss: 0.0113 ||:   8%|7         | 5/65 [00:54<10:52, 10.88s/it]
2022-04-11 13:33:49,035 - INFO - tqdm - accuracy: 0.9948, batch_loss: 0.0008, loss: 0.0096 ||:   9%|9         | 6/65 [01:05<10:42, 10.89s/it]
2022-04-11 13:33:59,952 - INFO - tqdm - accuracy: 0.9955, batch_loss: 0.0011, loss: 0.0084 ||:  11%|#         | 7/65 [01:16<10:32, 10.90s/it]
2022-04-11 13:34:10,870 - INFO - tqdm - accuracy: 0.9961, batch_loss: 0.0021, loss: 0.0076 ||:  12%|#2        | 8/65 [01:27<10:21, 10.91s/it]
2022-04-11 13:34:21,775 - INFO - tqdm - accuracy: 0.9965, batch_loss: 0.0008, loss: 0.0068 ||:  14%|#3        | 9/65 [01:37<10:10, 10.91s/it]
2022-04-11 13:34:32,687 - INFO - tqdm - accuracy: 0.9969, batch_loss: 0.0013, loss: 0.0063 ||:  15%|#5        | 10/65 [01:48<09:59, 10.91s/it]
2022-04-11 13:34:43,606 - INFO - tqdm - accuracy: 0.9972, batch_loss: 0.0017, loss: 0.0059 ||:  17%|#6        | 11/65 [01:59<09:49, 10.91s/it]
2022-04-11 13:34:54,522 - INFO - tqdm - accuracy: 0.9974, batch_loss: 0.0006, loss: 0.0054 ||:  18%|#8        | 12/65 [02:10<09:38, 10.91s/it]
2022-04-11 13:35:05,440 - INFO - tqdm - accuracy: 0.9976, batch_loss: 0.0145, loss: 0.0061 ||:  20%|##        | 13/65 [02:21<09:27, 10.91s/it]
2022-04-11 13:35:16,355 - INFO - tqdm - accuracy: 0.9978, batch_loss: 0.0053, loss: 0.0061 ||:  22%|##1       | 14/65 [02:32<09:16, 10.91s/it]
2022-04-11 13:35:27,266 - INFO - tqdm - accuracy: 0.9979, batch_loss: 0.0023, loss: 0.0058 ||:  23%|##3       | 15/65 [02:43<09:05, 10.91s/it]
2022-04-11 13:35:38,176 - INFO - tqdm - accuracy: 0.9980, batch_loss: 0.0087, loss: 0.0060 ||:  25%|##4       | 16/65 [02:54<08:54, 10.91s/it]
2022-04-11 13:35:49,101 - INFO - tqdm - accuracy: 0.9982, batch_loss: 0.0013, loss: 0.0057 ||:  26%|##6       | 17/65 [03:05<08:43, 10.92s/it]
2022-04-11 13:36:00,012 - INFO - tqdm - accuracy: 0.9983, batch_loss: 0.0007, loss: 0.0054 ||:  28%|##7       | 18/65 [03:16<08:32, 10.91s/it]
2022-04-11 13:36:10,937 - INFO - tqdm - accuracy: 0.9984, batch_loss: 0.0014, loss: 0.0052 ||:  29%|##9       | 19/65 [03:27<08:22, 10.92s/it]
2022-04-11 13:36:21,868 - INFO - tqdm - accuracy: 0.9984, batch_loss: 0.0011, loss: 0.0050 ||:  31%|###       | 20/65 [03:38<08:11, 10.92s/it]
2022-04-11 13:36:32,773 - INFO - tqdm - accuracy: 0.9985, batch_loss: 0.0006, loss: 0.0048 ||:  32%|###2      | 21/65 [03:48<08:00, 10.92s/it]
2022-04-11 13:36:43,568 - INFO - tqdm - accuracy: 0.9986, batch_loss: 0.0018, loss: 0.0047 ||:  34%|###3      | 22/65 [03:59<07:47, 10.88s/it]
2022-04-11 13:36:54,517 - INFO - tqdm - accuracy: 0.9986, batch_loss: 0.0008, loss: 0.0045 ||:  35%|###5      | 23/65 [04:10<07:37, 10.90s/it]
2022-04-11 13:37:05,476 - INFO - tqdm - accuracy: 0.9987, batch_loss: 0.0196, loss: 0.0051 ||:  37%|###6      | 24/65 [04:21<07:27, 10.92s/it]
2022-04-11 13:37:16,426 - INFO - tqdm - accuracy: 0.9975, batch_loss: 0.0319, loss: 0.0062 ||:  38%|###8      | 25/65 [04:32<07:17, 10.93s/it]
2022-04-11 13:37:27,397 - INFO - tqdm - accuracy: 0.9976, batch_loss: 0.0034, loss: 0.0061 ||:  40%|####      | 26/65 [04:43<07:06, 10.94s/it]
2022-04-11 13:37:38,376 - INFO - tqdm - accuracy: 0.9977, batch_loss: 0.0125, loss: 0.0063 ||:  42%|####1     | 27/65 [04:54<06:56, 10.95s/it]
2022-04-11 13:37:49,365 - INFO - tqdm - accuracy: 0.9978, batch_loss: 0.0009, loss: 0.0061 ||:  43%|####3     | 28/65 [05:05<06:45, 10.96s/it]
2022-04-11 13:38:00,356 - INFO - tqdm - accuracy: 0.9978, batch_loss: 0.0004, loss: 0.0059 ||:  45%|####4     | 29/65 [05:16<06:34, 10.97s/it]
2022-04-11 13:38:11,338 - INFO - tqdm - accuracy: 0.9979, batch_loss: 0.0009, loss: 0.0058 ||:  46%|####6     | 30/65 [05:27<06:24, 10.97s/it]
2022-04-11 13:38:22,338 - INFO - tqdm - accuracy: 0.9980, batch_loss: 0.0154, loss: 0.0061 ||:  48%|####7     | 31/65 [05:38<06:13, 10.98s/it]
2022-04-11 13:38:33,315 - INFO - tqdm - accuracy: 0.9980, batch_loss: 0.0007, loss: 0.0059 ||:  49%|####9     | 32/65 [05:49<06:02, 10.98s/it]
2022-04-11 13:38:44,308 - INFO - tqdm - accuracy: 0.9981, batch_loss: 0.0009, loss: 0.0058 ||:  51%|#####     | 33/65 [06:00<05:51, 10.98s/it]
2022-04-11 13:38:55,304 - INFO - tqdm - accuracy: 0.9982, batch_loss: 0.0020, loss: 0.0056 ||:  52%|#####2    | 34/65 [06:11<05:40, 10.99s/it]
2022-04-11 13:39:06,252 - INFO - tqdm - accuracy: 0.9982, batch_loss: 0.0004, loss: 0.0055 ||:  54%|#####3    | 35/65 [06:22<05:29, 10.98s/it]
2022-04-11 13:39:17,184 - INFO - tqdm - accuracy: 0.9983, batch_loss: 0.0010, loss: 0.0054 ||:  55%|#####5    | 36/65 [06:33<05:17, 10.96s/it]
2022-04-11 13:39:28,131 - INFO - tqdm - accuracy: 0.9983, batch_loss: 0.0040, loss: 0.0053 ||:  57%|#####6    | 37/65 [06:44<05:06, 10.96s/it]
2022-04-11 13:39:39,076 - INFO - tqdm - accuracy: 0.9984, batch_loss: 0.0008, loss: 0.0052 ||:  58%|#####8    | 38/65 [06:55<04:55, 10.95s/it]
2022-04-11 13:39:50,018 - INFO - tqdm - accuracy: 0.9976, batch_loss: 0.1054, loss: 0.0078 ||:  60%|######    | 39/65 [07:06<04:44, 10.95s/it]
2022-04-11 13:40:00,944 - INFO - tqdm - accuracy: 0.9977, batch_loss: 0.0016, loss: 0.0076 ||:  62%|######1   | 40/65 [07:17<04:33, 10.94s/it]
2022-04-11 13:40:11,877 - INFO - tqdm - accuracy: 0.9977, batch_loss: 0.0014, loss: 0.0075 ||:  63%|######3   | 41/65 [07:28<04:22, 10.94s/it]
2022-04-11 13:40:22,780 - INFO - tqdm - accuracy: 0.9978, batch_loss: 0.0004, loss: 0.0073 ||:  65%|######4   | 42/65 [07:39<04:11, 10.93s/it]
2022-04-11 13:40:33,698 - INFO - tqdm - accuracy: 0.9978, batch_loss: 0.0016, loss: 0.0072 ||:  66%|######6   | 43/65 [07:49<04:00, 10.93s/it]
2022-04-11 13:40:44,623 - INFO - tqdm - accuracy: 0.9979, batch_loss: 0.0007, loss: 0.0070 ||:  68%|######7   | 44/65 [08:00<03:49, 10.93s/it]
2022-04-11 13:40:55,551 - INFO - tqdm - accuracy: 0.9979, batch_loss: 0.0012, loss: 0.0069 ||:  69%|######9   | 45/65 [08:11<03:38, 10.93s/it]
2022-04-11 13:41:06,478 - INFO - tqdm - accuracy: 0.9980, batch_loss: 0.0015, loss: 0.0068 ||:  71%|#######   | 46/65 [08:22<03:27, 10.93s/it]
2022-04-11 13:41:17,433 - INFO - tqdm - accuracy: 0.9980, batch_loss: 0.0013, loss: 0.0067 ||:  72%|#######2  | 47/65 [08:33<03:16, 10.93s/it]
2022-04-11 13:41:28,388 - INFO - tqdm - accuracy: 0.9980, batch_loss: 0.0031, loss: 0.0066 ||:  74%|#######3  | 48/65 [08:44<03:05, 10.94s/it]
2022-04-11 13:41:39,353 - INFO - tqdm - accuracy: 0.9981, batch_loss: 0.0022, loss: 0.0065 ||:  75%|#######5  | 49/65 [08:55<02:55, 10.95s/it]
2022-04-11 13:41:50,308 - INFO - tqdm - accuracy: 0.9981, batch_loss: 0.0078, loss: 0.0065 ||:  77%|#######6  | 50/65 [09:06<02:44, 10.95s/it]
2022-04-11 13:42:01,282 - INFO - tqdm - accuracy: 0.9982, batch_loss: 0.0066, loss: 0.0065 ||:  78%|#######8  | 51/65 [09:17<02:33, 10.96s/it]
2022-04-11 13:42:11,805 - INFO - tqdm - accuracy: 0.9976, batch_loss: 0.0467, loss: 0.0073 ||:  80%|########  | 52/65 [09:28<02:20, 10.83s/it]
2022-04-11 13:42:22,764 - INFO - tqdm - accuracy: 0.9976, batch_loss: 0.0008, loss: 0.0072 ||:  82%|########1 | 53/65 [09:38<02:10, 10.87s/it]
2022-04-11 13:42:33,718 - INFO - tqdm - accuracy: 0.9977, batch_loss: 0.0026, loss: 0.0071 ||:  83%|########3 | 54/65 [09:49<01:59, 10.89s/it]
2022-04-11 13:42:44,674 - INFO - tqdm - accuracy: 0.9977, batch_loss: 0.0012, loss: 0.0070 ||:  85%|########4 | 55/65 [10:00<01:49, 10.91s/it]
2022-04-11 13:42:55,405 - INFO - tqdm - accuracy: 0.9978, batch_loss: 0.0021, loss: 0.0069 ||:  86%|########6 | 56/65 [10:11<01:37, 10.86s/it]
2022-04-11 13:43:06,332 - INFO - tqdm - accuracy: 0.9978, batch_loss: 0.0039, loss: 0.0068 ||:  88%|########7 | 57/65 [10:22<01:27, 10.88s/it]
2022-04-11 13:43:17,278 - INFO - tqdm - accuracy: 0.9978, batch_loss: 0.0203, loss: 0.0071 ||:  89%|########9 | 58/65 [10:33<01:16, 10.90s/it]
2022-04-11 13:43:28,220 - INFO - tqdm - accuracy: 0.9979, batch_loss: 0.0014, loss: 0.0070 ||:  91%|######### | 59/65 [10:44<01:05, 10.91s/it]
2022-04-11 13:43:39,160 - INFO - tqdm - accuracy: 0.9979, batch_loss: 0.0025, loss: 0.0069 ||:  92%|#########2| 60/65 [10:55<00:54, 10.92s/it]
2022-04-11 13:43:50,098 - INFO - tqdm - accuracy: 0.9979, batch_loss: 0.0009, loss: 0.0068 ||:  94%|#########3| 61/65 [11:06<00:43, 10.93s/it]
2022-04-11 13:44:01,024 - INFO - tqdm - accuracy: 0.9980, batch_loss: 0.0008, loss: 0.0067 ||:  95%|#########5| 62/65 [11:17<00:32, 10.93s/it]
2022-04-11 13:44:11,940 - INFO - tqdm - accuracy: 0.9980, batch_loss: 0.0014, loss: 0.0066 ||:  97%|#########6| 63/65 [11:28<00:21, 10.92s/it]
2022-04-11 13:44:22,857 - INFO - tqdm - accuracy: 0.9980, batch_loss: 0.0010, loss: 0.0065 ||:  98%|#########8| 64/65 [11:39<00:10, 10.92s/it]
2022-04-11 13:44:27,706 - INFO - tqdm - accuracy: 0.9981, batch_loss: 0.0112, loss: 0.0066 ||: 100%|##########| 65/65 [11:43<00:00,  9.10s/it]
2022-04-11 13:44:27,706 - INFO - tqdm - accuracy: 0.9981, batch_loss: 0.0112, loss: 0.0066 ||: 100%|##########| 65/65 [11:43<00:00, 10.83s/it]
2022-04-11 13:44:30,583 - INFO - allennlp.training.trainer - Validating
2022-04-11 13:44:30,584 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-11 13:44:40,628 - INFO - tqdm - accuracy: 0.6429, batch_loss: 3.6219, loss: 2.0603 ||:  37%|###6      | 42/115 [00:10<00:17,  4.17it/s]
2022-04-11 13:44:50,734 - INFO - tqdm - accuracy: 0.6568, batch_loss: 7.9242, loss: 2.0126 ||:  74%|#######3  | 85/115 [00:20<00:06,  4.67it/s]
2022-04-11 13:44:57,946 - INFO - tqdm - accuracy: 0.6419, batch_loss: 3.3024, loss: 2.1203 ||: 100%|##########| 115/115 [00:27<00:00,  4.17it/s]
2022-04-11 13:44:57,946 - INFO - tqdm - accuracy: 0.6419, batch_loss: 3.3024, loss: 2.1203 ||: 100%|##########| 115/115 [00:27<00:00,  4.20it/s]
2022-04-11 13:44:57,946 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-11 13:44:57,947 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.998  |     0.642
2022-04-11 13:44:57,947 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-11 13:44:57,947 - INFO - allennlp.training.tensorboard_writer - loss               |     0.007  |     2.120
2022-04-11 13:44:57,948 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5802.672  |       N/A
2022-04-11 13:45:04,480 - INFO - allennlp.training.trainer - Epoch duration: 0:12:20.704100
2022-04-11 13:45:04,481 - INFO - allennlp.training.checkpointer - loading best weights
2022-04-11 13:45:05,402 - INFO - allennlp.common.util - Metrics: {
  "best_epoch": 12,
  "peak_worker_0_memory_MB": 5802.671875,
  "peak_gpu_0_memory_MB": 9580.5712890625,
  "training_duration": "3:12:45.557691",
  "training_start_epoch": 0,
  "training_epochs": 14,
  "epoch": 14,
  "training_accuracy": 0.9980591945657448,
  "training_loss": 0.006613545342168856,
  "training_worker_0_memory_MB": 5802.671875,
  "training_gpu_0_memory_MB": 9580.5712890625,
  "validation_accuracy": 0.6419213973799127,
  "validation_loss": 2.120267742756013,
  "best_validation_accuracy": 0.6550218340611353,
  "best_validation_loss": 2.0729240289296444
}
2022-04-11 13:45:05,403 - INFO - allennlp.models.archival - archiving weights and vocabulary to ./output_ir-ora-d_hyper6/model.tar.gz
