2022-04-10 00:33:50,501 - INFO - allennlp.common.params - random_seed = 42
2022-04-10 00:33:50,501 - INFO - allennlp.common.params - numpy_seed = 42
2022-04-10 00:33:50,501 - INFO - allennlp.common.params - pytorch_seed = 42
2022-04-10 00:33:51,170 - INFO - allennlp.common.checks - Pytorch version: 1.7.1
2022-04-10 00:33:51,171 - INFO - allennlp.common.params - type = default
2022-04-10 00:33:51,171 - INFO - allennlp.common.params - dataset_reader.type = strategy_qa_reader
2022-04-10 00:33:51,172 - INFO - allennlp.common.params - dataset_reader.lazy = False
2022-04-10 00:33:51,172 - INFO - allennlp.common.params - dataset_reader.cache_directory = None
2022-04-10 00:33:51,172 - INFO - allennlp.common.params - dataset_reader.max_instances = None
2022-04-10 00:33:51,172 - INFO - allennlp.common.params - dataset_reader.manual_distributed_sharding = False
2022-04-10 00:33:51,172 - INFO - allennlp.common.params - dataset_reader.manual_multi_process_sharding = False
2022-04-10 00:33:51,172 - INFO - allennlp.common.params - dataset_reader.tokenizer_wrapper.type = default
2022-04-10 00:33:51,172 - INFO - allennlp.common.params - dataset_reader.tokenizer_wrapper.pretrained_model = roberta-large
2022-04-10 00:33:51,172 - INFO - allennlp.common.params - dataset_reader.tokenizer_wrapper.call_kwargs.return_offsets_mapping = True
2022-04-10 00:33:51,172 - INFO - allennlp.common.params - dataset_reader.tokenizer_wrapper.call_kwargs.return_special_tokens_mask = True
2022-04-10 00:33:51,172 - INFO - allennlp.common.params - dataset_reader.tokenizer_wrapper.call_kwargs.truncation = only_first
2022-04-10 00:33:52,378 - INFO - allennlp.common.params - dataset_reader.save_tokenizer = True
2022-04-10 00:33:52,378 - INFO - allennlp.common.params - dataset_reader.is_training = True
2022-04-10 00:33:52,378 - INFO - allennlp.common.params - dataset_reader.pickle.action = None
2022-04-10 00:33:52,379 - INFO - allennlp.common.params - dataset_reader.pickle.file_name = None
2022-04-10 00:33:52,379 - INFO - allennlp.common.params - dataset_reader.pickle.path = ../pickle
2022-04-10 00:33:52,379 - INFO - allennlp.common.params - dataset_reader.pickle.save_even_when_max_instances = False
2022-04-10 00:33:52,379 - INFO - allennlp.common.params - dataset_reader.paragraphs_source = IR-ORA-D
2022-04-10 00:33:52,379 - INFO - allennlp.common.params - dataset_reader.answer_last_decomposition_step = False
2022-04-10 00:33:52,379 - INFO - allennlp.common.params - dataset_reader.skip_if_context_missing = True
2022-04-10 00:33:52,379 - INFO - allennlp.common.params - dataset_reader.paragraphs_limit = 10
2022-04-10 00:33:52,379 - INFO - allennlp.common.params - dataset_reader.generated_decompositions_paths = None
2022-04-10 00:33:52,379 - INFO - allennlp.common.params - dataset_reader.save_elasticsearch_cache = True
2022-04-10 00:33:52,847 - INFO - filelock - Lock 140120752741392 acquired on cache.lock
2022-04-10 00:33:53,511 - INFO - filelock - Lock 140120752741392 released on cache.lock
2022-04-10 00:33:53,511 - INFO - allennlp.common.params - train_data_path = data/strategyqa/train.json
2022-04-10 00:33:53,511 - INFO - allennlp.common.params - vocabulary = <allennlp.common.lazy.Lazy object at 0x7f707274a190>
2022-04-10 00:33:53,511 - INFO - allennlp.common.params - datasets_for_vocab_creation = None
2022-04-10 00:33:53,512 - INFO - allennlp.common.params - validation_dataset_reader.type = strategy_qa_reader
2022-04-10 00:33:53,512 - INFO - allennlp.common.params - validation_dataset_reader.lazy = False
2022-04-10 00:33:53,512 - INFO - allennlp.common.params - validation_dataset_reader.cache_directory = None
2022-04-10 00:33:53,512 - INFO - allennlp.common.params - validation_dataset_reader.max_instances = None
2022-04-10 00:33:53,512 - INFO - allennlp.common.params - validation_dataset_reader.manual_distributed_sharding = False
2022-04-10 00:33:53,512 - INFO - allennlp.common.params - validation_dataset_reader.manual_multi_process_sharding = False
2022-04-10 00:33:53,512 - INFO - allennlp.common.params - validation_dataset_reader.tokenizer_wrapper.type = default
2022-04-10 00:33:53,512 - INFO - allennlp.common.params - validation_dataset_reader.tokenizer_wrapper.pretrained_model = roberta-large
2022-04-10 00:33:53,513 - INFO - allennlp.common.params - validation_dataset_reader.tokenizer_wrapper.call_kwargs.return_offsets_mapping = True
2022-04-10 00:33:53,513 - INFO - allennlp.common.params - validation_dataset_reader.tokenizer_wrapper.call_kwargs.return_special_tokens_mask = True
2022-04-10 00:33:53,513 - INFO - allennlp.common.params - validation_dataset_reader.tokenizer_wrapper.call_kwargs.truncation = only_first
2022-04-10 00:33:54,750 - INFO - allennlp.common.params - validation_dataset_reader.save_tokenizer = False
2022-04-10 00:33:54,750 - INFO - allennlp.common.params - validation_dataset_reader.is_training = False
2022-04-10 00:33:54,750 - INFO - allennlp.common.params - validation_dataset_reader.pickle.action = None
2022-04-10 00:33:54,750 - INFO - allennlp.common.params - validation_dataset_reader.pickle.file_name = None
2022-04-10 00:33:54,750 - INFO - allennlp.common.params - validation_dataset_reader.pickle.path = ../pickle
2022-04-10 00:33:54,750 - INFO - allennlp.common.params - validation_dataset_reader.pickle.save_even_when_max_instances = False
2022-04-10 00:33:54,750 - INFO - allennlp.common.params - validation_dataset_reader.paragraphs_source = IR-ORA-D
2022-04-10 00:33:54,750 - INFO - allennlp.common.params - validation_dataset_reader.answer_last_decomposition_step = False
2022-04-10 00:33:54,750 - INFO - allennlp.common.params - validation_dataset_reader.skip_if_context_missing = True
2022-04-10 00:33:54,751 - INFO - allennlp.common.params - validation_dataset_reader.paragraphs_limit = 10
2022-04-10 00:33:54,751 - INFO - allennlp.common.params - validation_dataset_reader.generated_decompositions_paths = None
2022-04-10 00:33:54,751 - INFO - allennlp.common.params - validation_dataset_reader.save_elasticsearch_cache = True
2022-04-10 00:33:54,751 - INFO - filelock - Lock 140120752673680 acquired on cache.lock
2022-04-10 00:33:55,615 - INFO - filelock - Lock 140120752673680 released on cache.lock
2022-04-10 00:33:55,616 - INFO - allennlp.common.params - validation_data_path = data/strategyqa/dev.json
2022-04-10 00:33:55,616 - INFO - allennlp.common.params - validation_data_loader = None
2022-04-10 00:33:55,616 - INFO - allennlp.common.params - test_data_path = None
2022-04-10 00:33:55,616 - INFO - allennlp.common.params - evaluate_on_test = True
2022-04-10 00:33:55,616 - INFO - allennlp.common.params - batch_weight_key = 
2022-04-10 00:33:55,616 - INFO - allennlp.training.util - Reading training data from data/strategyqa/train.json
2022-04-10 00:33:55,617 - INFO - tqdm - reading instances: 0it [00:00, ?it/s]
2022-04-10 00:33:55,617 - INFO - src.data.dataset_readers.base.base_dataset_reader - Reading the dataset from scratch
2022-04-10 00:33:55,617 - INFO - src.data.dataset_readers.strategy_qa_reader - Reading the dataset:
2022-04-10 00:33:55,617 - INFO - src.data.dataset_readers.strategy_qa_reader - Reading file at data/strategyqa/train.json
2022-04-10 00:34:03,168 - INFO - filelock - Lock 140120110626832 acquired on cache.lock
2022-04-10 00:34:03,947 - INFO - filelock - Lock 140120110626832 released on cache.lock
2022-04-10 00:34:03,984 - INFO - allennlp.training.util - Reading validation data from data/strategyqa/dev.json
2022-04-10 00:34:03,984 - INFO - tqdm - reading instances: 0it [00:00, ?it/s]
2022-04-10 00:34:03,985 - INFO - src.data.dataset_readers.base.base_dataset_reader - Reading the dataset from scratch
2022-04-10 00:34:03,985 - INFO - src.data.dataset_readers.strategy_qa_reader - Reading the dataset:
2022-04-10 00:34:03,985 - INFO - src.data.dataset_readers.strategy_qa_reader - Reading file at data/strategyqa/dev.json
2022-04-10 00:34:04,413 - INFO - src.data.dataset_readers.utils.elasticsearch_utils - Empty results from Elasticsearch for: actions commercial pilots take concerning engine noise arriving departing Orange County, California?
2022-04-10 00:34:04,738 - INFO - filelock - Lock 140120118044624 acquired on cache.lock
2022-04-10 00:34:05,489 - INFO - filelock - Lock 140120118044624 released on cache.lock
2022-04-10 00:34:05,523 - INFO - allennlp.common.params - type = from_instances
2022-04-10 00:34:05,523 - INFO - allennlp.common.params - min_count = None
2022-04-10 00:34:05,523 - INFO - allennlp.common.params - max_vocab_size = None
2022-04-10 00:34:05,523 - INFO - allennlp.common.params - non_padded_namespaces = ('*tags', '*labels')
2022-04-10 00:34:05,524 - INFO - allennlp.common.params - pretrained_files = None
2022-04-10 00:34:05,524 - INFO - allennlp.common.params - only_include_pretrained_words = False
2022-04-10 00:34:05,524 - INFO - allennlp.common.params - tokens_to_add = None
2022-04-10 00:34:05,524 - INFO - allennlp.common.params - min_pretrained_embeddings = None
2022-04-10 00:34:05,524 - INFO - allennlp.common.params - padding_token = @@PADDING@@
2022-04-10 00:34:05,524 - INFO - allennlp.common.params - oov_token = @@UNKNOWN@@
2022-04-10 00:34:05,524 - INFO - allennlp.data.vocabulary - Fitting token dictionary from dataset.
2022-04-10 00:34:05,524 - INFO - tqdm - building vocab: 0it [00:00, ?it/s]
2022-04-10 00:34:05,530 - INFO - allennlp.common.params - model.type = hf_classifier
2022-04-10 00:34:05,530 - INFO - allennlp.common.params - model.regularizer = None
2022-04-10 00:34:05,531 - INFO - allennlp.common.params - model.pretrained_model = roberta-large
2022-04-10 00:34:05,531 - INFO - allennlp.common.params - model.tokenizer_wrapper.type = default
2022-04-10 00:34:05,531 - INFO - allennlp.common.params - model.tokenizer_wrapper.pretrained_model = roberta-large
2022-04-10 00:34:05,531 - INFO - allennlp.common.params - model.tokenizer_wrapper.call_kwargs.return_offsets_mapping = True
2022-04-10 00:34:05,531 - INFO - allennlp.common.params - model.tokenizer_wrapper.call_kwargs.return_special_tokens_mask = True
2022-04-10 00:34:05,531 - INFO - allennlp.common.params - model.tokenizer_wrapper.call_kwargs.truncation = only_first
2022-04-10 00:34:06,685 - INFO - allennlp.common.params - model.num_labels = 2
2022-04-10 00:34:06,686 - INFO - allennlp.common.params - model.label_namespace = labels
2022-04-10 00:34:06,686 - INFO - allennlp.common.params - model.transformer_weights_path = None
2022-04-10 00:34:06,686 - INFO - allennlp.common.params - model.initializer.regexes.0.1.type = pretrained
2022-04-10 00:34:06,686 - INFO - allennlp.common.params - model.initializer.regexes.0.1.weights_file_path = output_roberta-star/model.tar.gz
2022-04-10 00:34:06,687 - INFO - allennlp.models.archival - extracting archive file output_roberta-star/model.tar.gz to temp dir /tmp/tmpylqw561h
2022-04-10 00:34:18,273 - INFO - allennlp.models.archival - removing temporary unarchived model dir at /tmp/tmpylqw561h
2022-04-10 00:34:18,445 - INFO - allennlp.common.params - model.initializer.prevent_regexes = None
2022-04-10 00:34:32,976 - INFO - allennlp.nn.initializers - Initializing parameters
2022-04-10 00:34:32,976 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.embeddings.word_embeddings.weight using .* initializer
2022-04-10 00:34:32,986 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.embeddings.position_embeddings.weight using .* initializer
2022-04-10 00:34:32,986 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.embeddings.token_type_embeddings.weight using .* initializer
2022-04-10 00:34:32,986 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.embeddings.LayerNorm.weight using .* initializer
2022-04-10 00:34:32,986 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.embeddings.LayerNorm.bias using .* initializer
2022-04-10 00:34:32,986 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.self.query.weight using .* initializer
2022-04-10 00:34:32,987 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.self.query.bias using .* initializer
2022-04-10 00:34:32,987 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.self.key.weight using .* initializer
2022-04-10 00:34:32,987 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.self.key.bias using .* initializer
2022-04-10 00:34:32,988 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.self.value.weight using .* initializer
2022-04-10 00:34:32,988 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.self.value.bias using .* initializer
2022-04-10 00:34:32,988 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.output.dense.weight using .* initializer
2022-04-10 00:34:32,988 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.output.dense.bias using .* initializer
2022-04-10 00:34:32,989 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.output.LayerNorm.weight using .* initializer
2022-04-10 00:34:32,989 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.attention.output.LayerNorm.bias using .* initializer
2022-04-10 00:34:32,989 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.intermediate.dense.weight using .* initializer
2022-04-10 00:34:32,990 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.intermediate.dense.bias using .* initializer
2022-04-10 00:34:32,990 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.output.dense.weight using .* initializer
2022-04-10 00:34:32,991 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.output.dense.bias using .* initializer
2022-04-10 00:34:32,991 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.output.LayerNorm.weight using .* initializer
2022-04-10 00:34:32,991 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.0.output.LayerNorm.bias using .* initializer
2022-04-10 00:34:32,991 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.self.query.weight using .* initializer
2022-04-10 00:34:32,991 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.self.query.bias using .* initializer
2022-04-10 00:34:32,992 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.self.key.weight using .* initializer
2022-04-10 00:34:32,992 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.self.key.bias using .* initializer
2022-04-10 00:34:32,992 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.self.value.weight using .* initializer
2022-04-10 00:34:32,992 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.self.value.bias using .* initializer
2022-04-10 00:34:32,993 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.output.dense.weight using .* initializer
2022-04-10 00:34:32,993 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.output.dense.bias using .* initializer
2022-04-10 00:34:32,993 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.output.LayerNorm.weight using .* initializer
2022-04-10 00:34:32,993 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.attention.output.LayerNorm.bias using .* initializer
2022-04-10 00:34:32,993 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.intermediate.dense.weight using .* initializer
2022-04-10 00:34:32,994 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.intermediate.dense.bias using .* initializer
2022-04-10 00:34:32,994 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.output.dense.weight using .* initializer
2022-04-10 00:34:32,995 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.output.dense.bias using .* initializer
2022-04-10 00:34:32,995 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.output.LayerNorm.weight using .* initializer
2022-04-10 00:34:32,996 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.1.output.LayerNorm.bias using .* initializer
2022-04-10 00:34:32,996 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.self.query.weight using .* initializer
2022-04-10 00:34:32,996 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.self.query.bias using .* initializer
2022-04-10 00:34:32,996 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.self.key.weight using .* initializer
2022-04-10 00:34:32,996 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.self.key.bias using .* initializer
2022-04-10 00:34:32,997 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.self.value.weight using .* initializer
2022-04-10 00:34:32,997 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.self.value.bias using .* initializer
2022-04-10 00:34:32,997 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.output.dense.weight using .* initializer
2022-04-10 00:34:32,997 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.output.dense.bias using .* initializer
2022-04-10 00:34:32,998 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.output.LayerNorm.weight using .* initializer
2022-04-10 00:34:32,998 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.attention.output.LayerNorm.bias using .* initializer
2022-04-10 00:34:32,998 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.intermediate.dense.weight using .* initializer
2022-04-10 00:34:32,999 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.intermediate.dense.bias using .* initializer
2022-04-10 00:34:32,999 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.output.dense.weight using .* initializer
2022-04-10 00:34:33,000 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.output.dense.bias using .* initializer
2022-04-10 00:34:33,000 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.output.LayerNorm.weight using .* initializer
2022-04-10 00:34:33,000 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.2.output.LayerNorm.bias using .* initializer
2022-04-10 00:34:33,000 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.self.query.weight using .* initializer
2022-04-10 00:34:33,000 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.self.query.bias using .* initializer
2022-04-10 00:34:33,000 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.self.key.weight using .* initializer
2022-04-10 00:34:33,001 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.self.key.bias using .* initializer
2022-04-10 00:34:33,001 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.self.value.weight using .* initializer
2022-04-10 00:34:33,001 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.self.value.bias using .* initializer
2022-04-10 00:34:33,001 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.output.dense.weight using .* initializer
2022-04-10 00:34:33,002 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.output.dense.bias using .* initializer
2022-04-10 00:34:33,002 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.output.LayerNorm.weight using .* initializer
2022-04-10 00:34:33,002 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.attention.output.LayerNorm.bias using .* initializer
2022-04-10 00:34:33,002 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.intermediate.dense.weight using .* initializer
2022-04-10 00:34:33,003 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.intermediate.dense.bias using .* initializer
2022-04-10 00:34:33,003 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.output.dense.weight using .* initializer
2022-04-10 00:34:33,004 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.output.dense.bias using .* initializer
2022-04-10 00:34:33,004 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.output.LayerNorm.weight using .* initializer
2022-04-10 00:34:33,004 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.3.output.LayerNorm.bias using .* initializer
2022-04-10 00:34:33,004 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.self.query.weight using .* initializer
2022-04-10 00:34:33,005 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.self.query.bias using .* initializer
2022-04-10 00:34:33,005 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.self.key.weight using .* initializer
2022-04-10 00:34:33,005 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.self.key.bias using .* initializer
2022-04-10 00:34:33,005 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.self.value.weight using .* initializer
2022-04-10 00:34:33,006 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.self.value.bias using .* initializer
2022-04-10 00:34:33,006 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.output.dense.weight using .* initializer
2022-04-10 00:34:33,006 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.output.dense.bias using .* initializer
2022-04-10 00:34:33,006 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.output.LayerNorm.weight using .* initializer
2022-04-10 00:34:33,006 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.attention.output.LayerNorm.bias using .* initializer
2022-04-10 00:34:33,006 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.intermediate.dense.weight using .* initializer
2022-04-10 00:34:33,007 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.intermediate.dense.bias using .* initializer
2022-04-10 00:34:33,007 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.output.dense.weight using .* initializer
2022-04-10 00:34:33,008 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.output.dense.bias using .* initializer
2022-04-10 00:34:33,008 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.output.LayerNorm.weight using .* initializer
2022-04-10 00:34:33,009 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.4.output.LayerNorm.bias using .* initializer
2022-04-10 00:34:33,009 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.self.query.weight using .* initializer
2022-04-10 00:34:33,009 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.self.query.bias using .* initializer
2022-04-10 00:34:33,009 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.self.key.weight using .* initializer
2022-04-10 00:34:33,009 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.self.key.bias using .* initializer
2022-04-10 00:34:33,010 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.self.value.weight using .* initializer
2022-04-10 00:34:33,010 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.self.value.bias using .* initializer
2022-04-10 00:34:33,010 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.output.dense.weight using .* initializer
2022-04-10 00:34:33,011 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.output.dense.bias using .* initializer
2022-04-10 00:34:33,011 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.output.LayerNorm.weight using .* initializer
2022-04-10 00:34:33,011 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.attention.output.LayerNorm.bias using .* initializer
2022-04-10 00:34:33,011 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.intermediate.dense.weight using .* initializer
2022-04-10 00:34:33,012 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.intermediate.dense.bias using .* initializer
2022-04-10 00:34:33,012 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.output.dense.weight using .* initializer
2022-04-10 00:34:33,013 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.output.dense.bias using .* initializer
2022-04-10 00:34:33,013 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.output.LayerNorm.weight using .* initializer
2022-04-10 00:34:33,013 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.5.output.LayerNorm.bias using .* initializer
2022-04-10 00:34:33,013 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.self.query.weight using .* initializer
2022-04-10 00:34:33,014 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.self.query.bias using .* initializer
2022-04-10 00:34:33,014 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.self.key.weight using .* initializer
2022-04-10 00:34:33,014 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.self.key.bias using .* initializer
2022-04-10 00:34:33,014 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.self.value.weight using .* initializer
2022-04-10 00:34:33,014 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.self.value.bias using .* initializer
2022-04-10 00:34:33,015 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.output.dense.weight using .* initializer
2022-04-10 00:34:33,015 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.output.dense.bias using .* initializer
2022-04-10 00:34:33,015 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.output.LayerNorm.weight using .* initializer
2022-04-10 00:34:33,015 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.attention.output.LayerNorm.bias using .* initializer
2022-04-10 00:34:33,016 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.intermediate.dense.weight using .* initializer
2022-04-10 00:34:33,016 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.intermediate.dense.bias using .* initializer
2022-04-10 00:34:33,017 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.output.dense.weight using .* initializer
2022-04-10 00:34:33,018 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.output.dense.bias using .* initializer
2022-04-10 00:34:33,018 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.output.LayerNorm.weight using .* initializer
2022-04-10 00:34:33,018 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.6.output.LayerNorm.bias using .* initializer
2022-04-10 00:34:33,018 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.self.query.weight using .* initializer
2022-04-10 00:34:33,018 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.self.query.bias using .* initializer
2022-04-10 00:34:33,018 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.self.key.weight using .* initializer
2022-04-10 00:34:33,019 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.self.key.bias using .* initializer
2022-04-10 00:34:33,019 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.self.value.weight using .* initializer
2022-04-10 00:34:33,019 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.self.value.bias using .* initializer
2022-04-10 00:34:33,019 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.output.dense.weight using .* initializer
2022-04-10 00:34:33,020 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.output.dense.bias using .* initializer
2022-04-10 00:34:33,020 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.output.LayerNorm.weight using .* initializer
2022-04-10 00:34:33,020 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.attention.output.LayerNorm.bias using .* initializer
2022-04-10 00:34:33,020 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.intermediate.dense.weight using .* initializer
2022-04-10 00:34:33,021 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.intermediate.dense.bias using .* initializer
2022-04-10 00:34:33,021 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.output.dense.weight using .* initializer
2022-04-10 00:34:33,022 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.output.dense.bias using .* initializer
2022-04-10 00:34:33,022 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.output.LayerNorm.weight using .* initializer
2022-04-10 00:34:33,022 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.7.output.LayerNorm.bias using .* initializer
2022-04-10 00:34:33,022 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.self.query.weight using .* initializer
2022-04-10 00:34:33,023 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.self.query.bias using .* initializer
2022-04-10 00:34:33,023 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.self.key.weight using .* initializer
2022-04-10 00:34:33,023 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.self.key.bias using .* initializer
2022-04-10 00:34:33,023 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.self.value.weight using .* initializer
2022-04-10 00:34:33,024 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.self.value.bias using .* initializer
2022-04-10 00:34:33,024 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.output.dense.weight using .* initializer
2022-04-10 00:34:33,024 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.output.dense.bias using .* initializer
2022-04-10 00:34:33,024 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.output.LayerNorm.weight using .* initializer
2022-04-10 00:34:33,024 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.attention.output.LayerNorm.bias using .* initializer
2022-04-10 00:34:33,025 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.intermediate.dense.weight using .* initializer
2022-04-10 00:34:33,025 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.intermediate.dense.bias using .* initializer
2022-04-10 00:34:33,026 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.output.dense.weight using .* initializer
2022-04-10 00:34:33,027 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.output.dense.bias using .* initializer
2022-04-10 00:34:33,027 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.output.LayerNorm.weight using .* initializer
2022-04-10 00:34:33,027 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.8.output.LayerNorm.bias using .* initializer
2022-04-10 00:34:33,027 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.self.query.weight using .* initializer
2022-04-10 00:34:33,027 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.self.query.bias using .* initializer
2022-04-10 00:34:33,027 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.self.key.weight using .* initializer
2022-04-10 00:34:33,028 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.self.key.bias using .* initializer
2022-04-10 00:34:33,028 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.self.value.weight using .* initializer
2022-04-10 00:34:33,028 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.self.value.bias using .* initializer
2022-04-10 00:34:33,028 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.output.dense.weight using .* initializer
2022-04-10 00:34:33,029 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.output.dense.bias using .* initializer
2022-04-10 00:34:33,029 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.output.LayerNorm.weight using .* initializer
2022-04-10 00:34:33,029 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.attention.output.LayerNorm.bias using .* initializer
2022-04-10 00:34:33,029 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.intermediate.dense.weight using .* initializer
2022-04-10 00:34:33,030 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.intermediate.dense.bias using .* initializer
2022-04-10 00:34:33,030 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.output.dense.weight using .* initializer
2022-04-10 00:34:33,031 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.output.dense.bias using .* initializer
2022-04-10 00:34:33,031 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.output.LayerNorm.weight using .* initializer
2022-04-10 00:34:33,031 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.9.output.LayerNorm.bias using .* initializer
2022-04-10 00:34:33,031 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.self.query.weight using .* initializer
2022-04-10 00:34:33,032 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.self.query.bias using .* initializer
2022-04-10 00:34:33,032 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.self.key.weight using .* initializer
2022-04-10 00:34:33,032 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.self.key.bias using .* initializer
2022-04-10 00:34:33,032 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.self.value.weight using .* initializer
2022-04-10 00:34:33,033 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.self.value.bias using .* initializer
2022-04-10 00:34:33,033 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.output.dense.weight using .* initializer
2022-04-10 00:34:33,033 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.output.dense.bias using .* initializer
2022-04-10 00:34:33,033 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.output.LayerNorm.weight using .* initializer
2022-04-10 00:34:33,033 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.attention.output.LayerNorm.bias using .* initializer
2022-04-10 00:34:33,034 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.intermediate.dense.weight using .* initializer
2022-04-10 00:34:33,034 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.intermediate.dense.bias using .* initializer
2022-04-10 00:34:33,035 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.output.dense.weight using .* initializer
2022-04-10 00:34:33,035 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.output.dense.bias using .* initializer
2022-04-10 00:34:33,036 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.output.LayerNorm.weight using .* initializer
2022-04-10 00:34:33,036 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.10.output.LayerNorm.bias using .* initializer
2022-04-10 00:34:33,036 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.self.query.weight using .* initializer
2022-04-10 00:34:33,036 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.self.query.bias using .* initializer
2022-04-10 00:34:33,036 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.self.key.weight using .* initializer
2022-04-10 00:34:33,037 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.self.key.bias using .* initializer
2022-04-10 00:34:33,037 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.self.value.weight using .* initializer
2022-04-10 00:34:33,037 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.self.value.bias using .* initializer
2022-04-10 00:34:33,037 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.output.dense.weight using .* initializer
2022-04-10 00:34:33,038 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.output.dense.bias using .* initializer
2022-04-10 00:34:33,038 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.output.LayerNorm.weight using .* initializer
2022-04-10 00:34:33,038 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.attention.output.LayerNorm.bias using .* initializer
2022-04-10 00:34:33,038 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.intermediate.dense.weight using .* initializer
2022-04-10 00:34:33,039 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.intermediate.dense.bias using .* initializer
2022-04-10 00:34:33,039 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.output.dense.weight using .* initializer
2022-04-10 00:34:33,040 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.output.dense.bias using .* initializer
2022-04-10 00:34:33,040 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.output.LayerNorm.weight using .* initializer
2022-04-10 00:34:33,040 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.11.output.LayerNorm.bias using .* initializer
2022-04-10 00:34:33,040 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.self.query.weight using .* initializer
2022-04-10 00:34:33,041 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.self.query.bias using .* initializer
2022-04-10 00:34:33,041 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.self.key.weight using .* initializer
2022-04-10 00:34:33,041 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.self.key.bias using .* initializer
2022-04-10 00:34:33,041 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.self.value.weight using .* initializer
2022-04-10 00:34:33,042 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.self.value.bias using .* initializer
2022-04-10 00:34:33,042 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.output.dense.weight using .* initializer
2022-04-10 00:34:33,042 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.output.dense.bias using .* initializer
2022-04-10 00:34:33,042 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.output.LayerNorm.weight using .* initializer
2022-04-10 00:34:33,042 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.attention.output.LayerNorm.bias using .* initializer
2022-04-10 00:34:33,042 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.intermediate.dense.weight using .* initializer
2022-04-10 00:34:33,043 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.intermediate.dense.bias using .* initializer
2022-04-10 00:34:33,043 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.output.dense.weight using .* initializer
2022-04-10 00:34:33,044 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.output.dense.bias using .* initializer
2022-04-10 00:34:33,045 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.output.LayerNorm.weight using .* initializer
2022-04-10 00:34:33,045 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.12.output.LayerNorm.bias using .* initializer
2022-04-10 00:34:33,045 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.self.query.weight using .* initializer
2022-04-10 00:34:33,045 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.self.query.bias using .* initializer
2022-04-10 00:34:33,045 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.self.key.weight using .* initializer
2022-04-10 00:34:33,046 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.self.key.bias using .* initializer
2022-04-10 00:34:33,046 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.self.value.weight using .* initializer
2022-04-10 00:34:33,046 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.self.value.bias using .* initializer
2022-04-10 00:34:33,046 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.output.dense.weight using .* initializer
2022-04-10 00:34:33,046 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.output.dense.bias using .* initializer
2022-04-10 00:34:33,047 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.output.LayerNorm.weight using .* initializer
2022-04-10 00:34:33,047 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.attention.output.LayerNorm.bias using .* initializer
2022-04-10 00:34:33,047 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.intermediate.dense.weight using .* initializer
2022-04-10 00:34:33,048 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.intermediate.dense.bias using .* initializer
2022-04-10 00:34:33,048 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.output.dense.weight using .* initializer
2022-04-10 00:34:33,049 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.output.dense.bias using .* initializer
2022-04-10 00:34:33,049 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.output.LayerNorm.weight using .* initializer
2022-04-10 00:34:33,049 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.13.output.LayerNorm.bias using .* initializer
2022-04-10 00:34:33,049 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.self.query.weight using .* initializer
2022-04-10 00:34:33,050 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.self.query.bias using .* initializer
2022-04-10 00:34:33,050 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.self.key.weight using .* initializer
2022-04-10 00:34:33,050 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.self.key.bias using .* initializer
2022-04-10 00:34:33,050 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.self.value.weight using .* initializer
2022-04-10 00:34:33,050 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.self.value.bias using .* initializer
2022-04-10 00:34:33,051 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.output.dense.weight using .* initializer
2022-04-10 00:34:33,051 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.output.dense.bias using .* initializer
2022-04-10 00:34:33,051 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.output.LayerNorm.weight using .* initializer
2022-04-10 00:34:33,051 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.attention.output.LayerNorm.bias using .* initializer
2022-04-10 00:34:33,051 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.intermediate.dense.weight using .* initializer
2022-04-10 00:34:33,052 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.intermediate.dense.bias using .* initializer
2022-04-10 00:34:33,052 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.output.dense.weight using .* initializer
2022-04-10 00:34:33,053 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.output.dense.bias using .* initializer
2022-04-10 00:34:33,053 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.output.LayerNorm.weight using .* initializer
2022-04-10 00:34:33,054 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.14.output.LayerNorm.bias using .* initializer
2022-04-10 00:34:33,054 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.self.query.weight using .* initializer
2022-04-10 00:34:33,054 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.self.query.bias using .* initializer
2022-04-10 00:34:33,054 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.self.key.weight using .* initializer
2022-04-10 00:34:33,054 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.self.key.bias using .* initializer
2022-04-10 00:34:33,055 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.self.value.weight using .* initializer
2022-04-10 00:34:33,055 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.self.value.bias using .* initializer
2022-04-10 00:34:33,055 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.output.dense.weight using .* initializer
2022-04-10 00:34:33,055 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.output.dense.bias using .* initializer
2022-04-10 00:34:33,056 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.output.LayerNorm.weight using .* initializer
2022-04-10 00:34:33,056 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.attention.output.LayerNorm.bias using .* initializer
2022-04-10 00:34:33,056 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.intermediate.dense.weight using .* initializer
2022-04-10 00:34:33,057 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.intermediate.dense.bias using .* initializer
2022-04-10 00:34:33,057 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.output.dense.weight using .* initializer
2022-04-10 00:34:33,058 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.output.dense.bias using .* initializer
2022-04-10 00:34:33,058 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.output.LayerNorm.weight using .* initializer
2022-04-10 00:34:33,058 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.15.output.LayerNorm.bias using .* initializer
2022-04-10 00:34:33,058 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.self.query.weight using .* initializer
2022-04-10 00:34:33,058 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.self.query.bias using .* initializer
2022-04-10 00:34:33,059 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.self.key.weight using .* initializer
2022-04-10 00:34:33,059 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.self.key.bias using .* initializer
2022-04-10 00:34:33,059 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.self.value.weight using .* initializer
2022-04-10 00:34:33,059 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.self.value.bias using .* initializer
2022-04-10 00:34:33,059 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.output.dense.weight using .* initializer
2022-04-10 00:34:33,060 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.output.dense.bias using .* initializer
2022-04-10 00:34:33,060 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.output.LayerNorm.weight using .* initializer
2022-04-10 00:34:33,060 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.attention.output.LayerNorm.bias using .* initializer
2022-04-10 00:34:33,060 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.intermediate.dense.weight using .* initializer
2022-04-10 00:34:33,061 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.intermediate.dense.bias using .* initializer
2022-04-10 00:34:33,061 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.output.dense.weight using .* initializer
2022-04-10 00:34:33,062 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.output.dense.bias using .* initializer
2022-04-10 00:34:33,062 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.output.LayerNorm.weight using .* initializer
2022-04-10 00:34:33,062 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.16.output.LayerNorm.bias using .* initializer
2022-04-10 00:34:33,063 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.self.query.weight using .* initializer
2022-04-10 00:34:33,063 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.self.query.bias using .* initializer
2022-04-10 00:34:33,063 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.self.key.weight using .* initializer
2022-04-10 00:34:33,063 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.self.key.bias using .* initializer
2022-04-10 00:34:33,064 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.self.value.weight using .* initializer
2022-04-10 00:34:33,064 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.self.value.bias using .* initializer
2022-04-10 00:34:33,064 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.output.dense.weight using .* initializer
2022-04-10 00:34:33,064 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.output.dense.bias using .* initializer
2022-04-10 00:34:33,064 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.output.LayerNorm.weight using .* initializer
2022-04-10 00:34:33,065 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.attention.output.LayerNorm.bias using .* initializer
2022-04-10 00:34:33,065 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.intermediate.dense.weight using .* initializer
2022-04-10 00:34:33,066 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.intermediate.dense.bias using .* initializer
2022-04-10 00:34:33,066 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.output.dense.weight using .* initializer
2022-04-10 00:34:33,067 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.output.dense.bias using .* initializer
2022-04-10 00:34:33,067 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.output.LayerNorm.weight using .* initializer
2022-04-10 00:34:33,067 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.17.output.LayerNorm.bias using .* initializer
2022-04-10 00:34:33,067 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.self.query.weight using .* initializer
2022-04-10 00:34:33,067 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.self.query.bias using .* initializer
2022-04-10 00:34:33,068 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.self.key.weight using .* initializer
2022-04-10 00:34:33,068 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.self.key.bias using .* initializer
2022-04-10 00:34:33,068 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.self.value.weight using .* initializer
2022-04-10 00:34:33,068 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.self.value.bias using .* initializer
2022-04-10 00:34:33,068 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.output.dense.weight using .* initializer
2022-04-10 00:34:33,069 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.output.dense.bias using .* initializer
2022-04-10 00:34:33,069 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.output.LayerNorm.weight using .* initializer
2022-04-10 00:34:33,069 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.attention.output.LayerNorm.bias using .* initializer
2022-04-10 00:34:33,069 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.intermediate.dense.weight using .* initializer
2022-04-10 00:34:33,070 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.intermediate.dense.bias using .* initializer
2022-04-10 00:34:33,070 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.output.dense.weight using .* initializer
2022-04-10 00:34:33,071 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.output.dense.bias using .* initializer
2022-04-10 00:34:33,071 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.output.LayerNorm.weight using .* initializer
2022-04-10 00:34:33,071 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.18.output.LayerNorm.bias using .* initializer
2022-04-10 00:34:33,072 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.self.query.weight using .* initializer
2022-04-10 00:34:33,072 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.self.query.bias using .* initializer
2022-04-10 00:34:33,072 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.self.key.weight using .* initializer
2022-04-10 00:34:33,073 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.self.key.bias using .* initializer
2022-04-10 00:34:33,073 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.self.value.weight using .* initializer
2022-04-10 00:34:33,073 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.self.value.bias using .* initializer
2022-04-10 00:34:33,073 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.output.dense.weight using .* initializer
2022-04-10 00:34:33,074 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.output.dense.bias using .* initializer
2022-04-10 00:34:33,074 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.output.LayerNorm.weight using .* initializer
2022-04-10 00:34:33,074 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.attention.output.LayerNorm.bias using .* initializer
2022-04-10 00:34:33,074 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.intermediate.dense.weight using .* initializer
2022-04-10 00:34:33,075 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.intermediate.dense.bias using .* initializer
2022-04-10 00:34:33,075 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.output.dense.weight using .* initializer
2022-04-10 00:34:33,076 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.output.dense.bias using .* initializer
2022-04-10 00:34:33,076 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.output.LayerNorm.weight using .* initializer
2022-04-10 00:34:33,076 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.19.output.LayerNorm.bias using .* initializer
2022-04-10 00:34:33,077 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.self.query.weight using .* initializer
2022-04-10 00:34:33,077 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.self.query.bias using .* initializer
2022-04-10 00:34:33,077 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.self.key.weight using .* initializer
2022-04-10 00:34:33,077 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.self.key.bias using .* initializer
2022-04-10 00:34:33,077 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.self.value.weight using .* initializer
2022-04-10 00:34:33,078 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.self.value.bias using .* initializer
2022-04-10 00:34:33,078 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.output.dense.weight using .* initializer
2022-04-10 00:34:33,078 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.output.dense.bias using .* initializer
2022-04-10 00:34:33,078 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.output.LayerNorm.weight using .* initializer
2022-04-10 00:34:33,078 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.attention.output.LayerNorm.bias using .* initializer
2022-04-10 00:34:33,079 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.intermediate.dense.weight using .* initializer
2022-04-10 00:34:33,080 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.intermediate.dense.bias using .* initializer
2022-04-10 00:34:33,080 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.output.dense.weight using .* initializer
2022-04-10 00:34:33,081 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.output.dense.bias using .* initializer
2022-04-10 00:34:33,081 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.output.LayerNorm.weight using .* initializer
2022-04-10 00:34:33,081 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.20.output.LayerNorm.bias using .* initializer
2022-04-10 00:34:33,081 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.self.query.weight using .* initializer
2022-04-10 00:34:33,082 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.self.query.bias using .* initializer
2022-04-10 00:34:33,082 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.self.key.weight using .* initializer
2022-04-10 00:34:33,082 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.self.key.bias using .* initializer
2022-04-10 00:34:33,082 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.self.value.weight using .* initializer
2022-04-10 00:34:33,082 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.self.value.bias using .* initializer
2022-04-10 00:34:33,083 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.output.dense.weight using .* initializer
2022-04-10 00:34:33,083 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.output.dense.bias using .* initializer
2022-04-10 00:34:33,083 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.output.LayerNorm.weight using .* initializer
2022-04-10 00:34:33,083 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.attention.output.LayerNorm.bias using .* initializer
2022-04-10 00:34:33,083 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.intermediate.dense.weight using .* initializer
2022-04-10 00:34:33,084 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.intermediate.dense.bias using .* initializer
2022-04-10 00:34:33,084 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.output.dense.weight using .* initializer
2022-04-10 00:34:33,085 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.output.dense.bias using .* initializer
2022-04-10 00:34:33,085 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.output.LayerNorm.weight using .* initializer
2022-04-10 00:34:33,086 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.21.output.LayerNorm.bias using .* initializer
2022-04-10 00:34:33,086 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.self.query.weight using .* initializer
2022-04-10 00:34:33,086 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.self.query.bias using .* initializer
2022-04-10 00:34:33,086 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.self.key.weight using .* initializer
2022-04-10 00:34:33,086 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.self.key.bias using .* initializer
2022-04-10 00:34:33,087 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.self.value.weight using .* initializer
2022-04-10 00:34:33,087 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.self.value.bias using .* initializer
2022-04-10 00:34:33,087 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.output.dense.weight using .* initializer
2022-04-10 00:34:33,087 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.output.dense.bias using .* initializer
2022-04-10 00:34:33,088 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.output.LayerNorm.weight using .* initializer
2022-04-10 00:34:33,088 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.attention.output.LayerNorm.bias using .* initializer
2022-04-10 00:34:33,088 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.intermediate.dense.weight using .* initializer
2022-04-10 00:34:33,089 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.intermediate.dense.bias using .* initializer
2022-04-10 00:34:33,089 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.output.dense.weight using .* initializer
2022-04-10 00:34:33,090 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.output.dense.bias using .* initializer
2022-04-10 00:34:33,090 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.output.LayerNorm.weight using .* initializer
2022-04-10 00:34:33,090 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.22.output.LayerNorm.bias using .* initializer
2022-04-10 00:34:33,090 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.self.query.weight using .* initializer
2022-04-10 00:34:33,090 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.self.query.bias using .* initializer
2022-04-10 00:34:33,091 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.self.key.weight using .* initializer
2022-04-10 00:34:33,091 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.self.key.bias using .* initializer
2022-04-10 00:34:33,091 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.self.value.weight using .* initializer
2022-04-10 00:34:33,091 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.self.value.bias using .* initializer
2022-04-10 00:34:33,092 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.output.dense.weight using .* initializer
2022-04-10 00:34:33,092 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.output.dense.bias using .* initializer
2022-04-10 00:34:33,092 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.output.LayerNorm.weight using .* initializer
2022-04-10 00:34:33,092 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.attention.output.LayerNorm.bias using .* initializer
2022-04-10 00:34:33,092 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.intermediate.dense.weight using .* initializer
2022-04-10 00:34:33,093 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.intermediate.dense.bias using .* initializer
2022-04-10 00:34:33,093 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.output.dense.weight using .* initializer
2022-04-10 00:34:33,094 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.output.dense.bias using .* initializer
2022-04-10 00:34:33,094 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.output.LayerNorm.weight using .* initializer
2022-04-10 00:34:33,094 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.encoder.layer.23.output.LayerNorm.bias using .* initializer
2022-04-10 00:34:33,095 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.pooler.dense.weight using .* initializer
2022-04-10 00:34:33,095 - INFO - allennlp.nn.initializers - Initializing _classifier.roberta.pooler.dense.bias using .* initializer
2022-04-10 00:34:33,095 - INFO - allennlp.nn.initializers - Initializing _classifier.classifier.dense.weight using .* initializer
2022-04-10 00:34:33,095 - INFO - allennlp.nn.initializers - Initializing _classifier.classifier.dense.bias using .* initializer
2022-04-10 00:34:33,096 - INFO - allennlp.nn.initializers - Initializing _classifier.classifier.out_proj.weight using .* initializer
2022-04-10 00:34:33,096 - INFO - allennlp.nn.initializers - Initializing _classifier.classifier.out_proj.bias using .* initializer
2022-04-10 00:34:33,096 - INFO - allennlp.nn.initializers - Done initializing parameters; the following parameters are using their default initialization from their code
2022-04-10 00:34:33,749 - INFO - filelock - Lock 140120110626000 acquired on ./output_ir-ora-d_hyper2/vocabulary/.lock
2022-04-10 00:34:33,749 - INFO - filelock - Lock 140120110626000 released on ./output_ir-ora-d_hyper2/vocabulary/.lock
2022-04-10 00:34:33,749 - INFO - allennlp.common.params - data_loader.type = pytorch_dataloader
2022-04-10 00:34:33,749 - INFO - allennlp.common.params - data_loader.batch_size = 1
2022-04-10 00:34:33,750 - INFO - allennlp.common.params - data_loader.shuffle = False
2022-04-10 00:34:33,750 - INFO - allennlp.common.params - data_loader.sampler = None
2022-04-10 00:34:33,750 - INFO - allennlp.common.params - data_loader.num_workers = 0
2022-04-10 00:34:33,750 - INFO - allennlp.common.params - data_loader.pin_memory = False
2022-04-10 00:34:33,750 - INFO - allennlp.common.params - data_loader.drop_last = False
2022-04-10 00:34:33,750 - INFO - allennlp.common.params - data_loader.timeout = 0
2022-04-10 00:34:33,750 - INFO - allennlp.common.params - data_loader.worker_init_fn = None
2022-04-10 00:34:33,750 - INFO - allennlp.common.params - data_loader.multiprocessing_context = None
2022-04-10 00:34:33,750 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None
2022-04-10 00:34:33,751 - INFO - allennlp.common.params - data_loader.batch_sampler.type = bucket
2022-04-10 00:34:33,751 - INFO - allennlp.common.params - data_loader.batch_sampler.batch_size = 2
2022-04-10 00:34:33,751 - INFO - allennlp.common.params - data_loader.batch_sampler.sorting_keys = None
2022-04-10 00:34:33,751 - INFO - allennlp.common.params - data_loader.batch_sampler.padding_noise = 0.1
2022-04-10 00:34:33,751 - INFO - allennlp.common.params - data_loader.batch_sampler.drop_last = False
2022-04-10 00:34:33,752 - INFO - allennlp.common.params - data_loader.type = pytorch_dataloader
2022-04-10 00:34:33,752 - INFO - allennlp.common.params - data_loader.batch_size = 1
2022-04-10 00:34:33,752 - INFO - allennlp.common.params - data_loader.shuffle = False
2022-04-10 00:34:33,752 - INFO - allennlp.common.params - data_loader.sampler = None
2022-04-10 00:34:33,752 - INFO - allennlp.common.params - data_loader.num_workers = 0
2022-04-10 00:34:33,752 - INFO - allennlp.common.params - data_loader.pin_memory = False
2022-04-10 00:34:33,753 - INFO - allennlp.common.params - data_loader.drop_last = False
2022-04-10 00:34:33,753 - INFO - allennlp.common.params - data_loader.timeout = 0
2022-04-10 00:34:33,753 - INFO - allennlp.common.params - data_loader.worker_init_fn = None
2022-04-10 00:34:33,753 - INFO - allennlp.common.params - data_loader.multiprocessing_context = None
2022-04-10 00:34:33,753 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None
2022-04-10 00:34:33,753 - INFO - allennlp.common.params - data_loader.batch_sampler.type = bucket
2022-04-10 00:34:33,753 - INFO - allennlp.common.params - data_loader.batch_sampler.batch_size = 2
2022-04-10 00:34:33,753 - INFO - allennlp.common.params - data_loader.batch_sampler.sorting_keys = None
2022-04-10 00:34:33,753 - INFO - allennlp.common.params - data_loader.batch_sampler.padding_noise = 0.1
2022-04-10 00:34:33,754 - INFO - allennlp.common.params - data_loader.batch_sampler.drop_last = False
2022-04-10 00:34:33,754 - INFO - allennlp.common.params - trainer.type = gradient_descent
2022-04-10 00:34:33,754 - INFO - allennlp.common.params - trainer.patience = None
2022-04-10 00:34:33,754 - INFO - allennlp.common.params - trainer.validation_metric = +accuracy
2022-04-10 00:34:33,754 - INFO - allennlp.common.params - trainer.num_epochs = 15
2022-04-10 00:34:33,755 - INFO - allennlp.common.params - trainer.cuda_device = 0
2022-04-10 00:34:33,755 - INFO - allennlp.common.params - trainer.grad_norm = None
2022-04-10 00:34:33,755 - INFO - allennlp.common.params - trainer.grad_clipping = None
2022-04-10 00:34:33,755 - INFO - allennlp.common.params - trainer.distributed = False
2022-04-10 00:34:33,755 - INFO - allennlp.common.params - trainer.world_size = 1
2022-04-10 00:34:33,755 - INFO - allennlp.common.params - trainer.num_gradient_accumulation_steps = 16
2022-04-10 00:34:33,755 - INFO - allennlp.common.params - trainer.use_amp = False
2022-04-10 00:34:33,755 - INFO - allennlp.common.params - trainer.no_grad = None
2022-04-10 00:34:33,756 - INFO - allennlp.common.params - trainer.momentum_scheduler = None
2022-04-10 00:34:33,756 - INFO - allennlp.common.params - trainer.tensorboard_writer = <allennlp.common.lazy.Lazy object at 0x7f70682f5450>
2022-04-10 00:34:33,756 - INFO - allennlp.common.params - trainer.moving_average = None
2022-04-10 00:34:33,756 - INFO - allennlp.common.params - trainer.checkpointer = <allennlp.common.lazy.Lazy object at 0x7f70682f54d0>
2022-04-10 00:34:33,756 - INFO - allennlp.common.params - trainer.batch_callbacks = None
2022-04-10 00:34:33,756 - INFO - allennlp.common.params - trainer.epoch_callbacks = None
2022-04-10 00:34:33,756 - INFO - allennlp.common.params - trainer.end_callbacks = None
2022-04-10 00:34:33,756 - INFO - allennlp.common.params - trainer.trainer_callbacks = None
2022-04-10 00:34:36,502 - INFO - allennlp.common.params - trainer.optimizer.type = huggingface_adamw
2022-04-10 00:34:36,502 - INFO - allennlp.common.params - trainer.optimizer.parameter_groups = None
2022-04-10 00:34:36,502 - INFO - allennlp.common.params - trainer.optimizer.lr = 1e-05
2022-04-10 00:34:36,502 - INFO - allennlp.common.params - trainer.optimizer.betas = [0.9, 0.98]
2022-04-10 00:34:36,502 - INFO - allennlp.common.params - trainer.optimizer.eps = 1e-06
2022-04-10 00:34:36,503 - INFO - allennlp.common.params - trainer.optimizer.weight_decay = 0.075
2022-04-10 00:34:36,503 - INFO - allennlp.common.params - trainer.optimizer.correct_bias = True
2022-04-10 00:34:36,503 - INFO - allennlp.training.optimizers - Number of trainable parameters: 356411394
2022-04-10 00:34:36,505 - INFO - allennlp.common.util - The following parameters are Frozen (without gradient):
2022-04-10 00:34:36,507 - INFO - allennlp.common.util - The following parameters are Tunable (with gradient):
2022-04-10 00:34:36,507 - INFO - allennlp.common.util - _classifier.roberta.embeddings.word_embeddings.weight
2022-04-10 00:34:36,508 - INFO - allennlp.common.util - _classifier.roberta.embeddings.position_embeddings.weight
2022-04-10 00:34:36,508 - INFO - allennlp.common.util - _classifier.roberta.embeddings.token_type_embeddings.weight
2022-04-10 00:34:36,508 - INFO - allennlp.common.util - _classifier.roberta.embeddings.LayerNorm.weight
2022-04-10 00:34:36,508 - INFO - allennlp.common.util - _classifier.roberta.embeddings.LayerNorm.bias
2022-04-10 00:34:36,508 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.self.query.weight
2022-04-10 00:34:36,508 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.self.query.bias
2022-04-10 00:34:36,508 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.self.key.weight
2022-04-10 00:34:36,508 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.self.key.bias
2022-04-10 00:34:36,508 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.self.value.weight
2022-04-10 00:34:36,508 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.self.value.bias
2022-04-10 00:34:36,508 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.output.dense.weight
2022-04-10 00:34:36,509 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.output.dense.bias
2022-04-10 00:34:36,509 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.output.LayerNorm.weight
2022-04-10 00:34:36,509 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.attention.output.LayerNorm.bias
2022-04-10 00:34:36,509 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.intermediate.dense.weight
2022-04-10 00:34:36,509 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.intermediate.dense.bias
2022-04-10 00:34:36,509 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.output.dense.weight
2022-04-10 00:34:36,509 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.output.dense.bias
2022-04-10 00:34:36,509 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.output.LayerNorm.weight
2022-04-10 00:34:36,509 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.0.output.LayerNorm.bias
2022-04-10 00:34:36,509 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.self.query.weight
2022-04-10 00:34:36,509 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.self.query.bias
2022-04-10 00:34:36,510 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.self.key.weight
2022-04-10 00:34:36,510 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.self.key.bias
2022-04-10 00:34:36,510 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.self.value.weight
2022-04-10 00:34:36,510 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.self.value.bias
2022-04-10 00:34:36,510 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.output.dense.weight
2022-04-10 00:34:36,510 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.output.dense.bias
2022-04-10 00:34:36,510 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.output.LayerNorm.weight
2022-04-10 00:34:36,510 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.attention.output.LayerNorm.bias
2022-04-10 00:34:36,510 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.intermediate.dense.weight
2022-04-10 00:34:36,510 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.intermediate.dense.bias
2022-04-10 00:34:36,510 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.output.dense.weight
2022-04-10 00:34:36,511 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.output.dense.bias
2022-04-10 00:34:36,511 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.output.LayerNorm.weight
2022-04-10 00:34:36,511 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.1.output.LayerNorm.bias
2022-04-10 00:34:36,511 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.self.query.weight
2022-04-10 00:34:36,511 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.self.query.bias
2022-04-10 00:34:36,511 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.self.key.weight
2022-04-10 00:34:36,511 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.self.key.bias
2022-04-10 00:34:36,511 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.self.value.weight
2022-04-10 00:34:36,511 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.self.value.bias
2022-04-10 00:34:36,511 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.output.dense.weight
2022-04-10 00:34:36,511 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.output.dense.bias
2022-04-10 00:34:36,512 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.output.LayerNorm.weight
2022-04-10 00:34:36,512 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.attention.output.LayerNorm.bias
2022-04-10 00:34:36,512 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.intermediate.dense.weight
2022-04-10 00:34:36,512 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.intermediate.dense.bias
2022-04-10 00:34:36,512 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.output.dense.weight
2022-04-10 00:34:36,512 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.output.dense.bias
2022-04-10 00:34:36,512 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.output.LayerNorm.weight
2022-04-10 00:34:36,512 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.2.output.LayerNorm.bias
2022-04-10 00:34:36,512 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.self.query.weight
2022-04-10 00:34:36,512 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.self.query.bias
2022-04-10 00:34:36,512 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.self.key.weight
2022-04-10 00:34:36,513 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.self.key.bias
2022-04-10 00:34:36,513 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.self.value.weight
2022-04-10 00:34:36,513 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.self.value.bias
2022-04-10 00:34:36,513 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.output.dense.weight
2022-04-10 00:34:36,513 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.output.dense.bias
2022-04-10 00:34:36,513 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.output.LayerNorm.weight
2022-04-10 00:34:36,513 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.attention.output.LayerNorm.bias
2022-04-10 00:34:36,513 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.intermediate.dense.weight
2022-04-10 00:34:36,513 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.intermediate.dense.bias
2022-04-10 00:34:36,513 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.output.dense.weight
2022-04-10 00:34:36,513 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.output.dense.bias
2022-04-10 00:34:36,514 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.output.LayerNorm.weight
2022-04-10 00:34:36,514 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.3.output.LayerNorm.bias
2022-04-10 00:34:36,514 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.self.query.weight
2022-04-10 00:34:36,514 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.self.query.bias
2022-04-10 00:34:36,514 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.self.key.weight
2022-04-10 00:34:36,514 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.self.key.bias
2022-04-10 00:34:36,514 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.self.value.weight
2022-04-10 00:34:36,514 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.self.value.bias
2022-04-10 00:34:36,514 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.output.dense.weight
2022-04-10 00:34:36,514 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.output.dense.bias
2022-04-10 00:34:36,514 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.output.LayerNorm.weight
2022-04-10 00:34:36,515 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.attention.output.LayerNorm.bias
2022-04-10 00:34:36,515 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.intermediate.dense.weight
2022-04-10 00:34:36,515 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.intermediate.dense.bias
2022-04-10 00:34:36,515 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.output.dense.weight
2022-04-10 00:34:36,515 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.output.dense.bias
2022-04-10 00:34:36,515 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.output.LayerNorm.weight
2022-04-10 00:34:36,515 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.4.output.LayerNorm.bias
2022-04-10 00:34:36,515 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.self.query.weight
2022-04-10 00:34:36,515 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.self.query.bias
2022-04-10 00:34:36,515 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.self.key.weight
2022-04-10 00:34:36,515 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.self.key.bias
2022-04-10 00:34:36,516 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.self.value.weight
2022-04-10 00:34:36,516 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.self.value.bias
2022-04-10 00:34:36,516 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.output.dense.weight
2022-04-10 00:34:36,516 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.output.dense.bias
2022-04-10 00:34:36,516 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.output.LayerNorm.weight
2022-04-10 00:34:36,516 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.attention.output.LayerNorm.bias
2022-04-10 00:34:36,516 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.intermediate.dense.weight
2022-04-10 00:34:36,516 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.intermediate.dense.bias
2022-04-10 00:34:36,516 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.output.dense.weight
2022-04-10 00:34:36,516 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.output.dense.bias
2022-04-10 00:34:36,517 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.output.LayerNorm.weight
2022-04-10 00:34:36,517 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.5.output.LayerNorm.bias
2022-04-10 00:34:36,517 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.self.query.weight
2022-04-10 00:34:36,517 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.self.query.bias
2022-04-10 00:34:36,517 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.self.key.weight
2022-04-10 00:34:36,517 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.self.key.bias
2022-04-10 00:34:36,517 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.self.value.weight
2022-04-10 00:34:36,517 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.self.value.bias
2022-04-10 00:34:36,517 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.output.dense.weight
2022-04-10 00:34:36,517 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.output.dense.bias
2022-04-10 00:34:36,517 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.output.LayerNorm.weight
2022-04-10 00:34:36,518 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.attention.output.LayerNorm.bias
2022-04-10 00:34:36,518 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.intermediate.dense.weight
2022-04-10 00:34:36,518 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.intermediate.dense.bias
2022-04-10 00:34:36,518 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.output.dense.weight
2022-04-10 00:34:36,518 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.output.dense.bias
2022-04-10 00:34:36,518 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.output.LayerNorm.weight
2022-04-10 00:34:36,518 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.6.output.LayerNorm.bias
2022-04-10 00:34:36,518 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.self.query.weight
2022-04-10 00:34:36,518 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.self.query.bias
2022-04-10 00:34:36,518 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.self.key.weight
2022-04-10 00:34:36,518 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.self.key.bias
2022-04-10 00:34:36,519 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.self.value.weight
2022-04-10 00:34:36,519 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.self.value.bias
2022-04-10 00:34:36,519 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.output.dense.weight
2022-04-10 00:34:36,519 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.output.dense.bias
2022-04-10 00:34:36,519 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.output.LayerNorm.weight
2022-04-10 00:34:36,519 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.attention.output.LayerNorm.bias
2022-04-10 00:34:36,519 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.intermediate.dense.weight
2022-04-10 00:34:36,519 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.intermediate.dense.bias
2022-04-10 00:34:36,519 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.output.dense.weight
2022-04-10 00:34:36,519 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.output.dense.bias
2022-04-10 00:34:36,519 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.output.LayerNorm.weight
2022-04-10 00:34:36,520 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.7.output.LayerNorm.bias
2022-04-10 00:34:36,520 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.self.query.weight
2022-04-10 00:34:36,520 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.self.query.bias
2022-04-10 00:34:36,520 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.self.key.weight
2022-04-10 00:34:36,520 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.self.key.bias
2022-04-10 00:34:36,520 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.self.value.weight
2022-04-10 00:34:36,520 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.self.value.bias
2022-04-10 00:34:36,520 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.output.dense.weight
2022-04-10 00:34:36,520 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.output.dense.bias
2022-04-10 00:34:36,520 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.output.LayerNorm.weight
2022-04-10 00:34:36,520 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.attention.output.LayerNorm.bias
2022-04-10 00:34:36,521 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.intermediate.dense.weight
2022-04-10 00:34:36,521 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.intermediate.dense.bias
2022-04-10 00:34:36,521 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.output.dense.weight
2022-04-10 00:34:36,521 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.output.dense.bias
2022-04-10 00:34:36,521 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.output.LayerNorm.weight
2022-04-10 00:34:36,521 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.8.output.LayerNorm.bias
2022-04-10 00:34:36,521 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.self.query.weight
2022-04-10 00:34:36,521 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.self.query.bias
2022-04-10 00:34:36,521 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.self.key.weight
2022-04-10 00:34:36,521 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.self.key.bias
2022-04-10 00:34:36,521 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.self.value.weight
2022-04-10 00:34:36,522 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.self.value.bias
2022-04-10 00:34:36,522 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.output.dense.weight
2022-04-10 00:34:36,522 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.output.dense.bias
2022-04-10 00:34:36,522 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.output.LayerNorm.weight
2022-04-10 00:34:36,522 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.attention.output.LayerNorm.bias
2022-04-10 00:34:36,522 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.intermediate.dense.weight
2022-04-10 00:34:36,522 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.intermediate.dense.bias
2022-04-10 00:34:36,522 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.output.dense.weight
2022-04-10 00:34:36,522 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.output.dense.bias
2022-04-10 00:34:36,522 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.output.LayerNorm.weight
2022-04-10 00:34:36,522 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.9.output.LayerNorm.bias
2022-04-10 00:34:36,523 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.self.query.weight
2022-04-10 00:34:36,523 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.self.query.bias
2022-04-10 00:34:36,523 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.self.key.weight
2022-04-10 00:34:36,523 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.self.key.bias
2022-04-10 00:34:36,523 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.self.value.weight
2022-04-10 00:34:36,523 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.self.value.bias
2022-04-10 00:34:36,523 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.output.dense.weight
2022-04-10 00:34:36,523 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.output.dense.bias
2022-04-10 00:34:36,523 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.output.LayerNorm.weight
2022-04-10 00:34:36,523 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.attention.output.LayerNorm.bias
2022-04-10 00:34:36,523 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.intermediate.dense.weight
2022-04-10 00:34:36,524 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.intermediate.dense.bias
2022-04-10 00:34:36,524 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.output.dense.weight
2022-04-10 00:34:36,524 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.output.dense.bias
2022-04-10 00:34:36,524 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.output.LayerNorm.weight
2022-04-10 00:34:36,524 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.10.output.LayerNorm.bias
2022-04-10 00:34:36,524 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.self.query.weight
2022-04-10 00:34:36,524 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.self.query.bias
2022-04-10 00:34:36,524 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.self.key.weight
2022-04-10 00:34:36,524 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.self.key.bias
2022-04-10 00:34:36,524 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.self.value.weight
2022-04-10 00:34:36,525 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.self.value.bias
2022-04-10 00:34:36,525 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.output.dense.weight
2022-04-10 00:34:36,525 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.output.dense.bias
2022-04-10 00:34:36,525 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.output.LayerNorm.weight
2022-04-10 00:34:36,525 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.attention.output.LayerNorm.bias
2022-04-10 00:34:36,525 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.intermediate.dense.weight
2022-04-10 00:34:36,525 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.intermediate.dense.bias
2022-04-10 00:34:36,525 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.output.dense.weight
2022-04-10 00:34:36,525 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.output.dense.bias
2022-04-10 00:34:36,525 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.output.LayerNorm.weight
2022-04-10 00:34:36,525 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.11.output.LayerNorm.bias
2022-04-10 00:34:36,525 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.self.query.weight
2022-04-10 00:34:36,526 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.self.query.bias
2022-04-10 00:34:36,526 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.self.key.weight
2022-04-10 00:34:36,526 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.self.key.bias
2022-04-10 00:34:36,526 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.self.value.weight
2022-04-10 00:34:36,526 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.self.value.bias
2022-04-10 00:34:36,526 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.output.dense.weight
2022-04-10 00:34:36,526 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.output.dense.bias
2022-04-10 00:34:36,526 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.output.LayerNorm.weight
2022-04-10 00:34:36,526 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.attention.output.LayerNorm.bias
2022-04-10 00:34:36,526 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.intermediate.dense.weight
2022-04-10 00:34:36,526 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.intermediate.dense.bias
2022-04-10 00:34:36,527 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.output.dense.weight
2022-04-10 00:34:36,527 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.output.dense.bias
2022-04-10 00:34:36,527 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.output.LayerNorm.weight
2022-04-10 00:34:36,527 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.12.output.LayerNorm.bias
2022-04-10 00:34:36,527 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.self.query.weight
2022-04-10 00:34:36,527 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.self.query.bias
2022-04-10 00:34:36,527 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.self.key.weight
2022-04-10 00:34:36,527 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.self.key.bias
2022-04-10 00:34:36,527 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.self.value.weight
2022-04-10 00:34:36,527 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.self.value.bias
2022-04-10 00:34:36,527 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.output.dense.weight
2022-04-10 00:34:36,528 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.output.dense.bias
2022-04-10 00:34:36,528 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.output.LayerNorm.weight
2022-04-10 00:34:36,528 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.attention.output.LayerNorm.bias
2022-04-10 00:34:36,528 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.intermediate.dense.weight
2022-04-10 00:34:36,528 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.intermediate.dense.bias
2022-04-10 00:34:36,528 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.output.dense.weight
2022-04-10 00:34:36,528 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.output.dense.bias
2022-04-10 00:34:36,528 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.output.LayerNorm.weight
2022-04-10 00:34:36,528 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.13.output.LayerNorm.bias
2022-04-10 00:34:36,528 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.self.query.weight
2022-04-10 00:34:36,528 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.self.query.bias
2022-04-10 00:34:36,529 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.self.key.weight
2022-04-10 00:34:36,529 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.self.key.bias
2022-04-10 00:34:36,529 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.self.value.weight
2022-04-10 00:34:36,529 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.self.value.bias
2022-04-10 00:34:36,529 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.output.dense.weight
2022-04-10 00:34:36,529 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.output.dense.bias
2022-04-10 00:34:36,529 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.output.LayerNorm.weight
2022-04-10 00:34:36,529 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.attention.output.LayerNorm.bias
2022-04-10 00:34:36,529 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.intermediate.dense.weight
2022-04-10 00:34:36,529 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.intermediate.dense.bias
2022-04-10 00:34:36,529 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.output.dense.weight
2022-04-10 00:34:36,530 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.output.dense.bias
2022-04-10 00:34:36,530 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.output.LayerNorm.weight
2022-04-10 00:34:36,530 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.14.output.LayerNorm.bias
2022-04-10 00:34:36,530 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.self.query.weight
2022-04-10 00:34:36,530 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.self.query.bias
2022-04-10 00:34:36,530 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.self.key.weight
2022-04-10 00:34:36,530 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.self.key.bias
2022-04-10 00:34:36,530 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.self.value.weight
2022-04-10 00:34:36,530 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.self.value.bias
2022-04-10 00:34:36,530 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.output.dense.weight
2022-04-10 00:34:36,530 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.output.dense.bias
2022-04-10 00:34:36,531 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.output.LayerNorm.weight
2022-04-10 00:34:36,531 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.attention.output.LayerNorm.bias
2022-04-10 00:34:36,531 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.intermediate.dense.weight
2022-04-10 00:34:36,531 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.intermediate.dense.bias
2022-04-10 00:34:36,531 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.output.dense.weight
2022-04-10 00:34:36,531 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.output.dense.bias
2022-04-10 00:34:36,531 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.output.LayerNorm.weight
2022-04-10 00:34:36,531 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.15.output.LayerNorm.bias
2022-04-10 00:34:36,531 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.self.query.weight
2022-04-10 00:34:36,531 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.self.query.bias
2022-04-10 00:34:36,532 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.self.key.weight
2022-04-10 00:34:36,532 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.self.key.bias
2022-04-10 00:34:36,532 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.self.value.weight
2022-04-10 00:34:36,532 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.self.value.bias
2022-04-10 00:34:36,532 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.output.dense.weight
2022-04-10 00:34:36,532 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.output.dense.bias
2022-04-10 00:34:36,532 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.output.LayerNorm.weight
2022-04-10 00:34:36,532 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.attention.output.LayerNorm.bias
2022-04-10 00:34:36,532 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.intermediate.dense.weight
2022-04-10 00:34:36,532 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.intermediate.dense.bias
2022-04-10 00:34:36,532 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.output.dense.weight
2022-04-10 00:34:36,533 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.output.dense.bias
2022-04-10 00:34:36,533 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.output.LayerNorm.weight
2022-04-10 00:34:36,533 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.16.output.LayerNorm.bias
2022-04-10 00:34:36,533 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.self.query.weight
2022-04-10 00:34:36,533 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.self.query.bias
2022-04-10 00:34:36,533 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.self.key.weight
2022-04-10 00:34:36,533 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.self.key.bias
2022-04-10 00:34:36,533 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.self.value.weight
2022-04-10 00:34:36,533 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.self.value.bias
2022-04-10 00:34:36,533 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.output.dense.weight
2022-04-10 00:34:36,533 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.output.dense.bias
2022-04-10 00:34:36,534 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.output.LayerNorm.weight
2022-04-10 00:34:36,534 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.attention.output.LayerNorm.bias
2022-04-10 00:34:36,534 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.intermediate.dense.weight
2022-04-10 00:34:36,534 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.intermediate.dense.bias
2022-04-10 00:34:36,534 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.output.dense.weight
2022-04-10 00:34:36,534 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.output.dense.bias
2022-04-10 00:34:36,534 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.output.LayerNorm.weight
2022-04-10 00:34:36,534 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.17.output.LayerNorm.bias
2022-04-10 00:34:36,534 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.self.query.weight
2022-04-10 00:34:36,534 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.self.query.bias
2022-04-10 00:34:36,534 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.self.key.weight
2022-04-10 00:34:36,535 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.self.key.bias
2022-04-10 00:34:36,535 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.self.value.weight
2022-04-10 00:34:36,535 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.self.value.bias
2022-04-10 00:34:36,535 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.output.dense.weight
2022-04-10 00:34:36,535 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.output.dense.bias
2022-04-10 00:34:36,535 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.output.LayerNorm.weight
2022-04-10 00:34:36,535 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.attention.output.LayerNorm.bias
2022-04-10 00:34:36,535 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.intermediate.dense.weight
2022-04-10 00:34:36,535 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.intermediate.dense.bias
2022-04-10 00:34:36,535 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.output.dense.weight
2022-04-10 00:34:36,535 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.output.dense.bias
2022-04-10 00:34:36,536 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.output.LayerNorm.weight
2022-04-10 00:34:36,536 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.18.output.LayerNorm.bias
2022-04-10 00:34:36,536 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.self.query.weight
2022-04-10 00:34:36,536 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.self.query.bias
2022-04-10 00:34:36,536 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.self.key.weight
2022-04-10 00:34:36,536 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.self.key.bias
2022-04-10 00:34:36,536 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.self.value.weight
2022-04-10 00:34:36,536 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.self.value.bias
2022-04-10 00:34:36,536 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.output.dense.weight
2022-04-10 00:34:36,536 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.output.dense.bias
2022-04-10 00:34:36,536 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.output.LayerNorm.weight
2022-04-10 00:34:36,537 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.attention.output.LayerNorm.bias
2022-04-10 00:34:36,537 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.intermediate.dense.weight
2022-04-10 00:34:36,537 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.intermediate.dense.bias
2022-04-10 00:34:36,537 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.output.dense.weight
2022-04-10 00:34:36,537 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.output.dense.bias
2022-04-10 00:34:36,537 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.output.LayerNorm.weight
2022-04-10 00:34:36,537 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.19.output.LayerNorm.bias
2022-04-10 00:34:36,537 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.self.query.weight
2022-04-10 00:34:36,537 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.self.query.bias
2022-04-10 00:34:36,537 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.self.key.weight
2022-04-10 00:34:36,537 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.self.key.bias
2022-04-10 00:34:36,538 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.self.value.weight
2022-04-10 00:34:36,538 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.self.value.bias
2022-04-10 00:34:36,538 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.output.dense.weight
2022-04-10 00:34:36,538 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.output.dense.bias
2022-04-10 00:34:36,538 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.output.LayerNorm.weight
2022-04-10 00:34:36,538 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.attention.output.LayerNorm.bias
2022-04-10 00:34:36,538 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.intermediate.dense.weight
2022-04-10 00:34:36,538 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.intermediate.dense.bias
2022-04-10 00:34:36,538 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.output.dense.weight
2022-04-10 00:34:36,538 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.output.dense.bias
2022-04-10 00:34:36,539 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.output.LayerNorm.weight
2022-04-10 00:34:36,539 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.20.output.LayerNorm.bias
2022-04-10 00:34:36,539 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.self.query.weight
2022-04-10 00:34:36,539 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.self.query.bias
2022-04-10 00:34:36,539 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.self.key.weight
2022-04-10 00:34:36,539 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.self.key.bias
2022-04-10 00:34:36,539 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.self.value.weight
2022-04-10 00:34:36,539 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.self.value.bias
2022-04-10 00:34:36,539 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.output.dense.weight
2022-04-10 00:34:36,539 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.output.dense.bias
2022-04-10 00:34:36,539 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.output.LayerNorm.weight
2022-04-10 00:34:36,540 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.attention.output.LayerNorm.bias
2022-04-10 00:34:36,540 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.intermediate.dense.weight
2022-04-10 00:34:36,540 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.intermediate.dense.bias
2022-04-10 00:34:36,540 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.output.dense.weight
2022-04-10 00:34:36,540 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.output.dense.bias
2022-04-10 00:34:36,540 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.output.LayerNorm.weight
2022-04-10 00:34:36,540 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.21.output.LayerNorm.bias
2022-04-10 00:34:36,540 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.self.query.weight
2022-04-10 00:34:36,540 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.self.query.bias
2022-04-10 00:34:36,540 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.self.key.weight
2022-04-10 00:34:36,541 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.self.key.bias
2022-04-10 00:34:36,541 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.self.value.weight
2022-04-10 00:34:36,541 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.self.value.bias
2022-04-10 00:34:36,541 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.output.dense.weight
2022-04-10 00:34:36,541 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.output.dense.bias
2022-04-10 00:34:36,541 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.output.LayerNorm.weight
2022-04-10 00:34:36,541 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.attention.output.LayerNorm.bias
2022-04-10 00:34:36,541 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.intermediate.dense.weight
2022-04-10 00:34:36,541 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.intermediate.dense.bias
2022-04-10 00:34:36,541 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.output.dense.weight
2022-04-10 00:34:36,541 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.output.dense.bias
2022-04-10 00:34:36,542 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.output.LayerNorm.weight
2022-04-10 00:34:36,542 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.22.output.LayerNorm.bias
2022-04-10 00:34:36,542 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.self.query.weight
2022-04-10 00:34:36,542 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.self.query.bias
2022-04-10 00:34:36,542 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.self.key.weight
2022-04-10 00:34:36,542 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.self.key.bias
2022-04-10 00:34:36,542 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.self.value.weight
2022-04-10 00:34:36,542 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.self.value.bias
2022-04-10 00:34:36,542 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.output.dense.weight
2022-04-10 00:34:36,542 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.output.dense.bias
2022-04-10 00:34:36,542 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.output.LayerNorm.weight
2022-04-10 00:34:36,543 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.attention.output.LayerNorm.bias
2022-04-10 00:34:36,543 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.intermediate.dense.weight
2022-04-10 00:34:36,543 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.intermediate.dense.bias
2022-04-10 00:34:36,543 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.output.dense.weight
2022-04-10 00:34:36,543 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.output.dense.bias
2022-04-10 00:34:36,543 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.output.LayerNorm.weight
2022-04-10 00:34:36,543 - INFO - allennlp.common.util - _classifier.roberta.encoder.layer.23.output.LayerNorm.bias
2022-04-10 00:34:36,543 - INFO - allennlp.common.util - _classifier.roberta.pooler.dense.weight
2022-04-10 00:34:36,543 - INFO - allennlp.common.util - _classifier.roberta.pooler.dense.bias
2022-04-10 00:34:36,543 - INFO - allennlp.common.util - _classifier.classifier.dense.weight
2022-04-10 00:34:36,543 - INFO - allennlp.common.util - _classifier.classifier.dense.bias
2022-04-10 00:34:36,544 - INFO - allennlp.common.util - _classifier.classifier.out_proj.weight
2022-04-10 00:34:36,544 - INFO - allennlp.common.util - _classifier.classifier.out_proj.bias
2022-04-10 00:34:36,544 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.type = slanted_triangular
2022-04-10 00:34:36,544 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.cut_frac = 0.06
2022-04-10 00:34:36,544 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.ratio = 32
2022-04-10 00:34:36,544 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.last_epoch = -1
2022-04-10 00:34:36,544 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.gradual_unfreezing = False
2022-04-10 00:34:36,545 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.discriminative_fine_tuning = False
2022-04-10 00:34:36,545 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.decay_factor = 0.38
2022-04-10 00:34:36,545 - INFO - allennlp.common.params - type = default
2022-04-10 00:34:36,545 - INFO - allennlp.common.params - keep_serialized_model_every_num_seconds = None
2022-04-10 00:34:36,545 - INFO - allennlp.common.params - num_serialized_models_to_keep = 2
2022-04-10 00:34:36,545 - INFO - allennlp.common.params - model_save_interval = None
2022-04-10 00:34:36,545 - INFO - allennlp.common.params - summary_interval = 100
2022-04-10 00:34:36,546 - INFO - allennlp.common.params - histogram_interval = None
2022-04-10 00:34:36,546 - INFO - allennlp.common.params - batch_size_interval = None
2022-04-10 00:34:36,546 - INFO - allennlp.common.params - should_log_parameter_statistics = True
2022-04-10 00:34:36,546 - INFO - allennlp.common.params - should_log_learning_rate = False
2022-04-10 00:34:36,546 - INFO - allennlp.common.params - get_batch_num_total = None
2022-04-10 00:34:36,548 - WARNING - allennlp.training.trainer - You provided a validation dataset but patience was set to None, meaning that early stopping is disabled
2022-04-10 00:34:36,640 - INFO - allennlp.training.trainer - Beginning training.
2022-04-10 00:34:36,640 - INFO - allennlp.training.trainer - Epoch 0/14
2022-04-10 00:34:36,640 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-10 00:34:36,640 - INFO - allennlp.training.trainer - GPU 0 memory usage: 1.3G
2022-04-10 00:34:36,641 - INFO - allennlp.training.trainer - Training
2022-04-10 00:34:36,641 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-10 00:34:36,642 - INFO - allennlp.data.samplers.bucket_batch_sampler - No sorting keys given; trying to guess a good one
2022-04-10 00:34:36,642 - INFO - allennlp.data.samplers.bucket_batch_sampler - Using ['tokens'] as the sorting keys
2022-04-10 00:34:46,916 - INFO - tqdm - accuracy: 0.4688, batch_loss: 1.5490, loss: 1.5490 ||:   2%|1         | 1/65 [00:10<10:57, 10.27s/it]
2022-04-10 00:34:57,339 - INFO - tqdm - accuracy: 0.5156, batch_loss: 1.5481, loss: 1.5485 ||:   3%|3         | 2/65 [00:20<10:52, 10.36s/it]
2022-04-10 00:35:07,907 - INFO - tqdm - accuracy: 0.5208, batch_loss: 2.0186, loss: 1.7052 ||:   5%|4         | 3/65 [00:31<10:48, 10.46s/it]
2022-04-10 00:35:18,540 - INFO - tqdm - accuracy: 0.5469, batch_loss: 1.3413, loss: 1.6143 ||:   6%|6         | 4/65 [00:41<10:42, 10.53s/it]
2022-04-10 00:35:29,158 - INFO - tqdm - accuracy: 0.5312, batch_loss: 1.2133, loss: 1.5341 ||:   8%|7         | 5/65 [00:52<10:33, 10.56s/it]
2022-04-10 00:35:39,863 - INFO - tqdm - accuracy: 0.5156, batch_loss: 1.6996, loss: 1.5617 ||:   9%|9         | 6/65 [01:03<10:25, 10.61s/it]
2022-04-10 00:35:50,658 - INFO - tqdm - accuracy: 0.5223, batch_loss: 1.4528, loss: 1.5461 ||:  11%|#         | 7/65 [01:14<10:18, 10.67s/it]
2022-04-10 00:36:01,522 - INFO - tqdm - accuracy: 0.5156, batch_loss: 1.5003, loss: 1.5404 ||:  12%|#2        | 8/65 [01:24<10:11, 10.73s/it]
2022-04-10 00:36:12,323 - INFO - tqdm - accuracy: 0.5104, batch_loss: 1.6756, loss: 1.5554 ||:  14%|#3        | 9/65 [01:35<10:02, 10.75s/it]
2022-04-10 00:36:23,056 - INFO - tqdm - accuracy: 0.4969, batch_loss: 1.4999, loss: 1.5499 ||:  15%|#5        | 10/65 [01:46<09:51, 10.75s/it]
2022-04-10 00:36:33,988 - INFO - tqdm - accuracy: 0.4943, batch_loss: 1.4280, loss: 1.5388 ||:  17%|#6        | 11/65 [01:57<09:43, 10.80s/it]
2022-04-10 00:36:44,995 - INFO - tqdm - accuracy: 0.5104, batch_loss: 0.9455, loss: 1.4893 ||:  18%|#8        | 12/65 [02:08<09:35, 10.87s/it]
2022-04-10 00:36:56,064 - INFO - tqdm - accuracy: 0.5144, batch_loss: 0.8639, loss: 1.4412 ||:  20%|##        | 13/65 [02:19<09:28, 10.93s/it]
2022-04-10 00:37:07,084 - INFO - tqdm - accuracy: 0.5134, batch_loss: 1.3290, loss: 1.4332 ||:  22%|##1       | 14/65 [02:30<09:18, 10.96s/it]
2022-04-10 00:37:18,310 - INFO - tqdm - accuracy: 0.5167, batch_loss: 1.0016, loss: 1.4044 ||:  23%|##3       | 15/65 [02:41<09:11, 11.04s/it]
2022-04-10 00:37:29,583 - INFO - tqdm - accuracy: 0.5059, batch_loss: 1.0293, loss: 1.3810 ||:  25%|##4       | 16/65 [02:52<09:04, 11.11s/it]
2022-04-10 00:37:40,909 - INFO - tqdm - accuracy: 0.5110, batch_loss: 0.5857, loss: 1.3342 ||:  26%|##6       | 17/65 [03:04<08:56, 11.17s/it]
2022-04-10 00:37:52,299 - INFO - tqdm - accuracy: 0.5156, batch_loss: 0.8142, loss: 1.3053 ||:  28%|##7       | 18/65 [03:15<08:48, 11.24s/it]
2022-04-10 00:38:03,714 - INFO - tqdm - accuracy: 0.5181, batch_loss: 0.8293, loss: 1.2803 ||:  29%|##9       | 19/65 [03:27<08:39, 11.29s/it]
2022-04-10 00:38:14,931 - INFO - tqdm - accuracy: 0.5234, batch_loss: 0.6349, loss: 1.2480 ||:  31%|###       | 20/65 [03:38<08:27, 11.27s/it]
2022-04-10 00:38:26,376 - INFO - tqdm - accuracy: 0.5193, batch_loss: 0.9824, loss: 1.2354 ||:  32%|###2      | 21/65 [03:49<08:18, 11.32s/it]
2022-04-10 00:38:37,655 - INFO - tqdm - accuracy: 0.5213, batch_loss: 0.7504, loss: 1.2133 ||:  34%|###3      | 22/65 [04:01<08:06, 11.31s/it]
2022-04-10 00:38:49,019 - INFO - tqdm - accuracy: 0.5204, batch_loss: 0.7836, loss: 1.1946 ||:  35%|###5      | 23/65 [04:12<07:55, 11.33s/it]
2022-04-10 00:39:00,393 - INFO - tqdm - accuracy: 0.5182, batch_loss: 0.7596, loss: 1.1765 ||:  37%|###6      | 24/65 [04:23<07:44, 11.34s/it]
2022-04-10 00:39:11,798 - INFO - tqdm - accuracy: 0.5138, batch_loss: 0.8454, loss: 1.1633 ||:  38%|###8      | 25/65 [04:35<07:34, 11.36s/it]
2022-04-10 00:39:23,224 - INFO - tqdm - accuracy: 0.5096, batch_loss: 0.7289, loss: 1.1466 ||:  40%|####      | 26/65 [04:46<07:23, 11.38s/it]
2022-04-10 00:39:34,664 - INFO - tqdm - accuracy: 0.5127, batch_loss: 0.6501, loss: 1.1282 ||:  42%|####1     | 27/65 [04:58<07:13, 11.40s/it]
2022-04-10 00:39:46,135 - INFO - tqdm - accuracy: 0.5123, batch_loss: 0.7310, loss: 1.1140 ||:  43%|####3     | 28/65 [05:09<07:02, 11.42s/it]
2022-04-10 00:39:57,602 - INFO - tqdm - accuracy: 0.5097, batch_loss: 0.7286, loss: 1.1007 ||:  45%|####4     | 29/65 [05:20<06:51, 11.43s/it]
2022-04-10 00:40:09,029 - INFO - tqdm - accuracy: 0.5073, batch_loss: 0.7452, loss: 1.0888 ||:  46%|####6     | 30/65 [05:32<06:40, 11.43s/it]
2022-04-10 00:40:20,472 - INFO - tqdm - accuracy: 0.5101, batch_loss: 0.6677, loss: 1.0753 ||:  48%|####7     | 31/65 [05:43<06:28, 11.44s/it]
2022-04-10 00:40:31,930 - INFO - tqdm - accuracy: 0.5088, batch_loss: 0.7434, loss: 1.0649 ||:  49%|####9     | 32/65 [05:55<06:17, 11.44s/it]
2022-04-10 00:40:43,381 - INFO - tqdm - accuracy: 0.5076, batch_loss: 0.7016, loss: 1.0539 ||:  51%|#####     | 33/65 [06:06<06:06, 11.44s/it]
2022-04-10 00:40:54,820 - INFO - tqdm - accuracy: 0.5074, batch_loss: 0.7002, loss: 1.0435 ||:  52%|#####2    | 34/65 [06:18<05:54, 11.44s/it]
2022-04-10 00:41:06,249 - INFO - tqdm - accuracy: 0.5080, batch_loss: 0.7122, loss: 1.0340 ||:  54%|#####3    | 35/65 [06:29<05:43, 11.44s/it]
2022-04-10 00:41:17,688 - INFO - tqdm - accuracy: 0.5069, batch_loss: 0.6845, loss: 1.0243 ||:  55%|#####5    | 36/65 [06:41<05:31, 11.44s/it]
2022-04-10 00:41:29,102 - INFO - tqdm - accuracy: 0.5034, batch_loss: 0.7112, loss: 1.0158 ||:  57%|#####6    | 37/65 [06:52<05:20, 11.43s/it]
2022-04-10 00:41:40,511 - INFO - tqdm - accuracy: 0.5058, batch_loss: 0.6741, loss: 1.0068 ||:  58%|#####8    | 38/65 [07:03<05:08, 11.42s/it]
2022-04-10 00:41:51,936 - INFO - tqdm - accuracy: 0.5064, batch_loss: 0.6901, loss: 0.9987 ||:  60%|######    | 39/65 [07:15<04:57, 11.42s/it]
2022-04-10 00:42:03,351 - INFO - tqdm - accuracy: 0.5094, batch_loss: 0.6665, loss: 0.9904 ||:  62%|######1   | 40/65 [07:26<04:45, 11.42s/it]
2022-04-10 00:42:14,745 - INFO - tqdm - accuracy: 0.5053, batch_loss: 0.7384, loss: 0.9843 ||:  63%|######3   | 41/65 [07:38<04:33, 11.41s/it]
2022-04-10 00:42:26,150 - INFO - tqdm - accuracy: 0.5074, batch_loss: 0.6601, loss: 0.9766 ||:  65%|######4   | 42/65 [07:49<04:22, 11.41s/it]
2022-04-10 00:42:37,574 - INFO - tqdm - accuracy: 0.5036, batch_loss: 0.7332, loss: 0.9709 ||:  66%|######6   | 43/65 [08:00<04:11, 11.41s/it]
2022-04-10 00:42:48,993 - INFO - tqdm - accuracy: 0.5064, batch_loss: 0.6484, loss: 0.9636 ||:  68%|######7   | 44/65 [08:12<03:59, 11.42s/it]
2022-04-10 00:43:00,394 - INFO - tqdm - accuracy: 0.5069, batch_loss: 0.6969, loss: 0.9576 ||:  69%|######9   | 45/65 [08:23<03:48, 11.41s/it]
2022-04-10 00:43:11,806 - INFO - tqdm - accuracy: 0.5075, batch_loss: 0.6834, loss: 0.9517 ||:  71%|#######   | 46/65 [08:35<03:36, 11.41s/it]
2022-04-10 00:43:23,220 - INFO - tqdm - accuracy: 0.5093, batch_loss: 0.6779, loss: 0.9459 ||:  72%|#######2  | 47/65 [08:46<03:25, 11.41s/it]
2022-04-10 00:43:34,619 - INFO - tqdm - accuracy: 0.5078, batch_loss: 0.7285, loss: 0.9413 ||:  74%|#######3  | 48/65 [08:57<03:13, 11.41s/it]
2022-04-10 00:43:46,027 - INFO - tqdm - accuracy: 0.5083, batch_loss: 0.6886, loss: 0.9362 ||:  75%|#######5  | 49/65 [09:09<03:02, 11.41s/it]
2022-04-10 00:43:57,419 - INFO - tqdm - accuracy: 0.5094, batch_loss: 0.6851, loss: 0.9311 ||:  77%|#######6  | 50/65 [09:20<02:51, 11.40s/it]
2022-04-10 00:44:08,815 - INFO - tqdm - accuracy: 0.5080, batch_loss: 0.7242, loss: 0.9271 ||:  78%|#######8  | 51/65 [09:32<02:39, 11.40s/it]
2022-04-10 00:44:20,230 - INFO - tqdm - accuracy: 0.5108, batch_loss: 0.6435, loss: 0.9216 ||:  80%|########  | 52/65 [09:43<02:28, 11.41s/it]
2022-04-10 00:44:31,628 - INFO - tqdm - accuracy: 0.5118, batch_loss: 0.6858, loss: 0.9172 ||:  82%|########1 | 53/65 [09:54<02:16, 11.40s/it]
2022-04-10 00:44:42,693 - INFO - tqdm - accuracy: 0.5165, batch_loss: 0.6167, loss: 0.9116 ||:  83%|########3 | 54/65 [10:06<02:04, 11.30s/it]
2022-04-10 00:44:54,104 - INFO - tqdm - accuracy: 0.5139, batch_loss: 0.7289, loss: 0.9083 ||:  85%|########4 | 55/65 [10:17<01:53, 11.33s/it]
2022-04-10 00:45:05,527 - INFO - tqdm - accuracy: 0.5142, batch_loss: 0.6923, loss: 0.9044 ||:  86%|########6 | 56/65 [10:28<01:42, 11.36s/it]
2022-04-10 00:45:16,945 - INFO - tqdm - accuracy: 0.5096, batch_loss: 0.7957, loss: 0.9025 ||:  88%|########7 | 57/65 [10:40<01:31, 11.38s/it]
2022-04-10 00:45:28,364 - INFO - tqdm - accuracy: 0.5094, batch_loss: 0.6757, loss: 0.8986 ||:  89%|########9 | 58/65 [10:51<01:19, 11.39s/it]
2022-04-10 00:45:39,770 - INFO - tqdm - accuracy: 0.5098, batch_loss: 0.7114, loss: 0.8954 ||:  91%|######### | 59/65 [11:03<01:08, 11.40s/it]
2022-04-10 00:45:51,177 - INFO - tqdm - accuracy: 0.5117, batch_loss: 0.6728, loss: 0.8917 ||:  92%|#########2| 60/65 [11:14<00:56, 11.40s/it]
2022-04-10 00:46:02,574 - INFO - tqdm - accuracy: 0.5115, batch_loss: 0.6857, loss: 0.8884 ||:  94%|#########3| 61/65 [11:25<00:45, 11.40s/it]
2022-04-10 00:46:14,002 - INFO - tqdm - accuracy: 0.5124, batch_loss: 0.6830, loss: 0.8850 ||:  95%|#########5| 62/65 [11:37<00:34, 11.41s/it]
2022-04-10 00:46:25,432 - INFO - tqdm - accuracy: 0.5117, batch_loss: 0.7103, loss: 0.8823 ||:  97%|#########6| 63/65 [11:48<00:22, 11.41s/it]
2022-04-10 00:46:36,842 - INFO - tqdm - accuracy: 0.5129, batch_loss: 0.6981, loss: 0.8794 ||:  98%|#########8| 64/65 [12:00<00:11, 11.41s/it]
2022-04-10 00:46:41,907 - INFO - tqdm - accuracy: 0.5124, batch_loss: 0.7236, loss: 0.8770 ||: 100%|##########| 65/65 [12:05<00:00,  9.51s/it]
2022-04-10 00:46:41,907 - INFO - tqdm - accuracy: 0.5124, batch_loss: 0.7236, loss: 0.8770 ||: 100%|##########| 65/65 [12:05<00:00, 11.16s/it]
2022-04-10 00:46:44,722 - INFO - allennlp.training.trainer - Validating
2022-04-10 00:46:44,724 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-10 00:46:44,724 - INFO - allennlp.data.samplers.bucket_batch_sampler - No sorting keys given; trying to guess a good one
2022-04-10 00:46:44,724 - INFO - allennlp.data.samplers.bucket_batch_sampler - Using ['tokens'] as the sorting keys
2022-04-10 00:46:54,736 - INFO - tqdm - accuracy: 0.5000, batch_loss: 0.6465, loss: 0.6972 ||:  35%|###4      | 40/115 [00:10<00:18,  3.99it/s]
2022-04-10 00:47:04,747 - INFO - tqdm - accuracy: 0.4625, batch_loss: 0.6495, loss: 0.7004 ||:  70%|######9   | 80/115 [00:20<00:08,  4.02it/s]
2022-04-10 00:47:13,550 - INFO - tqdm - accuracy: 0.4716, batch_loss: 0.7969, loss: 0.6994 ||: 100%|##########| 115/115 [00:28<00:00,  3.94it/s]
2022-04-10 00:47:13,551 - INFO - tqdm - accuracy: 0.4716, batch_loss: 0.7969, loss: 0.6994 ||: 100%|##########| 115/115 [00:28<00:00,  3.99it/s]
2022-04-10 00:47:13,551 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-10 00:47:13,552 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.512  |     0.472
2022-04-10 00:47:13,552 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  1359.606  |       N/A
2022-04-10 00:47:13,553 - INFO - allennlp.training.tensorboard_writer - loss               |     0.877  |     0.699
2022-04-10 00:47:13,554 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5813.805  |       N/A
2022-04-10 00:47:19,337 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to './output_ir-ora-d_hyper2/best.th'.
2022-04-10 00:47:20,614 - INFO - allennlp.training.trainer - Epoch duration: 0:12:43.973882
2022-04-10 00:47:20,614 - INFO - allennlp.training.trainer - Estimated training time remaining: 2:58:15
2022-04-10 00:47:20,614 - INFO - allennlp.training.trainer - Epoch 1/14
2022-04-10 00:47:20,614 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-10 00:47:20,614 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-10 00:47:20,616 - INFO - allennlp.training.trainer - Training
2022-04-10 00:47:20,616 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-10 00:47:31,871 - INFO - tqdm - accuracy: 0.3438, batch_loss: 0.7156, loss: 0.7156 ||:   2%|1         | 1/65 [00:11<12:00, 11.25s/it]
2022-04-10 00:47:43,367 - INFO - tqdm - accuracy: 0.4375, batch_loss: 0.6840, loss: 0.6998 ||:   3%|3         | 2/65 [00:22<11:57, 11.40s/it]
2022-04-10 00:47:55,052 - INFO - tqdm - accuracy: 0.4167, batch_loss: 0.7175, loss: 0.7057 ||:   5%|4         | 3/65 [00:34<11:54, 11.53s/it]
2022-04-10 00:48:06,498 - INFO - tqdm - accuracy: 0.4609, batch_loss: 0.6803, loss: 0.6994 ||:   6%|6         | 4/65 [00:45<11:41, 11.50s/it]
2022-04-10 00:48:17,824 - INFO - tqdm - accuracy: 0.4625, batch_loss: 0.6860, loss: 0.6967 ||:   8%|7         | 5/65 [00:57<11:26, 11.43s/it]
2022-04-10 00:48:29,205 - INFO - tqdm - accuracy: 0.4948, batch_loss: 0.6810, loss: 0.6941 ||:   9%|9         | 6/65 [01:08<11:13, 11.42s/it]
2022-04-10 00:48:40,578 - INFO - tqdm - accuracy: 0.5000, batch_loss: 0.6944, loss: 0.6941 ||:  11%|#         | 7/65 [01:19<11:01, 11.40s/it]
2022-04-10 00:48:51,936 - INFO - tqdm - accuracy: 0.5117, batch_loss: 0.6840, loss: 0.6929 ||:  12%|#2        | 8/65 [01:31<10:49, 11.39s/it]
2022-04-10 00:49:03,316 - INFO - tqdm - accuracy: 0.5243, batch_loss: 0.6671, loss: 0.6900 ||:  14%|#3        | 9/65 [01:42<10:37, 11.39s/it]
2022-04-10 00:49:14,714 - INFO - tqdm - accuracy: 0.5219, batch_loss: 0.6699, loss: 0.6880 ||:  15%|#5        | 10/65 [01:54<10:26, 11.39s/it]
2022-04-10 00:49:26,120 - INFO - tqdm - accuracy: 0.5170, batch_loss: 0.7288, loss: 0.6917 ||:  17%|#6        | 11/65 [02:05<10:15, 11.39s/it]
2022-04-10 00:49:37,548 - INFO - tqdm - accuracy: 0.5208, batch_loss: 0.6758, loss: 0.6904 ||:  18%|#8        | 12/65 [02:16<10:04, 11.40s/it]
2022-04-10 00:49:48,983 - INFO - tqdm - accuracy: 0.5288, batch_loss: 0.6703, loss: 0.6888 ||:  20%|##        | 13/65 [02:28<09:53, 11.41s/it]
2022-04-10 00:50:00,294 - INFO - tqdm - accuracy: 0.5268, batch_loss: 0.7197, loss: 0.6910 ||:  22%|##1       | 14/65 [02:39<09:40, 11.38s/it]
2022-04-10 00:50:11,745 - INFO - tqdm - accuracy: 0.5250, batch_loss: 0.6926, loss: 0.6911 ||:  23%|##3       | 15/65 [02:51<09:30, 11.40s/it]
2022-04-10 00:50:23,201 - INFO - tqdm - accuracy: 0.5273, batch_loss: 0.6836, loss: 0.6907 ||:  25%|##4       | 16/65 [03:02<09:19, 11.42s/it]
2022-04-10 00:50:34,654 - INFO - tqdm - accuracy: 0.5276, batch_loss: 0.7204, loss: 0.6924 ||:  26%|##6       | 17/65 [03:14<09:08, 11.43s/it]
2022-04-10 00:50:46,103 - INFO - tqdm - accuracy: 0.5330, batch_loss: 0.6279, loss: 0.6888 ||:  28%|##7       | 18/65 [03:25<08:57, 11.44s/it]
2022-04-10 00:50:57,546 - INFO - tqdm - accuracy: 0.5329, batch_loss: 0.6825, loss: 0.6885 ||:  29%|##9       | 19/65 [03:36<08:46, 11.44s/it]
2022-04-10 00:51:08,970 - INFO - tqdm - accuracy: 0.5391, batch_loss: 0.6687, loss: 0.6875 ||:  31%|###       | 20/65 [03:48<08:34, 11.43s/it]
2022-04-10 00:51:20,393 - INFO - tqdm - accuracy: 0.5417, batch_loss: 0.6470, loss: 0.6856 ||:  32%|###2      | 21/65 [03:59<08:22, 11.43s/it]
2022-04-10 00:51:31,693 - INFO - tqdm - accuracy: 0.5412, batch_loss: 0.6764, loss: 0.6852 ||:  34%|###3      | 22/65 [04:11<08:09, 11.39s/it]
2022-04-10 00:51:43,109 - INFO - tqdm - accuracy: 0.5435, batch_loss: 0.6677, loss: 0.6844 ||:  35%|###5      | 23/65 [04:22<07:58, 11.40s/it]
2022-04-10 00:51:54,517 - INFO - tqdm - accuracy: 0.5391, batch_loss: 0.7023, loss: 0.6851 ||:  37%|###6      | 24/65 [04:33<07:47, 11.40s/it]
2022-04-10 00:52:05,931 - INFO - tqdm - accuracy: 0.5437, batch_loss: 0.6434, loss: 0.6835 ||:  38%|###8      | 25/65 [04:45<07:36, 11.41s/it]
2022-04-10 00:52:17,339 - INFO - tqdm - accuracy: 0.5457, batch_loss: 0.6669, loss: 0.6828 ||:  40%|####      | 26/65 [04:56<07:24, 11.41s/it]
2022-04-10 00:52:28,729 - INFO - tqdm - accuracy: 0.5463, batch_loss: 0.6658, loss: 0.6822 ||:  42%|####1     | 27/65 [05:08<07:13, 11.40s/it]
2022-04-10 00:52:40,102 - INFO - tqdm - accuracy: 0.5502, batch_loss: 0.6636, loss: 0.6815 ||:  43%|####3     | 28/65 [05:19<07:01, 11.39s/it]
2022-04-10 00:52:51,495 - INFO - tqdm - accuracy: 0.5517, batch_loss: 0.6610, loss: 0.6808 ||:  45%|####4     | 29/65 [05:30<06:50, 11.39s/it]
2022-04-10 00:53:02,867 - INFO - tqdm - accuracy: 0.5521, batch_loss: 0.7032, loss: 0.6816 ||:  46%|####6     | 30/65 [05:42<06:38, 11.39s/it]
2022-04-10 00:53:14,229 - INFO - tqdm - accuracy: 0.5544, batch_loss: 0.6653, loss: 0.6810 ||:  48%|####7     | 31/65 [05:53<06:26, 11.38s/it]
2022-04-10 00:53:25,628 - INFO - tqdm - accuracy: 0.5537, batch_loss: 0.6500, loss: 0.6801 ||:  49%|####9     | 32/65 [06:05<06:15, 11.39s/it]
2022-04-10 00:53:37,025 - INFO - tqdm - accuracy: 0.5540, batch_loss: 0.6910, loss: 0.6804 ||:  51%|#####     | 33/65 [06:16<06:04, 11.39s/it]
2022-04-10 00:53:48,464 - INFO - tqdm - accuracy: 0.5570, batch_loss: 0.6645, loss: 0.6799 ||:  52%|#####2    | 34/65 [06:27<05:53, 11.40s/it]
2022-04-10 00:54:00,181 - INFO - tqdm - accuracy: 0.5571, batch_loss: 0.6718, loss: 0.6797 ||:  54%|#####3    | 35/65 [06:39<05:44, 11.50s/it]
2022-04-10 00:54:11,575 - INFO - tqdm - accuracy: 0.5651, batch_loss: 0.6231, loss: 0.6781 ||:  55%|#####5    | 36/65 [06:50<05:32, 11.47s/it]
2022-04-10 00:54:23,000 - INFO - tqdm - accuracy: 0.5676, batch_loss: 0.5886, loss: 0.6757 ||:  57%|#####6    | 37/65 [07:02<05:20, 11.45s/it]
2022-04-10 00:54:34,415 - INFO - tqdm - accuracy: 0.5691, batch_loss: 0.6806, loss: 0.6758 ||:  58%|#####8    | 38/65 [07:13<05:08, 11.44s/it]
2022-04-10 00:54:45,828 - INFO - tqdm - accuracy: 0.5697, batch_loss: 0.6622, loss: 0.6755 ||:  60%|######    | 39/65 [07:25<04:57, 11.43s/it]
2022-04-10 00:54:57,256 - INFO - tqdm - accuracy: 0.5734, batch_loss: 0.6267, loss: 0.6743 ||:  62%|######1   | 40/65 [07:36<04:45, 11.43s/it]
2022-04-10 00:55:08,641 - INFO - tqdm - accuracy: 0.5747, batch_loss: 0.6303, loss: 0.6732 ||:  63%|######3   | 41/65 [07:48<04:34, 11.42s/it]
2022-04-10 00:55:20,057 - INFO - tqdm - accuracy: 0.5781, batch_loss: 0.6368, loss: 0.6723 ||:  65%|######4   | 42/65 [07:59<04:22, 11.42s/it]
2022-04-10 00:55:31,463 - INFO - tqdm - accuracy: 0.5807, batch_loss: 0.6802, loss: 0.6725 ||:  66%|######6   | 43/65 [08:10<04:11, 11.41s/it]
2022-04-10 00:55:42,853 - INFO - tqdm - accuracy: 0.5817, batch_loss: 0.6643, loss: 0.6723 ||:  68%|######7   | 44/65 [08:22<03:59, 11.41s/it]
2022-04-10 00:55:54,250 - INFO - tqdm - accuracy: 0.5826, batch_loss: 0.5847, loss: 0.6704 ||:  69%|######9   | 45/65 [08:33<03:48, 11.40s/it]
2022-04-10 00:56:05,651 - INFO - tqdm - accuracy: 0.5849, batch_loss: 0.6609, loss: 0.6702 ||:  71%|#######   | 46/65 [08:45<03:36, 11.40s/it]
2022-04-10 00:56:17,052 - INFO - tqdm - accuracy: 0.5871, batch_loss: 0.6682, loss: 0.6701 ||:  72%|#######2  | 47/65 [08:56<03:25, 11.40s/it]
2022-04-10 00:56:28,465 - INFO - tqdm - accuracy: 0.5853, batch_loss: 0.6907, loss: 0.6706 ||:  74%|#######3  | 48/65 [09:07<03:13, 11.41s/it]
2022-04-10 00:56:39,872 - INFO - tqdm - accuracy: 0.5867, batch_loss: 0.7466, loss: 0.6721 ||:  75%|#######5  | 49/65 [09:19<03:02, 11.41s/it]
2022-04-10 00:56:51,289 - INFO - tqdm - accuracy: 0.5875, batch_loss: 0.6540, loss: 0.6717 ||:  77%|#######6  | 50/65 [09:30<02:51, 11.41s/it]
2022-04-10 00:57:02,694 - INFO - tqdm - accuracy: 0.5833, batch_loss: 0.7997, loss: 0.6743 ||:  78%|#######8  | 51/65 [09:42<02:39, 11.41s/it]
2022-04-10 00:57:14,089 - INFO - tqdm - accuracy: 0.5829, batch_loss: 0.6900, loss: 0.6746 ||:  80%|########  | 52/65 [09:53<02:28, 11.40s/it]
2022-04-10 00:57:25,483 - INFO - tqdm - accuracy: 0.5855, batch_loss: 0.6117, loss: 0.6734 ||:  82%|########1 | 53/65 [10:04<02:16, 11.40s/it]
2022-04-10 00:57:36,865 - INFO - tqdm - accuracy: 0.5845, batch_loss: 0.7239, loss: 0.6743 ||:  83%|########3 | 54/65 [10:16<02:05, 11.40s/it]
2022-04-10 00:57:48,234 - INFO - tqdm - accuracy: 0.5875, batch_loss: 0.5418, loss: 0.6719 ||:  85%|########4 | 55/65 [10:27<01:53, 11.39s/it]
2022-04-10 00:57:59,636 - INFO - tqdm - accuracy: 0.5887, batch_loss: 0.5845, loss: 0.6703 ||:  86%|########6 | 56/65 [10:39<01:42, 11.39s/it]
2022-04-10 00:58:10,784 - INFO - tqdm - accuracy: 0.5894, batch_loss: 0.6299, loss: 0.6696 ||:  88%|########7 | 57/65 [10:50<01:30, 11.32s/it]
2022-04-10 00:58:22,183 - INFO - tqdm - accuracy: 0.5878, batch_loss: 0.7449, loss: 0.6709 ||:  89%|########9 | 58/65 [11:01<01:19, 11.34s/it]
2022-04-10 00:58:33,592 - INFO - tqdm - accuracy: 0.5895, batch_loss: 0.6179, loss: 0.6700 ||:  91%|######### | 59/65 [11:12<01:08, 11.36s/it]
2022-04-10 00:58:45,004 - INFO - tqdm - accuracy: 0.5896, batch_loss: 0.7701, loss: 0.6717 ||:  92%|#########2| 60/65 [11:24<00:56, 11.38s/it]
2022-04-10 00:58:56,393 - INFO - tqdm - accuracy: 0.5922, batch_loss: 0.5547, loss: 0.6698 ||:  94%|#########3| 61/65 [11:35<00:45, 11.38s/it]
2022-04-10 00:59:07,775 - INFO - tqdm - accuracy: 0.5932, batch_loss: 0.5343, loss: 0.6676 ||:  95%|#########5| 62/65 [11:47<00:34, 11.38s/it]
2022-04-10 00:59:19,159 - INFO - tqdm - accuracy: 0.5938, batch_loss: 0.6320, loss: 0.6670 ||:  97%|#########6| 63/65 [11:58<00:22, 11.38s/it]
2022-04-10 00:59:30,197 - INFO - tqdm - accuracy: 0.5945, batch_loss: 0.6917, loss: 0.6674 ||:  98%|#########8| 64/65 [12:09<00:11, 11.28s/it]
2022-04-10 00:59:35,266 - INFO - tqdm - accuracy: 0.5949, batch_loss: 0.6497, loss: 0.6671 ||: 100%|##########| 65/65 [12:14<00:00,  9.42s/it]
2022-04-10 00:59:35,266 - INFO - tqdm - accuracy: 0.5949, batch_loss: 0.6497, loss: 0.6671 ||: 100%|##########| 65/65 [12:14<00:00, 11.30s/it]
2022-04-10 00:59:38,052 - INFO - allennlp.training.trainer - Validating
2022-04-10 00:59:38,054 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-10 00:59:48,227 - INFO - tqdm - accuracy: 0.6829, batch_loss: 0.6149, loss: 0.6302 ||:  36%|###5      | 41/115 [00:10<00:18,  4.00it/s]
2022-04-10 00:59:58,326 - INFO - tqdm - accuracy: 0.6728, batch_loss: 0.4194, loss: 0.6423 ||:  70%|#######   | 81/115 [00:20<00:08,  3.93it/s]
2022-04-10 01:00:06,891 - INFO - tqdm - accuracy: 0.6550, batch_loss: 0.6863, loss: 0.6583 ||: 100%|##########| 115/115 [00:28<00:00,  3.91it/s]
2022-04-10 01:00:06,891 - INFO - tqdm - accuracy: 0.6550, batch_loss: 0.6863, loss: 0.6583 ||: 100%|##########| 115/115 [00:28<00:00,  3.99it/s]
2022-04-10 01:00:06,891 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-10 01:00:06,892 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.595  |     0.655
2022-04-10 01:00:06,892 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-10 01:00:06,892 - INFO - allennlp.training.tensorboard_writer - loss               |     0.667  |     0.658
2022-04-10 01:00:06,892 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5813.805  |       N/A
2022-04-10 01:00:12,728 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to './output_ir-ora-d_hyper2/best.th'.
2022-04-10 01:00:26,002 - INFO - allennlp.training.trainer - Epoch duration: 0:13:05.388028
2022-04-10 01:00:26,002 - INFO - allennlp.training.trainer - Estimated training time remaining: 2:47:50
2022-04-10 01:00:26,002 - INFO - allennlp.training.trainer - Epoch 2/14
2022-04-10 01:00:26,003 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-10 01:00:26,003 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-10 01:00:26,004 - INFO - allennlp.training.trainer - Training
2022-04-10 01:00:26,005 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-10 01:00:37,122 - INFO - tqdm - accuracy: 0.5938, batch_loss: 0.6488, loss: 0.6488 ||:   2%|1         | 1/65 [00:11<11:51, 11.12s/it]
2022-04-10 01:00:48,540 - INFO - tqdm - accuracy: 0.7188, batch_loss: 0.5096, loss: 0.5792 ||:   3%|3         | 2/65 [00:22<11:51, 11.29s/it]
2022-04-10 01:01:00,206 - INFO - tqdm - accuracy: 0.7083, batch_loss: 0.5581, loss: 0.5721 ||:   5%|4         | 3/65 [00:34<11:50, 11.46s/it]
2022-04-10 01:01:11,847 - INFO - tqdm - accuracy: 0.7344, batch_loss: 0.5095, loss: 0.5565 ||:   6%|6         | 4/65 [00:45<11:43, 11.53s/it]
2022-04-10 01:01:23,231 - INFO - tqdm - accuracy: 0.7250, batch_loss: 0.5149, loss: 0.5482 ||:   8%|7         | 5/65 [00:57<11:28, 11.48s/it]
2022-04-10 01:01:34,371 - INFO - tqdm - accuracy: 0.7448, batch_loss: 0.5141, loss: 0.5425 ||:   9%|9         | 6/65 [01:08<11:10, 11.36s/it]
2022-04-10 01:01:45,657 - INFO - tqdm - accuracy: 0.7366, batch_loss: 0.5692, loss: 0.5463 ||:  11%|#         | 7/65 [01:19<10:57, 11.34s/it]
2022-04-10 01:01:57,065 - INFO - tqdm - accuracy: 0.7266, batch_loss: 0.6173, loss: 0.5552 ||:  12%|#2        | 8/65 [01:31<10:47, 11.36s/it]
2022-04-10 01:02:08,562 - INFO - tqdm - accuracy: 0.7361, batch_loss: 0.4756, loss: 0.5463 ||:  14%|#3        | 9/65 [01:42<10:38, 11.40s/it]
2022-04-10 01:02:20,022 - INFO - tqdm - accuracy: 0.7344, batch_loss: 0.5400, loss: 0.5457 ||:  15%|#5        | 10/65 [01:54<10:28, 11.42s/it]
2022-04-10 01:02:31,438 - INFO - tqdm - accuracy: 0.7244, batch_loss: 0.6103, loss: 0.5516 ||:  17%|#6        | 11/65 [02:05<10:16, 11.42s/it]
2022-04-10 01:02:42,814 - INFO - tqdm - accuracy: 0.7214, batch_loss: 0.5685, loss: 0.5530 ||:  18%|#8        | 12/65 [02:16<10:04, 11.41s/it]
2022-04-10 01:02:53,838 - INFO - tqdm - accuracy: 0.7012, batch_loss: 0.7767, loss: 0.5702 ||:  20%|##        | 13/65 [02:27<09:47, 11.29s/it]
2022-04-10 01:03:05,275 - INFO - tqdm - accuracy: 0.7002, batch_loss: 0.5853, loss: 0.5713 ||:  22%|##1       | 14/65 [02:39<09:38, 11.33s/it]
2022-04-10 01:03:16,743 - INFO - tqdm - accuracy: 0.6931, batch_loss: 0.6701, loss: 0.5779 ||:  23%|##3       | 15/65 [02:50<09:28, 11.37s/it]
2022-04-10 01:03:28,203 - INFO - tqdm - accuracy: 0.6967, batch_loss: 0.4895, loss: 0.5723 ||:  25%|##4       | 16/65 [03:02<09:18, 11.40s/it]
2022-04-10 01:03:39,625 - INFO - tqdm - accuracy: 0.6924, batch_loss: 0.6316, loss: 0.5758 ||:  26%|##6       | 17/65 [03:13<09:07, 11.41s/it]
2022-04-10 01:03:50,991 - INFO - tqdm - accuracy: 0.6835, batch_loss: 0.7830, loss: 0.5873 ||:  28%|##7       | 18/65 [03:24<08:55, 11.39s/it]
2022-04-10 01:04:02,373 - INFO - tqdm - accuracy: 0.6853, batch_loss: 0.5939, loss: 0.5877 ||:  29%|##9       | 19/65 [03:36<08:43, 11.39s/it]
2022-04-10 01:04:13,784 - INFO - tqdm - accuracy: 0.6870, batch_loss: 0.5565, loss: 0.5861 ||:  31%|###       | 20/65 [03:47<08:32, 11.40s/it]
2022-04-10 01:04:25,191 - INFO - tqdm - accuracy: 0.6811, batch_loss: 0.6495, loss: 0.5891 ||:  32%|###2      | 21/65 [03:59<08:21, 11.40s/it]
2022-04-10 01:04:36,636 - INFO - tqdm - accuracy: 0.6814, batch_loss: 0.5498, loss: 0.5873 ||:  34%|###3      | 22/65 [04:10<08:10, 11.41s/it]
2022-04-10 01:04:48,083 - INFO - tqdm - accuracy: 0.6816, batch_loss: 0.5476, loss: 0.5856 ||:  35%|###5      | 23/65 [04:22<07:59, 11.42s/it]
2022-04-10 01:04:59,424 - INFO - tqdm - accuracy: 0.6819, batch_loss: 0.6032, loss: 0.5864 ||:  37%|###6      | 24/65 [04:33<07:47, 11.40s/it]
2022-04-10 01:05:10,890 - INFO - tqdm - accuracy: 0.6846, batch_loss: 0.5366, loss: 0.5844 ||:  38%|###8      | 25/65 [04:44<07:36, 11.42s/it]
2022-04-10 01:05:22,331 - INFO - tqdm - accuracy: 0.6883, batch_loss: 0.5027, loss: 0.5812 ||:  40%|####      | 26/65 [04:56<07:25, 11.43s/it]
2022-04-10 01:05:33,769 - INFO - tqdm - accuracy: 0.6929, batch_loss: 0.4711, loss: 0.5771 ||:  42%|####1     | 27/65 [05:07<07:14, 11.43s/it]
2022-04-10 01:05:45,192 - INFO - tqdm - accuracy: 0.6972, batch_loss: 0.4726, loss: 0.5734 ||:  43%|####3     | 28/65 [05:19<07:02, 11.43s/it]
2022-04-10 01:05:56,595 - INFO - tqdm - accuracy: 0.6958, batch_loss: 0.5973, loss: 0.5742 ||:  45%|####4     | 29/65 [05:30<06:51, 11.42s/it]
2022-04-10 01:06:07,976 - INFO - tqdm - accuracy: 0.6955, batch_loss: 0.5012, loss: 0.5718 ||:  46%|####6     | 30/65 [05:41<06:39, 11.41s/it]
2022-04-10 01:06:19,349 - INFO - tqdm - accuracy: 0.6912, batch_loss: 0.5685, loss: 0.5717 ||:  48%|####7     | 31/65 [05:53<06:27, 11.40s/it]
2022-04-10 01:06:30,717 - INFO - tqdm - accuracy: 0.6931, batch_loss: 0.5295, loss: 0.5704 ||:  49%|####9     | 32/65 [06:04<06:15, 11.39s/it]
2022-04-10 01:06:42,101 - INFO - tqdm - accuracy: 0.6910, batch_loss: 0.6504, loss: 0.5728 ||:  51%|#####     | 33/65 [06:16<06:04, 11.39s/it]
2022-04-10 01:06:53,527 - INFO - tqdm - accuracy: 0.6900, batch_loss: 0.6167, loss: 0.5741 ||:  52%|#####2    | 34/65 [06:27<05:53, 11.40s/it]
2022-04-10 01:07:04,953 - INFO - tqdm - accuracy: 0.6917, batch_loss: 0.4984, loss: 0.5719 ||:  54%|#####3    | 35/65 [06:38<05:42, 11.41s/it]
2022-04-10 01:07:16,399 - INFO - tqdm - accuracy: 0.6933, batch_loss: 0.5384, loss: 0.5710 ||:  55%|#####5    | 36/65 [06:50<05:31, 11.42s/it]
2022-04-10 01:07:27,823 - INFO - tqdm - accuracy: 0.6923, batch_loss: 0.5796, loss: 0.5712 ||:  57%|#####6    | 37/65 [07:01<05:19, 11.42s/it]
2022-04-10 01:07:39,265 - INFO - tqdm - accuracy: 0.6955, batch_loss: 0.3537, loss: 0.5655 ||:  58%|#####8    | 38/65 [07:13<05:08, 11.43s/it]
2022-04-10 01:07:50,714 - INFO - tqdm - accuracy: 0.6953, batch_loss: 0.5117, loss: 0.5641 ||:  60%|######    | 39/65 [07:24<04:57, 11.43s/it]
2022-04-10 01:08:02,162 - INFO - tqdm - accuracy: 0.6951, batch_loss: 0.6199, loss: 0.5655 ||:  62%|######1   | 40/65 [07:36<04:45, 11.44s/it]
2022-04-10 01:08:13,633 - INFO - tqdm - accuracy: 0.6964, batch_loss: 0.4791, loss: 0.5634 ||:  63%|######3   | 41/65 [07:47<04:34, 11.45s/it]
2022-04-10 01:08:24,955 - INFO - tqdm - accuracy: 0.6992, batch_loss: 0.4611, loss: 0.5610 ||:  65%|######4   | 42/65 [07:58<04:22, 11.41s/it]
2022-04-10 01:08:36,390 - INFO - tqdm - accuracy: 0.6975, batch_loss: 0.6233, loss: 0.5624 ||:  66%|######6   | 43/65 [08:10<04:11, 11.42s/it]
2022-04-10 01:08:47,824 - INFO - tqdm - accuracy: 0.6979, batch_loss: 0.4903, loss: 0.5608 ||:  68%|######7   | 44/65 [08:21<03:59, 11.42s/it]
2022-04-10 01:08:59,230 - INFO - tqdm - accuracy: 0.6984, batch_loss: 0.5721, loss: 0.5610 ||:  69%|######9   | 45/65 [08:33<03:48, 11.42s/it]
2022-04-10 01:09:10,644 - INFO - tqdm - accuracy: 0.7002, batch_loss: 0.4441, loss: 0.5585 ||:  71%|#######   | 46/65 [08:44<03:36, 11.42s/it]
2022-04-10 01:09:22,043 - INFO - tqdm - accuracy: 0.6993, batch_loss: 0.6324, loss: 0.5601 ||:  72%|#######2  | 47/65 [08:56<03:25, 11.41s/it]
2022-04-10 01:09:33,447 - INFO - tqdm - accuracy: 0.6990, batch_loss: 0.6751, loss: 0.5625 ||:  74%|#######3  | 48/65 [09:07<03:13, 11.41s/it]
2022-04-10 01:09:44,827 - INFO - tqdm - accuracy: 0.6975, batch_loss: 0.5642, loss: 0.5625 ||:  75%|#######5  | 49/65 [09:18<03:02, 11.40s/it]
2022-04-10 01:09:56,221 - INFO - tqdm - accuracy: 0.6973, batch_loss: 0.6228, loss: 0.5637 ||:  77%|#######6  | 50/65 [09:30<02:50, 11.40s/it]
2022-04-10 01:10:07,609 - INFO - tqdm - accuracy: 0.6953, batch_loss: 0.6943, loss: 0.5663 ||:  78%|#######8  | 51/65 [09:41<02:39, 11.40s/it]
2022-04-10 01:10:19,016 - INFO - tqdm - accuracy: 0.6945, batch_loss: 0.5040, loss: 0.5651 ||:  80%|########  | 52/65 [09:53<02:28, 11.40s/it]
2022-04-10 01:10:30,408 - INFO - tqdm - accuracy: 0.6973, batch_loss: 0.4617, loss: 0.5631 ||:  82%|########1 | 53/65 [10:04<02:16, 11.40s/it]
2022-04-10 01:10:41,797 - INFO - tqdm - accuracy: 0.6983, batch_loss: 0.5383, loss: 0.5627 ||:  83%|########3 | 54/65 [10:15<02:05, 11.39s/it]
2022-04-10 01:10:53,161 - INFO - tqdm - accuracy: 0.6981, batch_loss: 0.6623, loss: 0.5645 ||:  85%|########4 | 55/65 [10:27<01:53, 11.39s/it]
2022-04-10 01:11:04,548 - INFO - tqdm - accuracy: 0.6979, batch_loss: 0.5668, loss: 0.5645 ||:  86%|########6 | 56/65 [10:38<01:42, 11.39s/it]
2022-04-10 01:11:15,940 - INFO - tqdm - accuracy: 0.6961, batch_loss: 0.7196, loss: 0.5672 ||:  88%|########7 | 57/65 [10:49<01:31, 11.39s/it]
2022-04-10 01:11:27,075 - INFO - tqdm - accuracy: 0.6960, batch_loss: 0.5215, loss: 0.5664 ||:  89%|########9 | 58/65 [11:01<01:19, 11.31s/it]
2022-04-10 01:11:38,465 - INFO - tqdm - accuracy: 0.6979, batch_loss: 0.4535, loss: 0.5645 ||:  91%|######### | 59/65 [11:12<01:08, 11.34s/it]
2022-04-10 01:11:49,839 - INFO - tqdm - accuracy: 0.6962, batch_loss: 0.6176, loss: 0.5654 ||:  92%|#########2| 60/65 [11:23<00:56, 11.35s/it]
2022-04-10 01:12:01,224 - INFO - tqdm - accuracy: 0.6966, batch_loss: 0.4785, loss: 0.5640 ||:  94%|#########3| 61/65 [11:35<00:45, 11.36s/it]
2022-04-10 01:12:12,622 - INFO - tqdm - accuracy: 0.6944, batch_loss: 0.6827, loss: 0.5659 ||:  95%|#########5| 62/65 [11:46<00:34, 11.37s/it]
2022-04-10 01:12:24,009 - INFO - tqdm - accuracy: 0.6943, batch_loss: 0.5405, loss: 0.5655 ||:  97%|#########6| 63/65 [11:58<00:22, 11.38s/it]
2022-04-10 01:12:35,385 - INFO - tqdm - accuracy: 0.6927, batch_loss: 0.6773, loss: 0.5672 ||:  98%|#########8| 64/65 [12:09<00:11, 11.38s/it]
2022-04-10 01:12:40,448 - INFO - tqdm - accuracy: 0.6929, batch_loss: 0.6007, loss: 0.5678 ||: 100%|##########| 65/65 [12:14<00:00,  9.48s/it]
2022-04-10 01:12:40,449 - INFO - tqdm - accuracy: 0.6929, batch_loss: 0.6007, loss: 0.5678 ||: 100%|##########| 65/65 [12:14<00:00, 11.30s/it]
2022-04-10 01:12:43,225 - INFO - allennlp.training.trainer - Validating
2022-04-10 01:12:43,226 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-10 01:12:53,242 - INFO - tqdm - accuracy: 0.5625, batch_loss: 0.8240, loss: 0.7595 ||:  35%|###4      | 40/115 [00:10<00:18,  3.99it/s]
2022-04-10 01:13:03,396 - INFO - tqdm - accuracy: 0.6025, batch_loss: 0.3206, loss: 0.6833 ||:  70%|#######   | 81/115 [00:20<00:08,  4.02it/s]
2022-04-10 01:13:12,047 - INFO - tqdm - accuracy: 0.6288, batch_loss: 1.3200, loss: 0.6960 ||: 100%|##########| 115/115 [00:28<00:00,  3.92it/s]
2022-04-10 01:13:12,047 - INFO - tqdm - accuracy: 0.6288, batch_loss: 1.3200, loss: 0.6960 ||: 100%|##########| 115/115 [00:28<00:00,  3.99it/s]
2022-04-10 01:13:12,047 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-10 01:13:12,048 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.693  |     0.629
2022-04-10 01:13:12,048 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-10 01:13:12,049 - INFO - allennlp.training.tensorboard_writer - loss               |     0.568  |     0.696
2022-04-10 01:13:12,050 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5813.805  |       N/A
2022-04-10 01:13:18,068 - INFO - allennlp.training.trainer - Epoch duration: 0:12:52.065782
2022-04-10 01:13:18,068 - INFO - allennlp.training.trainer - Estimated training time remaining: 2:34:45
2022-04-10 01:13:18,069 - INFO - allennlp.training.trainer - Epoch 3/14
2022-04-10 01:13:18,069 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-10 01:13:18,069 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-10 01:13:18,070 - INFO - allennlp.training.trainer - Training
2022-04-10 01:13:18,071 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-10 01:13:29,369 - INFO - tqdm - accuracy: 0.7812, batch_loss: 0.5004, loss: 0.5004 ||:   2%|1         | 1/65 [00:11<12:03, 11.30s/it]
2022-04-10 01:13:40,942 - INFO - tqdm - accuracy: 0.7344, batch_loss: 0.5965, loss: 0.5485 ||:   3%|3         | 2/65 [00:22<12:01, 11.46s/it]
2022-04-10 01:13:52,549 - INFO - tqdm - accuracy: 0.7500, batch_loss: 0.4793, loss: 0.5254 ||:   5%|4         | 3/65 [00:34<11:54, 11.53s/it]
2022-04-10 01:14:03,936 - INFO - tqdm - accuracy: 0.7734, batch_loss: 0.4107, loss: 0.4967 ||:   6%|6         | 4/65 [00:45<11:39, 11.47s/it]
2022-04-10 01:14:15,697 - INFO - tqdm - accuracy: 0.7875, batch_loss: 0.4243, loss: 0.4823 ||:   8%|7         | 5/65 [00:57<11:34, 11.58s/it]
2022-04-10 01:14:27,071 - INFO - tqdm - accuracy: 0.7969, batch_loss: 0.3938, loss: 0.4675 ||:   9%|9         | 6/65 [01:09<11:18, 11.51s/it]
2022-04-10 01:14:38,489 - INFO - tqdm - accuracy: 0.8125, batch_loss: 0.3888, loss: 0.4563 ||:  11%|#         | 7/65 [01:20<11:05, 11.48s/it]
2022-04-10 01:14:49,781 - INFO - tqdm - accuracy: 0.8320, batch_loss: 0.3100, loss: 0.4380 ||:  12%|#2        | 8/65 [01:31<10:50, 11.42s/it]
2022-04-10 01:15:01,213 - INFO - tqdm - accuracy: 0.8299, batch_loss: 0.4036, loss: 0.4342 ||:  14%|#3        | 9/65 [01:43<10:39, 11.42s/it]
2022-04-10 01:15:12,686 - INFO - tqdm - accuracy: 0.8438, batch_loss: 0.2938, loss: 0.4201 ||:  15%|#5        | 10/65 [01:54<10:29, 11.44s/it]
2022-04-10 01:15:24,013 - INFO - tqdm - accuracy: 0.8438, batch_loss: 0.3636, loss: 0.4150 ||:  17%|#6        | 11/65 [02:05<10:15, 11.40s/it]
2022-04-10 01:15:35,485 - INFO - tqdm - accuracy: 0.8464, batch_loss: 0.2559, loss: 0.4017 ||:  18%|#8        | 12/65 [02:17<10:05, 11.42s/it]
2022-04-10 01:15:46,949 - INFO - tqdm - accuracy: 0.8510, batch_loss: 0.2457, loss: 0.3897 ||:  20%|##        | 13/65 [02:28<09:54, 11.44s/it]
2022-04-10 01:15:58,421 - INFO - tqdm - accuracy: 0.8438, batch_loss: 0.4297, loss: 0.3926 ||:  22%|##1       | 14/65 [02:40<09:43, 11.45s/it]
2022-04-10 01:16:09,877 - INFO - tqdm - accuracy: 0.8417, batch_loss: 0.4032, loss: 0.3933 ||:  23%|##3       | 15/65 [02:51<09:32, 11.45s/it]
2022-04-10 01:16:21,331 - INFO - tqdm - accuracy: 0.8398, batch_loss: 0.3234, loss: 0.3889 ||:  25%|##4       | 16/65 [03:03<09:21, 11.45s/it]
2022-04-10 01:16:32,763 - INFO - tqdm - accuracy: 0.8346, batch_loss: 0.5659, loss: 0.3993 ||:  26%|##6       | 17/65 [03:14<09:09, 11.45s/it]
2022-04-10 01:16:44,206 - INFO - tqdm - accuracy: 0.8403, batch_loss: 0.1649, loss: 0.3863 ||:  28%|##7       | 18/65 [03:26<08:57, 11.44s/it]
2022-04-10 01:16:55,632 - INFO - tqdm - accuracy: 0.8355, batch_loss: 0.5346, loss: 0.3941 ||:  29%|##9       | 19/65 [03:37<08:46, 11.44s/it]
2022-04-10 01:17:07,053 - INFO - tqdm - accuracy: 0.8281, batch_loss: 0.6957, loss: 0.4092 ||:  31%|###       | 20/65 [03:48<08:34, 11.43s/it]
2022-04-10 01:17:18,495 - INFO - tqdm - accuracy: 0.8304, batch_loss: 0.3218, loss: 0.4050 ||:  32%|###2      | 21/65 [04:00<08:23, 11.44s/it]
2022-04-10 01:17:29,900 - INFO - tqdm - accuracy: 0.8310, batch_loss: 0.3714, loss: 0.4035 ||:  34%|###3      | 22/65 [04:11<08:11, 11.43s/it]
2022-04-10 01:17:41,318 - INFO - tqdm - accuracy: 0.8342, batch_loss: 0.2282, loss: 0.3959 ||:  35%|###5      | 23/65 [04:23<07:59, 11.42s/it]
2022-04-10 01:17:52,732 - INFO - tqdm - accuracy: 0.8346, batch_loss: 0.2974, loss: 0.3918 ||:  37%|###6      | 24/65 [04:34<07:48, 11.42s/it]
2022-04-10 01:18:04,150 - INFO - tqdm - accuracy: 0.8387, batch_loss: 0.2444, loss: 0.3859 ||:  38%|###8      | 25/65 [04:46<07:36, 11.42s/it]
2022-04-10 01:18:15,553 - INFO - tqdm - accuracy: 0.8425, batch_loss: 0.2291, loss: 0.3798 ||:  40%|####      | 26/65 [04:57<07:25, 11.41s/it]
2022-04-10 01:18:26,957 - INFO - tqdm - accuracy: 0.8403, batch_loss: 0.3445, loss: 0.3785 ||:  42%|####1     | 27/65 [05:08<07:13, 11.41s/it]
2022-04-10 01:18:38,355 - INFO - tqdm - accuracy: 0.8415, batch_loss: 0.3041, loss: 0.3759 ||:  43%|####3     | 28/65 [05:20<07:02, 11.41s/it]
2022-04-10 01:18:49,752 - INFO - tqdm - accuracy: 0.8427, batch_loss: 0.2774, loss: 0.3725 ||:  45%|####4     | 29/65 [05:31<06:50, 11.40s/it]
2022-04-10 01:19:01,135 - INFO - tqdm - accuracy: 0.8438, batch_loss: 0.2728, loss: 0.3692 ||:  46%|####6     | 30/65 [05:43<06:38, 11.40s/it]
2022-04-10 01:19:12,534 - INFO - tqdm - accuracy: 0.8458, batch_loss: 0.3956, loss: 0.3700 ||:  48%|####7     | 31/65 [05:54<06:27, 11.40s/it]
2022-04-10 01:19:23,802 - INFO - tqdm - accuracy: 0.8447, batch_loss: 0.3346, loss: 0.3689 ||:  49%|####9     | 32/65 [06:05<06:14, 11.36s/it]
2022-04-10 01:19:35,225 - INFO - tqdm - accuracy: 0.8428, batch_loss: 0.3619, loss: 0.3687 ||:  51%|#####     | 33/65 [06:17<06:04, 11.38s/it]
2022-04-10 01:19:46,656 - INFO - tqdm - accuracy: 0.8428, batch_loss: 0.3742, loss: 0.3689 ||:  52%|#####2    | 34/65 [06:28<05:53, 11.39s/it]
2022-04-10 01:19:58,057 - INFO - tqdm - accuracy: 0.8429, batch_loss: 0.3551, loss: 0.3685 ||:  54%|#####3    | 35/65 [06:39<05:41, 11.40s/it]
2022-04-10 01:20:09,444 - INFO - tqdm - accuracy: 0.8403, batch_loss: 0.4624, loss: 0.3711 ||:  55%|#####5    | 36/65 [06:51<05:30, 11.39s/it]
2022-04-10 01:20:20,828 - INFO - tqdm - accuracy: 0.8404, batch_loss: 0.2966, loss: 0.3691 ||:  57%|#####6    | 37/65 [07:02<05:18, 11.39s/it]
2022-04-10 01:20:32,232 - INFO - tqdm - accuracy: 0.8388, batch_loss: 0.3746, loss: 0.3692 ||:  58%|#####8    | 38/65 [07:14<05:07, 11.39s/it]
2022-04-10 01:20:43,622 - INFO - tqdm - accuracy: 0.8381, batch_loss: 0.4799, loss: 0.3720 ||:  60%|######    | 39/65 [07:25<04:56, 11.39s/it]
2022-04-10 01:20:55,017 - INFO - tqdm - accuracy: 0.8383, batch_loss: 0.3381, loss: 0.3712 ||:  62%|######1   | 40/65 [07:36<04:44, 11.39s/it]
2022-04-10 01:21:06,389 - INFO - tqdm - accuracy: 0.8377, batch_loss: 0.3518, loss: 0.3707 ||:  63%|######3   | 41/65 [07:48<04:33, 11.39s/it]
2022-04-10 01:21:17,798 - INFO - tqdm - accuracy: 0.8385, batch_loss: 0.3072, loss: 0.3692 ||:  65%|######4   | 42/65 [07:59<04:22, 11.39s/it]
2022-04-10 01:21:29,211 - INFO - tqdm - accuracy: 0.8408, batch_loss: 0.2589, loss: 0.3666 ||:  66%|######6   | 43/65 [08:11<04:10, 11.40s/it]
2022-04-10 01:21:40,635 - INFO - tqdm - accuracy: 0.8402, batch_loss: 0.5450, loss: 0.3707 ||:  68%|######7   | 44/65 [08:22<03:59, 11.41s/it]
2022-04-10 01:21:52,060 - INFO - tqdm - accuracy: 0.8389, batch_loss: 0.4480, loss: 0.3724 ||:  69%|######9   | 45/65 [08:33<03:48, 11.41s/it]
2022-04-10 01:22:03,501 - INFO - tqdm - accuracy: 0.8383, batch_loss: 0.3189, loss: 0.3713 ||:  71%|#######   | 46/65 [08:45<03:36, 11.42s/it]
2022-04-10 01:22:14,935 - INFO - tqdm - accuracy: 0.8384, batch_loss: 0.3531, loss: 0.3709 ||:  72%|#######2  | 47/65 [08:56<03:25, 11.42s/it]
2022-04-10 01:22:26,387 - INFO - tqdm - accuracy: 0.8418, batch_loss: 0.1416, loss: 0.3661 ||:  74%|#######3  | 48/65 [09:08<03:14, 11.43s/it]
2022-04-10 01:22:37,830 - INFO - tqdm - accuracy: 0.8406, batch_loss: 0.4027, loss: 0.3668 ||:  75%|#######5  | 49/65 [09:19<03:02, 11.44s/it]
2022-04-10 01:22:49,274 - INFO - tqdm - accuracy: 0.8394, batch_loss: 0.4725, loss: 0.3690 ||:  77%|#######6  | 50/65 [09:31<02:51, 11.44s/it]
2022-04-10 01:23:00,702 - INFO - tqdm - accuracy: 0.8370, batch_loss: 0.5621, loss: 0.3727 ||:  78%|#######8  | 51/65 [09:42<02:40, 11.44s/it]
2022-04-10 01:23:11,896 - INFO - tqdm - accuracy: 0.8359, batch_loss: 0.4710, loss: 0.3746 ||:  80%|########  | 52/65 [09:53<02:27, 11.36s/it]
2022-04-10 01:23:23,336 - INFO - tqdm - accuracy: 0.8343, batch_loss: 0.4131, loss: 0.3754 ||:  82%|########1 | 53/65 [10:05<02:16, 11.39s/it]
2022-04-10 01:23:34,761 - INFO - tqdm - accuracy: 0.8328, batch_loss: 0.4956, loss: 0.3776 ||:  83%|########3 | 54/65 [10:16<02:05, 11.40s/it]
2022-04-10 01:23:46,185 - INFO - tqdm - accuracy: 0.8324, batch_loss: 0.3622, loss: 0.3773 ||:  85%|########4 | 55/65 [10:28<01:54, 11.41s/it]
2022-04-10 01:23:57,626 - INFO - tqdm - accuracy: 0.8331, batch_loss: 0.3675, loss: 0.3771 ||:  86%|########6 | 56/65 [10:39<01:42, 11.42s/it]
2022-04-10 01:24:09,052 - INFO - tqdm - accuracy: 0.8333, batch_loss: 0.3967, loss: 0.3775 ||:  88%|########7 | 57/65 [10:50<01:31, 11.42s/it]
2022-04-10 01:24:20,483 - INFO - tqdm - accuracy: 0.8346, batch_loss: 0.2580, loss: 0.3754 ||:  89%|########9 | 58/65 [11:02<01:19, 11.42s/it]
2022-04-10 01:24:31,560 - INFO - tqdm - accuracy: 0.8341, batch_loss: 0.3814, loss: 0.3755 ||:  91%|######### | 59/65 [11:13<01:07, 11.32s/it]
2022-04-10 01:24:42,978 - INFO - tqdm - accuracy: 0.8348, batch_loss: 0.3221, loss: 0.3746 ||:  92%|#########2| 60/65 [11:24<00:56, 11.35s/it]
2022-04-10 01:24:54,388 - INFO - tqdm - accuracy: 0.8350, batch_loss: 0.3033, loss: 0.3735 ||:  94%|#########3| 61/65 [11:36<00:45, 11.37s/it]
2022-04-10 01:25:05,810 - INFO - tqdm - accuracy: 0.8356, batch_loss: 0.3402, loss: 0.3729 ||:  95%|#########5| 62/65 [11:47<00:34, 11.38s/it]
2022-04-10 01:25:17,224 - INFO - tqdm - accuracy: 0.8352, batch_loss: 0.3762, loss: 0.3730 ||:  97%|#########6| 63/65 [11:59<00:22, 11.39s/it]
2022-04-10 01:25:28,635 - INFO - tqdm - accuracy: 0.8354, batch_loss: 0.3126, loss: 0.3720 ||:  98%|#########8| 64/65 [12:10<00:11, 11.40s/it]
2022-04-10 01:25:33,710 - INFO - tqdm - accuracy: 0.8355, batch_loss: 0.4291, loss: 0.3729 ||: 100%|##########| 65/65 [12:15<00:00,  9.50s/it]
2022-04-10 01:25:33,710 - INFO - tqdm - accuracy: 0.8355, batch_loss: 0.4291, loss: 0.3729 ||: 100%|##########| 65/65 [12:15<00:00, 11.32s/it]
2022-04-10 01:25:36,524 - INFO - allennlp.training.trainer - Validating
2022-04-10 01:25:36,525 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-10 01:25:46,554 - INFO - tqdm - accuracy: 0.6375, batch_loss: 0.0184, loss: 1.1292 ||:  35%|###4      | 40/115 [00:10<00:18,  4.00it/s]
2022-04-10 01:25:56,576 - INFO - tqdm - accuracy: 0.6687, batch_loss: 0.6642, loss: 0.9597 ||:  70%|######9   | 80/115 [00:20<00:08,  3.94it/s]
2022-04-10 01:26:05,401 - INFO - tqdm - accuracy: 0.6594, batch_loss: 0.0766, loss: 0.9423 ||: 100%|##########| 115/115 [00:28<00:00,  3.98it/s]
2022-04-10 01:26:05,402 - INFO - tqdm - accuracy: 0.6594, batch_loss: 0.0766, loss: 0.9423 ||: 100%|##########| 115/115 [00:28<00:00,  3.98it/s]
2022-04-10 01:26:05,402 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-10 01:26:05,403 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.836  |     0.659
2022-04-10 01:26:05,403 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-10 01:26:05,404 - INFO - allennlp.training.tensorboard_writer - loss               |     0.373  |     0.942
2022-04-10 01:26:05,404 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5813.805  |       N/A
2022-04-10 01:26:11,150 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to './output_ir-ora-d_hyper2/best.th'.
2022-04-10 01:26:26,377 - INFO - allennlp.training.trainer - Epoch duration: 0:13:08.308222
2022-04-10 01:26:26,377 - INFO - allennlp.training.trainer - Estimated training time remaining: 2:22:31
2022-04-10 01:26:26,377 - INFO - allennlp.training.trainer - Epoch 4/14
2022-04-10 01:26:26,377 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-10 01:26:26,378 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-10 01:26:26,379 - INFO - allennlp.training.trainer - Training
2022-04-10 01:26:26,379 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-10 01:26:37,468 - INFO - tqdm - accuracy: 0.9062, batch_loss: 0.2413, loss: 0.2413 ||:   2%|1         | 1/65 [00:11<11:49, 11.09s/it]
2022-04-10 01:26:48,842 - INFO - tqdm - accuracy: 0.9219, batch_loss: 0.1511, loss: 0.1962 ||:   3%|3         | 2/65 [00:22<11:49, 11.26s/it]
2022-04-10 01:27:00,429 - INFO - tqdm - accuracy: 0.8958, batch_loss: 0.3070, loss: 0.2331 ||:   5%|4         | 3/65 [00:34<11:47, 11.41s/it]
2022-04-10 01:27:12,005 - INFO - tqdm - accuracy: 0.8984, batch_loss: 0.2683, loss: 0.2419 ||:   6%|6         | 4/65 [00:45<11:39, 11.47s/it]
2022-04-10 01:27:23,330 - INFO - tqdm - accuracy: 0.8938, batch_loss: 0.2664, loss: 0.2468 ||:   8%|7         | 5/65 [00:56<11:25, 11.42s/it]
2022-04-10 01:27:34,582 - INFO - tqdm - accuracy: 0.9115, batch_loss: 0.1296, loss: 0.2273 ||:   9%|9         | 6/65 [01:08<11:10, 11.36s/it]
2022-04-10 01:27:45,887 - INFO - tqdm - accuracy: 0.9107, batch_loss: 0.2013, loss: 0.2236 ||:  11%|#         | 7/65 [01:19<10:57, 11.34s/it]
2022-04-10 01:27:57,310 - INFO - tqdm - accuracy: 0.9180, batch_loss: 0.1391, loss: 0.2130 ||:  12%|#2        | 8/65 [01:30<10:48, 11.37s/it]
2022-04-10 01:28:08,836 - INFO - tqdm - accuracy: 0.9236, batch_loss: 0.1506, loss: 0.2061 ||:  14%|#3        | 9/65 [01:42<10:39, 11.42s/it]
2022-04-10 01:28:20,318 - INFO - tqdm - accuracy: 0.9250, batch_loss: 0.1474, loss: 0.2002 ||:  15%|#5        | 10/65 [01:53<10:29, 11.44s/it]
2022-04-10 01:28:31,724 - INFO - tqdm - accuracy: 0.9233, batch_loss: 0.2314, loss: 0.2030 ||:  17%|#6        | 11/65 [02:05<10:17, 11.43s/it]
2022-04-10 01:28:43,059 - INFO - tqdm - accuracy: 0.9271, batch_loss: 0.1271, loss: 0.1967 ||:  18%|#8        | 12/65 [02:16<10:04, 11.40s/it]
2022-04-10 01:28:54,478 - INFO - tqdm - accuracy: 0.9255, batch_loss: 0.2403, loss: 0.2001 ||:  20%|##        | 13/65 [02:28<09:53, 11.41s/it]
2022-04-10 01:29:05,942 - INFO - tqdm - accuracy: 0.9308, batch_loss: 0.0607, loss: 0.1901 ||:  22%|##1       | 14/65 [02:39<09:42, 11.42s/it]
2022-04-10 01:29:17,414 - INFO - tqdm - accuracy: 0.9313, batch_loss: 0.2254, loss: 0.1925 ||:  23%|##3       | 15/65 [02:51<09:31, 11.44s/it]
2022-04-10 01:29:28,832 - INFO - tqdm - accuracy: 0.9316, batch_loss: 0.1045, loss: 0.1870 ||:  25%|##4       | 16/65 [03:02<09:20, 11.43s/it]
2022-04-10 01:29:40,224 - INFO - tqdm - accuracy: 0.9338, batch_loss: 0.1039, loss: 0.1821 ||:  26%|##6       | 17/65 [03:13<09:08, 11.42s/it]
2022-04-10 01:29:51,604 - INFO - tqdm - accuracy: 0.9340, batch_loss: 0.1817, loss: 0.1821 ||:  28%|##7       | 18/65 [03:25<08:56, 11.41s/it]
2022-04-10 01:30:03,048 - INFO - tqdm - accuracy: 0.9375, batch_loss: 0.1031, loss: 0.1779 ||:  29%|##9       | 19/65 [03:36<08:45, 11.42s/it]
2022-04-10 01:30:14,527 - INFO - tqdm - accuracy: 0.9313, batch_loss: 0.3136, loss: 0.1847 ||:  31%|###       | 20/65 [03:48<08:34, 11.44s/it]
2022-04-10 01:30:25,995 - INFO - tqdm - accuracy: 0.9345, batch_loss: 0.0758, loss: 0.1795 ||:  32%|###2      | 21/65 [03:59<08:23, 11.45s/it]
2022-04-10 01:30:37,447 - INFO - tqdm - accuracy: 0.9332, batch_loss: 0.2091, loss: 0.1809 ||:  34%|###3      | 22/65 [04:11<08:12, 11.45s/it]
2022-04-10 01:30:48,850 - INFO - tqdm - accuracy: 0.9280, batch_loss: 0.3651, loss: 0.1889 ||:  35%|###5      | 23/65 [04:22<08:00, 11.43s/it]
2022-04-10 01:31:00,223 - INFO - tqdm - accuracy: 0.9284, batch_loss: 0.1290, loss: 0.1864 ||:  37%|###6      | 24/65 [04:33<07:48, 11.42s/it]
2022-04-10 01:31:11,603 - INFO - tqdm - accuracy: 0.9237, batch_loss: 0.3377, loss: 0.1924 ||:  38%|###8      | 25/65 [04:45<07:36, 11.41s/it]
2022-04-10 01:31:23,038 - INFO - tqdm - accuracy: 0.9255, batch_loss: 0.0900, loss: 0.1885 ||:  40%|####      | 26/65 [04:56<07:25, 11.41s/it]
2022-04-10 01:31:34,492 - INFO - tqdm - accuracy: 0.9282, batch_loss: 0.0456, loss: 0.1832 ||:  42%|####1     | 27/65 [05:08<07:14, 11.43s/it]
2022-04-10 01:31:45,965 - INFO - tqdm - accuracy: 0.9286, batch_loss: 0.3146, loss: 0.1879 ||:  43%|####3     | 28/65 [05:19<07:03, 11.44s/it]
2022-04-10 01:31:57,395 - INFO - tqdm - accuracy: 0.9300, batch_loss: 0.0950, loss: 0.1847 ||:  45%|####4     | 29/65 [05:31<06:51, 11.44s/it]
2022-04-10 01:32:08,672 - INFO - tqdm - accuracy: 0.9292, batch_loss: 0.2857, loss: 0.1880 ||:  46%|####6     | 30/65 [05:42<06:38, 11.39s/it]
2022-04-10 01:32:20,035 - INFO - tqdm - accuracy: 0.9304, batch_loss: 0.0730, loss: 0.1843 ||:  48%|####7     | 31/65 [05:53<06:26, 11.38s/it]
2022-04-10 01:32:31,435 - INFO - tqdm - accuracy: 0.9316, batch_loss: 0.1065, loss: 0.1819 ||:  49%|####9     | 32/65 [06:05<06:15, 11.39s/it]
2022-04-10 01:32:42,836 - INFO - tqdm - accuracy: 0.9328, batch_loss: 0.1529, loss: 0.1810 ||:  51%|#####     | 33/65 [06:16<06:04, 11.39s/it]
2022-04-10 01:32:54,282 - INFO - tqdm - accuracy: 0.9329, batch_loss: 0.2616, loss: 0.1834 ||:  52%|#####2    | 34/65 [06:27<05:53, 11.41s/it]
2022-04-10 01:33:05,728 - INFO - tqdm - accuracy: 0.9330, batch_loss: 0.1755, loss: 0.1832 ||:  54%|#####3    | 35/65 [06:39<05:42, 11.42s/it]
2022-04-10 01:33:17,193 - INFO - tqdm - accuracy: 0.9332, batch_loss: 0.1505, loss: 0.1823 ||:  55%|#####5    | 36/65 [06:50<05:31, 11.43s/it]
2022-04-10 01:33:28,311 - INFO - tqdm - accuracy: 0.9315, batch_loss: 0.2209, loss: 0.1833 ||:  57%|#####6    | 37/65 [07:01<05:17, 11.34s/it]
2022-04-10 01:33:39,752 - INFO - tqdm - accuracy: 0.9325, batch_loss: 0.0766, loss: 0.1805 ||:  58%|#####8    | 38/65 [07:13<05:06, 11.37s/it]
2022-04-10 01:33:51,130 - INFO - tqdm - accuracy: 0.9318, batch_loss: 0.1977, loss: 0.1809 ||:  60%|######    | 39/65 [07:24<04:55, 11.37s/it]
2022-04-10 01:34:02,898 - INFO - tqdm - accuracy: 0.9312, batch_loss: 0.2835, loss: 0.1835 ||:  62%|######1   | 40/65 [07:36<04:47, 11.49s/it]
2022-04-10 01:34:14,264 - INFO - tqdm - accuracy: 0.9314, batch_loss: 0.1138, loss: 0.1818 ||:  63%|######3   | 41/65 [07:47<04:34, 11.45s/it]
2022-04-10 01:34:25,708 - INFO - tqdm - accuracy: 0.9308, batch_loss: 0.2939, loss: 0.1845 ||:  65%|######4   | 42/65 [07:59<04:23, 11.45s/it]
2022-04-10 01:34:37,193 - INFO - tqdm - accuracy: 0.9316, batch_loss: 0.1213, loss: 0.1830 ||:  66%|######6   | 43/65 [08:10<04:12, 11.46s/it]
2022-04-10 01:34:48,606 - INFO - tqdm - accuracy: 0.9296, batch_loss: 0.3443, loss: 0.1867 ||:  68%|######7   | 44/65 [08:22<04:00, 11.45s/it]
2022-04-10 01:34:59,837 - INFO - tqdm - accuracy: 0.9298, batch_loss: 0.1618, loss: 0.1861 ||:  69%|######9   | 45/65 [08:33<03:47, 11.38s/it]
2022-04-10 01:35:11,272 - INFO - tqdm - accuracy: 0.9273, batch_loss: 0.4402, loss: 0.1916 ||:  71%|#######   | 46/65 [08:44<03:36, 11.40s/it]
2022-04-10 01:35:22,726 - INFO - tqdm - accuracy: 0.9255, batch_loss: 0.2761, loss: 0.1934 ||:  72%|#######2  | 47/65 [08:56<03:25, 11.41s/it]
2022-04-10 01:35:34,196 - INFO - tqdm - accuracy: 0.9244, batch_loss: 0.2082, loss: 0.1937 ||:  74%|#######3  | 48/65 [09:07<03:14, 11.43s/it]
2022-04-10 01:35:45,630 - INFO - tqdm - accuracy: 0.9253, batch_loss: 0.0855, loss: 0.1915 ||:  75%|#######5  | 49/65 [09:19<03:02, 11.43s/it]
2022-04-10 01:35:57,055 - INFO - tqdm - accuracy: 0.9256, batch_loss: 0.1711, loss: 0.1911 ||:  77%|#######6  | 50/65 [09:30<02:51, 11.43s/it]
2022-04-10 01:36:08,436 - INFO - tqdm - accuracy: 0.9264, batch_loss: 0.1108, loss: 0.1895 ||:  78%|#######8  | 51/65 [09:42<02:39, 11.42s/it]
2022-04-10 01:36:19,826 - INFO - tqdm - accuracy: 0.9278, batch_loss: 0.1387, loss: 0.1886 ||:  80%|########  | 52/65 [09:53<02:28, 11.41s/it]
2022-04-10 01:36:31,242 - INFO - tqdm - accuracy: 0.9274, batch_loss: 0.1929, loss: 0.1887 ||:  82%|########1 | 53/65 [10:04<02:16, 11.41s/it]
2022-04-10 01:36:42,691 - INFO - tqdm - accuracy: 0.9265, batch_loss: 0.2751, loss: 0.1903 ||:  83%|########3 | 54/65 [10:16<02:05, 11.42s/it]
2022-04-10 01:36:54,140 - INFO - tqdm - accuracy: 0.9261, batch_loss: 0.2072, loss: 0.1906 ||:  85%|########4 | 55/65 [10:27<01:54, 11.43s/it]
2022-04-10 01:37:05,601 - INFO - tqdm - accuracy: 0.9274, batch_loss: 0.0944, loss: 0.1888 ||:  86%|########6 | 56/65 [10:39<01:42, 11.44s/it]
2022-04-10 01:37:17,061 - INFO - tqdm - accuracy: 0.9259, batch_loss: 0.2891, loss: 0.1906 ||:  88%|########7 | 57/65 [10:50<01:31, 11.45s/it]
2022-04-10 01:37:28,525 - INFO - tqdm - accuracy: 0.9267, batch_loss: 0.2126, loss: 0.1910 ||:  89%|########9 | 58/65 [11:02<01:20, 11.45s/it]
2022-04-10 01:37:39,972 - INFO - tqdm - accuracy: 0.9274, batch_loss: 0.0951, loss: 0.1894 ||:  91%|######### | 59/65 [11:13<01:08, 11.45s/it]
2022-04-10 01:37:51,423 - INFO - tqdm - accuracy: 0.9260, batch_loss: 0.2748, loss: 0.1908 ||:  92%|#########2| 60/65 [11:25<00:57, 11.45s/it]
2022-04-10 01:38:02,847 - INFO - tqdm - accuracy: 0.9272, batch_loss: 0.0927, loss: 0.1892 ||:  94%|#########3| 61/65 [11:36<00:45, 11.44s/it]
2022-04-10 01:38:14,000 - INFO - tqdm - accuracy: 0.9284, batch_loss: 0.0945, loss: 0.1876 ||:  95%|#########5| 62/65 [11:47<00:34, 11.36s/it]
2022-04-10 01:38:25,408 - INFO - tqdm - accuracy: 0.9290, batch_loss: 0.0990, loss: 0.1862 ||:  97%|#########6| 63/65 [11:59<00:22, 11.37s/it]
2022-04-10 01:38:36,812 - INFO - tqdm - accuracy: 0.9282, batch_loss: 0.2813, loss: 0.1877 ||:  98%|#########8| 64/65 [12:10<00:11, 11.38s/it]
2022-04-10 01:38:41,862 - INFO - tqdm - accuracy: 0.9282, batch_loss: 0.2476, loss: 0.1886 ||: 100%|##########| 65/65 [12:15<00:00,  9.48s/it]
2022-04-10 01:38:41,862 - INFO - tqdm - accuracy: 0.9282, batch_loss: 0.2476, loss: 0.1886 ||: 100%|##########| 65/65 [12:15<00:00, 11.32s/it]
2022-04-10 01:38:44,654 - INFO - allennlp.training.trainer - Validating
2022-04-10 01:38:44,656 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-10 01:38:54,668 - INFO - tqdm - accuracy: 0.6000, batch_loss: 0.0008, loss: 1.3333 ||:  35%|###4      | 40/115 [00:10<00:18,  3.99it/s]
2022-04-10 01:39:04,777 - INFO - tqdm - accuracy: 0.6250, batch_loss: 0.0103, loss: 1.2688 ||:  70%|######9   | 80/115 [00:20<00:08,  3.93it/s]
2022-04-10 01:39:13,454 - INFO - tqdm - accuracy: 0.6419, batch_loss: 0.0053, loss: 1.2572 ||: 100%|##########| 115/115 [00:28<00:00,  3.95it/s]
2022-04-10 01:39:13,455 - INFO - tqdm - accuracy: 0.6419, batch_loss: 0.0053, loss: 1.2572 ||: 100%|##########| 115/115 [00:28<00:00,  3.99it/s]
2022-04-10 01:39:13,455 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-10 01:39:13,455 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.928  |     0.642
2022-04-10 01:39:13,456 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-10 01:39:13,457 - INFO - allennlp.training.tensorboard_writer - loss               |     0.189  |     1.257
2022-04-10 01:39:13,457 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5813.805  |       N/A
2022-04-10 01:39:19,440 - INFO - allennlp.training.trainer - Epoch duration: 0:12:53.062981
2022-04-10 01:39:19,440 - INFO - allennlp.training.trainer - Estimated training time remaining: 2:09:25
2022-04-10 01:39:19,441 - INFO - allennlp.training.trainer - Epoch 5/14
2022-04-10 01:39:19,441 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-10 01:39:19,441 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-10 01:39:19,442 - INFO - allennlp.training.trainer - Training
2022-04-10 01:39:19,443 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-10 01:39:30,631 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.0919, loss: 0.0919 ||:   2%|1         | 1/65 [00:11<11:56, 11.19s/it]
2022-04-10 01:39:42,210 - INFO - tqdm - accuracy: 0.9531, batch_loss: 0.0928, loss: 0.0923 ||:   3%|3         | 2/65 [00:22<11:59, 11.42s/it]
2022-04-10 01:39:53,845 - INFO - tqdm - accuracy: 0.9479, batch_loss: 0.1425, loss: 0.1091 ||:   5%|4         | 3/65 [00:34<11:54, 11.52s/it]
2022-04-10 01:40:05,214 - INFO - tqdm - accuracy: 0.9453, batch_loss: 0.1316, loss: 0.1147 ||:   6%|6         | 4/65 [00:45<11:38, 11.46s/it]
2022-04-10 01:40:16,595 - INFO - tqdm - accuracy: 0.9437, batch_loss: 0.1965, loss: 0.1310 ||:   8%|7         | 5/65 [00:57<11:25, 11.43s/it]
2022-04-10 01:40:27,986 - INFO - tqdm - accuracy: 0.9479, batch_loss: 0.0926, loss: 0.1246 ||:   9%|9         | 6/65 [01:08<11:13, 11.42s/it]
2022-04-10 01:40:39,255 - INFO - tqdm - accuracy: 0.9554, batch_loss: 0.0412, loss: 0.1127 ||:  11%|#         | 7/65 [01:19<10:59, 11.37s/it]
2022-04-10 01:40:50,678 - INFO - tqdm - accuracy: 0.9570, batch_loss: 0.0877, loss: 0.1096 ||:  12%|#2        | 8/65 [01:31<10:49, 11.39s/it]
2022-04-10 01:41:02,075 - INFO - tqdm - accuracy: 0.9618, batch_loss: 0.0302, loss: 0.1008 ||:  14%|#3        | 9/65 [01:42<10:37, 11.39s/it]
2022-04-10 01:41:13,494 - INFO - tqdm - accuracy: 0.9625, batch_loss: 0.0597, loss: 0.0967 ||:  15%|#5        | 10/65 [01:54<10:26, 11.40s/it]
2022-04-10 01:41:24,905 - INFO - tqdm - accuracy: 0.9602, batch_loss: 0.1167, loss: 0.0985 ||:  17%|#6        | 11/65 [02:05<10:15, 11.40s/it]
2022-04-10 01:41:36,340 - INFO - tqdm - accuracy: 0.9635, batch_loss: 0.0464, loss: 0.0941 ||:  18%|#8        | 12/65 [02:16<10:04, 11.41s/it]
2022-04-10 01:41:47,748 - INFO - tqdm - accuracy: 0.9567, batch_loss: 0.2341, loss: 0.1049 ||:  20%|##        | 13/65 [02:28<09:53, 11.41s/it]
2022-04-10 01:41:59,181 - INFO - tqdm - accuracy: 0.9509, batch_loss: 0.2092, loss: 0.1124 ||:  22%|##1       | 14/65 [02:39<09:42, 11.42s/it]
2022-04-10 01:42:10,610 - INFO - tqdm - accuracy: 0.9542, batch_loss: 0.0272, loss: 0.1067 ||:  23%|##3       | 15/65 [02:51<09:31, 11.42s/it]
2022-04-10 01:42:21,667 - INFO - tqdm - accuracy: 0.9550, batch_loss: 0.1168, loss: 0.1073 ||:  25%|##4       | 16/65 [03:02<09:14, 11.31s/it]
2022-04-10 01:42:33,083 - INFO - tqdm - accuracy: 0.9558, batch_loss: 0.0771, loss: 0.1055 ||:  26%|##6       | 17/65 [03:13<09:04, 11.34s/it]
2022-04-10 01:42:44,491 - INFO - tqdm - accuracy: 0.9513, batch_loss: 0.3534, loss: 0.1193 ||:  28%|##7       | 18/65 [03:25<08:54, 11.36s/it]
2022-04-10 01:42:55,912 - INFO - tqdm - accuracy: 0.9506, batch_loss: 0.2121, loss: 0.1242 ||:  29%|##9       | 19/65 [03:36<08:43, 11.38s/it]
2022-04-10 01:43:07,307 - INFO - tqdm - accuracy: 0.9531, batch_loss: 0.0671, loss: 0.1213 ||:  31%|###       | 20/65 [03:47<08:32, 11.38s/it]
2022-04-10 01:43:18,710 - INFO - tqdm - accuracy: 0.9493, batch_loss: 0.2819, loss: 0.1290 ||:  32%|###2      | 21/65 [03:59<08:21, 11.39s/it]
2022-04-10 01:43:30,092 - INFO - tqdm - accuracy: 0.9474, batch_loss: 0.1938, loss: 0.1319 ||:  34%|###3      | 22/65 [04:10<08:09, 11.39s/it]
2022-04-10 01:43:41,500 - INFO - tqdm - accuracy: 0.9429, batch_loss: 0.2657, loss: 0.1377 ||:  35%|###5      | 23/65 [04:22<07:58, 11.39s/it]
2022-04-10 01:43:52,914 - INFO - tqdm - accuracy: 0.9439, batch_loss: 0.0761, loss: 0.1352 ||:  37%|###6      | 24/65 [04:33<07:47, 11.40s/it]
2022-04-10 01:44:04,324 - INFO - tqdm - accuracy: 0.9449, batch_loss: 0.2209, loss: 0.1386 ||:  38%|###8      | 25/65 [04:44<07:36, 11.40s/it]
2022-04-10 01:44:15,728 - INFO - tqdm - accuracy: 0.9446, batch_loss: 0.0802, loss: 0.1364 ||:  40%|####      | 26/65 [04:56<07:24, 11.40s/it]
2022-04-10 01:44:27,111 - INFO - tqdm - accuracy: 0.9432, batch_loss: 0.1232, loss: 0.1359 ||:  42%|####1     | 27/65 [05:07<07:13, 11.40s/it]
2022-04-10 01:44:38,510 - INFO - tqdm - accuracy: 0.9441, batch_loss: 0.0740, loss: 0.1337 ||:  43%|####3     | 28/65 [05:19<07:01, 11.40s/it]
2022-04-10 01:44:49,906 - INFO - tqdm - accuracy: 0.9461, batch_loss: 0.0356, loss: 0.1303 ||:  45%|####4     | 29/65 [05:30<06:50, 11.40s/it]
2022-04-10 01:45:01,314 - INFO - tqdm - accuracy: 0.9447, batch_loss: 0.1539, loss: 0.1311 ||:  46%|####6     | 30/65 [05:41<06:39, 11.40s/it]
2022-04-10 01:45:12,726 - INFO - tqdm - accuracy: 0.9465, batch_loss: 0.0442, loss: 0.1283 ||:  48%|####7     | 31/65 [05:53<06:27, 11.40s/it]
2022-04-10 01:45:24,135 - INFO - tqdm - accuracy: 0.9472, batch_loss: 0.0797, loss: 0.1267 ||:  49%|####9     | 32/65 [06:04<06:16, 11.41s/it]
2022-04-10 01:45:35,549 - INFO - tqdm - accuracy: 0.9479, batch_loss: 0.0656, loss: 0.1249 ||:  51%|#####     | 33/65 [06:16<06:05, 11.41s/it]
2022-04-10 01:45:46,949 - INFO - tqdm - accuracy: 0.9485, batch_loss: 0.0858, loss: 0.1237 ||:  52%|#####2    | 34/65 [06:27<05:53, 11.41s/it]
2022-04-10 01:45:58,367 - INFO - tqdm - accuracy: 0.9491, batch_loss: 0.0901, loss: 0.1228 ||:  54%|#####3    | 35/65 [06:38<05:42, 11.41s/it]
2022-04-10 01:46:09,772 - INFO - tqdm - accuracy: 0.9496, batch_loss: 0.0834, loss: 0.1217 ||:  55%|#####5    | 36/65 [06:50<05:30, 11.41s/it]
2022-04-10 01:46:21,199 - INFO - tqdm - accuracy: 0.9510, batch_loss: 0.0609, loss: 0.1200 ||:  57%|#####6    | 37/65 [07:01<05:19, 11.41s/it]
2022-04-10 01:46:32,604 - INFO - tqdm - accuracy: 0.9514, batch_loss: 0.0719, loss: 0.1188 ||:  58%|#####8    | 38/65 [07:13<05:08, 11.41s/it]
2022-04-10 01:46:44,055 - INFO - tqdm - accuracy: 0.9519, batch_loss: 0.0543, loss: 0.1171 ||:  60%|######    | 39/65 [07:24<04:57, 11.42s/it]
2022-04-10 01:46:55,513 - INFO - tqdm - accuracy: 0.9500, batch_loss: 0.3263, loss: 0.1224 ||:  62%|######1   | 40/65 [07:36<04:45, 11.43s/it]
2022-04-10 01:47:06,961 - INFO - tqdm - accuracy: 0.9504, batch_loss: 0.0541, loss: 0.1207 ||:  63%|######3   | 41/65 [07:47<04:34, 11.44s/it]
2022-04-10 01:47:18,423 - INFO - tqdm - accuracy: 0.9494, batch_loss: 0.2330, loss: 0.1234 ||:  65%|######4   | 42/65 [07:58<04:23, 11.45s/it]
2022-04-10 01:47:29,873 - INFO - tqdm - accuracy: 0.9491, batch_loss: 0.1870, loss: 0.1248 ||:  66%|######6   | 43/65 [08:10<04:11, 11.45s/it]
2022-04-10 01:47:41,309 - INFO - tqdm - accuracy: 0.9481, batch_loss: 0.3066, loss: 0.1290 ||:  68%|######7   | 44/65 [08:21<04:00, 11.44s/it]
2022-04-10 01:47:52,496 - INFO - tqdm - accuracy: 0.9493, batch_loss: 0.0315, loss: 0.1268 ||:  69%|######9   | 45/65 [08:33<03:47, 11.37s/it]
2022-04-10 01:48:03,946 - INFO - tqdm - accuracy: 0.9490, batch_loss: 0.1353, loss: 0.1270 ||:  71%|#######   | 46/65 [08:44<03:36, 11.39s/it]
2022-04-10 01:48:15,384 - INFO - tqdm - accuracy: 0.9494, batch_loss: 0.0891, loss: 0.1262 ||:  72%|#######2  | 47/65 [08:55<03:25, 11.41s/it]
2022-04-10 01:48:26,797 - INFO - tqdm - accuracy: 0.9492, batch_loss: 0.1409, loss: 0.1265 ||:  74%|#######3  | 48/65 [09:07<03:13, 11.41s/it]
2022-04-10 01:48:38,232 - INFO - tqdm - accuracy: 0.9496, batch_loss: 0.0671, loss: 0.1253 ||:  75%|#######5  | 49/65 [09:18<03:02, 11.42s/it]
2022-04-10 01:48:49,673 - INFO - tqdm - accuracy: 0.9500, batch_loss: 0.0642, loss: 0.1241 ||:  77%|#######6  | 50/65 [09:30<02:51, 11.42s/it]
2022-04-10 01:49:01,089 - INFO - tqdm - accuracy: 0.9503, batch_loss: 0.0634, loss: 0.1229 ||:  78%|#######8  | 51/65 [09:41<02:39, 11.42s/it]
2022-04-10 01:49:12,501 - INFO - tqdm - accuracy: 0.9501, batch_loss: 0.1437, loss: 0.1233 ||:  80%|########  | 52/65 [09:53<02:28, 11.42s/it]
2022-04-10 01:49:23,929 - INFO - tqdm - accuracy: 0.9504, batch_loss: 0.0767, loss: 0.1224 ||:  82%|########1 | 53/65 [10:04<02:17, 11.42s/it]
2022-04-10 01:49:35,349 - INFO - tqdm - accuracy: 0.9508, batch_loss: 0.0831, loss: 0.1217 ||:  83%|########3 | 54/65 [10:15<02:05, 11.42s/it]
2022-04-10 01:49:46,769 - INFO - tqdm - accuracy: 0.9511, batch_loss: 0.0855, loss: 0.1210 ||:  85%|########4 | 55/65 [10:27<01:54, 11.42s/it]
2022-04-10 01:49:58,181 - INFO - tqdm - accuracy: 0.9520, batch_loss: 0.0586, loss: 0.1199 ||:  86%|########6 | 56/65 [10:38<01:42, 11.42s/it]
2022-04-10 01:50:09,600 - INFO - tqdm - accuracy: 0.9523, batch_loss: 0.0846, loss: 0.1193 ||:  88%|########7 | 57/65 [10:50<01:31, 11.42s/it]
2022-04-10 01:50:20,998 - INFO - tqdm - accuracy: 0.9520, batch_loss: 0.1525, loss: 0.1198 ||:  89%|########9 | 58/65 [11:01<01:19, 11.41s/it]
2022-04-10 01:50:32,294 - INFO - tqdm - accuracy: 0.9523, batch_loss: 0.0978, loss: 0.1195 ||:  91%|######### | 59/65 [11:12<01:08, 11.38s/it]
2022-04-10 01:50:43,693 - INFO - tqdm - accuracy: 0.9531, batch_loss: 0.0884, loss: 0.1190 ||:  92%|#########2| 60/65 [11:24<00:56, 11.38s/it]
2022-04-10 01:50:55,092 - INFO - tqdm - accuracy: 0.9528, batch_loss: 0.1528, loss: 0.1195 ||:  94%|#########3| 61/65 [11:35<00:45, 11.39s/it]
2022-04-10 01:51:06,512 - INFO - tqdm - accuracy: 0.9526, batch_loss: 0.1334, loss: 0.1197 ||:  95%|#########5| 62/65 [11:47<00:34, 11.40s/it]
2022-04-10 01:51:17,916 - INFO - tqdm - accuracy: 0.9533, batch_loss: 0.0654, loss: 0.1189 ||:  97%|#########6| 63/65 [11:58<00:22, 11.40s/it]
2022-04-10 01:51:29,310 - INFO - tqdm - accuracy: 0.9531, batch_loss: 0.1083, loss: 0.1187 ||:  98%|#########8| 64/65 [12:09<00:11, 11.40s/it]
2022-04-10 01:51:34,382 - INFO - tqdm - accuracy: 0.9529, batch_loss: 0.1647, loss: 0.1194 ||: 100%|##########| 65/65 [12:14<00:00,  9.50s/it]
2022-04-10 01:51:34,383 - INFO - tqdm - accuracy: 0.9529, batch_loss: 0.1647, loss: 0.1194 ||: 100%|##########| 65/65 [12:14<00:00, 11.31s/it]
2022-04-10 01:51:37,183 - INFO - allennlp.training.trainer - Validating
2022-04-10 01:51:37,184 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-10 01:51:47,431 - INFO - tqdm - accuracy: 0.6463, batch_loss: 0.0768, loss: 1.6644 ||:  36%|###5      | 41/115 [00:10<00:18,  4.00it/s]
2022-04-10 01:51:57,576 - INFO - tqdm - accuracy: 0.6748, batch_loss: 0.0269, loss: 1.4051 ||:  71%|#######1  | 82/115 [00:20<00:08,  4.02it/s]
2022-04-10 01:52:06,001 - INFO - tqdm - accuracy: 0.6507, batch_loss: 0.0149, loss: 1.5549 ||: 100%|##########| 115/115 [00:28<00:00,  3.91it/s]
2022-04-10 01:52:06,002 - INFO - tqdm - accuracy: 0.6507, batch_loss: 0.0149, loss: 1.5549 ||: 100%|##########| 115/115 [00:28<00:00,  3.99it/s]
2022-04-10 01:52:06,002 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-10 01:52:06,003 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.953  |     0.651
2022-04-10 01:52:06,003 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-10 01:52:06,004 - INFO - allennlp.training.tensorboard_writer - loss               |     0.119  |     1.555
2022-04-10 01:52:06,005 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5813.805  |       N/A
2022-04-10 01:52:12,019 - INFO - allennlp.training.trainer - Epoch duration: 0:12:52.577946
2022-04-10 01:52:12,019 - INFO - allennlp.training.trainer - Estimated training time remaining: 1:56:23
2022-04-10 01:52:12,019 - INFO - allennlp.training.trainer - Epoch 6/14
2022-04-10 01:52:12,019 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-10 01:52:12,019 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-10 01:52:12,021 - INFO - allennlp.training.trainer - Training
2022-04-10 01:52:12,021 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-10 01:52:23,314 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0483, loss: 0.0483 ||:   2%|1         | 1/65 [00:11<12:02, 11.29s/it]
2022-04-10 01:52:34,829 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0428, loss: 0.0455 ||:   3%|3         | 2/65 [00:22<11:59, 11.42s/it]
2022-04-10 01:52:46,461 - INFO - tqdm - accuracy: 0.9792, batch_loss: 0.0880, loss: 0.0597 ||:   5%|4         | 3/65 [00:34<11:54, 11.52s/it]
2022-04-10 01:52:57,889 - INFO - tqdm - accuracy: 0.9766, batch_loss: 0.1880, loss: 0.0918 ||:   6%|6         | 4/65 [00:45<11:40, 11.48s/it]
2022-04-10 01:53:09,264 - INFO - tqdm - accuracy: 0.9812, batch_loss: 0.0281, loss: 0.0790 ||:   8%|7         | 5/65 [00:57<11:26, 11.44s/it]
2022-04-10 01:53:20,595 - INFO - tqdm - accuracy: 0.9740, batch_loss: 0.0805, loss: 0.0793 ||:   9%|9         | 6/65 [01:08<11:12, 11.41s/it]
2022-04-10 01:53:31,976 - INFO - tqdm - accuracy: 0.9732, batch_loss: 0.0466, loss: 0.0746 ||:  11%|#         | 7/65 [01:19<11:01, 11.40s/it]
2022-04-10 01:53:43,396 - INFO - tqdm - accuracy: 0.9766, batch_loss: 0.0250, loss: 0.0684 ||:  12%|#2        | 8/65 [01:31<10:50, 11.40s/it]
2022-04-10 01:53:54,861 - INFO - tqdm - accuracy: 0.9722, batch_loss: 0.0815, loss: 0.0699 ||:  14%|#3        | 9/65 [01:42<10:39, 11.42s/it]
2022-04-10 01:54:06,734 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.1984, loss: 0.0827 ||:  15%|#5        | 10/65 [01:54<10:35, 11.56s/it]
2022-04-10 01:54:18,135 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.1458, loss: 0.0884 ||:  17%|#6        | 11/65 [02:06<10:21, 11.51s/it]
2022-04-10 01:54:29,563 - INFO - tqdm - accuracy: 0.9661, batch_loss: 0.1298, loss: 0.0919 ||:  18%|#8        | 12/65 [02:17<10:08, 11.49s/it]
2022-04-10 01:54:40,975 - INFO - tqdm - accuracy: 0.9663, batch_loss: 0.0572, loss: 0.0892 ||:  20%|##        | 13/65 [02:28<09:56, 11.46s/it]
2022-04-10 01:54:52,369 - INFO - tqdm - accuracy: 0.9621, batch_loss: 0.2226, loss: 0.0987 ||:  22%|##1       | 14/65 [02:40<09:43, 11.44s/it]
2022-04-10 01:55:03,746 - INFO - tqdm - accuracy: 0.9625, batch_loss: 0.1022, loss: 0.0990 ||:  23%|##3       | 15/65 [02:51<09:31, 11.42s/it]
2022-04-10 01:55:15,128 - INFO - tqdm - accuracy: 0.9609, batch_loss: 0.0771, loss: 0.0976 ||:  25%|##4       | 16/65 [03:03<09:19, 11.41s/it]
2022-04-10 01:55:26,494 - INFO - tqdm - accuracy: 0.9614, batch_loss: 0.1506, loss: 0.1007 ||:  26%|##6       | 17/65 [03:14<09:07, 11.40s/it]
2022-04-10 01:55:37,876 - INFO - tqdm - accuracy: 0.9618, batch_loss: 0.0670, loss: 0.0989 ||:  28%|##7       | 18/65 [03:25<08:55, 11.39s/it]
2022-04-10 01:55:49,304 - INFO - tqdm - accuracy: 0.9638, batch_loss: 0.0397, loss: 0.0957 ||:  29%|##9       | 19/65 [03:37<08:44, 11.40s/it]
2022-04-10 01:56:00,752 - INFO - tqdm - accuracy: 0.9641, batch_loss: 0.1241, loss: 0.0972 ||:  31%|###       | 20/65 [03:48<08:33, 11.42s/it]
2022-04-10 01:56:12,190 - INFO - tqdm - accuracy: 0.9658, batch_loss: 0.0217, loss: 0.0936 ||:  32%|###2      | 21/65 [04:00<08:22, 11.42s/it]
2022-04-10 01:56:23,634 - INFO - tqdm - accuracy: 0.9673, batch_loss: 0.0493, loss: 0.0916 ||:  34%|###3      | 22/65 [04:11<08:11, 11.43s/it]
2022-04-10 01:56:35,088 - INFO - tqdm - accuracy: 0.9660, batch_loss: 0.1444, loss: 0.0939 ||:  35%|###5      | 23/65 [04:23<08:00, 11.44s/it]
2022-04-10 01:56:46,533 - INFO - tqdm - accuracy: 0.9648, batch_loss: 0.2929, loss: 0.1021 ||:  37%|###6      | 24/65 [04:34<07:49, 11.44s/it]
2022-04-10 01:56:57,970 - INFO - tqdm - accuracy: 0.9637, batch_loss: 0.0882, loss: 0.1016 ||:  38%|###8      | 25/65 [04:45<07:37, 11.44s/it]
2022-04-10 01:57:09,408 - INFO - tqdm - accuracy: 0.9639, batch_loss: 0.0468, loss: 0.0995 ||:  40%|####      | 26/65 [04:57<07:26, 11.44s/it]
2022-04-10 01:57:20,832 - INFO - tqdm - accuracy: 0.9630, batch_loss: 0.1222, loss: 0.1003 ||:  42%|####1     | 27/65 [05:08<07:14, 11.43s/it]
2022-04-10 01:57:32,229 - INFO - tqdm - accuracy: 0.9643, batch_loss: 0.0522, loss: 0.0986 ||:  43%|####3     | 28/65 [05:20<07:02, 11.42s/it]
2022-04-10 01:57:43,644 - INFO - tqdm - accuracy: 0.9655, batch_loss: 0.0267, loss: 0.0961 ||:  45%|####4     | 29/65 [05:31<06:51, 11.42s/it]
2022-04-10 01:57:54,917 - INFO - tqdm - accuracy: 0.9646, batch_loss: 0.1918, loss: 0.0993 ||:  46%|####6     | 30/65 [05:42<06:38, 11.38s/it]
2022-04-10 01:58:06,306 - INFO - tqdm - accuracy: 0.9657, batch_loss: 0.0242, loss: 0.0969 ||:  48%|####7     | 31/65 [05:54<06:26, 11.38s/it]
2022-04-10 01:58:17,693 - INFO - tqdm - accuracy: 0.9658, batch_loss: 0.1133, loss: 0.0974 ||:  49%|####9     | 32/65 [06:05<06:15, 11.38s/it]
2022-04-10 01:58:29,084 - INFO - tqdm - accuracy: 0.9669, batch_loss: 0.0377, loss: 0.0956 ||:  51%|#####     | 33/65 [06:17<06:04, 11.38s/it]
2022-04-10 01:58:40,460 - INFO - tqdm - accuracy: 0.9678, batch_loss: 0.0210, loss: 0.0934 ||:  52%|#####2    | 34/65 [06:28<05:52, 11.38s/it]
2022-04-10 01:58:51,840 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.0281, loss: 0.0915 ||:  54%|#####3    | 35/65 [06:39<05:41, 11.38s/it]
2022-04-10 01:59:03,231 - INFO - tqdm - accuracy: 0.9696, batch_loss: 0.0253, loss: 0.0897 ||:  55%|#####5    | 36/65 [06:51<05:30, 11.38s/it]
2022-04-10 01:59:14,592 - INFO - tqdm - accuracy: 0.9696, batch_loss: 0.0529, loss: 0.0887 ||:  57%|#####6    | 37/65 [07:02<05:18, 11.38s/it]
2022-04-10 01:59:25,976 - INFO - tqdm - accuracy: 0.9696, batch_loss: 0.0804, loss: 0.0885 ||:  58%|#####8    | 38/65 [07:13<05:07, 11.38s/it]
2022-04-10 01:59:37,366 - INFO - tqdm - accuracy: 0.9704, batch_loss: 0.0675, loss: 0.0879 ||:  60%|######    | 39/65 [07:25<04:55, 11.38s/it]
2022-04-10 01:59:48,727 - INFO - tqdm - accuracy: 0.9703, batch_loss: 0.0712, loss: 0.0875 ||:  62%|######1   | 40/65 [07:36<04:44, 11.38s/it]
2022-04-10 01:59:59,770 - INFO - tqdm - accuracy: 0.9695, batch_loss: 0.0989, loss: 0.0878 ||:  63%|######3   | 41/65 [07:47<04:30, 11.28s/it]
2022-04-10 02:00:11,162 - INFO - tqdm - accuracy: 0.9695, batch_loss: 0.1008, loss: 0.0881 ||:  65%|######4   | 42/65 [07:59<04:20, 11.31s/it]
2022-04-10 02:00:22,309 - INFO - tqdm - accuracy: 0.9695, batch_loss: 0.1163, loss: 0.0888 ||:  66%|######6   | 43/65 [08:10<04:07, 11.26s/it]
2022-04-10 02:00:33,717 - INFO - tqdm - accuracy: 0.9694, batch_loss: 0.0929, loss: 0.0889 ||:  68%|######7   | 44/65 [08:21<03:57, 11.31s/it]
2022-04-10 02:00:45,095 - INFO - tqdm - accuracy: 0.9687, batch_loss: 0.1154, loss: 0.0895 ||:  69%|######9   | 45/65 [08:33<03:46, 11.33s/it]
2022-04-10 02:00:56,496 - INFO - tqdm - accuracy: 0.9680, batch_loss: 0.0942, loss: 0.0896 ||:  71%|#######   | 46/65 [08:44<03:35, 11.35s/it]
2022-04-10 02:01:07,895 - INFO - tqdm - accuracy: 0.9687, batch_loss: 0.0263, loss: 0.0882 ||:  72%|#######2  | 47/65 [08:55<03:24, 11.36s/it]
2022-04-10 02:01:19,308 - INFO - tqdm - accuracy: 0.9687, batch_loss: 0.0455, loss: 0.0873 ||:  74%|#######3  | 48/65 [09:07<03:13, 11.38s/it]
2022-04-10 02:01:30,743 - INFO - tqdm - accuracy: 0.9694, batch_loss: 0.0082, loss: 0.0857 ||:  75%|#######5  | 49/65 [09:18<03:02, 11.40s/it]
2022-04-10 02:01:42,187 - INFO - tqdm - accuracy: 0.9700, batch_loss: 0.0249, loss: 0.0845 ||:  77%|#######6  | 50/65 [09:30<02:51, 11.41s/it]
2022-04-10 02:01:53,636 - INFO - tqdm - accuracy: 0.9706, batch_loss: 0.0122, loss: 0.0831 ||:  78%|#######8  | 51/65 [09:41<02:39, 11.42s/it]
2022-04-10 02:02:04,974 - INFO - tqdm - accuracy: 0.9711, batch_loss: 0.0068, loss: 0.0816 ||:  80%|########  | 52/65 [09:52<02:28, 11.40s/it]
2022-04-10 02:02:16,459 - INFO - tqdm - accuracy: 0.9717, batch_loss: 0.0175, loss: 0.0804 ||:  82%|########1 | 53/65 [10:04<02:17, 11.42s/it]
2022-04-10 02:02:27,912 - INFO - tqdm - accuracy: 0.9716, batch_loss: 0.0302, loss: 0.0795 ||:  83%|########3 | 54/65 [10:15<02:05, 11.43s/it]
2022-04-10 02:02:39,343 - INFO - tqdm - accuracy: 0.9721, batch_loss: 0.0056, loss: 0.0781 ||:  85%|########4 | 55/65 [10:27<01:54, 11.43s/it]
2022-04-10 02:02:50,647 - INFO - tqdm - accuracy: 0.9715, batch_loss: 0.0934, loss: 0.0784 ||:  86%|########6 | 56/65 [10:38<01:42, 11.39s/it]
2022-04-10 02:03:02,067 - INFO - tqdm - accuracy: 0.9709, batch_loss: 0.1668, loss: 0.0799 ||:  88%|########7 | 57/65 [10:50<01:31, 11.40s/it]
2022-04-10 02:03:13,471 - INFO - tqdm - accuracy: 0.9714, batch_loss: 0.0037, loss: 0.0786 ||:  89%|########9 | 58/65 [11:01<01:19, 11.40s/it]
2022-04-10 02:03:24,863 - INFO - tqdm - accuracy: 0.9719, batch_loss: 0.0066, loss: 0.0774 ||:  91%|######### | 59/65 [11:12<01:08, 11.40s/it]
2022-04-10 02:03:36,240 - INFO - tqdm - accuracy: 0.9719, batch_loss: 0.0337, loss: 0.0767 ||:  92%|#########2| 60/65 [11:24<00:56, 11.39s/it]
2022-04-10 02:03:47,654 - INFO - tqdm - accuracy: 0.9723, batch_loss: 0.0301, loss: 0.0759 ||:  94%|#########3| 61/65 [11:35<00:45, 11.40s/it]
2022-04-10 02:03:59,045 - INFO - tqdm - accuracy: 0.9723, batch_loss: 0.0713, loss: 0.0758 ||:  95%|#########5| 62/65 [11:47<00:34, 11.40s/it]
2022-04-10 02:04:10,433 - INFO - tqdm - accuracy: 0.9722, batch_loss: 0.0531, loss: 0.0755 ||:  97%|#########6| 63/65 [11:58<00:22, 11.39s/it]
2022-04-10 02:04:21,821 - INFO - tqdm - accuracy: 0.9726, batch_loss: 0.0180, loss: 0.0746 ||:  98%|#########8| 64/65 [12:09<00:11, 11.39s/it]
2022-04-10 02:04:26,874 - INFO - tqdm - accuracy: 0.9723, batch_loss: 0.1648, loss: 0.0760 ||: 100%|##########| 65/65 [12:14<00:00,  9.49s/it]
2022-04-10 02:04:26,876 - INFO - tqdm - accuracy: 0.9723, batch_loss: 0.1648, loss: 0.0760 ||: 100%|##########| 65/65 [12:14<00:00, 11.31s/it]
2022-04-10 02:04:29,671 - INFO - allennlp.training.trainer - Validating
2022-04-10 02:04:29,673 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-10 02:04:39,806 - INFO - tqdm - accuracy: 0.5926, batch_loss: 4.5746, loss: 1.9183 ||:  36%|###5      | 41/115 [00:10<00:18,  4.01it/s]
2022-04-10 02:04:49,840 - INFO - tqdm - accuracy: 0.6087, batch_loss: 0.0076, loss: 1.8019 ||:  70%|#######   | 81/115 [00:20<00:07,  4.37it/s]
2022-04-10 02:04:58,505 - INFO - tqdm - accuracy: 0.6288, batch_loss: 0.8743, loss: 1.7256 ||: 100%|##########| 115/115 [00:28<00:00,  3.93it/s]
2022-04-10 02:04:58,505 - INFO - tqdm - accuracy: 0.6288, batch_loss: 0.8743, loss: 1.7256 ||: 100%|##########| 115/115 [00:28<00:00,  3.99it/s]
2022-04-10 02:04:58,505 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-10 02:04:58,506 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.972  |     0.629
2022-04-10 02:04:58,507 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-10 02:04:58,508 - INFO - allennlp.training.tensorboard_writer - loss               |     0.076  |     1.726
2022-04-10 02:04:58,508 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5813.805  |       N/A
2022-04-10 02:05:04,695 - INFO - allennlp.training.trainer - Epoch duration: 0:12:52.675532
2022-04-10 02:05:04,695 - INFO - allennlp.training.trainer - Estimated training time remaining: 1:43:23
2022-04-10 02:05:04,695 - INFO - allennlp.training.trainer - Epoch 7/14
2022-04-10 02:05:04,695 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-10 02:05:04,695 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-10 02:05:04,697 - INFO - allennlp.training.trainer - Training
2022-04-10 02:05:04,697 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-10 02:05:15,970 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0149, loss: 0.0149 ||:   2%|1         | 1/65 [00:11<12:01, 11.27s/it]
2022-04-10 02:05:27,517 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0616, loss: 0.0382 ||:   3%|3         | 2/65 [00:22<12:00, 11.43s/it]
2022-04-10 02:05:39,158 - INFO - tqdm - accuracy: 0.9583, batch_loss: 0.2937, loss: 0.1234 ||:   5%|4         | 3/65 [00:34<11:54, 11.53s/it]
2022-04-10 02:05:50,581 - INFO - tqdm - accuracy: 0.9609, batch_loss: 0.0353, loss: 0.1013 ||:   6%|6         | 4/65 [00:45<11:40, 11.49s/it]
2022-04-10 02:06:01,957 - INFO - tqdm - accuracy: 0.9625, batch_loss: 0.0515, loss: 0.0914 ||:   8%|7         | 5/65 [00:57<11:26, 11.45s/it]
2022-04-10 02:06:13,380 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.0100, loss: 0.0778 ||:   9%|9         | 6/65 [01:08<11:14, 11.44s/it]
2022-04-10 02:06:24,779 - INFO - tqdm - accuracy: 0.9732, batch_loss: 0.0451, loss: 0.0732 ||:  11%|#         | 7/65 [01:20<11:02, 11.43s/it]
2022-04-10 02:06:36,198 - INFO - tqdm - accuracy: 0.9766, batch_loss: 0.0073, loss: 0.0649 ||:  12%|#2        | 8/65 [01:31<10:51, 11.42s/it]
2022-04-10 02:06:47,605 - INFO - tqdm - accuracy: 0.9757, batch_loss: 0.0482, loss: 0.0631 ||:  14%|#3        | 9/65 [01:42<10:39, 11.42s/it]
2022-04-10 02:06:59,016 - INFO - tqdm - accuracy: 0.9781, batch_loss: 0.0148, loss: 0.0582 ||:  15%|#5        | 10/65 [01:54<10:27, 11.42s/it]
2022-04-10 02:07:10,427 - INFO - tqdm - accuracy: 0.9773, batch_loss: 0.0734, loss: 0.0596 ||:  17%|#6        | 11/65 [02:05<10:16, 11.41s/it]
2022-04-10 02:07:21,830 - INFO - tqdm - accuracy: 0.9792, batch_loss: 0.0166, loss: 0.0560 ||:  18%|#8        | 12/65 [02:17<10:04, 11.41s/it]
2022-04-10 02:07:33,233 - INFO - tqdm - accuracy: 0.9808, batch_loss: 0.0115, loss: 0.0526 ||:  20%|##        | 13/65 [02:28<09:53, 11.41s/it]
2022-04-10 02:07:44,654 - INFO - tqdm - accuracy: 0.9821, batch_loss: 0.0123, loss: 0.0497 ||:  22%|##1       | 14/65 [02:39<09:42, 11.41s/it]
2022-04-10 02:07:55,934 - INFO - tqdm - accuracy: 0.9833, batch_loss: 0.0142, loss: 0.0474 ||:  23%|##3       | 15/65 [02:51<09:28, 11.37s/it]
2022-04-10 02:08:07,333 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0339, loss: 0.0465 ||:  25%|##4       | 16/65 [03:02<09:17, 11.38s/it]
2022-04-10 02:08:18,600 - INFO - tqdm - accuracy: 0.9835, batch_loss: 0.0533, loss: 0.0469 ||:  26%|##6       | 17/65 [03:13<09:04, 11.35s/it]
2022-04-10 02:08:30,002 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0028, loss: 0.0445 ||:  28%|##7       | 18/65 [03:25<08:54, 11.36s/it]
2022-04-10 02:08:41,426 - INFO - tqdm - accuracy: 0.9852, batch_loss: 0.0095, loss: 0.0426 ||:  29%|##9       | 19/65 [03:36<08:43, 11.38s/it]
2022-04-10 02:08:52,711 - INFO - tqdm - accuracy: 0.9859, batch_loss: 0.0117, loss: 0.0411 ||:  31%|###       | 20/65 [03:48<08:30, 11.35s/it]
2022-04-10 02:09:04,113 - INFO - tqdm - accuracy: 0.9866, batch_loss: 0.0248, loss: 0.0403 ||:  32%|###2      | 21/65 [03:59<08:20, 11.37s/it]
2022-04-10 02:09:15,530 - INFO - tqdm - accuracy: 0.9872, batch_loss: 0.0235, loss: 0.0395 ||:  34%|###3      | 22/65 [04:10<08:09, 11.38s/it]
2022-04-10 02:09:26,933 - INFO - tqdm - accuracy: 0.9878, batch_loss: 0.0112, loss: 0.0383 ||:  35%|###5      | 23/65 [04:22<07:58, 11.39s/it]
2022-04-10 02:09:38,321 - INFO - tqdm - accuracy: 0.9883, batch_loss: 0.0026, loss: 0.0368 ||:  37%|###6      | 24/65 [04:33<07:46, 11.39s/it]
2022-04-10 02:09:49,707 - INFO - tqdm - accuracy: 0.9888, batch_loss: 0.0134, loss: 0.0359 ||:  38%|###8      | 25/65 [04:45<07:35, 11.39s/it]
2022-04-10 02:10:01,122 - INFO - tqdm - accuracy: 0.9880, batch_loss: 0.0423, loss: 0.0361 ||:  40%|####      | 26/65 [04:56<07:24, 11.40s/it]
2022-04-10 02:10:12,531 - INFO - tqdm - accuracy: 0.9884, batch_loss: 0.0166, loss: 0.0354 ||:  42%|####1     | 27/65 [05:07<07:13, 11.40s/it]
2022-04-10 02:10:23,923 - INFO - tqdm - accuracy: 0.9888, batch_loss: 0.0047, loss: 0.0343 ||:  43%|####3     | 28/65 [05:19<07:01, 11.40s/it]
2022-04-10 02:10:35,321 - INFO - tqdm - accuracy: 0.9881, batch_loss: 0.1039, loss: 0.0367 ||:  45%|####4     | 29/65 [05:30<06:50, 11.40s/it]
2022-04-10 02:10:46,736 - INFO - tqdm - accuracy: 0.9885, batch_loss: 0.0468, loss: 0.0371 ||:  46%|####6     | 30/65 [05:42<06:39, 11.40s/it]
2022-04-10 02:10:58,153 - INFO - tqdm - accuracy: 0.9879, batch_loss: 0.0349, loss: 0.0370 ||:  48%|####7     | 31/65 [05:53<06:27, 11.41s/it]
2022-04-10 02:11:09,551 - INFO - tqdm - accuracy: 0.9873, batch_loss: 0.0499, loss: 0.0374 ||:  49%|####9     | 32/65 [06:04<06:16, 11.40s/it]
2022-04-10 02:11:20,936 - INFO - tqdm - accuracy: 0.9877, batch_loss: 0.0085, loss: 0.0365 ||:  51%|#####     | 33/65 [06:16<06:04, 11.40s/it]
2022-04-10 02:11:32,352 - INFO - tqdm - accuracy: 0.9871, batch_loss: 0.0707, loss: 0.0375 ||:  52%|#####2    | 34/65 [06:27<05:53, 11.40s/it]
2022-04-10 02:11:43,751 - INFO - tqdm - accuracy: 0.9875, batch_loss: 0.0273, loss: 0.0372 ||:  54%|#####3    | 35/65 [06:39<05:42, 11.40s/it]
2022-04-10 02:11:55,151 - INFO - tqdm - accuracy: 0.9878, batch_loss: 0.0072, loss: 0.0364 ||:  55%|#####5    | 36/65 [06:50<05:30, 11.40s/it]
2022-04-10 02:12:06,565 - INFO - tqdm - accuracy: 0.9873, batch_loss: 0.1653, loss: 0.0399 ||:  57%|#####6    | 37/65 [07:01<05:19, 11.41s/it]
2022-04-10 02:12:17,971 - INFO - tqdm - accuracy: 0.9877, batch_loss: 0.0062, loss: 0.0390 ||:  58%|#####8    | 38/65 [07:13<05:07, 11.41s/it]
2022-04-10 02:12:29,372 - INFO - tqdm - accuracy: 0.9880, batch_loss: 0.0084, loss: 0.0382 ||:  60%|######    | 39/65 [07:24<04:56, 11.40s/it]
2022-04-10 02:12:40,776 - INFO - tqdm - accuracy: 0.9883, batch_loss: 0.0103, loss: 0.0375 ||:  62%|######1   | 40/65 [07:36<04:45, 11.40s/it]
2022-04-10 02:12:52,192 - INFO - tqdm - accuracy: 0.9886, batch_loss: 0.0212, loss: 0.0371 ||:  63%|######3   | 41/65 [07:47<04:33, 11.41s/it]
2022-04-10 02:13:03,590 - INFO - tqdm - accuracy: 0.9888, batch_loss: 0.0090, loss: 0.0364 ||:  65%|######4   | 42/65 [07:58<04:22, 11.40s/it]
2022-04-10 02:13:15,017 - INFO - tqdm - accuracy: 0.9891, batch_loss: 0.0252, loss: 0.0362 ||:  66%|######6   | 43/65 [08:10<04:11, 11.41s/it]
2022-04-10 02:13:26,407 - INFO - tqdm - accuracy: 0.9893, batch_loss: 0.0048, loss: 0.0355 ||:  68%|######7   | 44/65 [08:21<03:59, 11.41s/it]
2022-04-10 02:13:38,200 - INFO - tqdm - accuracy: 0.9889, batch_loss: 0.0325, loss: 0.0354 ||:  69%|######9   | 45/65 [08:33<03:50, 11.52s/it]
2022-04-10 02:13:49,560 - INFO - tqdm - accuracy: 0.9885, batch_loss: 0.0452, loss: 0.0356 ||:  71%|#######   | 46/65 [08:44<03:37, 11.47s/it]
2022-04-10 02:14:00,983 - INFO - tqdm - accuracy: 0.9887, batch_loss: 0.0102, loss: 0.0351 ||:  72%|#######2  | 47/65 [08:56<03:26, 11.46s/it]
2022-04-10 02:14:12,413 - INFO - tqdm - accuracy: 0.9883, batch_loss: 0.0388, loss: 0.0352 ||:  74%|#######3  | 48/65 [09:07<03:14, 11.45s/it]
2022-04-10 02:14:23,847 - INFO - tqdm - accuracy: 0.9885, batch_loss: 0.0184, loss: 0.0348 ||:  75%|#######5  | 49/65 [09:19<03:03, 11.45s/it]
2022-04-10 02:14:35,296 - INFO - tqdm - accuracy: 0.9888, batch_loss: 0.0037, loss: 0.0342 ||:  77%|#######6  | 50/65 [09:30<02:51, 11.45s/it]
2022-04-10 02:14:46,757 - INFO - tqdm - accuracy: 0.9890, batch_loss: 0.0177, loss: 0.0339 ||:  78%|#######8  | 51/65 [09:42<02:40, 11.45s/it]
2022-04-10 02:14:58,227 - INFO - tqdm - accuracy: 0.9892, batch_loss: 0.0222, loss: 0.0336 ||:  80%|########  | 52/65 [09:53<02:28, 11.46s/it]
2022-04-10 02:15:09,675 - INFO - tqdm - accuracy: 0.9894, batch_loss: 0.0106, loss: 0.0332 ||:  82%|########1 | 53/65 [10:04<02:17, 11.45s/it]
2022-04-10 02:15:21,155 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.0270, loss: 0.0331 ||:  83%|########3 | 54/65 [10:16<02:06, 11.46s/it]
2022-04-10 02:15:32,624 - INFO - tqdm - accuracy: 0.9898, batch_loss: 0.0228, loss: 0.0329 ||:  85%|########4 | 55/65 [10:27<01:54, 11.46s/it]
2022-04-10 02:15:44,066 - INFO - tqdm - accuracy: 0.9894, batch_loss: 0.0279, loss: 0.0328 ||:  86%|########6 | 56/65 [10:39<01:43, 11.46s/it]
2022-04-10 02:15:55,485 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.0080, loss: 0.0324 ||:  88%|########7 | 57/65 [10:50<01:31, 11.45s/it]
2022-04-10 02:16:06,905 - INFO - tqdm - accuracy: 0.9892, batch_loss: 0.0896, loss: 0.0334 ||:  89%|########9 | 58/65 [11:02<01:20, 11.44s/it]
2022-04-10 02:16:17,716 - INFO - tqdm - accuracy: 0.9894, batch_loss: 0.0104, loss: 0.0330 ||:  91%|######### | 59/65 [11:13<01:07, 11.25s/it]
2022-04-10 02:16:29,096 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.0085, loss: 0.0326 ||:  92%|#########2| 60/65 [11:24<00:56, 11.29s/it]
2022-04-10 02:16:40,490 - INFO - tqdm - accuracy: 0.9897, batch_loss: 0.0067, loss: 0.0321 ||:  94%|#########3| 61/65 [11:35<00:45, 11.32s/it]
2022-04-10 02:16:51,862 - INFO - tqdm - accuracy: 0.9889, batch_loss: 0.1463, loss: 0.0340 ||:  95%|#########5| 62/65 [11:47<00:34, 11.34s/it]
2022-04-10 02:17:03,256 - INFO - tqdm - accuracy: 0.9886, batch_loss: 0.0767, loss: 0.0347 ||:  97%|#########6| 63/65 [11:58<00:22, 11.35s/it]
2022-04-10 02:17:14,661 - INFO - tqdm - accuracy: 0.9878, batch_loss: 0.1033, loss: 0.0357 ||:  98%|#########8| 64/65 [12:09<00:11, 11.37s/it]
2022-04-10 02:17:19,739 - INFO - tqdm - accuracy: 0.9879, batch_loss: 0.0028, loss: 0.0352 ||: 100%|##########| 65/65 [12:15<00:00,  9.48s/it]
2022-04-10 02:17:19,739 - INFO - tqdm - accuracy: 0.9879, batch_loss: 0.0028, loss: 0.0352 ||: 100%|##########| 65/65 [12:15<00:00, 11.31s/it]
2022-04-10 02:17:22,552 - INFO - allennlp.training.trainer - Validating
2022-04-10 02:17:22,553 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-10 02:17:32,746 - INFO - tqdm - accuracy: 0.7317, batch_loss: 4.0248, loss: 1.5165 ||:  36%|###5      | 41/115 [00:10<00:18,  4.01it/s]
2022-04-10 02:17:42,766 - INFO - tqdm - accuracy: 0.6708, batch_loss: 4.1064, loss: 1.8448 ||:  70%|#######   | 81/115 [00:20<00:08,  3.92it/s]
2022-04-10 02:17:51,431 - INFO - tqdm - accuracy: 0.6681, batch_loss: 3.9722, loss: 1.9012 ||: 100%|##########| 115/115 [00:28<00:00,  3.93it/s]
2022-04-10 02:17:51,431 - INFO - tqdm - accuracy: 0.6681, batch_loss: 3.9722, loss: 1.9012 ||: 100%|##########| 115/115 [00:28<00:00,  3.98it/s]
2022-04-10 02:17:51,432 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-10 02:17:51,432 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.988  |     0.668
2022-04-10 02:17:51,433 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-10 02:17:51,433 - INFO - allennlp.training.tensorboard_writer - loss               |     0.035  |     1.901
2022-04-10 02:17:51,434 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5813.805  |       N/A
2022-04-10 02:17:57,065 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to './output_ir-ora-d_hyper2/best.th'.
2022-04-10 02:18:12,696 - INFO - allennlp.training.trainer - Epoch duration: 0:13:08.000691
2022-04-10 02:18:12,696 - INFO - allennlp.training.trainer - Estimated training time remaining: 1:30:39
2022-04-10 02:18:12,696 - INFO - allennlp.training.trainer - Epoch 8/14
2022-04-10 02:18:12,696 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-10 02:18:12,697 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-10 02:18:12,698 - INFO - allennlp.training.trainer - Training
2022-04-10 02:18:12,698 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-10 02:18:23,776 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.1225, loss: 0.1225 ||:   2%|1         | 1/65 [00:11<11:48, 11.08s/it]
2022-04-10 02:18:35,150 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0029, loss: 0.0627 ||:   3%|3         | 2/65 [00:22<11:48, 11.25s/it]
2022-04-10 02:18:46,751 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.0174, loss: 0.0476 ||:   5%|4         | 3/65 [00:34<11:47, 11.41s/it]
2022-04-10 02:18:58,352 - INFO - tqdm - accuracy: 0.9922, batch_loss: 0.0116, loss: 0.0386 ||:   6%|6         | 4/65 [00:45<11:40, 11.49s/it]
2022-04-10 02:19:09,747 - INFO - tqdm - accuracy: 0.9938, batch_loss: 0.0170, loss: 0.0343 ||:   8%|7         | 5/65 [00:57<11:27, 11.45s/it]
2022-04-10 02:19:21,048 - INFO - tqdm - accuracy: 0.9948, batch_loss: 0.0210, loss: 0.0321 ||:   9%|9         | 6/65 [01:08<11:12, 11.40s/it]
2022-04-10 02:19:32,368 - INFO - tqdm - accuracy: 0.9955, batch_loss: 0.0273, loss: 0.0314 ||:  11%|#         | 7/65 [01:19<10:59, 11.37s/it]
2022-04-10 02:19:43,762 - INFO - tqdm - accuracy: 0.9922, batch_loss: 0.0257, loss: 0.0307 ||:  12%|#2        | 8/65 [01:31<10:48, 11.38s/it]
2022-04-10 02:19:55,251 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.0275, loss: 0.0303 ||:  14%|#3        | 9/65 [01:42<10:39, 11.41s/it]
2022-04-10 02:20:06,696 - INFO - tqdm - accuracy: 0.9906, batch_loss: 0.0037, loss: 0.0277 ||:  15%|#5        | 10/65 [01:53<10:28, 11.42s/it]
2022-04-10 02:20:18,087 - INFO - tqdm - accuracy: 0.9886, batch_loss: 0.0261, loss: 0.0275 ||:  17%|#6        | 11/65 [02:05<10:16, 11.41s/it]
2022-04-10 02:20:29,464 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.0225, loss: 0.0271 ||:  18%|#8        | 12/65 [02:16<10:04, 11.40s/it]
2022-04-10 02:20:40,911 - INFO - tqdm - accuracy: 0.9880, batch_loss: 0.0830, loss: 0.0314 ||:  20%|##        | 13/65 [02:28<09:53, 11.42s/it]
2022-04-10 02:20:52,373 - INFO - tqdm - accuracy: 0.9888, batch_loss: 0.0118, loss: 0.0300 ||:  22%|##1       | 14/65 [02:39<09:42, 11.43s/it]
2022-04-10 02:21:03,840 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.0047, loss: 0.0283 ||:  23%|##3       | 15/65 [02:51<09:32, 11.44s/it]
2022-04-10 02:21:15,277 - INFO - tqdm - accuracy: 0.9902, batch_loss: 0.0111, loss: 0.0272 ||:  25%|##4       | 16/65 [03:02<09:20, 11.44s/it]
2022-04-10 02:21:26,703 - INFO - tqdm - accuracy: 0.9908, batch_loss: 0.0228, loss: 0.0270 ||:  26%|##6       | 17/65 [03:14<09:08, 11.44s/it]
2022-04-10 02:21:38,120 - INFO - tqdm - accuracy: 0.9861, batch_loss: 0.1819, loss: 0.0356 ||:  28%|##7       | 18/65 [03:25<08:57, 11.43s/it]
2022-04-10 02:21:49,499 - INFO - tqdm - accuracy: 0.9868, batch_loss: 0.0088, loss: 0.0342 ||:  29%|##9       | 19/65 [03:36<08:45, 11.41s/it]
2022-04-10 02:22:00,893 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.2789, loss: 0.0464 ||:  31%|###       | 20/65 [03:48<08:33, 11.41s/it]
2022-04-10 02:22:12,292 - INFO - tqdm - accuracy: 0.9851, batch_loss: 0.0150, loss: 0.0449 ||:  32%|###2      | 21/65 [03:59<08:21, 11.41s/it]
2022-04-10 02:22:23,698 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0770, loss: 0.0464 ||:  34%|###3      | 22/65 [04:10<08:10, 11.41s/it]
2022-04-10 02:22:35,086 - INFO - tqdm - accuracy: 0.9851, batch_loss: 0.0129, loss: 0.0449 ||:  35%|###5      | 23/65 [04:22<07:58, 11.40s/it]
2022-04-10 02:22:46,495 - INFO - tqdm - accuracy: 0.9857, batch_loss: 0.0163, loss: 0.0437 ||:  37%|###6      | 24/65 [04:33<07:47, 11.40s/it]
2022-04-10 02:22:57,895 - INFO - tqdm - accuracy: 0.9850, batch_loss: 0.1019, loss: 0.0460 ||:  38%|###8      | 25/65 [04:45<07:36, 11.40s/it]
2022-04-10 02:23:09,299 - INFO - tqdm - accuracy: 0.9856, batch_loss: 0.0085, loss: 0.0446 ||:  40%|####      | 26/65 [04:56<07:24, 11.40s/it]
2022-04-10 02:23:20,433 - INFO - tqdm - accuracy: 0.9850, batch_loss: 0.0653, loss: 0.0454 ||:  42%|####1     | 27/65 [05:07<07:10, 11.32s/it]
2022-04-10 02:23:31,825 - INFO - tqdm - accuracy: 0.9855, batch_loss: 0.0040, loss: 0.0439 ||:  43%|####3     | 28/65 [05:19<06:59, 11.34s/it]
2022-04-10 02:23:43,223 - INFO - tqdm - accuracy: 0.9860, batch_loss: 0.0288, loss: 0.0434 ||:  45%|####4     | 29/65 [05:30<06:48, 11.36s/it]
2022-04-10 02:23:54,632 - INFO - tqdm - accuracy: 0.9854, batch_loss: 0.0977, loss: 0.0452 ||:  46%|####6     | 30/65 [05:41<06:38, 11.37s/it]
2022-04-10 02:24:06,028 - INFO - tqdm - accuracy: 0.9859, batch_loss: 0.0025, loss: 0.0438 ||:  48%|####7     | 31/65 [05:53<06:26, 11.38s/it]
2022-04-10 02:24:17,438 - INFO - tqdm - accuracy: 0.9863, batch_loss: 0.0154, loss: 0.0429 ||:  49%|####9     | 32/65 [06:04<06:15, 11.39s/it]
2022-04-10 02:24:28,840 - INFO - tqdm - accuracy: 0.9867, batch_loss: 0.0252, loss: 0.0424 ||:  51%|#####     | 33/65 [06:16<06:04, 11.39s/it]
2022-04-10 02:24:40,226 - INFO - tqdm - accuracy: 0.9871, batch_loss: 0.0329, loss: 0.0421 ||:  52%|#####2    | 34/65 [06:27<05:53, 11.39s/it]
2022-04-10 02:24:51,635 - INFO - tqdm - accuracy: 0.9875, batch_loss: 0.0013, loss: 0.0409 ||:  54%|#####3    | 35/65 [06:38<05:41, 11.40s/it]
2022-04-10 02:25:03,034 - INFO - tqdm - accuracy: 0.9878, batch_loss: 0.0020, loss: 0.0399 ||:  55%|#####5    | 36/65 [06:50<05:30, 11.40s/it]
2022-04-10 02:25:14,431 - INFO - tqdm - accuracy: 0.9873, batch_loss: 0.0312, loss: 0.0396 ||:  57%|#####6    | 37/65 [07:01<05:19, 11.40s/it]
2022-04-10 02:25:25,838 - INFO - tqdm - accuracy: 0.9877, batch_loss: 0.0199, loss: 0.0391 ||:  58%|#####8    | 38/65 [07:13<05:07, 11.40s/it]
2022-04-10 02:25:37,016 - INFO - tqdm - accuracy: 0.9872, batch_loss: 0.0670, loss: 0.0398 ||:  60%|######    | 39/65 [07:24<04:54, 11.33s/it]
2022-04-10 02:25:48,437 - INFO - tqdm - accuracy: 0.9875, batch_loss: 0.0112, loss: 0.0391 ||:  62%|######1   | 40/65 [07:35<04:43, 11.36s/it]
2022-04-10 02:25:59,876 - INFO - tqdm - accuracy: 0.9878, batch_loss: 0.0352, loss: 0.0390 ||:  63%|######3   | 41/65 [07:47<04:33, 11.38s/it]
2022-04-10 02:26:11,330 - INFO - tqdm - accuracy: 0.9881, batch_loss: 0.0107, loss: 0.0383 ||:  65%|######4   | 42/65 [07:58<04:22, 11.40s/it]
2022-04-10 02:26:22,791 - INFO - tqdm - accuracy: 0.9884, batch_loss: 0.0092, loss: 0.0377 ||:  66%|######6   | 43/65 [08:10<04:11, 11.42s/it]
2022-04-10 02:26:34,258 - INFO - tqdm - accuracy: 0.9879, batch_loss: 0.0503, loss: 0.0379 ||:  68%|######7   | 44/65 [08:21<04:00, 11.44s/it]
2022-04-10 02:26:45,710 - INFO - tqdm - accuracy: 0.9875, batch_loss: 0.0652, loss: 0.0386 ||:  69%|######9   | 45/65 [08:33<03:48, 11.44s/it]
2022-04-10 02:26:57,149 - INFO - tqdm - accuracy: 0.9878, batch_loss: 0.0079, loss: 0.0379 ||:  71%|#######   | 46/65 [08:44<03:37, 11.44s/it]
2022-04-10 02:27:08,604 - INFO - tqdm - accuracy: 0.9880, batch_loss: 0.0050, loss: 0.0372 ||:  72%|#######2  | 47/65 [08:55<03:25, 11.44s/it]
2022-04-10 02:27:20,063 - INFO - tqdm - accuracy: 0.9883, batch_loss: 0.0098, loss: 0.0366 ||:  74%|#######3  | 48/65 [09:07<03:14, 11.45s/it]
2022-04-10 02:27:31,498 - INFO - tqdm - accuracy: 0.9885, batch_loss: 0.0036, loss: 0.0359 ||:  75%|#######5  | 49/65 [09:18<03:03, 11.44s/it]
2022-04-10 02:27:42,934 - INFO - tqdm - accuracy: 0.9888, batch_loss: 0.0093, loss: 0.0354 ||:  77%|#######6  | 50/65 [09:30<02:51, 11.44s/it]
2022-04-10 02:27:54,370 - INFO - tqdm - accuracy: 0.9890, batch_loss: 0.0291, loss: 0.0353 ||:  78%|#######8  | 51/65 [09:41<02:40, 11.44s/it]
2022-04-10 02:28:05,813 - INFO - tqdm - accuracy: 0.9892, batch_loss: 0.0053, loss: 0.0347 ||:  80%|########  | 52/65 [09:53<02:28, 11.44s/it]
2022-04-10 02:28:17,243 - INFO - tqdm - accuracy: 0.9894, batch_loss: 0.0024, loss: 0.0341 ||:  82%|########1 | 53/65 [10:04<02:17, 11.44s/it]
2022-04-10 02:28:28,663 - INFO - tqdm - accuracy: 0.9890, batch_loss: 0.0729, loss: 0.0348 ||:  83%|########3 | 54/65 [10:15<02:05, 11.43s/it]
2022-04-10 02:28:40,080 - INFO - tqdm - accuracy: 0.9892, batch_loss: 0.0017, loss: 0.0342 ||:  85%|########4 | 55/65 [10:27<01:54, 11.43s/it]
2022-04-10 02:28:51,501 - INFO - tqdm - accuracy: 0.9888, batch_loss: 0.0481, loss: 0.0345 ||:  86%|########6 | 56/65 [10:38<01:42, 11.43s/it]
2022-04-10 02:29:02,901 - INFO - tqdm - accuracy: 0.9890, batch_loss: 0.0033, loss: 0.0339 ||:  88%|########7 | 57/65 [10:50<01:31, 11.42s/it]
2022-04-10 02:29:14,315 - INFO - tqdm - accuracy: 0.9892, batch_loss: 0.0042, loss: 0.0334 ||:  89%|########9 | 58/65 [11:01<01:19, 11.42s/it]
2022-04-10 02:29:25,725 - INFO - tqdm - accuracy: 0.9894, batch_loss: 0.0202, loss: 0.0332 ||:  91%|######### | 59/65 [11:13<01:08, 11.41s/it]
2022-04-10 02:29:36,790 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.0292, loss: 0.0331 ||:  92%|#########2| 60/65 [11:24<00:56, 11.31s/it]
2022-04-10 02:29:48,202 - INFO - tqdm - accuracy: 0.9897, batch_loss: 0.0064, loss: 0.0327 ||:  94%|#########3| 61/65 [11:35<00:45, 11.34s/it]
2022-04-10 02:29:59,605 - INFO - tqdm - accuracy: 0.9894, batch_loss: 0.1441, loss: 0.0345 ||:  95%|#########5| 62/65 [11:46<00:34, 11.36s/it]
2022-04-10 02:30:11,003 - INFO - tqdm - accuracy: 0.9891, batch_loss: 0.0658, loss: 0.0350 ||:  97%|#########6| 63/65 [11:58<00:22, 11.37s/it]
2022-04-10 02:30:22,403 - INFO - tqdm - accuracy: 0.9893, batch_loss: 0.0036, loss: 0.0345 ||:  98%|#########8| 64/65 [12:09<00:11, 11.38s/it]
2022-04-10 02:30:27,340 - INFO - tqdm - accuracy: 0.9893, batch_loss: 0.0023, loss: 0.0340 ||: 100%|##########| 65/65 [12:14<00:00,  9.45s/it]
2022-04-10 02:30:27,340 - INFO - tqdm - accuracy: 0.9893, batch_loss: 0.0023, loss: 0.0340 ||: 100%|##########| 65/65 [12:14<00:00, 11.30s/it]
2022-04-10 02:30:30,156 - INFO - allennlp.training.trainer - Validating
2022-04-10 02:30:30,158 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-10 02:30:40,173 - INFO - tqdm - accuracy: 0.6000, batch_loss: 1.1704, loss: 2.1420 ||:  35%|###4      | 40/115 [00:10<00:18,  3.99it/s]
2022-04-10 02:30:50,273 - INFO - tqdm - accuracy: 0.6188, batch_loss: 2.7453, loss: 2.1646 ||:  70%|######9   | 80/115 [00:20<00:08,  3.92it/s]
2022-04-10 02:30:59,024 - INFO - tqdm - accuracy: 0.6419, batch_loss: 0.0007, loss: 2.0321 ||: 100%|##########| 115/115 [00:28<00:00,  3.96it/s]
2022-04-10 02:30:59,024 - INFO - tqdm - accuracy: 0.6419, batch_loss: 0.0007, loss: 2.0321 ||: 100%|##########| 115/115 [00:28<00:00,  3.98it/s]
2022-04-10 02:30:59,025 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-10 02:30:59,025 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.989  |     0.642
2022-04-10 02:30:59,026 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-10 02:30:59,027 - INFO - allennlp.training.tensorboard_writer - loss               |     0.034  |     2.032
2022-04-10 02:30:59,027 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5813.805  |       N/A
2022-04-10 02:31:05,353 - INFO - allennlp.training.trainer - Epoch duration: 0:12:52.657430
2022-04-10 02:31:05,354 - INFO - allennlp.training.trainer - Estimated training time remaining: 1:17:39
2022-04-10 02:31:05,354 - INFO - allennlp.training.trainer - Epoch 9/14
2022-04-10 02:31:05,354 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-10 02:31:05,354 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-10 02:31:05,356 - INFO - allennlp.training.trainer - Training
2022-04-10 02:31:05,356 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-10 02:31:16,591 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0021, loss: 0.0021 ||:   2%|1         | 1/65 [00:11<11:58, 11.23s/it]
2022-04-10 02:31:28,042 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0050, loss: 0.0035 ||:   3%|3         | 2/65 [00:22<11:55, 11.36s/it]
2022-04-10 02:31:39,724 - INFO - tqdm - accuracy: 0.9792, batch_loss: 0.1049, loss: 0.0373 ||:   5%|4         | 3/65 [00:34<11:53, 11.51s/it]
2022-04-10 02:31:51,092 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0030, loss: 0.0287 ||:   6%|6         | 4/65 [00:45<11:38, 11.45s/it]
2022-04-10 02:32:02,539 - INFO - tqdm - accuracy: 0.9812, batch_loss: 0.0816, loss: 0.0393 ||:   8%|7         | 5/65 [00:57<11:27, 11.45s/it]
2022-04-10 02:32:13,928 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0057, loss: 0.0337 ||:   9%|9         | 6/65 [01:08<11:14, 11.43s/it]
2022-04-10 02:32:25,305 - INFO - tqdm - accuracy: 0.9866, batch_loss: 0.0068, loss: 0.0299 ||:  11%|#         | 7/65 [01:19<11:01, 11.41s/it]
2022-04-10 02:32:36,746 - INFO - tqdm - accuracy: 0.9805, batch_loss: 0.1237, loss: 0.0416 ||:  12%|#2        | 8/65 [01:31<10:51, 11.42s/it]
2022-04-10 02:32:48,216 - INFO - tqdm - accuracy: 0.9826, batch_loss: 0.0010, loss: 0.0371 ||:  14%|#3        | 9/65 [01:42<10:40, 11.44s/it]
2022-04-10 02:32:59,689 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0015, loss: 0.0335 ||:  15%|#5        | 10/65 [01:54<10:29, 11.45s/it]
2022-04-10 02:33:11,115 - INFO - tqdm - accuracy: 0.9858, batch_loss: 0.0225, loss: 0.0325 ||:  17%|#6        | 11/65 [02:05<10:17, 11.44s/it]
2022-04-10 02:33:22,512 - INFO - tqdm - accuracy: 0.9870, batch_loss: 0.0060, loss: 0.0303 ||:  18%|#8        | 12/65 [02:17<10:05, 11.43s/it]
2022-04-10 02:33:33,908 - INFO - tqdm - accuracy: 0.9856, batch_loss: 0.0886, loss: 0.0348 ||:  20%|##        | 13/65 [02:28<09:53, 11.42s/it]
2022-04-10 02:33:45,352 - INFO - tqdm - accuracy: 0.9866, batch_loss: 0.0101, loss: 0.0330 ||:  22%|##1       | 14/65 [02:39<09:42, 11.43s/it]
2022-04-10 02:33:57,297 - INFO - tqdm - accuracy: 0.9875, batch_loss: 0.0039, loss: 0.0311 ||:  23%|##3       | 15/65 [02:51<09:39, 11.58s/it]
2022-04-10 02:34:08,750 - INFO - tqdm - accuracy: 0.9883, batch_loss: 0.0048, loss: 0.0294 ||:  25%|##4       | 16/65 [03:03<09:25, 11.54s/it]
2022-04-10 02:34:20,211 - INFO - tqdm - accuracy: 0.9890, batch_loss: 0.0080, loss: 0.0282 ||:  26%|##6       | 17/65 [03:14<09:12, 11.52s/it]
2022-04-10 02:34:31,638 - INFO - tqdm - accuracy: 0.9878, batch_loss: 0.0759, loss: 0.0308 ||:  28%|##7       | 18/65 [03:26<09:00, 11.49s/it]
2022-04-10 02:34:43,046 - INFO - tqdm - accuracy: 0.9868, batch_loss: 0.0950, loss: 0.0342 ||:  29%|##9       | 19/65 [03:37<08:47, 11.47s/it]
2022-04-10 02:34:54,414 - INFO - tqdm - accuracy: 0.9875, batch_loss: 0.0184, loss: 0.0334 ||:  31%|###       | 20/65 [03:49<08:34, 11.44s/it]
2022-04-10 02:35:05,807 - INFO - tqdm - accuracy: 0.9881, batch_loss: 0.0163, loss: 0.0326 ||:  32%|###2      | 21/65 [04:00<08:22, 11.42s/it]
2022-04-10 02:35:17,200 - INFO - tqdm - accuracy: 0.9886, batch_loss: 0.0141, loss: 0.0318 ||:  34%|###3      | 22/65 [04:11<08:10, 11.41s/it]
2022-04-10 02:35:28,631 - INFO - tqdm - accuracy: 0.9891, batch_loss: 0.0198, loss: 0.0312 ||:  35%|###5      | 23/65 [04:23<07:59, 11.42s/it]
2022-04-10 02:35:40,074 - INFO - tqdm - accuracy: 0.9870, batch_loss: 0.1399, loss: 0.0358 ||:  37%|###6      | 24/65 [04:34<07:48, 11.43s/it]
2022-04-10 02:35:51,526 - INFO - tqdm - accuracy: 0.9875, batch_loss: 0.0170, loss: 0.0350 ||:  38%|###8      | 25/65 [04:46<07:37, 11.43s/it]
2022-04-10 02:36:02,989 - INFO - tqdm - accuracy: 0.9880, batch_loss: 0.0014, loss: 0.0337 ||:  40%|####      | 26/65 [04:57<07:26, 11.44s/it]
2022-04-10 02:36:14,456 - INFO - tqdm - accuracy: 0.9873, batch_loss: 0.1008, loss: 0.0362 ||:  42%|####1     | 27/65 [05:09<07:15, 11.45s/it]
2022-04-10 02:36:25,919 - INFO - tqdm - accuracy: 0.9877, batch_loss: 0.0074, loss: 0.0352 ||:  43%|####3     | 28/65 [05:20<07:03, 11.45s/it]
2022-04-10 02:36:37,372 - INFO - tqdm - accuracy: 0.9881, batch_loss: 0.0184, loss: 0.0346 ||:  45%|####4     | 29/65 [05:32<06:52, 11.45s/it]
2022-04-10 02:36:48,691 - INFO - tqdm - accuracy: 0.9885, batch_loss: 0.0016, loss: 0.0335 ||:  46%|####6     | 30/65 [05:43<06:39, 11.41s/it]
2022-04-10 02:37:00,098 - INFO - tqdm - accuracy: 0.9889, batch_loss: 0.0245, loss: 0.0332 ||:  48%|####7     | 31/65 [05:54<06:27, 11.41s/it]
2022-04-10 02:37:11,496 - INFO - tqdm - accuracy: 0.9893, batch_loss: 0.0026, loss: 0.0323 ||:  49%|####9     | 32/65 [06:06<06:16, 11.41s/it]
2022-04-10 02:37:22,885 - INFO - tqdm - accuracy: 0.9886, batch_loss: 0.0911, loss: 0.0340 ||:  51%|#####     | 33/65 [06:17<06:04, 11.40s/it]
2022-04-10 02:37:34,269 - INFO - tqdm - accuracy: 0.9890, batch_loss: 0.0112, loss: 0.0334 ||:  52%|#####2    | 34/65 [06:28<05:53, 11.40s/it]
2022-04-10 02:37:45,321 - INFO - tqdm - accuracy: 0.9893, batch_loss: 0.0047, loss: 0.0326 ||:  54%|#####3    | 35/65 [06:39<05:38, 11.29s/it]
2022-04-10 02:37:56,706 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.0028, loss: 0.0317 ||:  55%|#####5    | 36/65 [06:51<05:28, 11.32s/it]
2022-04-10 02:38:08,075 - INFO - tqdm - accuracy: 0.9882, batch_loss: 0.1625, loss: 0.0353 ||:  57%|#####6    | 37/65 [07:02<05:17, 11.34s/it]
2022-04-10 02:38:19,470 - INFO - tqdm - accuracy: 0.9885, batch_loss: 0.0116, loss: 0.0346 ||:  58%|#####8    | 38/65 [07:14<05:06, 11.35s/it]
2022-04-10 02:38:30,899 - INFO - tqdm - accuracy: 0.9888, batch_loss: 0.0044, loss: 0.0339 ||:  60%|######    | 39/65 [07:25<04:55, 11.38s/it]
2022-04-10 02:38:42,325 - INFO - tqdm - accuracy: 0.9891, batch_loss: 0.0034, loss: 0.0331 ||:  62%|######1   | 40/65 [07:36<04:44, 11.39s/it]
2022-04-10 02:38:53,774 - INFO - tqdm - accuracy: 0.9893, batch_loss: 0.0238, loss: 0.0329 ||:  63%|######3   | 41/65 [07:48<04:33, 11.41s/it]
2022-04-10 02:39:05,224 - INFO - tqdm - accuracy: 0.9888, batch_loss: 0.0599, loss: 0.0335 ||:  65%|######4   | 42/65 [07:59<04:22, 11.42s/it]
2022-04-10 02:39:16,689 - INFO - tqdm - accuracy: 0.9891, batch_loss: 0.0013, loss: 0.0328 ||:  66%|######6   | 43/65 [08:11<04:11, 11.43s/it]
2022-04-10 02:39:28,135 - INFO - tqdm - accuracy: 0.9893, batch_loss: 0.0327, loss: 0.0328 ||:  68%|######7   | 44/65 [08:22<04:00, 11.44s/it]
2022-04-10 02:39:39,592 - INFO - tqdm - accuracy: 0.9889, batch_loss: 0.0558, loss: 0.0333 ||:  69%|######9   | 45/65 [08:34<03:48, 11.44s/it]
2022-04-10 02:39:51,063 - INFO - tqdm - accuracy: 0.9884, batch_loss: 0.0549, loss: 0.0337 ||:  71%|#######   | 46/65 [08:45<03:37, 11.45s/it]
2022-04-10 02:40:02,522 - INFO - tqdm - accuracy: 0.9874, batch_loss: 0.1560, loss: 0.0363 ||:  72%|#######2  | 47/65 [08:57<03:26, 11.45s/it]
2022-04-10 02:40:13,969 - INFO - tqdm - accuracy: 0.9876, batch_loss: 0.0100, loss: 0.0358 ||:  74%|#######3  | 48/65 [09:08<03:14, 11.45s/it]
2022-04-10 02:40:25,413 - INFO - tqdm - accuracy: 0.9879, batch_loss: 0.0300, loss: 0.0357 ||:  75%|#######5  | 49/65 [09:20<03:03, 11.45s/it]
2022-04-10 02:40:36,582 - INFO - tqdm - accuracy: 0.9875, batch_loss: 0.0423, loss: 0.0358 ||:  77%|#######6  | 50/65 [09:31<02:50, 11.37s/it]
2022-04-10 02:40:48,036 - INFO - tqdm - accuracy: 0.9877, batch_loss: 0.0201, loss: 0.0355 ||:  78%|#######8  | 51/65 [09:42<02:39, 11.39s/it]
2022-04-10 02:40:59,463 - INFO - tqdm - accuracy: 0.9874, batch_loss: 0.0308, loss: 0.0354 ||:  80%|########  | 52/65 [09:54<02:28, 11.40s/it]
2022-04-10 02:41:10,894 - INFO - tqdm - accuracy: 0.9870, batch_loss: 0.0574, loss: 0.0358 ||:  82%|########1 | 53/65 [10:05<02:16, 11.41s/it]
2022-04-10 02:41:22,333 - INFO - tqdm - accuracy: 0.9873, batch_loss: 0.0089, loss: 0.0353 ||:  83%|########3 | 54/65 [10:16<02:05, 11.42s/it]
2022-04-10 02:41:33,747 - INFO - tqdm - accuracy: 0.9875, batch_loss: 0.0046, loss: 0.0348 ||:  85%|########4 | 55/65 [10:28<01:54, 11.42s/it]
2022-04-10 02:41:45,199 - INFO - tqdm - accuracy: 0.9877, batch_loss: 0.0066, loss: 0.0343 ||:  86%|########6 | 56/65 [10:39<01:42, 11.43s/it]
2022-04-10 02:41:56,614 - INFO - tqdm - accuracy: 0.9879, batch_loss: 0.0063, loss: 0.0338 ||:  88%|########7 | 57/65 [10:51<01:31, 11.42s/it]
2022-04-10 02:42:08,037 - INFO - tqdm - accuracy: 0.9881, batch_loss: 0.0094, loss: 0.0334 ||:  89%|########9 | 58/65 [11:02<01:19, 11.42s/it]
2022-04-10 02:42:19,451 - INFO - tqdm - accuracy: 0.9883, batch_loss: 0.0055, loss: 0.0329 ||:  91%|######### | 59/65 [11:14<01:08, 11.42s/it]
2022-04-10 02:42:30,837 - INFO - tqdm - accuracy: 0.9885, batch_loss: 0.0204, loss: 0.0327 ||:  92%|#########2| 60/65 [11:25<00:57, 11.41s/it]
2022-04-10 02:42:42,262 - INFO - tqdm - accuracy: 0.9887, batch_loss: 0.0077, loss: 0.0323 ||:  94%|#########3| 61/65 [11:36<00:45, 11.41s/it]
2022-04-10 02:42:53,701 - INFO - tqdm - accuracy: 0.9889, batch_loss: 0.0093, loss: 0.0319 ||:  95%|#########5| 62/65 [11:48<00:34, 11.42s/it]
2022-04-10 02:43:05,149 - INFO - tqdm - accuracy: 0.9891, batch_loss: 0.0091, loss: 0.0315 ||:  97%|#########6| 63/65 [11:59<00:22, 11.43s/it]
2022-04-10 02:43:16,593 - INFO - tqdm - accuracy: 0.9893, batch_loss: 0.0042, loss: 0.0311 ||:  98%|#########8| 64/65 [12:11<00:11, 11.43s/it]
2022-04-10 02:43:21,664 - INFO - tqdm - accuracy: 0.9893, batch_loss: 0.0349, loss: 0.0312 ||: 100%|##########| 65/65 [12:16<00:00,  9.53s/it]
2022-04-10 02:43:21,664 - INFO - tqdm - accuracy: 0.9893, batch_loss: 0.0349, loss: 0.0312 ||: 100%|##########| 65/65 [12:16<00:00, 11.33s/it]
2022-04-10 02:43:24,464 - INFO - allennlp.training.trainer - Validating
2022-04-10 02:43:24,465 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-10 02:43:34,564 - INFO - tqdm - accuracy: 0.6049, batch_loss: 3.5901, loss: 2.2864 ||:  36%|###5      | 41/115 [00:10<00:18,  3.97it/s]
2022-04-10 02:43:44,689 - INFO - tqdm - accuracy: 0.6025, batch_loss: 5.3868, loss: 2.1735 ||:  70%|#######   | 81/115 [00:20<00:08,  3.95it/s]
2022-04-10 02:43:53,292 - INFO - tqdm - accuracy: 0.6245, batch_loss: 0.1923, loss: 2.0175 ||: 100%|##########| 115/115 [00:28<00:00,  3.96it/s]
2022-04-10 02:43:53,292 - INFO - tqdm - accuracy: 0.6245, batch_loss: 0.1923, loss: 2.0175 ||: 100%|##########| 115/115 [00:28<00:00,  3.99it/s]
2022-04-10 02:43:53,292 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-10 02:43:53,293 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.989  |     0.624
2022-04-10 02:43:53,293 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-10 02:43:53,294 - INFO - allennlp.training.tensorboard_writer - loss               |     0.031  |     2.018
2022-04-10 02:43:53,295 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5813.805  |       N/A
2022-04-10 02:43:59,462 - INFO - allennlp.training.trainer - Epoch duration: 0:12:54.107863
2022-04-10 02:43:59,462 - INFO - allennlp.training.trainer - Estimated training time remaining: 1:04:41
2022-04-10 02:43:59,462 - INFO - allennlp.training.trainer - Epoch 10/14
2022-04-10 02:43:59,462 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-10 02:43:59,463 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-10 02:43:59,464 - INFO - allennlp.training.trainer - Training
2022-04-10 02:43:59,464 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-10 02:44:10,779 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.0364, loss: 0.0364 ||:   2%|1         | 1/65 [00:11<12:04, 11.31s/it]
2022-04-10 02:44:22,086 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0072, loss: 0.0218 ||:   3%|3         | 2/65 [00:22<11:52, 11.31s/it]
2022-04-10 02:44:33,746 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.0071, loss: 0.0169 ||:   5%|4         | 3/65 [00:34<11:51, 11.47s/it]
2022-04-10 02:44:45,148 - INFO - tqdm - accuracy: 0.9922, batch_loss: 0.0136, loss: 0.0161 ||:   6%|6         | 4/65 [00:45<11:38, 11.44s/it]
2022-04-10 02:44:56,395 - INFO - tqdm - accuracy: 0.9938, batch_loss: 0.0171, loss: 0.0163 ||:   8%|7         | 5/65 [00:56<11:22, 11.37s/it]
2022-04-10 02:45:07,780 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.1312, loss: 0.0354 ||:   9%|9         | 6/65 [01:08<11:11, 11.38s/it]
2022-04-10 02:45:19,193 - INFO - tqdm - accuracy: 0.9911, batch_loss: 0.0093, loss: 0.0317 ||:  11%|#         | 7/65 [01:19<11:00, 11.39s/it]
2022-04-10 02:45:30,614 - INFO - tqdm - accuracy: 0.9922, batch_loss: 0.0022, loss: 0.0280 ||:  12%|#2        | 8/65 [01:31<10:49, 11.40s/it]
2022-04-10 02:45:42,062 - INFO - tqdm - accuracy: 0.9931, batch_loss: 0.0015, loss: 0.0251 ||:  14%|#3        | 9/65 [01:42<10:39, 11.41s/it]
2022-04-10 02:45:53,524 - INFO - tqdm - accuracy: 0.9906, batch_loss: 0.0247, loss: 0.0250 ||:  15%|#5        | 10/65 [01:54<10:28, 11.43s/it]
2022-04-10 02:46:04,978 - INFO - tqdm - accuracy: 0.9858, batch_loss: 0.1608, loss: 0.0374 ||:  17%|#6        | 11/65 [02:05<10:17, 11.44s/it]
2022-04-10 02:46:16,437 - INFO - tqdm - accuracy: 0.9870, batch_loss: 0.0065, loss: 0.0348 ||:  18%|#8        | 12/65 [02:16<10:06, 11.44s/it]
2022-04-10 02:46:27,914 - INFO - tqdm - accuracy: 0.9880, batch_loss: 0.0034, loss: 0.0324 ||:  20%|##        | 13/65 [02:28<09:55, 11.45s/it]
2022-04-10 02:46:39,356 - INFO - tqdm - accuracy: 0.9888, batch_loss: 0.0181, loss: 0.0314 ||:  22%|##1       | 14/65 [02:39<09:43, 11.45s/it]
2022-04-10 02:46:50,800 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.0017, loss: 0.0294 ||:  23%|##3       | 15/65 [02:51<09:32, 11.45s/it]
2022-04-10 02:47:02,245 - INFO - tqdm - accuracy: 0.9902, batch_loss: 0.0037, loss: 0.0278 ||:  25%|##4       | 16/65 [03:02<09:20, 11.45s/it]
2022-04-10 02:47:13,682 - INFO - tqdm - accuracy: 0.9908, batch_loss: 0.0016, loss: 0.0262 ||:  26%|##6       | 17/65 [03:14<09:09, 11.44s/it]
2022-04-10 02:47:25,081 - INFO - tqdm - accuracy: 0.9913, batch_loss: 0.0299, loss: 0.0264 ||:  28%|##7       | 18/65 [03:25<08:57, 11.43s/it]
2022-04-10 02:47:36,509 - INFO - tqdm - accuracy: 0.9918, batch_loss: 0.0183, loss: 0.0260 ||:  29%|##9       | 19/65 [03:37<08:45, 11.43s/it]
2022-04-10 02:47:47,915 - INFO - tqdm - accuracy: 0.9906, batch_loss: 0.0665, loss: 0.0280 ||:  31%|###       | 20/65 [03:48<08:34, 11.42s/it]
2022-04-10 02:47:59,303 - INFO - tqdm - accuracy: 0.9911, batch_loss: 0.0114, loss: 0.0272 ||:  32%|###2      | 21/65 [03:59<08:22, 11.41s/it]
2022-04-10 02:48:10,688 - INFO - tqdm - accuracy: 0.9915, batch_loss: 0.0121, loss: 0.0266 ||:  34%|###3      | 22/65 [04:11<08:10, 11.40s/it]
2022-04-10 02:48:21,735 - INFO - tqdm - accuracy: 0.9918, batch_loss: 0.0097, loss: 0.0258 ||:  35%|###5      | 23/65 [04:22<07:54, 11.30s/it]
2022-04-10 02:48:33,086 - INFO - tqdm - accuracy: 0.9922, batch_loss: 0.0029, loss: 0.0249 ||:  37%|###6      | 24/65 [04:33<07:43, 11.31s/it]
2022-04-10 02:48:44,518 - INFO - tqdm - accuracy: 0.9925, batch_loss: 0.0027, loss: 0.0240 ||:  38%|###8      | 25/65 [04:45<07:33, 11.35s/it]
2022-04-10 02:48:55,801 - INFO - tqdm - accuracy: 0.9928, batch_loss: 0.0031, loss: 0.0232 ||:  40%|####      | 26/65 [04:56<07:21, 11.33s/it]
2022-04-10 02:49:07,239 - INFO - tqdm - accuracy: 0.9930, batch_loss: 0.0051, loss: 0.0225 ||:  42%|####1     | 27/65 [05:07<07:11, 11.36s/it]
2022-04-10 02:49:18,687 - INFO - tqdm - accuracy: 0.9933, batch_loss: 0.0316, loss: 0.0228 ||:  43%|####3     | 28/65 [05:19<07:01, 11.39s/it]
2022-04-10 02:49:30,136 - INFO - tqdm - accuracy: 0.9924, batch_loss: 0.0266, loss: 0.0230 ||:  45%|####4     | 29/65 [05:30<06:50, 11.41s/it]
2022-04-10 02:49:41,593 - INFO - tqdm - accuracy: 0.9927, batch_loss: 0.0149, loss: 0.0227 ||:  46%|####6     | 30/65 [05:42<06:39, 11.42s/it]
2022-04-10 02:49:53,037 - INFO - tqdm - accuracy: 0.9929, batch_loss: 0.0032, loss: 0.0221 ||:  48%|####7     | 31/65 [05:53<06:28, 11.43s/it]
2022-04-10 02:50:04,511 - INFO - tqdm - accuracy: 0.9932, batch_loss: 0.0094, loss: 0.0217 ||:  49%|####9     | 32/65 [06:05<06:17, 11.44s/it]
2022-04-10 02:50:15,958 - INFO - tqdm - accuracy: 0.9934, batch_loss: 0.0151, loss: 0.0215 ||:  51%|#####     | 33/65 [06:16<06:06, 11.44s/it]
2022-04-10 02:50:27,388 - INFO - tqdm - accuracy: 0.9936, batch_loss: 0.0017, loss: 0.0209 ||:  52%|#####2    | 34/65 [06:27<05:54, 11.44s/it]
2022-04-10 02:50:38,817 - INFO - tqdm - accuracy: 0.9937, batch_loss: 0.0036, loss: 0.0204 ||:  54%|#####3    | 35/65 [06:39<05:43, 11.44s/it]
2022-04-10 02:50:50,260 - INFO - tqdm - accuracy: 0.9939, batch_loss: 0.0024, loss: 0.0199 ||:  55%|#####5    | 36/65 [06:50<05:31, 11.44s/it]
2022-04-10 02:51:01,731 - INFO - tqdm - accuracy: 0.9941, batch_loss: 0.0009, loss: 0.0194 ||:  57%|#####6    | 37/65 [07:02<05:20, 11.45s/it]
2022-04-10 02:51:13,147 - INFO - tqdm - accuracy: 0.9942, batch_loss: 0.0057, loss: 0.0190 ||:  58%|#####8    | 38/65 [07:13<05:08, 11.44s/it]
2022-04-10 02:51:24,576 - INFO - tqdm - accuracy: 0.9944, batch_loss: 0.0030, loss: 0.0186 ||:  60%|######    | 39/65 [07:25<04:57, 11.44s/it]
2022-04-10 02:51:36,004 - INFO - tqdm - accuracy: 0.9945, batch_loss: 0.0156, loss: 0.0185 ||:  62%|######1   | 40/65 [07:36<04:45, 11.43s/it]
2022-04-10 02:51:47,451 - INFO - tqdm - accuracy: 0.9947, batch_loss: 0.0034, loss: 0.0182 ||:  63%|######3   | 41/65 [07:47<04:34, 11.44s/it]
2022-04-10 02:51:58,863 - INFO - tqdm - accuracy: 0.9948, batch_loss: 0.0010, loss: 0.0178 ||:  65%|######4   | 42/65 [07:59<04:22, 11.43s/it]
2022-04-10 02:52:10,263 - INFO - tqdm - accuracy: 0.9949, batch_loss: 0.0097, loss: 0.0176 ||:  66%|######6   | 43/65 [08:10<04:11, 11.42s/it]
2022-04-10 02:52:21,673 - INFO - tqdm - accuracy: 0.9950, batch_loss: 0.0021, loss: 0.0172 ||:  68%|######7   | 44/65 [08:22<03:59, 11.42s/it]
2022-04-10 02:52:33,102 - INFO - tqdm - accuracy: 0.9951, batch_loss: 0.0246, loss: 0.0174 ||:  69%|######9   | 45/65 [08:33<03:48, 11.42s/it]
2022-04-10 02:52:44,517 - INFO - tqdm - accuracy: 0.9952, batch_loss: 0.0075, loss: 0.0172 ||:  71%|#######   | 46/65 [08:45<03:36, 11.42s/it]
2022-04-10 02:52:55,945 - INFO - tqdm - accuracy: 0.9953, batch_loss: 0.0073, loss: 0.0170 ||:  72%|#######2  | 47/65 [08:56<03:25, 11.42s/it]
2022-04-10 02:53:07,399 - INFO - tqdm - accuracy: 0.9954, batch_loss: 0.0027, loss: 0.0167 ||:  74%|#######3  | 48/65 [09:07<03:14, 11.43s/it]
2022-04-10 02:53:18,825 - INFO - tqdm - accuracy: 0.9955, batch_loss: 0.0017, loss: 0.0164 ||:  75%|#######5  | 49/65 [09:19<03:02, 11.43s/it]
2022-04-10 02:53:30,700 - INFO - tqdm - accuracy: 0.9956, batch_loss: 0.0048, loss: 0.0161 ||:  77%|#######6  | 50/65 [09:31<02:53, 11.56s/it]
2022-04-10 02:53:42,099 - INFO - tqdm - accuracy: 0.9957, batch_loss: 0.0024, loss: 0.0159 ||:  78%|#######8  | 51/65 [09:42<02:41, 11.51s/it]
2022-04-10 02:53:53,549 - INFO - tqdm - accuracy: 0.9958, batch_loss: 0.0021, loss: 0.0156 ||:  80%|########  | 52/65 [09:54<02:29, 11.49s/it]
2022-04-10 02:54:05,015 - INFO - tqdm - accuracy: 0.9959, batch_loss: 0.0070, loss: 0.0154 ||:  82%|########1 | 53/65 [10:05<02:17, 11.49s/it]
2022-04-10 02:54:16,478 - INFO - tqdm - accuracy: 0.9959, batch_loss: 0.0222, loss: 0.0156 ||:  83%|########3 | 54/65 [10:17<02:06, 11.48s/it]
2022-04-10 02:54:27,922 - INFO - tqdm - accuracy: 0.9955, batch_loss: 0.1057, loss: 0.0172 ||:  85%|########4 | 55/65 [10:28<01:54, 11.47s/it]
2022-04-10 02:54:39,364 - INFO - tqdm - accuracy: 0.9955, batch_loss: 0.0135, loss: 0.0171 ||:  86%|########6 | 56/65 [10:39<01:43, 11.46s/it]
2022-04-10 02:54:50,809 - INFO - tqdm - accuracy: 0.9956, batch_loss: 0.0028, loss: 0.0169 ||:  88%|########7 | 57/65 [10:51<01:31, 11.46s/it]
2022-04-10 02:55:02,256 - INFO - tqdm - accuracy: 0.9951, batch_loss: 0.1158, loss: 0.0186 ||:  89%|########9 | 58/65 [11:02<01:20, 11.45s/it]
2022-04-10 02:55:13,696 - INFO - tqdm - accuracy: 0.9952, batch_loss: 0.0122, loss: 0.0185 ||:  91%|######### | 59/65 [11:14<01:08, 11.45s/it]
2022-04-10 02:55:25,140 - INFO - tqdm - accuracy: 0.9953, batch_loss: 0.0066, loss: 0.0183 ||:  92%|#########2| 60/65 [11:25<00:57, 11.45s/it]
2022-04-10 02:55:36,576 - INFO - tqdm - accuracy: 0.9954, batch_loss: 0.0025, loss: 0.0180 ||:  94%|#########3| 61/65 [11:37<00:45, 11.44s/it]
2022-04-10 02:55:47,998 - INFO - tqdm - accuracy: 0.9955, batch_loss: 0.0026, loss: 0.0178 ||:  95%|#########5| 62/65 [11:48<00:34, 11.44s/it]
2022-04-10 02:55:59,410 - INFO - tqdm - accuracy: 0.9955, batch_loss: 0.0011, loss: 0.0175 ||:  97%|#########6| 63/65 [11:59<00:22, 11.43s/it]
2022-04-10 02:56:10,835 - INFO - tqdm - accuracy: 0.9956, batch_loss: 0.0211, loss: 0.0176 ||:  98%|#########8| 64/65 [12:11<00:11, 11.43s/it]
2022-04-10 02:56:15,899 - INFO - tqdm - accuracy: 0.9956, batch_loss: 0.0011, loss: 0.0173 ||: 100%|##########| 65/65 [12:16<00:00,  9.52s/it]
2022-04-10 02:56:15,900 - INFO - tqdm - accuracy: 0.9956, batch_loss: 0.0011, loss: 0.0173 ||: 100%|##########| 65/65 [12:16<00:00, 11.33s/it]
2022-04-10 02:56:18,672 - INFO - allennlp.training.trainer - Validating
2022-04-10 02:56:18,674 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-10 02:56:28,699 - INFO - tqdm - accuracy: 0.7125, batch_loss: 0.0001, loss: 1.6440 ||:  35%|###4      | 40/115 [00:10<00:18,  4.00it/s]
2022-04-10 02:56:38,860 - INFO - tqdm - accuracy: 0.5963, batch_loss: 3.1998, loss: 2.2527 ||:  70%|#######   | 81/115 [00:20<00:08,  3.96it/s]
2022-04-10 02:56:47,503 - INFO - tqdm - accuracy: 0.6026, batch_loss: 4.3130, loss: 2.2918 ||: 100%|##########| 115/115 [00:28<00:00,  3.96it/s]
2022-04-10 02:56:47,503 - INFO - tqdm - accuracy: 0.6026, batch_loss: 4.3130, loss: 2.2918 ||: 100%|##########| 115/115 [00:28<00:00,  3.99it/s]
2022-04-10 02:56:47,504 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-10 02:56:47,504 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.996  |     0.603
2022-04-10 02:56:47,505 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-10 02:56:47,506 - INFO - allennlp.training.tensorboard_writer - loss               |     0.017  |     2.292
2022-04-10 02:56:47,506 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5813.805  |       N/A
2022-04-10 02:56:53,457 - INFO - allennlp.training.trainer - Epoch duration: 0:12:53.994840
2022-04-10 02:56:53,457 - INFO - allennlp.training.trainer - Estimated training time remaining: 0:51:44
2022-04-10 02:56:53,457 - INFO - allennlp.training.trainer - Epoch 11/14
2022-04-10 02:56:53,457 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-10 02:56:53,458 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-10 02:56:53,459 - INFO - allennlp.training.trainer - Training
2022-04-10 02:56:53,459 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-10 02:57:04,747 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0008, loss: 0.0008 ||:   2%|1         | 1/65 [00:11<12:02, 11.29s/it]
2022-04-10 02:57:16,276 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0069, loss: 0.0039 ||:   3%|3         | 2/65 [00:22<12:00, 11.43s/it]
2022-04-10 02:57:27,900 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.0485, loss: 0.0188 ||:   5%|4         | 3/65 [00:34<11:54, 11.52s/it]
2022-04-10 02:57:39,324 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.2149, loss: 0.0678 ||:   6%|6         | 4/65 [00:45<11:40, 11.48s/it]
2022-04-10 02:57:50,697 - INFO - tqdm - accuracy: 0.9875, batch_loss: 0.0093, loss: 0.0561 ||:   8%|7         | 5/65 [00:57<11:26, 11.44s/it]
2022-04-10 02:58:02,111 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.1193, loss: 0.0666 ||:   9%|9         | 6/65 [01:08<11:14, 11.43s/it]
2022-04-10 02:58:13,496 - INFO - tqdm - accuracy: 0.9866, batch_loss: 0.0038, loss: 0.0577 ||:  11%|#         | 7/65 [01:20<11:02, 11.42s/it]
2022-04-10 02:58:24,888 - INFO - tqdm - accuracy: 0.9883, batch_loss: 0.0013, loss: 0.0506 ||:  12%|#2        | 8/65 [01:31<10:50, 11.41s/it]
2022-04-10 02:58:36,295 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.0010, loss: 0.0451 ||:  14%|#3        | 9/65 [01:42<10:38, 11.41s/it]
2022-04-10 02:58:47,710 - INFO - tqdm - accuracy: 0.9906, batch_loss: 0.0022, loss: 0.0408 ||:  15%|#5        | 10/65 [01:54<10:27, 11.41s/it]
2022-04-10 02:58:59,120 - INFO - tqdm - accuracy: 0.9915, batch_loss: 0.0080, loss: 0.0378 ||:  17%|#6        | 11/65 [02:05<10:16, 11.41s/it]
2022-04-10 02:59:10,564 - INFO - tqdm - accuracy: 0.9922, batch_loss: 0.0122, loss: 0.0357 ||:  18%|#8        | 12/65 [02:17<10:05, 11.42s/it]
2022-04-10 02:59:22,009 - INFO - tqdm - accuracy: 0.9928, batch_loss: 0.0292, loss: 0.0352 ||:  20%|##        | 13/65 [02:28<09:54, 11.43s/it]
2022-04-10 02:59:33,454 - INFO - tqdm - accuracy: 0.9933, batch_loss: 0.0010, loss: 0.0328 ||:  22%|##1       | 14/65 [02:39<09:43, 11.43s/it]
2022-04-10 02:59:44,912 - INFO - tqdm - accuracy: 0.9938, batch_loss: 0.0014, loss: 0.0307 ||:  23%|##3       | 15/65 [02:51<09:32, 11.44s/it]
2022-04-10 02:59:56,245 - INFO - tqdm - accuracy: 0.9941, batch_loss: 0.0009, loss: 0.0288 ||:  25%|##4       | 16/65 [03:02<09:19, 11.41s/it]
2022-04-10 03:00:07,709 - INFO - tqdm - accuracy: 0.9945, batch_loss: 0.0010, loss: 0.0272 ||:  26%|##6       | 17/65 [03:14<09:08, 11.42s/it]
2022-04-10 03:00:19,179 - INFO - tqdm - accuracy: 0.9931, batch_loss: 0.0311, loss: 0.0274 ||:  28%|##7       | 18/65 [03:25<08:57, 11.44s/it]
2022-04-10 03:00:30,652 - INFO - tqdm - accuracy: 0.9934, batch_loss: 0.0018, loss: 0.0260 ||:  29%|##9       | 19/65 [03:37<08:46, 11.45s/it]
2022-04-10 03:00:42,104 - INFO - tqdm - accuracy: 0.9938, batch_loss: 0.0018, loss: 0.0248 ||:  31%|###       | 20/65 [03:48<08:35, 11.45s/it]
2022-04-10 03:00:53,567 - INFO - tqdm - accuracy: 0.9940, batch_loss: 0.0039, loss: 0.0238 ||:  32%|###2      | 21/65 [04:00<08:23, 11.45s/it]
2022-04-10 03:01:05,012 - INFO - tqdm - accuracy: 0.9943, batch_loss: 0.0015, loss: 0.0228 ||:  34%|###3      | 22/65 [04:11<08:12, 11.45s/it]
2022-04-10 03:01:16,461 - INFO - tqdm - accuracy: 0.9946, batch_loss: 0.0018, loss: 0.0219 ||:  35%|###5      | 23/65 [04:23<08:00, 11.45s/it]
2022-04-10 03:01:27,902 - INFO - tqdm - accuracy: 0.9935, batch_loss: 0.0495, loss: 0.0231 ||:  37%|###6      | 24/65 [04:34<07:49, 11.45s/it]
2022-04-10 03:01:39,340 - INFO - tqdm - accuracy: 0.9938, batch_loss: 0.0057, loss: 0.0224 ||:  38%|###8      | 25/65 [04:45<07:37, 11.44s/it]
2022-04-10 03:01:50,779 - INFO - tqdm - accuracy: 0.9928, batch_loss: 0.0406, loss: 0.0231 ||:  40%|####      | 26/65 [04:57<07:26, 11.44s/it]
2022-04-10 03:02:02,237 - INFO - tqdm - accuracy: 0.9931, batch_loss: 0.0371, loss: 0.0236 ||:  42%|####1     | 27/65 [05:08<07:14, 11.45s/it]
2022-04-10 03:02:13,651 - INFO - tqdm - accuracy: 0.9933, batch_loss: 0.0047, loss: 0.0229 ||:  43%|####3     | 28/65 [05:20<07:03, 11.44s/it]
2022-04-10 03:02:25,089 - INFO - tqdm - accuracy: 0.9935, batch_loss: 0.0081, loss: 0.0224 ||:  45%|####4     | 29/65 [05:31<06:51, 11.44s/it]
2022-04-10 03:02:36,517 - INFO - tqdm - accuracy: 0.9938, batch_loss: 0.0019, loss: 0.0217 ||:  46%|####6     | 30/65 [05:43<06:40, 11.43s/it]
2022-04-10 03:02:47,921 - INFO - tqdm - accuracy: 0.9940, batch_loss: 0.0008, loss: 0.0210 ||:  48%|####7     | 31/65 [05:54<06:28, 11.43s/it]
2022-04-10 03:02:59,327 - INFO - tqdm - accuracy: 0.9941, batch_loss: 0.0045, loss: 0.0205 ||:  49%|####9     | 32/65 [06:05<06:16, 11.42s/it]
2022-04-10 03:03:10,764 - INFO - tqdm - accuracy: 0.9943, batch_loss: 0.0035, loss: 0.0200 ||:  51%|#####     | 33/65 [06:17<06:05, 11.42s/it]
2022-04-10 03:03:22,175 - INFO - tqdm - accuracy: 0.9945, batch_loss: 0.0071, loss: 0.0196 ||:  52%|#####2    | 34/65 [06:28<05:54, 11.42s/it]
2022-04-10 03:03:33,596 - INFO - tqdm - accuracy: 0.9938, batch_loss: 0.0496, loss: 0.0205 ||:  54%|#####3    | 35/65 [06:40<05:42, 11.42s/it]
2022-04-10 03:03:45,018 - INFO - tqdm - accuracy: 0.9939, batch_loss: 0.0022, loss: 0.0200 ||:  55%|#####5    | 36/65 [06:51<05:31, 11.42s/it]
2022-04-10 03:03:56,477 - INFO - tqdm - accuracy: 0.9941, batch_loss: 0.0024, loss: 0.0195 ||:  57%|#####6    | 37/65 [07:03<05:20, 11.43s/it]
2022-04-10 03:04:07,931 - INFO - tqdm - accuracy: 0.9942, batch_loss: 0.0051, loss: 0.0191 ||:  58%|#####8    | 38/65 [07:14<05:08, 11.44s/it]
2022-04-10 03:04:19,366 - INFO - tqdm - accuracy: 0.9944, batch_loss: 0.0069, loss: 0.0188 ||:  60%|######    | 39/65 [07:25<04:57, 11.44s/it]
2022-04-10 03:04:30,829 - INFO - tqdm - accuracy: 0.9945, batch_loss: 0.0028, loss: 0.0184 ||:  62%|######1   | 40/65 [07:37<04:46, 11.45s/it]
2022-04-10 03:04:42,274 - INFO - tqdm - accuracy: 0.9947, batch_loss: 0.0054, loss: 0.0181 ||:  63%|######3   | 41/65 [07:48<04:34, 11.45s/it]
2022-04-10 03:04:53,721 - INFO - tqdm - accuracy: 0.9948, batch_loss: 0.0010, loss: 0.0177 ||:  65%|######4   | 42/65 [08:00<04:23, 11.45s/it]
2022-04-10 03:05:05,023 - INFO - tqdm - accuracy: 0.9949, batch_loss: 0.0009, loss: 0.0173 ||:  66%|######6   | 43/65 [08:11<04:10, 11.40s/it]
2022-04-10 03:05:16,448 - INFO - tqdm - accuracy: 0.9943, batch_loss: 0.0366, loss: 0.0177 ||:  68%|######7   | 44/65 [08:22<03:59, 11.41s/it]
2022-04-10 03:05:27,523 - INFO - tqdm - accuracy: 0.9944, batch_loss: 0.0014, loss: 0.0174 ||:  69%|######9   | 45/65 [08:34<03:46, 11.31s/it]
2022-04-10 03:05:38,944 - INFO - tqdm - accuracy: 0.9946, batch_loss: 0.0046, loss: 0.0171 ||:  71%|#######   | 46/65 [08:45<03:35, 11.34s/it]
2022-04-10 03:05:50,376 - INFO - tqdm - accuracy: 0.9947, batch_loss: 0.0136, loss: 0.0170 ||:  72%|#######2  | 47/65 [08:56<03:24, 11.37s/it]
2022-04-10 03:06:01,814 - INFO - tqdm - accuracy: 0.9948, batch_loss: 0.0079, loss: 0.0168 ||:  74%|#######3  | 48/65 [09:08<03:13, 11.39s/it]
2022-04-10 03:06:13,216 - INFO - tqdm - accuracy: 0.9943, batch_loss: 0.0338, loss: 0.0172 ||:  75%|#######5  | 49/65 [09:19<03:02, 11.39s/it]
2022-04-10 03:06:24,626 - INFO - tqdm - accuracy: 0.9944, batch_loss: 0.0007, loss: 0.0168 ||:  77%|#######6  | 50/65 [09:31<02:50, 11.40s/it]
2022-04-10 03:06:36,039 - INFO - tqdm - accuracy: 0.9939, batch_loss: 0.0915, loss: 0.0183 ||:  78%|#######8  | 51/65 [09:42<02:39, 11.40s/it]
2022-04-10 03:06:47,433 - INFO - tqdm - accuracy: 0.9940, batch_loss: 0.0030, loss: 0.0180 ||:  80%|########  | 52/65 [09:53<02:28, 11.40s/it]
2022-04-10 03:06:58,844 - INFO - tqdm - accuracy: 0.9941, batch_loss: 0.0039, loss: 0.0177 ||:  82%|########1 | 53/65 [10:05<02:16, 11.40s/it]
2022-04-10 03:07:10,243 - INFO - tqdm - accuracy: 0.9942, batch_loss: 0.0018, loss: 0.0175 ||:  83%|########3 | 54/65 [10:16<02:05, 11.40s/it]
2022-04-10 03:07:21,613 - INFO - tqdm - accuracy: 0.9943, batch_loss: 0.0024, loss: 0.0172 ||:  85%|########4 | 55/65 [10:28<01:53, 11.39s/it]
2022-04-10 03:07:33,000 - INFO - tqdm - accuracy: 0.9944, batch_loss: 0.0041, loss: 0.0169 ||:  86%|########6 | 56/65 [10:39<01:42, 11.39s/it]
2022-04-10 03:07:44,138 - INFO - tqdm - accuracy: 0.9945, batch_loss: 0.0010, loss: 0.0167 ||:  88%|########7 | 57/65 [10:50<01:30, 11.31s/it]
2022-04-10 03:07:55,534 - INFO - tqdm - accuracy: 0.9941, batch_loss: 0.0352, loss: 0.0170 ||:  89%|########9 | 58/65 [11:02<01:19, 11.34s/it]
2022-04-10 03:08:06,929 - INFO - tqdm - accuracy: 0.9931, batch_loss: 0.1419, loss: 0.0191 ||:  91%|######### | 59/65 [11:13<01:08, 11.36s/it]
2022-04-10 03:08:18,322 - INFO - tqdm - accuracy: 0.9932, batch_loss: 0.0225, loss: 0.0192 ||:  92%|#########2| 60/65 [11:24<00:56, 11.37s/it]
2022-04-10 03:08:29,729 - INFO - tqdm - accuracy: 0.9933, batch_loss: 0.0012, loss: 0.0189 ||:  94%|#########3| 61/65 [11:36<00:45, 11.38s/it]
2022-04-10 03:08:41,148 - INFO - tqdm - accuracy: 0.9934, batch_loss: 0.0008, loss: 0.0186 ||:  95%|#########5| 62/65 [11:47<00:34, 11.39s/it]
2022-04-10 03:08:52,578 - INFO - tqdm - accuracy: 0.9935, batch_loss: 0.0010, loss: 0.0183 ||:  97%|#########6| 63/65 [11:59<00:22, 11.40s/it]
2022-04-10 03:09:04,004 - INFO - tqdm - accuracy: 0.9936, batch_loss: 0.0017, loss: 0.0180 ||:  98%|#########8| 64/65 [12:10<00:11, 11.41s/it]
2022-04-10 03:09:09,080 - INFO - tqdm - accuracy: 0.9937, batch_loss: 0.0097, loss: 0.0179 ||: 100%|##########| 65/65 [12:15<00:00,  9.51s/it]
2022-04-10 03:09:09,081 - INFO - tqdm - accuracy: 0.9937, batch_loss: 0.0097, loss: 0.0179 ||: 100%|##########| 65/65 [12:15<00:00, 11.32s/it]
2022-04-10 03:09:11,880 - INFO - allennlp.training.trainer - Validating
2022-04-10 03:09:11,882 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-10 03:09:21,945 - INFO - tqdm - accuracy: 0.5625, batch_loss: 0.1872, loss: 2.7675 ||:  35%|###4      | 40/115 [00:10<00:18,  3.98it/s]
2022-04-10 03:09:31,961 - INFO - tqdm - accuracy: 0.6038, batch_loss: 3.4455, loss: 2.3761 ||:  70%|######9   | 80/115 [00:20<00:07,  4.60it/s]
2022-04-10 03:09:40,765 - INFO - tqdm - accuracy: 0.6201, batch_loss: 3.1683, loss: 2.2103 ||: 100%|##########| 115/115 [00:28<00:00,  3.95it/s]
2022-04-10 03:09:40,765 - INFO - tqdm - accuracy: 0.6201, batch_loss: 3.1683, loss: 2.2103 ||: 100%|##########| 115/115 [00:28<00:00,  3.98it/s]
2022-04-10 03:09:40,765 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-10 03:09:40,766 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.994  |     0.620
2022-04-10 03:09:40,767 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-10 03:09:40,767 - INFO - allennlp.training.tensorboard_writer - loss               |     0.018  |     2.210
2022-04-10 03:09:40,768 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5813.805  |       N/A
2022-04-10 03:09:46,890 - INFO - allennlp.training.trainer - Epoch duration: 0:12:53.432216
2022-04-10 03:09:46,890 - INFO - allennlp.training.trainer - Estimated training time remaining: 0:38:47
2022-04-10 03:09:46,890 - INFO - allennlp.training.trainer - Epoch 12/14
2022-04-10 03:09:46,890 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-10 03:09:46,891 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-10 03:09:46,892 - INFO - allennlp.training.trainer - Training
2022-04-10 03:09:46,893 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-10 03:09:58,159 - INFO - tqdm - accuracy: 0.9688, batch_loss: 0.0971, loss: 0.0971 ||:   2%|1         | 1/65 [00:11<12:01, 11.27s/it]
2022-04-10 03:10:09,647 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0116, loss: 0.0543 ||:   3%|3         | 2/65 [00:22<11:57, 11.40s/it]
2022-04-10 03:10:21,211 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.0012, loss: 0.0366 ||:   5%|4         | 3/65 [00:34<11:51, 11.47s/it]
2022-04-10 03:10:32,663 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0535, loss: 0.0408 ||:   6%|6         | 4/65 [00:45<11:39, 11.46s/it]
2022-04-10 03:10:44,064 - INFO - tqdm - accuracy: 0.9812, batch_loss: 0.0454, loss: 0.0418 ||:   8%|7         | 5/65 [00:57<11:26, 11.44s/it]
2022-04-10 03:10:55,370 - INFO - tqdm - accuracy: 0.9844, batch_loss: 0.0024, loss: 0.0352 ||:   9%|9         | 6/65 [01:08<11:12, 11.40s/it]
2022-04-10 03:11:06,734 - INFO - tqdm - accuracy: 0.9866, batch_loss: 0.0030, loss: 0.0306 ||:  11%|#         | 7/65 [01:19<11:00, 11.39s/it]
2022-04-10 03:11:18,181 - INFO - tqdm - accuracy: 0.9883, batch_loss: 0.0006, loss: 0.0269 ||:  12%|#2        | 8/65 [01:31<10:50, 11.40s/it]
2022-04-10 03:11:29,700 - INFO - tqdm - accuracy: 0.9896, batch_loss: 0.0017, loss: 0.0241 ||:  14%|#3        | 9/65 [01:42<10:40, 11.44s/it]
2022-04-10 03:11:41,153 - INFO - tqdm - accuracy: 0.9906, batch_loss: 0.0005, loss: 0.0217 ||:  15%|#5        | 10/65 [01:54<10:29, 11.44s/it]
2022-04-10 03:11:52,506 - INFO - tqdm - accuracy: 0.9915, batch_loss: 0.0056, loss: 0.0202 ||:  17%|#6        | 11/65 [02:05<10:16, 11.42s/it]
2022-04-10 03:12:03,885 - INFO - tqdm - accuracy: 0.9922, batch_loss: 0.0006, loss: 0.0186 ||:  18%|#8        | 12/65 [02:16<10:04, 11.40s/it]
2022-04-10 03:12:14,932 - INFO - tqdm - accuracy: 0.9928, batch_loss: 0.0017, loss: 0.0173 ||:  20%|##        | 13/65 [02:28<09:47, 11.30s/it]
2022-04-10 03:12:26,362 - INFO - tqdm - accuracy: 0.9933, batch_loss: 0.0036, loss: 0.0163 ||:  22%|##1       | 14/65 [02:39<09:38, 11.34s/it]
2022-04-10 03:12:37,815 - INFO - tqdm - accuracy: 0.9937, batch_loss: 0.0011, loss: 0.0153 ||:  23%|##3       | 15/65 [02:50<09:28, 11.37s/it]
2022-04-10 03:12:49,276 - INFO - tqdm - accuracy: 0.9941, batch_loss: 0.0093, loss: 0.0149 ||:  25%|##4       | 16/65 [03:02<09:18, 11.40s/it]
2022-04-10 03:13:00,733 - INFO - tqdm - accuracy: 0.9945, batch_loss: 0.0053, loss: 0.0144 ||:  26%|##6       | 17/65 [03:13<09:07, 11.42s/it]
2022-04-10 03:13:12,191 - INFO - tqdm - accuracy: 0.9930, batch_loss: 0.0311, loss: 0.0153 ||:  28%|##7       | 18/65 [03:25<08:57, 11.43s/it]
2022-04-10 03:13:23,637 - INFO - tqdm - accuracy: 0.9918, batch_loss: 0.0741, loss: 0.0184 ||:  29%|##9       | 19/65 [03:36<08:45, 11.43s/it]
2022-04-10 03:13:35,493 - INFO - tqdm - accuracy: 0.9922, batch_loss: 0.0017, loss: 0.0176 ||:  31%|###       | 20/65 [03:48<08:40, 11.56s/it]
2022-04-10 03:13:46,889 - INFO - tqdm - accuracy: 0.9925, batch_loss: 0.0027, loss: 0.0169 ||:  32%|###2      | 21/65 [03:59<08:26, 11.51s/it]
2022-04-10 03:13:58,323 - INFO - tqdm - accuracy: 0.9929, batch_loss: 0.0009, loss: 0.0161 ||:  34%|###3      | 22/65 [04:11<08:13, 11.49s/it]
2022-04-10 03:14:09,782 - INFO - tqdm - accuracy: 0.9932, batch_loss: 0.0007, loss: 0.0155 ||:  35%|###5      | 23/65 [04:22<08:02, 11.48s/it]
2022-04-10 03:14:21,241 - INFO - tqdm - accuracy: 0.9935, batch_loss: 0.0044, loss: 0.0150 ||:  37%|###6      | 24/65 [04:34<07:50, 11.47s/it]
2022-04-10 03:14:32,445 - INFO - tqdm - accuracy: 0.9925, batch_loss: 0.2041, loss: 0.0226 ||:  38%|###8      | 25/65 [04:45<07:35, 11.39s/it]
2022-04-10 03:14:43,927 - INFO - tqdm - accuracy: 0.9928, batch_loss: 0.0012, loss: 0.0217 ||:  40%|####      | 26/65 [04:57<07:25, 11.42s/it]
2022-04-10 03:14:55,373 - INFO - tqdm - accuracy: 0.9930, batch_loss: 0.0028, loss: 0.0210 ||:  42%|####1     | 27/65 [05:08<07:14, 11.43s/it]
2022-04-10 03:15:06,819 - INFO - tqdm - accuracy: 0.9933, batch_loss: 0.0030, loss: 0.0204 ||:  43%|####3     | 28/65 [05:19<07:03, 11.43s/it]
2022-04-10 03:15:18,264 - INFO - tqdm - accuracy: 0.9935, batch_loss: 0.0147, loss: 0.0202 ||:  45%|####4     | 29/65 [05:31<06:51, 11.44s/it]
2022-04-10 03:15:29,709 - INFO - tqdm - accuracy: 0.9937, batch_loss: 0.0034, loss: 0.0196 ||:  46%|####6     | 30/65 [05:42<06:40, 11.44s/it]
2022-04-10 03:15:41,147 - INFO - tqdm - accuracy: 0.9939, batch_loss: 0.0006, loss: 0.0190 ||:  48%|####7     | 31/65 [05:54<06:28, 11.44s/it]
2022-04-10 03:15:52,465 - INFO - tqdm - accuracy: 0.9941, batch_loss: 0.0009, loss: 0.0185 ||:  49%|####9     | 32/65 [06:05<06:16, 11.40s/it]
2022-04-10 03:16:03,876 - INFO - tqdm - accuracy: 0.9943, batch_loss: 0.0029, loss: 0.0180 ||:  51%|#####     | 33/65 [06:16<06:04, 11.41s/it]
2022-04-10 03:16:15,312 - INFO - tqdm - accuracy: 0.9945, batch_loss: 0.0011, loss: 0.0175 ||:  52%|#####2    | 34/65 [06:28<05:53, 11.41s/it]
2022-04-10 03:16:26,747 - INFO - tqdm - accuracy: 0.9946, batch_loss: 0.0106, loss: 0.0173 ||:  54%|#####3    | 35/65 [06:39<05:42, 11.42s/it]
2022-04-10 03:16:38,149 - INFO - tqdm - accuracy: 0.9948, batch_loss: 0.0040, loss: 0.0169 ||:  55%|#####5    | 36/65 [06:51<05:31, 11.42s/it]
2022-04-10 03:16:49,555 - INFO - tqdm - accuracy: 0.9941, batch_loss: 0.0598, loss: 0.0181 ||:  57%|#####6    | 37/65 [07:02<05:19, 11.41s/it]
2022-04-10 03:17:00,962 - INFO - tqdm - accuracy: 0.9942, batch_loss: 0.0034, loss: 0.0177 ||:  58%|#####8    | 38/65 [07:14<05:08, 11.41s/it]
2022-04-10 03:17:12,379 - INFO - tqdm - accuracy: 0.9936, batch_loss: 0.0344, loss: 0.0181 ||:  60%|######    | 39/65 [07:25<04:56, 11.41s/it]
2022-04-10 03:17:23,775 - INFO - tqdm - accuracy: 0.9930, batch_loss: 0.0359, loss: 0.0186 ||:  62%|######1   | 40/65 [07:36<04:45, 11.41s/it]
2022-04-10 03:17:35,202 - INFO - tqdm - accuracy: 0.9931, batch_loss: 0.0010, loss: 0.0181 ||:  63%|######3   | 41/65 [07:48<04:33, 11.41s/it]
2022-04-10 03:17:46,587 - INFO - tqdm - accuracy: 0.9933, batch_loss: 0.0050, loss: 0.0178 ||:  65%|######4   | 42/65 [07:59<04:22, 11.40s/it]
2022-04-10 03:17:57,991 - INFO - tqdm - accuracy: 0.9935, batch_loss: 0.0007, loss: 0.0174 ||:  66%|######6   | 43/65 [08:11<04:10, 11.40s/it]
2022-04-10 03:18:09,376 - INFO - tqdm - accuracy: 0.9936, batch_loss: 0.0006, loss: 0.0170 ||:  68%|######7   | 44/65 [08:22<03:59, 11.40s/it]
2022-04-10 03:18:20,796 - INFO - tqdm - accuracy: 0.9937, batch_loss: 0.0010, loss: 0.0167 ||:  69%|######9   | 45/65 [08:33<03:48, 11.41s/it]
2022-04-10 03:18:32,190 - INFO - tqdm - accuracy: 0.9939, batch_loss: 0.0012, loss: 0.0163 ||:  71%|#######   | 46/65 [08:45<03:36, 11.40s/it]
2022-04-10 03:18:43,583 - INFO - tqdm - accuracy: 0.9940, batch_loss: 0.0011, loss: 0.0160 ||:  72%|#######2  | 47/65 [08:56<03:25, 11.40s/it]
2022-04-10 03:18:54,966 - INFO - tqdm - accuracy: 0.9941, batch_loss: 0.0034, loss: 0.0158 ||:  74%|#######3  | 48/65 [09:08<03:13, 11.39s/it]
2022-04-10 03:19:06,357 - INFO - tqdm - accuracy: 0.9943, batch_loss: 0.0007, loss: 0.0155 ||:  75%|#######5  | 49/65 [09:19<03:02, 11.39s/it]
2022-04-10 03:19:17,770 - INFO - tqdm - accuracy: 0.9944, batch_loss: 0.0008, loss: 0.0152 ||:  77%|#######6  | 50/65 [09:30<02:50, 11.40s/it]
2022-04-10 03:19:29,181 - INFO - tqdm - accuracy: 0.9945, batch_loss: 0.0011, loss: 0.0149 ||:  78%|#######8  | 51/65 [09:42<02:39, 11.40s/it]
2022-04-10 03:19:40,617 - INFO - tqdm - accuracy: 0.9946, batch_loss: 0.0114, loss: 0.0148 ||:  80%|########  | 52/65 [09:53<02:28, 11.41s/it]
2022-04-10 03:19:52,064 - INFO - tqdm - accuracy: 0.9947, batch_loss: 0.0008, loss: 0.0146 ||:  82%|########1 | 53/65 [10:05<02:17, 11.42s/it]
2022-04-10 03:20:03,504 - INFO - tqdm - accuracy: 0.9948, batch_loss: 0.0029, loss: 0.0143 ||:  83%|########3 | 54/65 [10:16<02:05, 11.43s/it]
2022-04-10 03:20:14,951 - INFO - tqdm - accuracy: 0.9949, batch_loss: 0.0009, loss: 0.0141 ||:  85%|########4 | 55/65 [10:28<01:54, 11.43s/it]
2022-04-10 03:20:26,401 - INFO - tqdm - accuracy: 0.9950, batch_loss: 0.0011, loss: 0.0139 ||:  86%|########6 | 56/65 [10:39<01:42, 11.44s/it]
2022-04-10 03:20:37,861 - INFO - tqdm - accuracy: 0.9951, batch_loss: 0.0031, loss: 0.0137 ||:  88%|########7 | 57/65 [10:50<01:31, 11.45s/it]
2022-04-10 03:20:49,327 - INFO - tqdm - accuracy: 0.9951, batch_loss: 0.0018, loss: 0.0135 ||:  89%|########9 | 58/65 [11:02<01:20, 11.45s/it]
2022-04-10 03:21:00,796 - INFO - tqdm - accuracy: 0.9952, batch_loss: 0.0043, loss: 0.0133 ||:  91%|######### | 59/65 [11:13<01:08, 11.46s/it]
2022-04-10 03:21:12,273 - INFO - tqdm - accuracy: 0.9953, batch_loss: 0.0027, loss: 0.0131 ||:  92%|#########2| 60/65 [11:25<00:57, 11.46s/it]
2022-04-10 03:21:23,700 - INFO - tqdm - accuracy: 0.9954, batch_loss: 0.0006, loss: 0.0129 ||:  94%|#########3| 61/65 [11:36<00:45, 11.45s/it]
2022-04-10 03:21:35,119 - INFO - tqdm - accuracy: 0.9955, batch_loss: 0.0013, loss: 0.0127 ||:  95%|#########5| 62/65 [11:48<00:34, 11.44s/it]
2022-04-10 03:21:46,496 - INFO - tqdm - accuracy: 0.9955, batch_loss: 0.0042, loss: 0.0126 ||:  97%|#########6| 63/65 [11:59<00:22, 11.42s/it]
2022-04-10 03:21:57,752 - INFO - tqdm - accuracy: 0.9956, batch_loss: 0.0025, loss: 0.0124 ||:  98%|#########8| 64/65 [12:10<00:11, 11.37s/it]
2022-04-10 03:22:02,805 - INFO - tqdm - accuracy: 0.9956, batch_loss: 0.0008, loss: 0.0123 ||: 100%|##########| 65/65 [12:15<00:00,  9.48s/it]
2022-04-10 03:22:02,807 - INFO - tqdm - accuracy: 0.9956, batch_loss: 0.0008, loss: 0.0123 ||: 100%|##########| 65/65 [12:15<00:00, 11.32s/it]
2022-04-10 03:22:05,625 - INFO - allennlp.training.trainer - Validating
2022-04-10 03:22:05,626 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-10 03:22:15,685 - INFO - tqdm - accuracy: 0.6914, batch_loss: 4.7369, loss: 1.7115 ||:  36%|###5      | 41/115 [00:10<00:18,  4.02it/s]
2022-04-10 03:22:25,817 - INFO - tqdm - accuracy: 0.6646, batch_loss: 0.0832, loss: 1.8366 ||:  70%|#######   | 81/115 [00:20<00:08,  3.92it/s]
2022-04-10 03:22:34,535 - INFO - tqdm - accuracy: 0.6376, batch_loss: 0.0150, loss: 2.0587 ||: 100%|##########| 115/115 [00:28<00:00,  3.90it/s]
2022-04-10 03:22:34,535 - INFO - tqdm - accuracy: 0.6376, batch_loss: 0.0150, loss: 2.0587 ||: 100%|##########| 115/115 [00:28<00:00,  3.98it/s]
2022-04-10 03:22:34,536 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-10 03:22:34,536 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.996  |     0.638
2022-04-10 03:22:34,537 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-10 03:22:34,538 - INFO - allennlp.training.tensorboard_writer - loss               |     0.012  |     2.059
2022-04-10 03:22:34,538 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5813.805  |       N/A
2022-04-10 03:22:40,497 - INFO - allennlp.training.trainer - Epoch duration: 0:12:53.606894
2022-04-10 03:22:40,497 - INFO - allennlp.training.trainer - Estimated training time remaining: 0:25:51
2022-04-10 03:22:40,498 - INFO - allennlp.training.trainer - Epoch 13/14
2022-04-10 03:22:40,498 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-10 03:22:40,498 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-10 03:22:40,500 - INFO - allennlp.training.trainer - Training
2022-04-10 03:22:40,500 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-10 03:22:51,732 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0009, loss: 0.0009 ||:   2%|1         | 1/65 [00:11<11:58, 11.23s/it]
2022-04-10 03:23:03,176 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0039, loss: 0.0024 ||:   3%|3         | 2/65 [00:22<11:55, 11.36s/it]
2022-04-10 03:23:14,846 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0006, loss: 0.0018 ||:   5%|4         | 3/65 [00:34<11:52, 11.50s/it]
2022-04-10 03:23:26,340 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0027, loss: 0.0020 ||:   6%|6         | 4/65 [00:45<11:41, 11.50s/it]
2022-04-10 03:23:37,767 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0021, loss: 0.0020 ||:   8%|7         | 5/65 [00:57<11:28, 11.47s/it]
2022-04-10 03:23:49,147 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0033, loss: 0.0023 ||:   9%|9         | 6/65 [01:08<11:14, 11.44s/it]
2022-04-10 03:24:00,539 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0009, loss: 0.0021 ||:  11%|#         | 7/65 [01:20<11:02, 11.42s/it]
2022-04-10 03:24:11,959 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0141, loss: 0.0036 ||:  12%|#2        | 8/65 [01:31<10:51, 11.42s/it]
2022-04-10 03:24:23,438 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0007, loss: 0.0032 ||:  14%|#3        | 9/65 [01:42<10:40, 11.44s/it]
2022-04-10 03:24:34,898 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0020, loss: 0.0031 ||:  15%|#5        | 10/65 [01:54<10:29, 11.45s/it]
2022-04-10 03:24:46,305 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0014, loss: 0.0030 ||:  17%|#6        | 11/65 [02:05<10:17, 11.43s/it]
2022-04-10 03:24:57,691 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0010, loss: 0.0028 ||:  18%|#8        | 12/65 [02:17<10:05, 11.42s/it]
2022-04-10 03:25:09,085 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0008, loss: 0.0026 ||:  20%|##        | 13/65 [02:28<09:53, 11.41s/it]
2022-04-10 03:25:20,535 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0026, loss: 0.0026 ||:  22%|##1       | 14/65 [02:40<09:42, 11.42s/it]
2022-04-10 03:25:32,027 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0006, loss: 0.0025 ||:  23%|##3       | 15/65 [02:51<09:32, 11.44s/it]
2022-04-10 03:25:43,468 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0014, loss: 0.0024 ||:  25%|##4       | 16/65 [03:02<09:20, 11.44s/it]
2022-04-10 03:25:54,896 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0005, loss: 0.0023 ||:  26%|##6       | 17/65 [03:14<09:09, 11.44s/it]
2022-04-10 03:26:05,946 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0006, loss: 0.0022 ||:  28%|##7       | 18/65 [03:25<08:52, 11.32s/it]
2022-04-10 03:26:17,323 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0027, loss: 0.0023 ||:  29%|##9       | 19/65 [03:36<08:41, 11.34s/it]
2022-04-10 03:26:28,746 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0028, loss: 0.0023 ||:  31%|###       | 20/65 [03:48<08:31, 11.36s/it]
2022-04-10 03:26:40,211 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0011, loss: 0.0022 ||:  32%|###2      | 21/65 [03:59<08:21, 11.39s/it]
2022-04-10 03:26:51,689 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0026, loss: 0.0023 ||:  34%|###3      | 22/65 [04:11<08:11, 11.42s/it]
2022-04-10 03:27:03,102 - INFO - tqdm - accuracy: 0.9986, batch_loss: 0.0290, loss: 0.0034 ||:  35%|###5      | 23/65 [04:22<07:59, 11.42s/it]
2022-04-10 03:27:14,437 - INFO - tqdm - accuracy: 0.9987, batch_loss: 0.0011, loss: 0.0033 ||:  37%|###6      | 24/65 [04:33<07:47, 11.39s/it]
2022-04-10 03:27:25,815 - INFO - tqdm - accuracy: 0.9987, batch_loss: 0.0016, loss: 0.0032 ||:  38%|###8      | 25/65 [04:45<07:35, 11.39s/it]
2022-04-10 03:27:37,249 - INFO - tqdm - accuracy: 0.9988, batch_loss: 0.0018, loss: 0.0032 ||:  40%|####      | 26/65 [04:56<07:24, 11.40s/it]
2022-04-10 03:27:48,736 - INFO - tqdm - accuracy: 0.9988, batch_loss: 0.0026, loss: 0.0032 ||:  42%|####1     | 27/65 [05:08<07:14, 11.43s/it]
2022-04-10 03:28:00,207 - INFO - tqdm - accuracy: 0.9989, batch_loss: 0.0016, loss: 0.0031 ||:  43%|####3     | 28/65 [05:19<07:03, 11.44s/it]
2022-04-10 03:28:11,631 - INFO - tqdm - accuracy: 0.9978, batch_loss: 0.0792, loss: 0.0057 ||:  45%|####4     | 29/65 [05:31<06:51, 11.44s/it]
2022-04-10 03:28:23,022 - INFO - tqdm - accuracy: 0.9969, batch_loss: 0.0384, loss: 0.0068 ||:  46%|####6     | 30/65 [05:42<06:39, 11.42s/it]
2022-04-10 03:28:34,400 - INFO - tqdm - accuracy: 0.9970, batch_loss: 0.0012, loss: 0.0066 ||:  48%|####7     | 31/65 [05:53<06:27, 11.41s/it]
2022-04-10 03:28:45,815 - INFO - tqdm - accuracy: 0.9971, batch_loss: 0.0016, loss: 0.0065 ||:  49%|####9     | 32/65 [06:05<06:16, 11.41s/it]
2022-04-10 03:28:57,224 - INFO - tqdm - accuracy: 0.9962, batch_loss: 0.1122, loss: 0.0097 ||:  51%|#####     | 33/65 [06:16<06:05, 11.41s/it]
2022-04-10 03:29:08,493 - INFO - tqdm - accuracy: 0.9963, batch_loss: 0.0017, loss: 0.0095 ||:  52%|#####2    | 34/65 [06:27<05:52, 11.37s/it]
2022-04-10 03:29:19,916 - INFO - tqdm - accuracy: 0.9964, batch_loss: 0.0100, loss: 0.0095 ||:  54%|#####3    | 35/65 [06:39<05:41, 11.38s/it]
2022-04-10 03:29:31,351 - INFO - tqdm - accuracy: 0.9957, batch_loss: 0.0436, loss: 0.0104 ||:  55%|#####5    | 36/65 [06:50<05:30, 11.40s/it]
2022-04-10 03:29:42,810 - INFO - tqdm - accuracy: 0.9958, batch_loss: 0.0004, loss: 0.0101 ||:  57%|#####6    | 37/65 [07:02<05:19, 11.42s/it]
2022-04-10 03:29:54,285 - INFO - tqdm - accuracy: 0.9959, batch_loss: 0.0023, loss: 0.0099 ||:  58%|#####8    | 38/65 [07:13<05:08, 11.43s/it]
2022-04-10 03:30:05,732 - INFO - tqdm - accuracy: 0.9960, batch_loss: 0.0012, loss: 0.0097 ||:  60%|######    | 39/65 [07:25<04:57, 11.44s/it]
2022-04-10 03:30:17,203 - INFO - tqdm - accuracy: 0.9961, batch_loss: 0.0034, loss: 0.0096 ||:  62%|######1   | 40/65 [07:36<04:46, 11.45s/it]
2022-04-10 03:30:28,672 - INFO - tqdm - accuracy: 0.9962, batch_loss: 0.0018, loss: 0.0094 ||:  63%|######3   | 41/65 [07:48<04:34, 11.45s/it]
2022-04-10 03:30:40,129 - INFO - tqdm - accuracy: 0.9963, batch_loss: 0.0015, loss: 0.0092 ||:  65%|######4   | 42/65 [07:59<04:23, 11.46s/it]
2022-04-10 03:30:51,578 - INFO - tqdm - accuracy: 0.9964, batch_loss: 0.0006, loss: 0.0090 ||:  66%|######6   | 43/65 [08:11<04:11, 11.45s/it]
2022-04-10 03:31:03,014 - INFO - tqdm - accuracy: 0.9964, batch_loss: 0.0014, loss: 0.0088 ||:  68%|######7   | 44/65 [08:22<04:00, 11.45s/it]
2022-04-10 03:31:14,424 - INFO - tqdm - accuracy: 0.9965, batch_loss: 0.0046, loss: 0.0087 ||:  69%|######9   | 45/65 [08:33<03:48, 11.44s/it]
2022-04-10 03:31:25,547 - INFO - tqdm - accuracy: 0.9966, batch_loss: 0.0006, loss: 0.0085 ||:  71%|#######   | 46/65 [08:45<03:35, 11.34s/it]
2022-04-10 03:31:36,934 - INFO - tqdm - accuracy: 0.9967, batch_loss: 0.0011, loss: 0.0084 ||:  72%|#######2  | 47/65 [08:56<03:24, 11.36s/it]
2022-04-10 03:31:48,336 - INFO - tqdm - accuracy: 0.9967, batch_loss: 0.0033, loss: 0.0083 ||:  74%|#######3  | 48/65 [09:07<03:13, 11.37s/it]
2022-04-10 03:31:59,729 - INFO - tqdm - accuracy: 0.9968, batch_loss: 0.0046, loss: 0.0082 ||:  75%|#######5  | 49/65 [09:19<03:02, 11.38s/it]
2022-04-10 03:32:11,034 - INFO - tqdm - accuracy: 0.9969, batch_loss: 0.0017, loss: 0.0081 ||:  77%|#######6  | 50/65 [09:30<02:50, 11.36s/it]
2022-04-10 03:32:22,466 - INFO - tqdm - accuracy: 0.9969, batch_loss: 0.0006, loss: 0.0079 ||:  78%|#######8  | 51/65 [09:41<02:39, 11.38s/it]
2022-04-10 03:32:33,910 - INFO - tqdm - accuracy: 0.9964, batch_loss: 0.2268, loss: 0.0121 ||:  80%|########  | 52/65 [09:53<02:28, 11.40s/it]
2022-04-10 03:32:45,337 - INFO - tqdm - accuracy: 0.9965, batch_loss: 0.0015, loss: 0.0119 ||:  82%|########1 | 53/65 [10:04<02:16, 11.41s/it]
2022-04-10 03:32:56,782 - INFO - tqdm - accuracy: 0.9965, batch_loss: 0.0017, loss: 0.0117 ||:  83%|########3 | 54/65 [10:16<02:05, 11.42s/it]
2022-04-10 03:33:08,617 - INFO - tqdm - accuracy: 0.9966, batch_loss: 0.0033, loss: 0.0116 ||:  85%|########4 | 55/65 [10:28<01:55, 11.54s/it]
2022-04-10 03:33:20,012 - INFO - tqdm - accuracy: 0.9966, batch_loss: 0.0017, loss: 0.0114 ||:  86%|########6 | 56/65 [10:39<01:43, 11.50s/it]
2022-04-10 03:33:31,435 - INFO - tqdm - accuracy: 0.9967, batch_loss: 0.0021, loss: 0.0113 ||:  88%|########7 | 57/65 [10:50<01:31, 11.48s/it]
2022-04-10 03:33:42,839 - INFO - tqdm - accuracy: 0.9968, batch_loss: 0.0007, loss: 0.0111 ||:  89%|########9 | 58/65 [11:02<01:20, 11.45s/it]
2022-04-10 03:33:54,255 - INFO - tqdm - accuracy: 0.9968, batch_loss: 0.0006, loss: 0.0109 ||:  91%|######### | 59/65 [11:13<01:08, 11.44s/it]
2022-04-10 03:34:05,670 - INFO - tqdm - accuracy: 0.9969, batch_loss: 0.0006, loss: 0.0107 ||:  92%|#########2| 60/65 [11:25<00:57, 11.43s/it]
2022-04-10 03:34:17,093 - INFO - tqdm - accuracy: 0.9969, batch_loss: 0.0013, loss: 0.0106 ||:  94%|#########3| 61/65 [11:36<00:45, 11.43s/it]
2022-04-10 03:34:28,511 - INFO - tqdm - accuracy: 0.9970, batch_loss: 0.0011, loss: 0.0104 ||:  95%|#########5| 62/65 [11:48<00:34, 11.43s/it]
2022-04-10 03:34:39,927 - INFO - tqdm - accuracy: 0.9970, batch_loss: 0.0018, loss: 0.0103 ||:  97%|#########6| 63/65 [11:59<00:22, 11.42s/it]
2022-04-10 03:34:51,340 - INFO - tqdm - accuracy: 0.9971, batch_loss: 0.0082, loss: 0.0102 ||:  98%|#########8| 64/65 [12:10<00:11, 11.42s/it]
2022-04-10 03:34:56,402 - INFO - tqdm - accuracy: 0.9971, batch_loss: 0.0405, loss: 0.0107 ||: 100%|##########| 65/65 [12:15<00:00,  9.51s/it]
2022-04-10 03:34:56,402 - INFO - tqdm - accuracy: 0.9971, batch_loss: 0.0405, loss: 0.0107 ||: 100%|##########| 65/65 [12:15<00:00, 11.32s/it]
2022-04-10 03:34:59,222 - INFO - allennlp.training.trainer - Validating
2022-04-10 03:34:59,224 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-10 03:35:09,359 - INFO - tqdm - accuracy: 0.7531, batch_loss: 0.0001, loss: 1.4110 ||:  36%|###5      | 41/115 [00:10<00:18,  3.99it/s]
2022-04-10 03:35:19,468 - INFO - tqdm - accuracy: 0.6149, batch_loss: 1.2982, loss: 2.2179 ||:  70%|#######   | 81/115 [00:20<00:08,  3.92it/s]
2022-04-10 03:35:28,057 - INFO - tqdm - accuracy: 0.6288, batch_loss: 0.4832, loss: 2.0988 ||: 100%|##########| 115/115 [00:28<00:00,  3.94it/s]
2022-04-10 03:35:28,058 - INFO - tqdm - accuracy: 0.6288, batch_loss: 0.4832, loss: 2.0988 ||: 100%|##########| 115/115 [00:28<00:00,  3.99it/s]
2022-04-10 03:35:28,059 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-10 03:35:28,059 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.997  |     0.629
2022-04-10 03:35:28,060 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-10 03:35:28,061 - INFO - allennlp.training.tensorboard_writer - loss               |     0.011  |     2.099
2022-04-10 03:35:28,061 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5813.805  |       N/A
2022-04-10 03:35:34,108 - INFO - allennlp.training.trainer - Epoch duration: 0:12:53.610665
2022-04-10 03:35:34,108 - INFO - allennlp.training.trainer - Estimated training time remaining: 0:12:55
2022-04-10 03:35:34,109 - INFO - allennlp.training.trainer - Epoch 14/14
2022-04-10 03:35:34,109 - INFO - allennlp.training.trainer - Worker 0 memory usage: 5.7G
2022-04-10 03:35:34,109 - INFO - allennlp.training.trainer - GPU 0 memory usage: 9.4G
2022-04-10 03:35:34,110 - INFO - allennlp.training.trainer - Training
2022-04-10 03:35:34,111 - INFO - tqdm - 0%|          | 0/65 [00:00<?, ?it/s]
2022-04-10 03:35:45,393 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0006, loss: 0.0006 ||:   2%|1         | 1/65 [00:11<12:02, 11.28s/it]
2022-04-10 03:35:56,919 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0032, loss: 0.0019 ||:   3%|3         | 2/65 [00:22<11:59, 11.43s/it]
2022-04-10 03:36:08,588 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0015, loss: 0.0018 ||:   5%|4         | 3/65 [00:34<11:55, 11.54s/it]
2022-04-10 03:36:20,102 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0078, loss: 0.0033 ||:   6%|6         | 4/65 [00:45<11:43, 11.53s/it]
2022-04-10 03:36:31,499 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0021, loss: 0.0030 ||:   8%|7         | 5/65 [00:57<11:28, 11.48s/it]
2022-04-10 03:36:42,885 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0042, loss: 0.0032 ||:   9%|9         | 6/65 [01:08<11:15, 11.45s/it]
2022-04-10 03:36:54,291 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0015, loss: 0.0030 ||:  11%|#         | 7/65 [01:20<11:03, 11.43s/it]
2022-04-10 03:37:05,737 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0007, loss: 0.0027 ||:  12%|#2        | 8/65 [01:31<10:51, 11.44s/it]
2022-04-10 03:37:17,247 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0016, loss: 0.0026 ||:  14%|#3        | 9/65 [01:43<10:41, 11.46s/it]
2022-04-10 03:37:28,705 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0007, loss: 0.0024 ||:  15%|#5        | 10/65 [01:54<10:30, 11.46s/it]
2022-04-10 03:37:40,125 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0011, loss: 0.0023 ||:  17%|#6        | 11/65 [02:06<10:18, 11.45s/it]
2022-04-10 03:37:51,505 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0009, loss: 0.0022 ||:  18%|#8        | 12/65 [02:17<10:05, 11.43s/it]
2022-04-10 03:38:02,878 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0021, loss: 0.0021 ||:  20%|##        | 13/65 [02:28<09:53, 11.41s/it]
2022-04-10 03:38:14,305 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0013, loss: 0.0021 ||:  22%|##1       | 14/65 [02:40<09:42, 11.42s/it]
2022-04-10 03:38:25,766 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0031, loss: 0.0022 ||:  23%|##3       | 15/65 [02:51<09:31, 11.43s/it]
2022-04-10 03:38:37,224 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0081, loss: 0.0025 ||:  25%|##4       | 16/65 [03:03<09:20, 11.44s/it]
2022-04-10 03:38:48,647 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0038, loss: 0.0026 ||:  26%|##6       | 17/65 [03:14<09:08, 11.43s/it]
2022-04-10 03:39:00,031 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0015, loss: 0.0025 ||:  28%|##7       | 18/65 [03:25<08:56, 11.42s/it]
2022-04-10 03:39:11,407 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0006, loss: 0.0024 ||:  29%|##9       | 19/65 [03:37<08:44, 11.41s/it]
2022-04-10 03:39:22,857 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0009, loss: 0.0024 ||:  31%|###       | 20/65 [03:48<08:33, 11.42s/it]
2022-04-10 03:39:34,329 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0009, loss: 0.0023 ||:  32%|###2      | 21/65 [04:00<08:23, 11.43s/it]
2022-04-10 03:39:45,651 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0021, loss: 0.0023 ||:  34%|###3      | 22/65 [04:11<08:10, 11.40s/it]
2022-04-10 03:39:57,107 - INFO - tqdm - accuracy: 1.0000, batch_loss: 0.0006, loss: 0.0022 ||:  35%|###5      | 23/65 [04:22<07:59, 11.42s/it]
2022-04-10 03:40:08,539 - INFO - tqdm - accuracy: 0.9987, batch_loss: 0.2005, loss: 0.0105 ||:  37%|###6      | 24/65 [04:34<07:48, 11.42s/it]
2022-04-10 03:40:19,980 - INFO - tqdm - accuracy: 0.9988, batch_loss: 0.0072, loss: 0.0103 ||:  38%|###8      | 25/65 [04:45<07:37, 11.43s/it]
2022-04-10 03:40:31,396 - INFO - tqdm - accuracy: 0.9988, batch_loss: 0.0006, loss: 0.0100 ||:  40%|####      | 26/65 [04:57<07:25, 11.42s/it]
2022-04-10 03:40:42,801 - INFO - tqdm - accuracy: 0.9988, batch_loss: 0.0041, loss: 0.0098 ||:  42%|####1     | 27/65 [05:08<07:13, 11.42s/it]
2022-04-10 03:40:54,225 - INFO - tqdm - accuracy: 0.9989, batch_loss: 0.0028, loss: 0.0095 ||:  43%|####3     | 28/65 [05:20<07:02, 11.42s/it]
2022-04-10 03:41:05,646 - INFO - tqdm - accuracy: 0.9989, batch_loss: 0.0016, loss: 0.0092 ||:  45%|####4     | 29/65 [05:31<06:51, 11.42s/it]
2022-04-10 03:41:17,057 - INFO - tqdm - accuracy: 0.9990, batch_loss: 0.0068, loss: 0.0092 ||:  46%|####6     | 30/65 [05:42<06:39, 11.42s/it]
2022-04-10 03:41:28,469 - INFO - tqdm - accuracy: 0.9990, batch_loss: 0.0038, loss: 0.0090 ||:  48%|####7     | 31/65 [05:54<06:28, 11.42s/it]
2022-04-10 03:41:39,884 - INFO - tqdm - accuracy: 0.9990, batch_loss: 0.0007, loss: 0.0087 ||:  49%|####9     | 32/65 [06:05<06:16, 11.42s/it]
2022-04-10 03:41:51,289 - INFO - tqdm - accuracy: 0.9991, batch_loss: 0.0008, loss: 0.0085 ||:  51%|#####     | 33/65 [06:17<06:05, 11.41s/it]
2022-04-10 03:42:02,693 - INFO - tqdm - accuracy: 0.9991, batch_loss: 0.0004, loss: 0.0082 ||:  52%|#####2    | 34/65 [06:28<05:53, 11.41s/it]
2022-04-10 03:42:14,109 - INFO - tqdm - accuracy: 0.9991, batch_loss: 0.0017, loss: 0.0081 ||:  54%|#####3    | 35/65 [06:39<05:42, 11.41s/it]
2022-04-10 03:42:25,508 - INFO - tqdm - accuracy: 0.9991, batch_loss: 0.0023, loss: 0.0079 ||:  55%|#####5    | 36/65 [06:51<05:30, 11.41s/it]
2022-04-10 03:42:36,912 - INFO - tqdm - accuracy: 0.9983, batch_loss: 0.1548, loss: 0.0119 ||:  57%|#####6    | 37/65 [07:02<05:19, 11.41s/it]
2022-04-10 03:42:48,327 - INFO - tqdm - accuracy: 0.9984, batch_loss: 0.0061, loss: 0.0117 ||:  58%|#####8    | 38/65 [07:14<05:08, 11.41s/it]
2022-04-10 03:42:59,728 - INFO - tqdm - accuracy: 0.9984, batch_loss: 0.0045, loss: 0.0115 ||:  60%|######    | 39/65 [07:25<04:56, 11.41s/it]
2022-04-10 03:43:11,133 - INFO - tqdm - accuracy: 0.9984, batch_loss: 0.0098, loss: 0.0115 ||:  62%|######1   | 40/65 [07:37<04:45, 11.41s/it]
2022-04-10 03:43:22,536 - INFO - tqdm - accuracy: 0.9985, batch_loss: 0.0008, loss: 0.0112 ||:  63%|######3   | 41/65 [07:48<04:33, 11.41s/it]
2022-04-10 03:43:33,950 - INFO - tqdm - accuracy: 0.9985, batch_loss: 0.0009, loss: 0.0110 ||:  65%|######4   | 42/65 [07:59<04:22, 11.41s/it]
2022-04-10 03:43:45,369 - INFO - tqdm - accuracy: 0.9985, batch_loss: 0.0068, loss: 0.0109 ||:  66%|######6   | 43/65 [08:11<04:11, 11.41s/it]
2022-04-10 03:43:56,819 - INFO - tqdm - accuracy: 0.9986, batch_loss: 0.0006, loss: 0.0106 ||:  68%|######7   | 44/65 [08:22<03:59, 11.42s/it]
2022-04-10 03:44:08,265 - INFO - tqdm - accuracy: 0.9986, batch_loss: 0.0010, loss: 0.0104 ||:  69%|######9   | 45/65 [08:34<03:48, 11.43s/it]
2022-04-10 03:44:19,729 - INFO - tqdm - accuracy: 0.9986, batch_loss: 0.0006, loss: 0.0102 ||:  71%|#######   | 46/65 [08:45<03:37, 11.44s/it]
2022-04-10 03:44:31,179 - INFO - tqdm - accuracy: 0.9987, batch_loss: 0.0015, loss: 0.0100 ||:  72%|#######2  | 47/65 [08:57<03:25, 11.44s/it]
2022-04-10 03:44:42,637 - INFO - tqdm - accuracy: 0.9980, batch_loss: 0.1994, loss: 0.0140 ||:  74%|#######3  | 48/65 [09:08<03:14, 11.45s/it]
2022-04-10 03:44:54,097 - INFO - tqdm - accuracy: 0.9981, batch_loss: 0.0018, loss: 0.0137 ||:  75%|#######5  | 49/65 [09:19<03:03, 11.45s/it]
2022-04-10 03:45:05,560 - INFO - tqdm - accuracy: 0.9981, batch_loss: 0.0011, loss: 0.0135 ||:  77%|#######6  | 50/65 [09:31<02:51, 11.45s/it]
2022-04-10 03:45:17,008 - INFO - tqdm - accuracy: 0.9982, batch_loss: 0.0007, loss: 0.0132 ||:  78%|#######8  | 51/65 [09:42<02:40, 11.45s/it]
2022-04-10 03:45:27,982 - INFO - tqdm - accuracy: 0.9982, batch_loss: 0.0012, loss: 0.0130 ||:  80%|########  | 52/65 [09:53<02:27, 11.31s/it]
2022-04-10 03:45:39,386 - INFO - tqdm - accuracy: 0.9982, batch_loss: 0.0006, loss: 0.0128 ||:  82%|########1 | 53/65 [10:05<02:16, 11.34s/it]
2022-04-10 03:45:50,829 - INFO - tqdm - accuracy: 0.9977, batch_loss: 0.0680, loss: 0.0138 ||:  83%|########3 | 54/65 [10:16<02:05, 11.37s/it]
2022-04-10 03:46:02,254 - INFO - tqdm - accuracy: 0.9977, batch_loss: 0.0021, loss: 0.0136 ||:  85%|########4 | 55/65 [10:28<01:53, 11.39s/it]
2022-04-10 03:46:13,409 - INFO - tqdm - accuracy: 0.9978, batch_loss: 0.0085, loss: 0.0135 ||:  86%|########6 | 56/65 [10:39<01:41, 11.32s/it]
2022-04-10 03:46:24,770 - INFO - tqdm - accuracy: 0.9978, batch_loss: 0.0043, loss: 0.0133 ||:  88%|########7 | 57/65 [10:50<01:30, 11.33s/it]
2022-04-10 03:46:36,178 - INFO - tqdm - accuracy: 0.9973, batch_loss: 0.0454, loss: 0.0139 ||:  89%|########9 | 58/65 [11:02<01:19, 11.35s/it]
2022-04-10 03:46:47,582 - INFO - tqdm - accuracy: 0.9974, batch_loss: 0.0033, loss: 0.0137 ||:  91%|######### | 59/65 [11:13<01:08, 11.37s/it]
2022-04-10 03:46:58,970 - INFO - tqdm - accuracy: 0.9974, batch_loss: 0.0013, loss: 0.0135 ||:  92%|#########2| 60/65 [11:24<00:56, 11.37s/it]
2022-04-10 03:47:10,359 - INFO - tqdm - accuracy: 0.9974, batch_loss: 0.0019, loss: 0.0133 ||:  94%|#########3| 61/65 [11:36<00:45, 11.38s/it]
2022-04-10 03:47:21,765 - INFO - tqdm - accuracy: 0.9975, batch_loss: 0.0007, loss: 0.0131 ||:  95%|#########5| 62/65 [11:47<00:34, 11.39s/it]
2022-04-10 03:47:33,163 - INFO - tqdm - accuracy: 0.9975, batch_loss: 0.0082, loss: 0.0130 ||:  97%|#########6| 63/65 [11:59<00:22, 11.39s/it]
2022-04-10 03:47:44,598 - INFO - tqdm - accuracy: 0.9971, batch_loss: 0.0723, loss: 0.0139 ||:  98%|#########8| 64/65 [12:10<00:11, 11.40s/it]
2022-04-10 03:47:49,659 - INFO - tqdm - accuracy: 0.9971, batch_loss: 0.0029, loss: 0.0138 ||: 100%|##########| 65/65 [12:15<00:00,  9.50s/it]
2022-04-10 03:47:49,659 - INFO - tqdm - accuracy: 0.9971, batch_loss: 0.0029, loss: 0.0138 ||: 100%|##########| 65/65 [12:15<00:00, 11.32s/it]
2022-04-10 03:47:52,494 - INFO - allennlp.training.trainer - Validating
2022-04-10 03:47:52,496 - INFO - tqdm - 0%|          | 0/115 [00:00<?, ?it/s]
2022-04-10 03:48:02,546 - INFO - tqdm - accuracy: 0.6875, batch_loss: 3.9255, loss: 1.7596 ||:  35%|###4      | 40/115 [00:10<00:18,  3.99it/s]
2022-04-10 03:48:12,676 - INFO - tqdm - accuracy: 0.6813, batch_loss: 5.0796, loss: 1.8895 ||:  70%|######9   | 80/115 [00:20<00:08,  3.93it/s]
2022-04-10 03:48:21,351 - INFO - tqdm - accuracy: 0.6550, batch_loss: 2.9918, loss: 2.0781 ||: 100%|##########| 115/115 [00:28<00:00,  3.95it/s]
2022-04-10 03:48:21,352 - INFO - tqdm - accuracy: 0.6550, batch_loss: 2.9918, loss: 2.0781 ||: 100%|##########| 115/115 [00:28<00:00,  3.99it/s]
2022-04-10 03:48:21,352 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation
2022-04-10 03:48:21,353 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.997  |     0.655
2022-04-10 03:48:21,354 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  9580.571  |       N/A
2022-04-10 03:48:21,354 - INFO - allennlp.training.tensorboard_writer - loss               |     0.014  |     2.078
2022-04-10 03:48:21,355 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |  5813.805  |       N/A
2022-04-10 03:48:27,420 - INFO - allennlp.training.trainer - Epoch duration: 0:12:53.311615
2022-04-10 03:48:27,420 - INFO - allennlp.training.checkpointer - loading best weights
2022-04-10 03:48:28,389 - INFO - allennlp.common.util - Metrics: {
  "best_epoch": 7,
  "peak_worker_0_memory_MB": 5813.8046875,
  "peak_gpu_0_memory_MB": 9580.5712890625,
  "training_duration": "3:13:44.715511",
  "training_start_epoch": 0,
  "training_epochs": 14,
  "epoch": 14,
  "training_accuracy": 0.9970887918486172,
  "training_loss": 0.01377611345871908,
  "training_worker_0_memory_MB": 5813.8046875,
  "training_gpu_0_memory_MB": 9580.5712890625,
  "validation_accuracy": 0.6550218340611353,
  "validation_loss": 2.0781353716775217,
  "best_validation_accuracy": 0.6681222707423581,
  "best_validation_loss": 1.9012036622558852
}
2022-04-10 03:48:28,389 - INFO - allennlp.models.archival - archiving weights and vocabulary to ./output_ir-ora-d_hyper2/model.tar.gz
